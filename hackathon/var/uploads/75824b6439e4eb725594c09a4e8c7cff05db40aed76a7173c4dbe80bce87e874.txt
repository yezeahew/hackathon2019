An Inﬁnitely Large Napkin

http://web.evanchen.cc/napkin.html

Evan Chen

Version: v1.5.20190718

When introduced to a new idea, always ask why you should care.

Do not expect an answer right away, but demand one eventually.

— Ravi Vakil [Va17]

If you like this book and want to support me,

please consider buying me a coﬀee!

http://ko-fi.com/evanchen/

For Brian and Lisa, who ﬁnally got me to write it.

© 2019 Evan Chen. All rights reserved. Personal use only.
This is (still!) an incomplete draft. Please send corrections, comments, pictures of kittens, etc.
to evan@evanchen.cc, or pull-request at https://github.com/vEnhance/napkin.

Last updated July 18, 2019.

Preface

The origin of the name “Napkin” comes from the following quote of mine.

I’ll be eating a quick lunch with some friends of mine who are still in high school.
They’ll ask me what I’ve been up to the last few weeks, and I’ll tell them that I’ve been
learning category theory. They’ll ask me what category theory is about. I tell them
it’s about abstracting things by looking at just the structure-preserving morphisms
between them, rather than the objects themselves. I’ll try to give them the standard
example Grp, but then I’ll realize that they don’t know what a homomorphism is.
So then I’ll start trying to explain what a homomorphism is, but then I’ll remember
that they haven’t learned what a group is. So then I’ll start trying to explain what a
group is, but by the time I ﬁnish writing the group axioms on my napkin, they’ve
already forgotten why I was talking about groups in the ﬁrst place. And then it’s
1PM, people need to go places, and I can’t help but think:
“Man, if I had forty hours instead of forty minutes, I bet I could actually have explained
this all”.

This book was initially my attempt at those forty hours, but has grown considerably
since then.

About this book

The Inﬁnitely Large Napkin is a light but mostly self-contained introduction to a large
amount of higher math.

I should say at once that this book is not intended as a replacement for dedicated
books or courses; the amount of depth is not comparable. On the ﬂip side, the beneﬁt of
this “light” approach is that it becomes accessible to a larger audience, since the goal is
merely to give the reader a feeling for any particular topic rather than to emulate a full
semester of lectures.

I initially wrote this book with talented high-school students in mind, particularly
those with math-olympiad type backgrounds. Some remnants of that cultural bias can
still be felt throughout the book, particularly in assorted challenge problems which are
taken from mathematical competitions. However, in general I think this would be a
good reference for anyone with some amount of mathematical maturity and curiosity.
Examples include but certainly not limited to: math undergraduate majors, physics/CS
majors, math PhD students who want to hear a little bit about ﬁelds other than their
own, high school students who like math but not math contests, and unusually intelligent
kittens ﬂuent in English.

Philosophy behind the Napkin approach

As far as I can tell, higher math for high-school students comes in two ﬂavors:

 Someone tells you about the hairy ball theorem in the form “you can’t comb the
hair on a spherical cat” then doesn’t tell you anything about why it should be true,
what it means to actually “comb the hair”, or any of the underlying theory, leaving
you with just some vague notion in your head.

 You take a class and prove every result in full detail, and at some point you stop

caring about what the professor is saying.

v

vi

Napkin, by Evan Chen (v1.5.20190718)

Presumably you already know how unsatisfying the ﬁrst approach is. So the second
approach seems to be the default, but I really think there should be some sort of middle
ground here.

Unlike university, it is not the purpose of this book to train you to solve exercises or
write proofs1, or prepare you for research in the ﬁeld. Instead I just want to show you
some interesting math. The things that are presented should be memorable and worth
caring about. For that reason, proofs that would be included for completeness in any
ordinary textbook are often omitted here, unless there is some idea in the proof which
I think is worth seeing. In particular, I place a strong emphasis over explaining why a
theorem should be true rather than writing down its proof. This is a recurrent theme of
this book:

Natural explanations supersede proofs.

My hope is that after reading any particular chapter in Napkin, one might get the

following out of it:

 Knowing what the precise deﬁnitions are of the main characters,

 Being acquainted with the few really major examples,

 Knowing the precise statements of famous theorems, and having a sense of why

they should be true.

Understanding “why” something is true can have many forms. This is sometimes
accomplished with a complete rigorous proof; in other cases, it is given by the idea of the
proof; in still other cases, it is just a few key examples with extensive cheerleading.

Obviously this is nowhere near enough if you want to e.g. do research in a ﬁeld; but if
you are just a curious outsider, I hope that it’s more satisfying than the elevator pitch or
Wikipedia articles. Even if you do want to learn a topic with serious depth, I hope that
it can be a good zoomed-out overview before you really dive in, because in many senses
the choice of material is “what I wish someone had told me before I started”.

More pedagogical comments and references

The preface would become too long if I talked about some of my pedagogical decisions
chapter by chapter, so Appendix A contains those comments instead.

In particular, I often name speciﬁc references, and then end of that appendix has more

references. So this is a good place to look if you want further reading.

Historical and personal notes

I began writing this book in December of 2014, after having ﬁnished my ﬁrst semester of
undergraduate at Harvard. It became my main focus for about 18 months after that,
as I became immersed in higher math. I essentially took only math classes, (gleefully
ignoring all my other graduation requirements) and merged as much of it as I could (as
well as lots of other math I learned on my own time) into the Napkin.

Towards August of 2016, though, I ﬁnally lost steam. The ﬁrst public drafts went
online then, and I decided to step back. Having burnt out slightly, I then took a break

1Which is not to say problem-solving isn’t valuable; I myself am a math olympiad coach at heart. It’s

just not the point of this book.

add more
add more
acknowledg-
acknowledg-
ments
ments

Preface

vii

from higher math, and spent the remaining two undergraduate years2 working extensively
as a coach for the American math olympiad team, and trying to spend as much time
with my friends as I could before they graduated and went their own ways.

During those two years, readers sent me many kind words of gratitude, many reports
of errors, and many suggestions for additions. So in November of 2018, some weeks into
my ﬁrst semester as a math PhD student, I decided I should ﬁnish what I had started.
Some months later, here is what I have.

Source code

The project is hosted on GitHub at https://github.com/vEnhance/napkin. Pull
requests are quite welcome! I am also happy to receive suggestions and corrections by
email.

Acknowledgements

I am indebted to countless people for this work. Here is a partial (surely incomplete) list.

 Thanks to all my teachers and professors for teaching me much of the material
covered in these notes, as well as the authors of all the references I have cited here.
A special call-out to [Ga14], [Le14], [Sj05], [Ga03], [Ll15], [Et11], [Ko14], [Va17], [Pu02],
[Go18], which were especially inﬂuential.

 Thanks also to dozens of friends and strangers who read through preview copies
of my draft, and pointed out errors and gave other suggestions. Special mention
to Andrej Vukovi´c and Alexander Chua for together catching over a thousand
errors. Thanks also to Brian Gu and Tom Tseng for many corrections. (If you ﬁnd
mistakes or have suggestions yourself, I would love to hear them!)

 I’d also like to express my gratitude for many, many kind words I received during
the development of this project. These generous comments led me to keep working
on this, and were largely responsible for my decision in November 2018 to begin
updating the Napkin again.

Finally, a huge thanks to the math olympiad community, from which the Napkin
(and me) has its roots. All the enthusiasm, encouragement, and thank-you notes I have
received over the years led me to begin writing this in the ﬁrst place. I otherwise would
never have the arrogance to dream a project like this was at all possible. And of course I
would be nowhere near where I am today were it not for the life-changing journey I took
in chasing my dreams to the IMO. Forever TWN2!

2Alternatively: “ . . . and spent the next two years forgetting everything I had painstakingly learned”.

Which made me grateful for all the past notes in the Napkin!

Advice for the reader

§1 Prerequisites

As explained in the preface, the main prerequisite is some amount of mathematical
maturity. This means I expect the reader to know how to read and write a proof, follow
logical arguments, and so on.

I also assume the reader is familiar with basic terminology about sets and functions

(e.g. “what is a bijection?”). If not, one should consult Appendix E.

§2 Deciding what to read

There is no need to read this book in linear order: it covers all sorts of areas in mathematics,
and there are many paths you can take. In Chapter 0, I give a short overview for each
part explaining what you might expect to see in that part.

For now, here is a brief chart showing how the chapters depend on each other; again
see Chapter 0 for details. Dependencies are indicated by arrows; dotted lines are optional
dependencies. I suggest that you simply pick a chapter you ﬁnd interesting,
and then ﬁnd the shortest path. With that in mind, I hope the length of the entire
PDF is not intimidating.

Ch 81-87
Set Theory

Ch 1,3-5
Abs Alg

Ch 9-15,18
Lin Alg

Ch 2,6-8
Topology

Ch 23-25
Quantum

Ch 16
Grp Act

Ch 19-22
Rep Th

Ch 60-63
Cat Th

Ch 42-45
Diﬀ Geo

Ch 17
Grp Classif

Ch 26-30
Calc

Ch 31-33
Cmplx Ana

Ch 34-41
Measure/Pr

Ch 70-74
Alg Geo 1

Ch 57-59
Alg Top 1

Ch 46-51
Alg NT 1

Ch 75-80
Alg Geo 2-3

Ch 64-69
Alg Top 2

Ch 52-56
Alg NT 2

ix

x

Napkin, by Evan Chen (v1.5.20190718)

§3 Questions, exercises, and problems

In this book, there are three hierarchies:

 An inline question is intended to be oﬀensively easy, mostly a chance to help you
internalize deﬁnitions. If you ﬁnd yourself unable to answer one or two of them, it
probably means I explained it badly and you should complain to me. But if you
can’t answer many, you likely missed something important: read back.

 An inline exercise is more meaty than a question, but shouldn’t have any “tricky”
steps. Often I leave proofs of theorems and propositions as exercises if they are
instructive and at least somewhat interesting.

 Each chapter features several trickier problems at the end. Some are reasonable,
but others are legitimately diﬃcult olympiad-style problems. Harder problems are
marked with up to three chili peppers (

), like this paragraph.

In addition to diﬃculty annotations, the problems are also marked by how important
they are to the big picture.

– Normal problems, which are hopefully fun but non-central.

– Daggered problems, which are (usually interesting) results that one should

know, but won’t be used directly later.

– Starred problems, which are results which will be used later on in the book.1

Several hints and solutions can be found in Appendices B and C.

1This is to avoid the classic “we are done by PSet 4, Problem 8” that happens in college sometimes, as

if I remembered what that was.

Image from [Go08]

Advice for the reader

§4 Paper

At the risk of being blunt,

Read this book with pencil and paper.

Here’s why:

xi

Image from [Or]

You are not God. You cannot keep everything in your head.2 If you’ve printed out a
hard copy, then write in the margins. If you’re trying to save paper, grab a notebook or
something along with the ride. Somehow, some way, make sure you can write. Thanks.

§5 On the importance of examples

I am pathologically obsessed with examples. In this book, I place all examples in large
boxes to draw emphasis to them, which leads to some pages of the book simply consisting
of sequences of boxes one after another. I hope the reader doesn’t mind.

I also often highlight a “prototypical example” for some sections, and reserve the color
red for such a note. The philosophy is that any time the reader sees a deﬁnition or a
theorem about such an object, they should test it against the prototypical example. If
the example is a good prototype, it should be immediately clear why this deﬁnition is
intuitive, or why the theorem should be true, or why the theorem is interesting, et cetera.
Let me tell you a secret. Whenever I wrote a deﬁnition or a theorem in this book, I
would have to recall the exact statement from my (quite poor) memory. So instead I
often consider the prototypical example, and then only after that do I remember what
the deﬁnition or the theorem is. Incidentally, this is also how I learned all the deﬁnitions
in the ﬁrst place. I hope you’ll ﬁnd it useful as well.

2 See also https://usamo.wordpress.com/2015/03/14/writing/ and the source above.

xii

Napkin, by Evan Chen (v1.5.20190718)

§6 Conventions and notations

This part describes some of the less familiar notations and deﬁnitions and settles for
once and for all some annoying issues (“is zero a natural number?”). Most of these are
“remarks for experts”: if something doesn’t make sense, you probably don’t have to worry
about it for now.

A full glossary of notation used can be found in the appendix.

§6.i Sets and equivalence relations

This is brief, intended as a reminder for experts. Consult Appendix E for full details.
An equivalence relation on a set X is a relation ∼ which is symmetric, reﬂexive, and
transitive. An equivalence relation partitions X into several equivalence classes. We
will denote this by X/∼. An element of such an equivalence class is a representative
of that equivalence class.
I always use ∼= for an “isomorphism”-style relation (formally: a relation which is an
isomorphism in a reasonable category). The only time (cid:39) is used in the Napkin is for
homotopic paths.
I generally use ⊆ and (cid:40) since these are non-ambiguous, unlike ⊂. I only use ⊂ on
rare occasions in which equality obviously does not hold yet pointing it out would be
distracting. For example, I write Q ⊂ R since “Q (cid:40) R” is distracting.

I prefer S \ T to S − T .
The power set of S (i.e., the set of subsets of S), is denoted either by 2S or P(S).

§6.ii Functions

This is brief, intended as a reminder for experts. Consult Appendix E for full details.

Let X

f

−→ Y be a function:

 By f pre(T ) I mean the pre-image

f pre(T ) := {x ∈ X | f (x) ∈ T} .

This is in contrast to the f−1(T ) used in the rest of the world; I only use f−1 for
an inverse function.
By abuse of notation, we may abbreviate f pre({y}) to f pre(y). We call f pre(y) a
ﬁber.

 By f img(S) I mean the image

f img(S) := {f (x) | x ∈ S} .

Almost everyone else in the world uses f (S) (though f [S] sees some use, and f “(S)
is often used in logic) but this is abuse of notation, and I prefer f img(S) for emphasis.
This image notation is not standard.

 If S ⊆ X, then the restriction of f to S is denoted f(cid:22)S, i.e. it is the function
f(cid:22)S : S → Y .
 Sometimes functions f : X → Y are injective or surjective; I may emphasize this
sometimes by writing f : X (cid:44)→ Y or f : X (cid:16) Y , respectively.

Advice for the reader

§6.iii Rings

xiii

All rings have a multiplicative identity 1 unless otherwise speciﬁed. We allow 0 = 1 in
general rings but not in integral domains.

All rings are commutative unless otherwise speciﬁed. There is an elaborate
scheme for naming rings which are not commutative, used only in the chapter on
cohomology rings:

1 not required

Graded

graded pseudo-ring

Not Graded
pseudo-ring

Anticommutative, 1 not required

anticommutative pseudo-ring

Has 1

Anticommutative with 1

Commutative with 1

graded ring

anticommutative ring

commutative graded ring

N/A
N/A
N/A
ring

On the other hand, an algebra always has 1, but it need not be commutative.

§6.iv Natural numbers are positive

The set N is the set of positive integers, not including 0. In the set theory chapters, we
use ω = {0, 1, . . .} instead, for consistency with the rest of the book.
§6.v Choice

We accept the Axiom of Choice, and use it freely.

§7 Further reading

The appendix Appendix A contains a list of resources I like, and explanations of peda-
gogical choices that I made for each chapter. I encourage you to check it out.

In particular, this is where you should go for further reading! There are some topics
that should be covered in the Napkin, but are not, due to my own ignorance or laziness.
The references provided in this appendix should hopefully help partially atone for my
omissions.

Contents

Preface

v

Advice for the reader

ix
ix
Prerequisites . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
ix
Deciding what to read . . . . . . . . . . . . . . . . . . . . . . . . . . . .
x
Questions, exercises, and problems
. . . . . . . . . . . . . . . . . . . . .
xi
Paper
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
On the importance of examples . . . . . . . . . . . . . . . . . . . . . . .
xi
Conventions and notations . . . . . . . . . . . . . . . . . . . . . . . . . . xii
Further reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiii

1
2
3
4
5
6
7

I

Starting Out

33

0 Sales pitches

35
The basics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
0.1
0.2 Abstract algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
0.3
Real and complex analysis . . . . . . . . . . . . . . . . . . . . . . . . . . 37
0.4 Algebraic number theory . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
0.5 Algebraic topology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
0.6 Algebraic geometry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
0.7
Set theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40

1 Groups

41
1.1 Deﬁnition and examples of groups . . . . . . . . . . . . . . . . . . . . . . 41
1.2
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
Properties of groups
1.3
Isomorphisms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
1.4 Orders of groups, and Lagrange’s theorem . . . . . . . . . . . . . . . . . 48
1.5
Subgroups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
1.6 Groups of small orders . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
1.7 Unimportant long digression . . . . . . . . . . . . . . . . . . . . . . . . . 51
1.8 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 51

2 Metric spaces

53
2.1 Deﬁnition and examples of metric spaces . . . . . . . . . . . . . . . . . . 53
2.2
. . . . . . . . . . . . . . . . . . . . . . . . 55
Convergence in metric spaces
2.3
Continuous maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
2.4 Homeomorphisms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
Extended example/deﬁnition: product metric . . . . . . . . . . . . . . . 58
2.5
2.6 Open sets
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
Closed sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
2.7
2.8 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 62

15

16

II

Basic Abstract Algebra

65

Napkin, by Evan Chen (v1.5.20190718)

3 Homomorphisms and quotient groups

67
3.1 Generators and group presentations . . . . . . . . . . . . . . . . . . . . . 67
3.2 Homomorphisms
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
Cosets and modding out . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
3.3
(Optional) Proof of Lagrange’s theorem . . . . . . . . . . . . . . . . . . 73
3.4
3.5
Eliminating the homomorphism . . . . . . . . . . . . . . . . . . . . . . . 73
(Digression) The ﬁrst isomorphism theorem . . . . . . . . . . . . . . . . 75
3.6
3.7 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 76

4 Rings and ideals

79
Some motivational metaphors about rings vs groups
. . . . . . . . . . . 79
4.1
4.2
(Optional) Pedagogical notes on motivation . . . . . . . . . . . . . . . . 79
4.3 Deﬁnition and examples of rings . . . . . . . . . . . . . . . . . . . . . . . 79
4.4 Homomorphisms
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
4.5
Ideals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
4.6 Generating ideals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84
4.7
Principal ideal domains . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
4.8 Noetherian rings
4.9 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 88

5 Flavors of rings

91
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
Fields
Integral domains
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92
Prime ideals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
. . . . . . . . . . . . . . . . . . . 96
. . . . . . . . . . . . . . . . . . . 97

5.1
5.2
5.3
5.4 Maximal ideals
5.5
5.6 Unique factorization domains (UFD’s)
5.7 A few harder problems to think about

Field of fractions

III

Basic Topology

99

6 Properties of metric spaces

101
Boundedness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
Completeness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
Let the buyer beware . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
Subspaces, and (inb4) a confusing linguistic point . . . . . . . . . . . . . 104
. . . . . . . . . . . . . . . . . . . 105

6.1
6.2
6.3
6.4
6.5 A few harder problems to think about

7 Topological spaces

107
Forgetting the metric . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
Re-deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
Subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
Connected spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
Path-connected spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
. . . . . . . . . . . . . . . . . . 112
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
. . . . . . . . . . . . . . . . . . . 115

7.1
7.2
7.3 Hausdorﬀ spaces
7.4
7.5
7.6
7.7 Homotopy and simply connected spaces
7.8
7.9 A few harder problems to think about

Bases of spaces

Contents

17

8 Compactness

117
. . . . . . . . . . . . . . . . . . . . 117
8.1 Deﬁnition of sequential compactness
Criteria for compactness . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
8.2
Compactness using open covers . . . . . . . . . . . . . . . . . . . . . . . 119
8.3
8.4 Applications of compactness . . . . . . . . . . . . . . . . . . . . . . . . . 121
(Optional) Equivalence of formulations of compactness . . . . . . . . . . 123
8.5
8.6 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 124

IV

Linear Algebra

127

9 Vector spaces

131
9.1
The deﬁnitions of a ring and ﬁeld . . . . . . . . . . . . . . . . . . . . . . 131
9.2 Modules and vector spaces . . . . . . . . . . . . . . . . . . . . . . . . . . 131
9.3 Direct sums . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
Linear independence, spans, and basis
9.4
. . . . . . . . . . . . . . . . . . . 135
Linear maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
9.5
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
9.6 What is a matrix?
9.7
. . . . . . . . . . . . . . . . . . 140
9.8 A cute application: Lagrange interpolation . . . . . . . . . . . . . . . . . 142
(Digression) Arrays of numbers are evil . . . . . . . . . . . . . . . . . . . 143
9.9
. . . . . . . . . . . . . . . . . . . . . . . . . 144
9.10 A word on general modules
9.11 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 145

Subspaces and picking convenient bases

10 Eigen-things

147
10.1 Why you should care . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
10.2 Warning on assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
10.3 Eigenvectors and eigenvalues . . . . . . . . . . . . . . . . . . . . . . . . . 148
10.4 The Jordan form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
10.5 Nilpotent maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
10.6 Reducing to the nilpotent case . . . . . . . . . . . . . . . . . . . . . . . . 152
10.7
(Optional) Proof of nilpotent Jordan . . . . . . . . . . . . . . . . . . . . 153
10.8 Algebraic and geometric multiplicity . . . . . . . . . . . . . . . . . . . . 154
10.9 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 155

157
11 Dual space and trace
11.1 Tensor product
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
11.2 Dual space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
11.3 V ∨ ⊗ W gives matrices from V to W . . . . . . . . . . . . . . . . . . . . 161
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
11.4 The trace
11.5 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 163

12 Determinant

165
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
12.1 Wedge product
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
12.2 The determinant
12.3 Characteristic polynomials, and Cayley-Hamilton . . . . . . . . . . . . . 169
12.4 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 171

13 Inner product spaces

173
13.1 The inner product
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
13.2 Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176

18

Napkin, by Evan Chen (v1.5.20190718)

13.3 Orthogonality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177
13.4 Hilbert spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
13.5 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 180

14 Bonus: Fourier analysis

181
14.1 Synopsis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181
14.2 A reminder on Hilbert spaces
. . . . . . . . . . . . . . . . . . . . . . . . 181
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182
14.3 Common examples
14.4 Summary, and another teaser
. . . . . . . . . . . . . . . . . . . . . . . . 186
14.5 Parseval and friends
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186
14.6 Application: Basel problem . . . . . . . . . . . . . . . . . . . . . . . . . 187
14.7 Application: Arrow’s Impossibility Theorem . . . . . . . . . . . . . . . . 188
. . . . . . . . . . . . . . . . . . . 190
14.8 A few harder problems to think about

15 Duals, adjoint, and transposes

191
15.1 Dual of a map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
15.2
Identifying with the dual space . . . . . . . . . . . . . . . . . . . . . . . 192
15.3 The adjoint (conjugate transpose) . . . . . . . . . . . . . . . . . . . . . . 193
15.4 Eigenvalues of normal maps . . . . . . . . . . . . . . . . . . . . . . . . . 195
15.5 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 196

V

More on Groups

199

16 Group actions overkill AIME problems

201
16.1 Deﬁnition of a group action . . . . . . . . . . . . . . . . . . . . . . . . . 201
16.2 Stabilizers and orbits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202
16.3 Burnside’s lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
16.4 Conjugation of elements . . . . . . . . . . . . . . . . . . . . . . . . . . . 204
16.5 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 205

17 Find all groups

207
17.1 Sylow theorems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207
(Optional) Proving Sylow’s theorem . . . . . . . . . . . . . . . . . . . . 208
17.2
17.3
(Optional) Simple groups and Jordan-H¨older . . . . . . . . . . . . . . . . 210
. . . . . . . . . . . . . . . . . . . 211
17.4 A few harder problems to think about

18 The PID structure theorem

213
18.1 Finitely generated abelian groups . . . . . . . . . . . . . . . . . . . . . . 213
18.2 Some ring theory prerequisites . . . . . . . . . . . . . . . . . . . . . . . . 214
18.3 The structure theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
18.4 Reduction to maps of free R-modules . . . . . . . . . . . . . . . . . . . . 216
18.5 Smith normal form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217
. . . . . . . . . . . . . . . . . . . 219
18.6 A few harder problems to think about

VI

Representation Theory

221

19 Representations of algebras

223
19.1 Algebras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223
19.2 Representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224
19.3 Direct sums . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226

Contents

19

19.4
19.5 Morphisms of representations
19.6 The representations of Matd(k)
19.7 A few harder problems to think about

Irreducible and indecomposable representations . . . . . . . . . . . . . . 227
. . . . . . . . . . . . . . . . . . . . . . . . 228
. . . . . . . . . . . . . . . . . . . . . . . 230
. . . . . . . . . . . . . . . . . . . 231

20 Semisimple algebras

233
20.1 Schur’s lemma continued . . . . . . . . . . . . . . . . . . . . . . . . . . . 233
20.2 Density theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234
20.3 Semisimple algebras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236
20.4 Maschke’s theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
20.5 Example: the representations of C[S3]
. . . . . . . . . . . . . . . . . . . 237
20.6 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 238

21 Characters

241
21.1 Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241
21.2 The dual space modulo the commutator
. . . . . . . . . . . . . . . . . . 242
21.3 Orthogonality of characters
. . . . . . . . . . . . . . . . . . . . . . . . . 243
21.4 Examples of character tables . . . . . . . . . . . . . . . . . . . . . . . . . 245
21.5 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 247

22 Some applications

249
22.1 Frobenius divisibility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249
22.2 Burnside’s theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250
22.3 Frobenius determinant . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251

VII

Quantum Algorithms

253

23 Quantum states and measurements

255
23.1 Bra-ket notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255
23.2 The state space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256
23.3 Observations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256
23.4 Entanglement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259
23.5 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 262

24 Quantum circuits

263
24.1 Classical logic gates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
24.2 Reversible classical logic . . . . . . . . . . . . . . . . . . . . . . . . . . . 264
24.3 Quantum logic gates
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266
24.4 Deutsch-Jozsa algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . 268
24.5 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 269

25 Shor’s algorithm

271
25.1 The classical (inverse) Fourier transform . . . . . . . . . . . . . . . . . . 271
25.2 The quantum Fourier transform . . . . . . . . . . . . . . . . . . . . . . . 272
25.3 Shor’s algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274

VIII Calculus 101

277

26 Limits and series

279
26.1 Completeness and inf/sup . . . . . . . . . . . . . . . . . . . . . . . . . . 279

20

Napkin, by Evan Chen (v1.5.20190718)

Inﬁnite series

26.2 Proofs of the two key completeness properties of R . . . . . . . . . . . . 280
26.3 Monotonic sequences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282
26.4
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283
26.5 Series addition is not commutative: a horror story . . . . . . . . . . . . 286
26.6 Limits of functions at points . . . . . . . . . . . . . . . . . . . . . . . . . 287
26.7 Limits of functions at inﬁnity . . . . . . . . . . . . . . . . . . . . . . . . 289
26.8 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 289

27 Bonus: A hint of p-adic numbers

291
27.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 291
27.2 Algebraic perspective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292
27.3 Analytic perspective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299
27.4 Mahler coeﬃcients
27.5 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 301

28 Diﬀerentiation

303
28.1 Deﬁnition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303
28.2 How to compute them . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304
28.3 Local (and global) maximums . . . . . . . . . . . . . . . . . . . . . . . . 307
28.4 Rolle and friends
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309
28.5 Smooth functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312
28.6 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 312

29 Power series and Taylor series

315
29.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 315
29.2 Power series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 316
29.3 Diﬀerentiating them . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317
29.4 Analytic functions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 318
29.5 A deﬁnition of Euler’s constant and exponentiation . . . . . . . . . . . . 320
29.6 This all works over complex numbers as well, except also complex analysis

is heaven . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321
. . . . . . . . . . . . . . . . . . . 321

29.7 A few harder problems to think about

30 Riemann integrals

323
30.1 Uniform continuity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323
30.2 Dense sets and extension . . . . . . . . . . . . . . . . . . . . . . . . . . . 324
30.3 Deﬁning the Riemann integral . . . . . . . . . . . . . . . . . . . . . . . . 325
30.4 Meshes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327
. . . . . . . . . . . . . . . . . . . 328
30.5 A few harder problems to think about

IX

Complex Analysis

331

31 Holomorphic functions

333
31.1 The nicest functions on earth . . . . . . . . . . . . . . . . . . . . . . . . 333
31.2 Complex diﬀerentiation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335
31.3 Contour integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336
31.4 Cauchy-Goursat theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . 338
31.5 Cauchy’s integral theorem . . . . . . . . . . . . . . . . . . . . . . . . . . 339
31.6 Holomorphic functions are analytic . . . . . . . . . . . . . . . . . . . . . 341
31.7 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 343

Contents

21

32 Meromorphic functions

345
32.1 The second nicest functions on earth . . . . . . . . . . . . . . . . . . . . 345
32.2 Meromorphic functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345
32.3 Winding numbers and the residue theorem . . . . . . . . . . . . . . . . . 348
32.4 Argument principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 350
32.5 Philosophy: why are holomorphic functions so nice? . . . . . . . . . . . . 351
32.6 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 352

33 Holomorphic square roots and logarithms

353
33.1 Motivation: square root of a complex number . . . . . . . . . . . . . . . 353
33.2 Square roots of holomorphic functions
. . . . . . . . . . . . . . . . . . . 355
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356
33.3 Covering projections
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356
33.4 Complex logarithms
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357
33.5 Some special cases
33.6 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 358

X

Measure Theory

359

34 Measure spaces

361
34.1 Motivating measure spaces via random variables . . . . . . . . . . . . . . 361
34.2 Motivating measure spaces geometrically . . . . . . . . . . . . . . . . . . 362
34.3 σ-algebras and measurable spaces . . . . . . . . . . . . . . . . . . . . . . 363
34.4 Measure spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 364
34.5 A hint of Banach-Tarski
. . . . . . . . . . . . . . . . . . . . . . . . . . . 365
34.6 Measurable functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 366
. . . . . . . . . . . . . . . . . . . . . . 366
34.7 On the word “almost” (TO DO)
34.8 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 366

35 Constructing the Borel and Lebesgue measure

367
35.1 Pre-measures
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367
35.2 Outer measures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 368
35.3 Carath´eodory extension for outer measures . . . . . . . . . . . . . . . . . 370
35.4 Deﬁning the Lebesgue measure
. . . . . . . . . . . . . . . . . . . . . . . 372
35.5 A fourth row: Carath´eodory for pre-measures . . . . . . . . . . . . . . . 374
35.6 From now on, we assume the Borel measure . . . . . . . . . . . . . . . . 375
35.7 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 375

36 Lebesgue integration

377
36.1 The deﬁnition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377
36.2 Relation to Riemann integrals (or: actually computing Lebesgue integrals)379
. . . . . . . . . . . . . . . . . . . 380
36.3 A few harder problems to think about

37 Swapping order with Lebesgue integrals

381
37.1 Motivating limit interchange . . . . . . . . . . . . . . . . . . . . . . . . . 381
37.2 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382
37.3 Fatou’s lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382
37.4 Everything else . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384
37.5 Fubini and Tonelli
37.6 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 384

22

Napkin, by Evan Chen (v1.5.20190718)

38 Bonus: A hint of Pontryagin duality

385
38.1 LCA groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 385
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 386
38.2 The Pontryagin dual
38.3 The orthonormal basis in the compact case
. . . . . . . . . . . . . . . . 387
38.4 The Fourier transform of the non-compact case . . . . . . . . . . . . . . 388
38.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 388
38.6 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 389

XI

Probability (TO DO)

391

39 Random variables (TO DO)

393
39.1 Random variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 393
39.2 Distribution functions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 394
39.3 Examples of random variables . . . . . . . . . . . . . . . . . . . . . . . . 394
39.4 Characteristic functions
. . . . . . . . . . . . . . . . . . . . . . . . . . . 394
Independent random variables . . . . . . . . . . . . . . . . . . . . . . . . 394
39.5
39.6 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 394

40 Large number laws (TO DO)

395
40.1 Notions of convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395
40.2 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 396

41 Stopped martingales (TO DO)

XII

Diﬀerential Geometry

397

399

42 Multivariable calculus done correctly

401
42.1 The total derivative . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 401
42.2 The projection principle . . . . . . . . . . . . . . . . . . . . . . . . . . . 403
42.3 Total and partial derivatives . . . . . . . . . . . . . . . . . . . . . . . . . 404
42.4
(Optional) A word on higher derivatives . . . . . . . . . . . . . . . . . . 406
42.5 Towards diﬀerential forms . . . . . . . . . . . . . . . . . . . . . . . . . . 407
. . . . . . . . . . . . . . . . . . . 407
42.6 A few harder problems to think about

43 Diﬀerential forms

409
43.1 Pictures of diﬀerential forms . . . . . . . . . . . . . . . . . . . . . . . . . 409
43.2 Pictures of exterior derivatives . . . . . . . . . . . . . . . . . . . . . . . . 411
43.3 Diﬀerential forms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 412
43.4 Exterior derivatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 413
43.5 Closed and exact forms . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415
. . . . . . . . . . . . . . . . . . . 416
43.6 A few harder problems to think about

44 Integrating diﬀerential forms

417
44.1 Motivation: line integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . 417
44.2 Pullbacks
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 418
44.3 Cells . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 419
44.4 Boundaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 421
44.5 Stokes’ theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 423
44.6 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 423

Contents

23

45 A bit of manifolds

425
45.1 Topological manifolds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 425
45.2 Smooth manifolds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 426
45.3 Regular value theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . 427
45.4 Diﬀerential forms on manifolds
. . . . . . . . . . . . . . . . . . . . . . . 428
45.5 Orientations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 429
45.6 Stokes’ theorem for manifolds . . . . . . . . . . . . . . . . . . . . . . . . 430
45.7
(Optional) The tangent and contangent space . . . . . . . . . . . . . . . 430
. . . . . . . . . . . . . . . . . . . 433
45.8 A few harder problems to think about

XIII Algebraic NT I: Rings of Integers

435

46 Algebraic integers

437
46.1 Motivation from high school algebra . . . . . . . . . . . . . . . . . . . . 437
46.2 Algebraic numbers and algebraic integers . . . . . . . . . . . . . . . . . . 438
46.3 Number ﬁelds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 439
46.4 Primitive element theorem, and monogenic extensions
. . . . . . . . . . 440
. . . . . . . . . . . . . . . . . . . 441
46.5 A few harder problems to think about

47 The ring of integers

443
47.1 Norms and traces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 443
47.2 The ring of integers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 446
47.3 On monogenic extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . 450
47.4 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 450

48 Unique factorization (ﬁnally!)

451
48.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 451
48.2
Ideal arithmetic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 452
48.3 Dedekind domains
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 453
48.4 Unique factorization works . . . . . . . . . . . . . . . . . . . . . . . . . . 454
48.5 The factoring algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . 455
48.6 Fractional ideals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 458
48.7 The ideal norm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 459
48.8 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 460

49 Minkowski bound and class groups

461
49.1 The class group . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 461
49.2 The discriminant of a number ﬁeld . . . . . . . . . . . . . . . . . . . . . 461
49.3 The signature of a number ﬁeld . . . . . . . . . . . . . . . . . . . . . . . 464
49.4 Minkowski’s theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 466
49.5 The trap box . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 467
49.6 The Minkowski bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . 468
49.7 The class group is ﬁnite
. . . . . . . . . . . . . . . . . . . . . . . . . . . 469
49.8 Computation of class numbers . . . . . . . . . . . . . . . . . . . . . . . . 470
49.9 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 473

50 More properties of the discriminant

50.1 A few harder problems to think about

475
. . . . . . . . . . . . . . . . . . . 475

24

Napkin, by Evan Chen (v1.5.20190718)

51 Bonus: Let’s solve Pell’s equation!

477
51.1 Units . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 477
51.2 Dirichlet’s unit theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . 478
51.3 Finding fundamental units . . . . . . . . . . . . . . . . . . . . . . . . . . 479
51.4 Pell’s equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 480
51.5 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 481

XIV Algebraic NT II: Galois and Ramiﬁcation Theory

483

52 Things Galois

485
52.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 485
52.2 Field extensions, algebraic closures, and splitting ﬁelds . . . . . . . . . . 486
52.3 Embeddings into algebraic closures for number ﬁelds . . . . . . . . . . . 487
52.4 Everyone hates characteristic 2: separable vs irreducible . . . . . . . . . 488
52.5 Automorphism groups and Galois extensions . . . . . . . . . . . . . . . . 490
52.6 Fundamental theorem of Galois theory . . . . . . . . . . . . . . . . . . . 493
52.7 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 494
(Optional) Proof that Galois extensions are splitting . . . . . . . . . . . 495
52.8

53 Finite ﬁelds

497
53.1 Example of a ﬁnite ﬁeld . . . . . . . . . . . . . . . . . . . . . . . . . . . 497
53.2 Finite ﬁelds have prime power order
. . . . . . . . . . . . . . . . . . . . 498
53.3 All ﬁnite ﬁelds are isomorphic . . . . . . . . . . . . . . . . . . . . . . . . 499
53.4 The Galois theory of ﬁnite ﬁelds . . . . . . . . . . . . . . . . . . . . . . . 500
53.5 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 501

54 Ramiﬁcation theory

503
54.1 Ramiﬁed / inert / split primes . . . . . . . . . . . . . . . . . . . . . . . . 503
54.2 Primes ramify if and only if they divide ∆K . . . . . . . . . . . . . . . . 504
54.3
Inertial degrees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 504
. . . . . . . . . . . . . . . . . . . . . . . 505
54.4 The magic of Galois extensions
54.5
. . . . . . . . . . . . . . . 507
54.6 Tangential remark: more general Galois extensions . . . . . . . . . . . . 509
. . . . . . . . . . . . . . . . . . . 509
54.7 A few harder problems to think about

(Optional) Decomposition and inertia groups

55 The Frobenius element

511
55.1 Frobenius elements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 511
55.2 Conjugacy classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 512
55.3 Chebotarev density theorem . . . . . . . . . . . . . . . . . . . . . . . . . 514
55.4 Example: Frobenius elements of cyclotomic ﬁelds . . . . . . . . . . . . . 514
55.5 Frobenius elements behave well with restriction . . . . . . . . . . . . . . 515
55.6 Application: Quadratic reciprocity . . . . . . . . . . . . . . . . . . . . . 516
55.7 Frobenius elements control factorization . . . . . . . . . . . . . . . . . . 518
55.8 Example application: IMO 2003 problem 6 . . . . . . . . . . . . . . . . . 521
. . . . . . . . . . . . . . . . . . . 522
55.9 A few harder problems to think about

56 Bonus: A Bit on Artin Reciprocity

523
Inﬁnite primes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 523
. . . . . . . . . . . . . . . . . . 523
Inﬁnite primes in extensions . . . . . . . . . . . . . . . . . . . . . . . . . 525

56.1
56.2 Modular arithmetic with inﬁnite primes
56.3

Contents

25

56.4 Frobenius element and Artin symbol
. . . . . . . . . . . . . . . . . . . . 526
56.5 Artin reciprocity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 528
56.6 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 531

XV

Algebraic Topology I: Homotopy

533

57 Some topological constructions

535
57.1 Spheres . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 535
57.2 Quotient topology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 535
57.3 Product topology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 536
57.4 Disjoint union and wedge sum . . . . . . . . . . . . . . . . . . . . . . . . 537
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 538
57.5 CW complexes
57.6 The torus, Klein bottle, RPn, CPn
. . . . . . . . . . . . . . . . . . . . . 539
57.7 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 545

58 Fundamental groups

547
58.1 Fusing paths together . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 547
58.2 Fundamental groups
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 548
58.3 Fundamental groups are functorial
. . . . . . . . . . . . . . . . . . . . . 552
58.4 Higher homotopy groups . . . . . . . . . . . . . . . . . . . . . . . . . . . 553
58.5 Homotopy equivalent spaces . . . . . . . . . . . . . . . . . . . . . . . . . 554
58.6 The pointed homotopy category . . . . . . . . . . . . . . . . . . . . . . . 556
. . . . . . . . . . . . . . . . . . . 557
58.7 A few harder problems to think about

59 Covering projections

559
. . . . . . . . . . . . . . . . . . 559
59.1 Even coverings and covering projections
59.2 Lifting theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 561
59.3 Lifting correspondence . . . . . . . . . . . . . . . . . . . . . . . . . . . . 563
59.4 Regular coverings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 564
59.5 The algebra of fundamental groups . . . . . . . . . . . . . . . . . . . . . 566
. . . . . . . . . . . . . . . . . . . 568
59.6 A few harder problems to think about

XVI Category Theory

569

60 Objects and morphisms

571
60.1 Motivation: isomorphisms . . . . . . . . . . . . . . . . . . . . . . . . . . 571
60.2 Categories, and examples thereof
. . . . . . . . . . . . . . . . . . . . . . 571
60.3 Special objects in categories . . . . . . . . . . . . . . . . . . . . . . . . . 575
60.4 Binary products . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 576
60.5 Monic and epic maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 580
. . . . . . . . . . . . . . . . . . . 581
60.6 A few harder problems to think about

61 Functors and natural transformations

583
61.1 Many examples of functors . . . . . . . . . . . . . . . . . . . . . . . . . . 583
61.2 Covariant functors
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 584
61.3 Contravariant functors . . . . . . . . . . . . . . . . . . . . . . . . . . . . 586
61.4 Equivalence of categories . . . . . . . . . . . . . . . . . . . . . . . . . . . 588
(Optional) Natural transformations . . . . . . . . . . . . . . . . . . . . . 588
61.5
61.6
(Optional) The Yoneda lemma . . . . . . . . . . . . . . . . . . . . . . . . 590

26

Napkin, by Evan Chen (v1.5.20190718)

61.7 A few harder problems to think about

. . . . . . . . . . . . . . . . . . . 592

62 Limits in categories (TO DO)

593
62.1 Equalizers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 593
62.2 Pullback squares (TO DO) . . . . . . . . . . . . . . . . . . . . . . . . . . 594
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 594
62.3 Limits
62.4 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 594

63 Abelian categories

595
63.1 Zero objects, kernels, cokernels, and images
. . . . . . . . . . . . . . . . 595
63.2 Additive and abelian categories . . . . . . . . . . . . . . . . . . . . . . . 596
63.3 Exact sequences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 598
63.4 The Freyd-Mitchell embedding theorem . . . . . . . . . . . . . . . . . . 599
63.5 Breaking long exact sequences . . . . . . . . . . . . . . . . . . . . . . . . 600
63.6 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 601

XVII Algebraic Topology II: Homology

603

64 Singular homology

605
64.1 Simplices and boundaries . . . . . . . . . . . . . . . . . . . . . . . . . . . 605
64.2 The singular homology groups . . . . . . . . . . . . . . . . . . . . . . . . 607
64.3 The homology functor and chain complexes
. . . . . . . . . . . . . . . . 610
. . . . . . . . . . . . . . . . . . . . . 614
64.4 More examples of chain complexes
. . . . . . . . . . . . . . . . . . . 615
64.5 A few harder problems to think about

65 The long exact sequence

617
65.1 Short exact sequences and four examples . . . . . . . . . . . . . . . . . . 617
65.2 The long exact sequence of homology groups . . . . . . . . . . . . . . . . 619
. . . . . . . . . . . . . . . . . . . . . . . . 621
65.3 The Mayer-Vietoris sequence
65.4 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 626

66 Excision and relative homology

627
66.1 The long exact sequences . . . . . . . . . . . . . . . . . . . . . . . . . . . 627
66.2 The category of pairs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 628
66.3 Excision . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 629
66.4 Some applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 630
Invariance of dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . 631
66.5
66.6 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 632

67 Bonus: Cellular homology

633
67.1 Degrees
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 633
67.2 Cellular chain complex . . . . . . . . . . . . . . . . . . . . . . . . . . . . 634
67.3 The cellular boundary formula . . . . . . . . . . . . . . . . . . . . . . . . 637
67.4 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 639

68 Singular cohomology

641
68.1 Cochain complexes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 641
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 642
68.2 Cohomology of spaces
68.3 Cohomology of spaces is functorial
. . . . . . . . . . . . . . . . . . . . . 643
68.4 Universal coeﬃcient theorem . . . . . . . . . . . . . . . . . . . . . . . . . 644
68.5 Example computation of cohomology groups . . . . . . . . . . . . . . . . 645

Contents

27

68.6 Relative cohomology groups . . . . . . . . . . . . . . . . . . . . . . . . . 646
. . . . . . . . . . . . . . . . . . . 647
68.7 A few harder problems to think about

69 Application of cohomology

649
69.1 Poincar´e duality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 649
69.2 de Rham cohomology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 649
69.3 Graded rings
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 650
69.4 Cup products . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 652
69.5 Relative cohomology pseudo-rings . . . . . . . . . . . . . . . . . . . . . . 654
69.6 Wedge sums . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 654
69.7 K¨unneth formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 656
69.8 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 657

XVIII Algebraic Geometry I: Classical Varieties

659

70 Aﬃne varieties

661
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 661
70.1 Aﬃne varieties
70.2 Naming aﬃne varieties via ideals
. . . . . . . . . . . . . . . . . . . . . . 662
70.3 Radical ideals and Hilbert’s Nullstellensatz . . . . . . . . . . . . . . . . . 663
70.4 Pictures of varieties in A1
. . . . . . . . . . . . . . . . . . . . . . . . . . 664
70.5 Prime ideals correspond to irreducible aﬃne varieties . . . . . . . . . . . 665
70.6 Pictures in A2 and A3
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 666
70.7 Maximal ideals
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 667
70.8 Motivating schemes with non-radical ideals . . . . . . . . . . . . . . . . . 668
70.9 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 668

71 Aﬃne varieties as ringed spaces

669
71.1 Synopsis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 669
71.2 The Zariski topology on An . . . . . . . . . . . . . . . . . . . . . . . . . 669
71.3 The Zariski topology on aﬃne varieties . . . . . . . . . . . . . . . . . . . 671
71.4 Coordinate rings
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 672
71.5 The sheaf of regular functions . . . . . . . . . . . . . . . . . . . . . . . . 673
71.6 Regular functions on distinguished open sets . . . . . . . . . . . . . . . . 674
71.7 Baby ringed spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 675
71.8 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 676

677
72 Projective varieties
72.1 Graded rings
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 677
72.2 The ambient space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 678
72.3 Homogeneous ideals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 680
72.4 As ringed spaces
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 681
72.5 Examples of regular functions . . . . . . . . . . . . . . . . . . . . . . . . 682
72.6 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 683

73 Bonus: B´ezout’s theorem

685
73.1 Non-radical ideals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 685
73.2 Hilbert functions of ﬁnitely many points . . . . . . . . . . . . . . . . . . 686
73.3 Hilbert polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 688
73.4 B´ezout’s theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 690
73.5 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 690

28

Napkin, by Evan Chen (v1.5.20190718)

73.6 A few harder problems to think about

. . . . . . . . . . . . . . . . . . . 691

74 Morphisms of varieties

693
74.1 Deﬁning morphisms of baby ringed spaces . . . . . . . . . . . . . . . . . 693
74.2 Classifying the simplest examples . . . . . . . . . . . . . . . . . . . . . . 694
74.3 Some more applications and examples
. . . . . . . . . . . . . . . . . . . 696
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 697
74.4 The hyperbola eﬀect
74.5 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 699

XIX Algebraic Geometry II: Aﬃne Schemes

701

75 Sheaves and ringed spaces

705
75.1 Motivation and warnings . . . . . . . . . . . . . . . . . . . . . . . . . . . 705
75.2 Pre-sheaves
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 705
75.3 Stalks and germs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 707
75.4 Sheaves
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 711
75.5 For sheaves, sections “are” sequences of germs . . . . . . . . . . . . . . . 712
75.6 Sheaﬁﬁcation (optional)
. . . . . . . . . . . . . . . . . . . . . . . . . . . 714
. . . . . . . . . . . . . . . . . . . 715
75.7 A few harder problems to think about

76 Localization

717
76.1 Spoilers
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 717
76.2 The deﬁnition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 718
76.3 Localization away from an element . . . . . . . . . . . . . . . . . . . . . 719
76.4 Localization at a prime ideal . . . . . . . . . . . . . . . . . . . . . . . . . 720
76.5 Prime ideals of localizations . . . . . . . . . . . . . . . . . . . . . . . . . 722
76.6 Prime ideals of quotients . . . . . . . . . . . . . . . . . . . . . . . . . . . 723
76.7 Localization commute with quotients . . . . . . . . . . . . . . . . . . . . 723
76.8 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 725

77 Aﬃne schemes: the Zariski topology

727
77.1 Some more advertising . . . . . . . . . . . . . . . . . . . . . . . . . . . . 727
77.2 The set of points
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 727
77.3 The Zariski topology on the spectrum . . . . . . . . . . . . . . . . . . . 729
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 732
77.4 On radicals
77.5 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 733

78 Aﬃne schemes: the sheaf

735
78.1 A useless deﬁnition of the structure sheaf . . . . . . . . . . . . . . . . . . 735
78.2 The value of distinguished open sets (or: how to actually compute sections)736
78.3 The stalks of the structure sheaf . . . . . . . . . . . . . . . . . . . . . . . 738
78.4 Local rings and residue ﬁelds: linking germs to values . . . . . . . . . . . 739
78.5 Recap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 742
78.6 Functions are determined by germs, not values . . . . . . . . . . . . . . . 742
. . . . . . . . . . . . . . . . . . . 743
78.7 A few harder problems to think about

79 Interlude: eighteen examples of aﬃne schemes

745
79.1 Example: Spec k, a single point . . . . . . . . . . . . . . . . . . . . . . . 745
79.2 Spec C[x], a one-dimensional line . . . . . . . . . . . . . . . . . . . . . . 745

Contents

29

79.3 Spec R[x], a one-dimensional line with complex conjugates glued (no fear

nullstellensatz)

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 746
79.4 Spec k[x], over any ground ﬁeld . . . . . . . . . . . . . . . . . . . . . . . 747
79.5 Spec Z, a one-dimensional scheme . . . . . . . . . . . . . . . . . . . . . . 747
79.6 Spec k[x]/(x2 − x), two points . . . . . . . . . . . . . . . . . . . . . . . . 748
79.7 Spec k[x]/(x2), the double point . . . . . . . . . . . . . . . . . . . . . . . 748
79.8 Spec k[x]/(x3 − x), a double point and a single point . . . . . . . . . . . 749
79.9 Spec Z/60Z, a scheme with three points
. . . . . . . . . . . . . . . . . . 749
79.10 Spec k[x, y], the two-dimensional plane . . . . . . . . . . . . . . . . . . . 750
79.11 Spec Z[x], a two-dimensional scheme, and Mumford’s picture . . . . . . . 751
79.12 Spec k[x, y]/(y − x2), the parabola
. . . . . . . . . . . . . . . . . . . . . 752
79.13 Spec Z[i], the Gaussian integers (one-dimensional) . . . . . . . . . . . . . 753
79.14 Long example: Spec k[x, y]/(xy), two axes . . . . . . . . . . . . . . . . . 754
79.15 Spec k[x, x−1], the punctured line (or hyperbola)
. . . . . . . . . . . . . 756
79.16 Spec k[x](x), zooming in to the origin of the line . . . . . . . . . . . . . . 757
79.17 Spec k[x, y](x,y), zooming in to the origin of the plane . . . . . . . . . . . 758
79.18 Spec k[x, y](0) = Spec k(x, y), the stalk above the generic point . . . . . . 758
79.19 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 758

80 Morphisms of locally ringed spaces

759
80.1 Morphisms of ringed spaces via sections
. . . . . . . . . . . . . . . . . . 759
80.2 Morphisms of ringed spaces via stalks . . . . . . . . . . . . . . . . . . . . 760
80.3 Morphisms of locally ringed spaces
. . . . . . . . . . . . . . . . . . . . . 761
80.4 A few examples of morphisms between aﬃne schemes . . . . . . . . . . . 762
80.5 The big theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 765
80.6 More examples of scheme morphisms . . . . . . . . . . . . . . . . . . . . 767
80.7 A little bit on non-aﬃne schemes . . . . . . . . . . . . . . . . . . . . . . 768
80.8 Where to go from here . . . . . . . . . . . . . . . . . . . . . . . . . . . . 770
80.9 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 770

XX

Algebraic Geometry III: Schemes (TO DO)

XXI

Set Theory I: ZFC, Ordinals, and Cardinals

771

773

81 Interlude: Cauchy’s functional equation and Zorn’s lemma

775
. . . . . . . . . . . . . . . . . . . . . . . . . . 775
81.1 Let’s construct a monster
81.2 Review of ﬁnite induction . . . . . . . . . . . . . . . . . . . . . . . . . . 776
81.3 Transﬁnite induction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 776
81.4 Wrapping up functional equations . . . . . . . . . . . . . . . . . . . . . . 778
81.5 Zorn’s lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 779
. . . . . . . . . . . . . . . . . . . 781
81.6 A few harder problems to think about

82 Zermelo-Fraenkel with choice

783
82.1 The ultimate functional equation . . . . . . . . . . . . . . . . . . . . . . 783
82.2 Cantor’s paradox . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 783
82.3 The language of set theory . . . . . . . . . . . . . . . . . . . . . . . . . . 784
82.4 The axioms of ZFC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 785
82.5 Encoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 787
82.6 Choice and well-ordering . . . . . . . . . . . . . . . . . . . . . . . . . . . 787

30

Napkin, by Evan Chen (v1.5.20190718)

82.7 Sets vs classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 788
. . . . . . . . . . . . . . . . . . . 789
82.8 A few harder problems to think about

83 Ordinals

791
. . . . . . . . . . . . . . . . . . . . . . . . . . 791
83.1 Counting for preschoolers
83.2 Counting for set theorists
. . . . . . . . . . . . . . . . . . . . . . . . . . 792
83.3 Deﬁnition of an ordinal . . . . . . . . . . . . . . . . . . . . . . . . . . . . 794
83.4 Ordinals are “tall” . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 795
83.5 Transﬁnite induction and recursion . . . . . . . . . . . . . . . . . . . . . 796
83.6 Ordinal arithmetic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 797
83.7 The hierarchy of sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 798
83.8 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 800

84 Cardinals

801
84.1 Equinumerous sets and cardinals
. . . . . . . . . . . . . . . . . . . . . . 801
84.2 Cardinalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 802
84.3 Aleph numbers
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 802
84.4 Cardinal arithmetic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 803
84.5 Cardinal exponentiation . . . . . . . . . . . . . . . . . . . . . . . . . . . 805
84.6 Coﬁnality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 805
Inaccessible cardinals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 807
84.7
84.8 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 807

XXII Set Theory II: Model Theory and Forcing

809

85 Inner model theory

811
85.1 Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 811
85.2 Sentences and satisfaction . . . . . . . . . . . . . . . . . . . . . . . . . . 812
85.3 The Levy hierarchy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 814
85.4 Substructures, and Tarski-Vaught . . . . . . . . . . . . . . . . . . . . . . 815
85.5 Obtaining the axioms of ZFC . . . . . . . . . . . . . . . . . . . . . . . . 816
85.6 Mostowski collapse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 817
85.7 Adding an inaccessible . . . . . . . . . . . . . . . . . . . . . . . . . . . . 817
85.8 FAQ’s on countable models
. . . . . . . . . . . . . . . . . . . . . . . . . 819
85.9 Picturing inner models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 820
85.10 A few harder problems to think about
. . . . . . . . . . . . . . . . . . . 821

86 Forcing

823
86.1 Setting up posets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 824
86.2 More properties of posets
. . . . . . . . . . . . . . . . . . . . . . . . . . 825
86.3 Names, and the generic extension . . . . . . . . . . . . . . . . . . . . . . 826
86.4 Fundamental theorem of forcing . . . . . . . . . . . . . . . . . . . . . . . 829
(Optional) Deﬁning the relation . . . . . . . . . . . . . . . . . . . . . . . 829
86.5
86.6 The remaining axioms
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 831
. . . . . . . . . . . . . . . . . . . 831
86.7 A few harder problems to think about

87 Breaking the continuum hypothesis

833
87.1 Adding in reals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 833
87.2 The countable chain condition . . . . . . . . . . . . . . . . . . . . . . . . 834
87.3 Preserving cardinals
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 835

Contents

31

87.4
87.5 A few harder problems to think about

Inﬁnite combinatorics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 836
. . . . . . . . . . . . . . . . . . . 837

XXIII Backmatter

839

A Pedagogical comments and references

841
A.1 Basic algebra and topology . . . . . . . . . . . . . . . . . . . . . . . . . . 841
A.2
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 842
A.3 Advanced topics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 843
A.4 Topics not in Napkin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 844

Second-year topics

B Hints to selected problems

C Sketches of selected solutions

845

855

D Glossary of notations

877
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 877
D.1 General
D.2 Functions and sets
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 877
D.3 Abstract and linear algebra . . . . . . . . . . . . . . . . . . . . . . . . . 878
D.4 Quantum computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 879
D.5 Topology and real/complex analysis . . . . . . . . . . . . . . . . . . . . . 879
D.6 Measure theory and probability . . . . . . . . . . . . . . . . . . . . . . . 880
D.7 Algebraic topology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 880
D.8 Category theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 881
D.9 Diﬀerential geometry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 882
D.10 Algebraic number theory . . . . . . . . . . . . . . . . . . . . . . . . . . . 882
D.11 Representation theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . 883
D.12 Algebraic geometry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 884
D.13 Set theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 885

E Terminology on sets and functions

887
Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 887
E.1
E.2
Functions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 888
E.3 Equivalence relations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 890

I

Starting Out

Part I: Contents

0 Sales pitches
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
0.1 The basics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
0.2 Abstract algebra
0.3 Real and complex analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
0.4 Algebraic number theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
0.5 Algebraic topology
0.6 Algebraic geometry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
0.7 Set theory

1 Groups

. . . . . . . . . . . . . . . . . . . . . . . . . .
1.1 Deﬁnition and examples of groups
1.2 Properties of groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Isomorphisms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.3
1.4 Orders of groups, and Lagrange’s theorem . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.5 Subgroups
1.6 Groups of small orders . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.7 Unimportant long digression . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
1.8 A few harder problems to think about

2 Metric spaces

2.1 Deﬁnition and examples of metric spaces . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 Convergence in metric spaces
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3 Continuous maps
2.4 Homeomorphisms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.5 Extended example/deﬁnition: product metric . . . . . . . . . . . . . . . . . . . . .
2.6 Open sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.7 Closed sets
. . . . . . . . . . . . . . . . . . . . . . . .
2.8 A few harder problems to think about

35
35
36
37
38
39
39
40

41
41
44
46
48
49
50
51
51

53
53
55
56
57
58
59
61
62

0 Sales pitches

This chapter contains a pitch for each part, to help you decide what you want to read

and to elaborate more on how they are interconnected.

For convenience, here is again the dependency plot that appeared in the frontmatter.

Ch 81-87
Set Theory

Ch 1,3-5
Abs Alg

Ch 9-15,18
Lin Alg

Ch 2,6-8
Topology

Ch 23-25
Quantum

Ch 16
Grp Act

Ch 19-22
Rep Th

Ch 60-63
Cat Th

Ch 42-45
Diﬀ Geo

Ch 17
Grp Classif

Ch 26-30
Calc

Ch 31-33
Cmplx Ana

Ch 34-41
Measure/Pr

Ch 70-74
Alg Geo 1

Ch 57-59
Alg Top 1

Ch 46-51
Alg NT 1

Ch 75-80
Alg Geo 2-3

Ch 64-69
Alg Top 2

Ch 52-56
Alg NT 2

§0.1 The basics

I. Starting Out.

I made a design decision that the ﬁrst part should have a little bit both of algebra
and topology: so this ﬁrst chapter begins by deﬁning a group, while the second
chapter begins by deﬁning a metric space. The intention is so that newcomers
get to see two diﬀerent examples of “sets with additional structure” in somewhat
diﬀerent contexts, and to have a minimal amount of literacy as these sorts of
deﬁnitions appear over and over1

II. Basic Abstract Algebra.

The algebraically inclined can then delve into further types of algebraic structures:
some more details of groups, and then rings and ﬁelds — which will let you
generalize Z, Q, R, C. So you’ll learn to become familiar with all sorts of other
nouns that appear in algebra, unlocking a whole host of objects that one couldn’t
talk about before.

1In particular, I think it’s easier to learn what a homeomorphism is after seeing group isomorphism,

and what a homomorphism is after seeing continuous map.

35

36

Napkin, by Evan Chen (v1.5.20190718)

We’ll also come into ideals, which generalize the GCD in Z that you might know
of. For example, you know in Z that any integer can be written in the form 3a + 5b
for a, b ∈ Z, since gcd(3, 5) = 1. We’ll see that this statement is really a statement
of ideals: “(3, 5) = 1 in Z”, and thus we’ll understand in what situations it can be
generalized, e.g. to polynomials.

III. Basic Topology.

The more analytically inclined can instead move into topology, learning more about
spaces. We’ll ﬁnd out that “metric spaces” are actually too speciﬁc, and that it’s
better to work with topological spaces, which are based on the so-called open
sets. You’ll then get to see the buddings of some geometrical ideals, ending with
the really great notion of compactness, a powerful notion that makes real analysis
tick.

One example of an application of compactness to tempt you now: a continuous
function f : [0, 1] → R always achieves a maximum value. (In contrast, f : (0, 1) → R
by x (cid:55)→ 1/x does not.) We’ll see the reason is that [0, 1] is compact.

§0.2 Abstract algebra

IV. Linear Algebra.

In high school, linear algebra is often really unsatisfying. You are given these arrays
of numbers, and they’re manipulated in some ways that don’t really make sense.
For example, the determinant is deﬁned as this funny-looking sum with a bunch of
products that seems to come out of thin air. Where does it come from? Why does
det(AB) = det A det B with such a bizarre formula?

Well, it turns out that you can explain all of these things! The trick is to not think
of linear algebra as the study of matrices, but instead as the study of linear maps.
In earlier chapters we saw that we got great generalizations by speaking of “sets
with enriched structure” and “maps between them”. This time, our sets are vector
space and our maps are linear maps. We’ll ﬁnd out that a matrix is actually
just a way of writing down a linear map as an array of numbers, but using the
“intrinsic” deﬁnitions we’ll de-mystify all the strange formulas from high school and
show you where they all come from.

In particular, we’ll see easy proofs that column rank equals row rank, determinant
is multiplicative, trace is the sum of the diagonal entries, how the dot product
works, and all the words starting with “eigen-”. We’ll even have a bonus chapter
for Fourier analysis showing that you can also explain all the big buzz-words by
just being comfortable with vector spaces.

V. More on Groups.

Some of you might be interested in more about groups, and this chapter will give
you a way to play further. It starts with an exploration of group actions, then
goes into a bit on Sylow theorems, which are the tools that let us try to classify
all groups.

VI. Representation Theory.

If G is a group, we can try to understand it by implementing it as a matrix, i.e.
considering embeddings G (cid:44)→ GLn(C). These are called representations of G; it
turns out that they can be decomposed into irreducible ones. Astonishingly we

0 Sales pitches

37

will ﬁnd that we can basically characterize all of them: the results turn out to be
short and completely unexpected.

For example, we will ﬁnd out that there are ﬁnitely many irreducible representations
of a given ﬁnite group G; if we label them V1, V2, . . . , Vr, then we will ﬁnd that r
is the number of conjugacy classes of G, and moreover that

|G| = (dim V1)2 + ··· + (dim Vr)2

which comes out of nowhere!

The last chapter of this part will show you some unexpected corollaries. Here is
one of them: let G be a ﬁnite group and create variables xg for each g ∈ G. A
|G| × |G| matrix M is deﬁned by setting the (g, h)th entry to be the variable xg·h.
Then this determinant will turn out to factor, and the factors will correspond to
the Vi we described above: there will be an irreducible factor of degree dim Vi
appearing dim Vi times. This result, called the Frobenius determinant, is said
to have given birth to representation theory.

VII. Quantum Algorithms.

If you ever wondered what Shor’s algorithm is, this chapter will use the built-up
linear algebra to tell you!

§0.3 Real and complex analysis

VIII. Calculus 101.

In this part, we’ll use our built-up knowledge of metric and topological spaces to
give short, rigorous deﬁnitions and theorems typical of high school calculus. That
is, we’ll really deﬁne and prove most everything you’ve seen about limits, series,
derivatives, and integrals.

Although this might seem intimidating, it turns out that actually, by the time we
start this chapter, the hard work has already been done: the notion of limits, open
sets, and compactness will make short work of what was swept under the rug in AP
calculus. Most of the proofs will thus actually be quite short, instead, we sit back
and watch all the pieces slowly come together, the reward for our careful study of
topology beforehand.

That said, if you are willing to suspend belief, you can actually read most of the
other parts without knowing the exact details of all the calculus here, so in some
sense this part is “optional”.

IX. Complex Analysis.

It turns out that holomorphic functions (complex-diﬀerentiable functions) are
close to the nicest things ever: they turn out to be given by a Taylor series (i.e. are
basically polynomials). This means we’ll be able to prove unreasonably nice results
about holomorphic functions C → C, like

– they are determined by just a few inputs,

– their contour integrals are all zero,

– they can’t be bounded unless they are constant,

– . . . .

38

Napkin, by Evan Chen (v1.5.20190718)

We then introduce meromorphic functions, which are like quotients of holomor-
phic functions, and ﬁnd that we can detect their zeros by simply drawing loops
in the plane and integrating over them: the famous residue theorem appears.
(In the practice problems, you will see this even gives us a way to evaluate real
integrals that can’t be evaluated otherwise.)

X. Measure Theory.

Measure theory is the upgraded version of integration. The Riemann integration is
for a lot of purposes not really suﬃcient; for example, if f is the function equalling 1
0 f (x) dx = 0,

at rational numbers but 0 at irrational numbers, we would hope that(cid:82) 1

but the Riemann integral is not capable of handling this function f .

The Lebesgue integral will handle these mistakes by assigning a measure to a
generic space Ω, making it into a measure space. This will let us develop a richer
theory of integration where the above integral does work out to zero because the
“rational numbers have measure zero”. Even the development of the measure will
be an achievement, because it means we’ve developed a rigorous, complete way of
talking about what notions like area and volume mean — on any space, not just
Rn! So for example the Lebesgue integral will let us integrate functions over any
measure space.

XI. Probability (TO DO).

Using the tools of measure theory, we’ll be able to start giving rigorous deﬁnitions
of probability, too. We’ll see that a random variable is actually a function
from a measure space of worlds to R, giving us a rigorous way to talk about its
probabilities. We can then start actually stating results like the law of large
numbers and central limit theorem in ways that make them both easy to state
and straightforward to prove.

XII. Diﬀerential Geometry.

Multivariable calculus is often confusing because of all the partial derivatives. But
we’ll ﬁnd out that, armed with our good understanding of linear algebra, that we’re
really looking at a total derivative: at every point of a function f : Rn → R we
can associate a linear map Df which captures in one object the notion of partial
derivatives. Set up this way, we’ll get to see versions of diﬀerential forms and
Stoke’s theorem, and we ﬁnally will know what the notation dx really means. In
the end, we’ll say a little bit about manifolds in general.

§0.4 Algebraic number theory

XIII. Algebraic NT I: Rings of Integers.

used in Pell equations just happens to be multiplicative? Why is it we can do

Why is 3 +√5 the conjugate of 3−√5? How come the norm(cid:13)(cid:13)a + b√5(cid:13)(cid:13) = a2 − 5b2
factoring into primes in Z[i] but not in Z[√−5]? All these questions and more will
be answered in this part, when we learn about number ﬁelds, a generalization
of Q and Z to things like Q(√5) and Z[√5]. We’ll ﬁnd out that we have unique
factorization into prime ideals, that there is a real multiplicative norm in play here,
and so on. We’ll also see that Pell’s equation falls out of this theory.

XIV. Algebraic NT II: Galois and Ramiﬁcation Theory.

0 Sales pitches

39

All the big buzz-words come out now: Galois groups, the Frobenius, and friends.
We’ll see quadratic reciprocity is just a shadow of the behavior of the Frobenius
element, and meet the Chebotarev density theorem, which generalizes greatly
the Dirichlet theorem on the inﬁnitude of primes which are a (mod n). Towards
the end, we’ll also state Artin reciprocity, one of the great results of class ﬁeld
theory, and how it generalizes quadratic reciprocity and cubic reciprocity.

§0.5 Algebraic topology

XV. Algebraic Topology I: Homotopy.

What’s the diﬀerence between an annulus and disk? Well, one of them has a “hole”
in it, but if we are just given intrinsic topological spaces it’s hard to make this
notion precise. The fundamental group π1(X) and more general homotopy
group will make this precise — we’ll ﬁnd a way to deﬁne an abelian group π1(X)
for every topological space X which captures the idea there is a hole in the space,
by throwing lassos into the space and seeing if we can reel them in.

Amazingly, the fundamental group π1(X) will, under mild conditions, tell you about
ways to cover X with a so-called covering projection. One picture is that one
can wrap a real line R into a helix shape and then project it down into the circle
S1. This will turn out to correspond to the fact that π1(S1) = Z which has only
one subgroup. More generally the subgroups of π1(X) will be in bijection with
ways to cover the space X!

XVI. Category Theory.

What do ﬁelds, groups, manifolds, metric spaces, measure spaces, modules, repre-
sentations, rings, topological spaces, vector spaces, all have in common? Answer:
they are all “objects with additional structure”, with maps between them.

The notion of category will appropriately generalize all of them. We’ll see all
sorts of constructions and ideas can be abstracted into the framework of a category,
in which we only think about objects and arrows between them, without probing
too hard into the details of what those objects are. This results in drawing many
commutative diagrams.

For example, any way of taking an objection in one category and getting another
one (for example π1 as above, from the category of spaces into the category of
groups) will probably be a functor. We’ll unify G× H, X × Y , R× S, and anything
with the × symbol into the notion of a product, and then even more generally into
a limit. Towards the end, we talk about abelian categories and talk about the
famous snake lemma, ﬁve lemma, and so on.

XVII. Algebraic Topology II: Homology.

Using the language of category theory, we then resume our adventures in algebraic
topology, in which we deﬁne the homology groups which give a diﬀerent way of
noticing holes in a space, in a way that is longer to deﬁne but easier to compute in
practice. We’ll then reverse the construction to get so-called cohomology rings
instead, which give us an even ﬁner invariant for telling spaces apart.

§0.6 Algebraic geometry

XVIII. Algebraic Geometry I: Classical Varieties.

40

Napkin, by Evan Chen (v1.5.20190718)

We begin with a classical study of classical complex varieties: the study of
intersections of polynomial equations over C. This will naturally lead us into the
geometry of rings, giving ways to draw pictures of ideals, and motivating Hilbert’s
nullstellensatz. The Zariski topology will show its face, and then we’ll play
with projective varities and quasi-projective varieties, with a bonus detour
into Bezout’s theorem. All this prepares us for our journey into schemes.

XIX. Algebraic Geometry II: Aﬃne Schemes.

We now get serious and delve into Grothendiek’s deﬁnition of an aﬃne scheme:
a generalization of our classical varieties that lets us start with any ring A and
construct a space Spec A on it. We’ll equip it with its own Zariski topology and
then a sheaf of functions on it, making it into a locally ringed space; we will ﬁnd
that the sheaf can be understood eﬀectively in terms of localization on it. We’ll
ﬁnd that the language of commutative algebra provides elegant generalizations of
what’s going on geometrically: prime ideals correspond to irreducible closed subsets,
radical ideals correspond to closed subsets, maximal ideals correspond to closed
points, and so on. We’ll draw lots of pictures of spaces and examples to accompany
this.

XX. Algebraic Geometry III: Schemes (TO DO).

Not yet written! Wait for v2.

§0.7 Set theory

XXI. Set Theory I: ZFC, Ordinals, and Cardinals.

Why is Russell’s paradox such a big deal and how is it resolved? What is this
Zorn’s lemma that everyone keeps talking about? In this part we’ll learn the
answers to these questions by giving a real description of the Zermelo-Frankel
axioms, and the axiom of choice, delving into the details of how math is built
axiomatically at the very bottom foundations. We’ll meet the ordinal numbers
and cardinal numbers and learn how to do transﬁnite induction with them.

XXII. Set Theory II: Model Theory and Forcing.

The continuum hypothesis states that there are no cardinalities between the
size of the natural numbers and the size of the real numbers. It was shown to be
independent of the axioms — one cannot prove or disprove it. How could a result
like that possibly be proved? Using our understanding of the ZF axioms, we’ll
develop a bit of model theory and then use forcing in order to show how to
construct entire models of the universe in which the continuum hypothesis is true
or false.

1 Groups

A group is one of the most basic structures in higher mathematics. In this chapter I
will tell you only the bare minimum: what a group is, and when two groups are the same.

§1.1 Deﬁnition and examples of groups

Prototypical example for this section: The additive group of integers (Z, +) and the cyclic
group Z/mZ. Just don’t let yourself forget that most groups are non-commutative.

A group consists of two pieces of data: a set G, and an associative binary operation
(cid:63) with some properties. Before I write down the deﬁnition of a group, let me give two
examples.

Example 1.1.1 (Additive integers)
The pair (Z, +) is a group: Z = {. . . ,−2,−1, 0, 1, 2, . . .} is the set and the associative
operation is addition. Note that

 The element 0 ∈ Z is an identity: a + 0 = 0 + a = a for any a.
 Every element a ∈ Z has an additive inverse: a + (−a) = (−a) + a = 0.

We call this group Z.

Example 1.1.2 (Nonzero rationals)
Let Q× be the set of nonzero rational numbers. The pair (Q×,·) is a group: the set
is Q× and the associative operation is multiplication.

Again we see the same two nice properties.
 The element 1 ∈ Q× is an identity: for any rational number, a · 1 = 1 · a = a.
 For any rational number x ∈ Q×, we have an inverse x−1, such that

x · x−1 = x−1 · x = 1.

From this you might already have a guess what the deﬁnition of a group is.

Deﬁnition 1.1.3. A group is a pair G = (G, (cid:63)) consisting of a set of elements G, and
a binary operation (cid:63) on G, such that:

 G has an identity element, usually denoted 1G or just 1, with the property that

1G (cid:63) g = g (cid:63) 1G = g for all g ∈ G.

 The operation is associative, meaning (a (cid:63) b) (cid:63) c = a (cid:63) (b (cid:63) c) for any a, b, c ∈ G.

Consequently we generally don’t write the parentheses.

 Each element g ∈ G has an inverse, that is, an element h ∈ G such that

g (cid:63) h = h (cid:63) g = 1G.

41

42

Napkin, by Evan Chen (v1.5.20190718)

Remark 1.1.4 (Unimportant pedantic point) — Some authors like to add a “closure”
axiom, i.e. to say explicitly that g (cid:63) h ∈ G. This is implied already by the fact that
(cid:63) is a binary operation on G, but is worth keeping in mind for the examples below.

Remark 1.1.5 — It is not required that (cid:63) is commutative (a (cid:63) b = b (cid:63) a). So we say
that a group is abelian if the operation is commutative and non-abelian otherwise.

Example 1.1.6 (Non-Examples of groups)

 The pair (Q,·) is NOT a group. (Here Q is rational numbers.) While there is

an identity element, the element 0 ∈ Q does not have an inverse.

 The pair (Z,·) is also NOT a group. (Why?)
 Let Mat2×2(R) be the set of 2 × 2 real matrices. Then (Mat2×2(R),·) (where
· is matrix multiplication) is NOT a group. Indeed, even though we have an
identity matrix

(cid:20)1 0
0 1(cid:21)

we still run into the same issue as before: the zero matrix does not have a
multiplicative inverse.

(Even if we delete the zero matrix from the set, the resulting structure is still
not a group: those of you that know some linear algebra might recall that any
matrix with determinant zero cannot have an inverse.)

Let’s resume writing down examples. Here are some more abelian examples of

groups:

Example 1.1.7 (Complex unit circle)
Let S1 denote the set of complex numbers z with absolute value one; that is

S1 := {z ∈ C | |z| = 1} .

Then (S1,×) is a group because

 The complex number 1 ∈ S1 serves as the identity, and
 Each complex number z ∈ S1 has an inverse 1

z which is also in S1, since

(cid:12)(cid:12)z−1(cid:12)(cid:12) = |z|−1 = 1.

There is one thing I ought to also check: that z1 × z2 is actually still in S1. But this
follows from the fact that |z1z2| = |z1||z2| = 1.

Example 1.1.8 (Addition mod n)
Here is an example from number theory: Let n > 1 be an integer, and consider the
residues (remainders) modulo n. These form a group under addition. We call this
the cyclic group of order n, and denote it as Z/nZ, with elements 0, 1, . . . .

1 Groups

43

Example 1.1.9 (Multiplication mod p)
Let p be a prime. Consider the nonzero residues modulo p, which we denote by
(Z/pZ)×. Then ((Z/pZ)×,×) is a group.

Question 1.1.10. Why do we need the fact that p is prime?

(Digression: the notation Z/nZ and (Z/pZ)× may seem strange but will make sense when
we talk about rings and ideals. Set aside your worry for now.)

Here are some non-abelian examples:

Example 1.1.11 (General linear group)
Let n be a positive integer. Then GLn(R) is deﬁned as the set of n × n real matrices
which have nonzero determinant. It turns out that with this condition, every matrix
does indeed have an inverse, so (GLn(R),×) is a group, called the general linear
group.
(The fact that GLn(R) is closed under × follows from the linear algebra fact that

det(AB) = det A det B, proved in later chapters.)

Example 1.1.12 (Special linear group)
Following the example above, let SLn(R) denote the set of n × n matrices whose
determinant is actually 1. Again, for linear algebra reasons it turns out that
(SLn(R),×) is also a group, called the special linear group.

Example 1.1.13 (Symmetric groups)
Let Sn be the set of permutations of {1, . . . , n}. By viewing these permutations as
functions from {1, . . . , n} to itself, we can consider compositions of permutations.
Then the pair (Sn,◦) (here ◦ is function composition) is also a group, because

 There is an identity permutation, and

 Each permutation has an inverse.

The group Sn is called the symmetric group on n elements.

Example 1.1.14 (Dihedral group)
The dihedral group of order 2n, denoted D2n, is the group of symmetries of a
regular n-gon A1A2 . . . An, which includes rotations and reﬂections. It consists of
the 2n elements

(cid:8)1, r, r2, . . . , rn−1, s, sr, sr2, . . . , srn−1(cid:9) .

The element r corresponds to rotating the n-gon by 2π
n , while s corresponds to
reﬂecting it across the line OA1 (here O is the center of the polygon). So rs mean
“reﬂect then rotate” (like with function composition, we read from right to left).

In particular, rn = s2 = 1. You can also see that rks = sr−k.

44

Napkin, by Evan Chen (v1.5.20190718)

Here is a picture of some elements of D10.

Trivia: the dihedral group D12 is my favorite example of a non-abelian group, and is the
ﬁrst group I try for any exam question of the form “ﬁnd an example. . . ”.

More examples:

Example 1.1.15 (Products of groups)
Let (G, (cid:63)) and (H,∗) be groups. We can deﬁne a product group (G × H,·), as
follows. The elements of the group will be ordered pairs (g, h) ∈ G × H. Then

(g1, h1) · (g2, h2) = (g1 (cid:63) g2, h1 ∗ h2) ∈ G × H

is the group operation.

Question 1.1.16. What are the identity and inverses of the product group?

Example 1.1.17 (Trivial group)
The trivial group, often denoted 0 or 1, is the group with only an identity element.
I will use the notation {1}.

Exercise 1.1.18. Which of these are groups?

(a) Rational numbers with odd denominators (in simplest form), where the operation is

addition. (This includes integers, written as n/1, and 0 = 0/1).

(b) The set of rational numbers with denominator at most 2, where the operation is addition.

(c) The set of rational numbers with denominator at most 2, where the operation is

multiplication.

(d) The set of nonnegative integers, where the operation is addition.

§1.2 Properties of groups

Prototypical example for this section: (Z/pZ)× is possibly best.

Abuse of Notation 1.2.1. From now on, we’ll often refer to a group (G, (cid:63)) by just G.
Moreover, we’ll abbreviate a (cid:63) b to just ab. Also, because the operation (cid:63) is associative,
we will omit unnecessary parentheses: (ab)c = a(bc) = abc.
Abuse of Notation 1.2.2. From now on, for any g ∈ G and n ∈ N we abbreviate

Moreover, we let g−1 denote the inverse of g, and g−n = (g−1)n.

gn = g (cid:63) ··· (cid:63) g
(cid:125)
(cid:123)(cid:122)

n times

.

(cid:124)

12345151234r15432s54321sr21543rs1 Groups

45

If you did functional equations at the IMO, you might know that you can actually
determine a lot about a function just by knowing a few properties of it. For example, if
you know that f : Q → R satisﬁes f (x + y) = f (x) + f (y), then you actually can show
that f (x) = cx for some c ∈ R. (This is called Cauchy’s functional equation.)
In the same vein, we can try to deduce some properties that a group must have just
from Deﬁnition 1.1.3. (In fact, this really is a functional equation: (cid:63) is just a function
G × G → G.)

It is a law in Guam and 37 other states that I now state the following proposition.

Fact 1.2.3. Let G be a group.

(a) The identity of a group is unique.

(b) The inverse of any element is unique.

(c) For any g ∈ G, (g−1)−1 = g.
Proof. This is mostly just some formal “functional-equation” style manipulations, and
you needn’t feel bad skipping it on a ﬁrst read.

(a) If 1 and 1(cid:48) are identities, then 1 = 1 (cid:63) 1(cid:48) = 1(cid:48).

(b) If h and h(cid:48) are inverses to g, then 1G = g (cid:63) h =⇒ h(cid:48) = (h(cid:48) (cid:63) g) (cid:63) h = 1G (cid:63) h = h.
(c) Trivial; omitted.

Now we state a slightly more useful proposition.

Proposition 1.2.4 (Inverse of products)
Let G be a group, and a, b ∈ G. Then (ab)−1 = b−1a−1.

Proof. Direct computation. We have

(ab)(b−1a−1) = a(bb−1)a−1 = aa−1 = 1G.

Hence (ab)−1 = b−1a−1. Similarly, (b−1a−1)(ab) = 1G as well.

Finally, we state a very important lemma about groups, which highlights why having

an inverse is so valuable.

Lemma 1.2.5 (Left multiplication is a bijection)
Let G be a group, and pick a g ∈ G. Then the map G → G given by x (cid:55)→ gx is a
bijection.

Exercise 1.2.6. Check this by showing injectivity and surjectivity directly. (If you don’t
know what these words mean, consult Appendix E.)

46

Napkin, by Evan Chen (v1.5.20190718)

Example 1.2.7
Let G = (Z/7Z)× (as in Example 1.1.9) and pick g = 3. The above lemma states
that the map x (cid:55)→ 3 · x is a bijection, and we can see this explicitly:

1 ×3
(cid:55)−→ 3
2 ×3
(cid:55)−→ 6
3 ×3
(cid:55)−→ 2
4 ×3
(cid:55)−→ 5
5 ×3
(cid:55)−→ 1
6 ×3
(cid:55)−→ 4

(mod 7)

(mod 7)

(mod 7)

(mod 7)

(mod 7)

(mod 7).

The fact that the map is injective is often called the cancellation law. (Why do you

think so?)

§1.3 Isomorphisms

Prototypical example for this section: Z ∼= 10Z.

First, let me talk about what it means for groups to be isomorphic. Consider the two

groups

 Z = ({. . . ,−2,−1, 0, 1, 2, . . .} , +).
 10Z = ({. . . ,−20,−10, 0, 10, 20, . . .} , +).

These groups are “diﬀerent”, but only superﬁcially so – you might even say they only
diﬀer in the names of the elements. Think about what this might mean formally for a
moment.

Speciﬁcally the map

is a bijection of the underlying sets which respects the group action. In symbols,

φ : Z → 10Z by x (cid:55)→ 10x

φ(x + y) = φ(x) + φ(y).

In other words, φ is a way of re-assigning names of the elements without changing the
structure of the group. That’s all just formalism for capturing the obvious fact that
(Z, +) and (10Z, +) are the same thing.
Now, let’s do the general deﬁnition.

Deﬁnition 1.3.1. Let G = (G, (cid:63)) and H = (H,∗) be groups. A bijection φ : G → H is
called an isomorphism if

φ(g1 (cid:63) g2) = φ(g1) ∗ φ(g2)

for all g1, g2 ∈ G.

If there exists an isomorphism from G to H, then we say G and H are isomorphic and
write G ∼= H.

Note that in this deﬁnition, the left-hand side φ(g1 (cid:63) g2) uses the operation of G while

the right-hand side φ(g1) ∗ φ(g2) uses the operation of H.

1 Groups

47

Example 1.3.2 (Examples of isomorphisms)
Let G and H be groups. We have the following isomorphisms.
(a) Z ∼= 10Z, as above.
(b) There is an isomorphism

G × H ∼= H × G

by the map (g, h) (cid:55)→ (h, g).

(c) The identity map id : G → G is an isomorphism, hence G ∼= G.
(d) There is another isomorphism of Z to itself: send every x to −x.

Example 1.3.3 (Primitive roots modulo 7)
As a nontrivial example, we claim that Z/6Z ∼= (Z/7Z)×. The bijection is

φ(a mod 6) = 3a mod 7.

To check that this is an isomorphism, we need to verify several things.

 First, we need to check this map actually makes sense: why is it the case that
if a ≡ b (mod 6), then 3a ≡ 3b (mod 7)? The reason is that Fermat’s little
theorem guarantees that 36 ≡ 1 (mod 7).

 Next, we need to check that this map is a bijection. You can do this explicitly:

(31, 32, 33, 34, 35, 36) ≡ (3, 2, 6, 4, 5, 1)

(mod 7).

 Finally, we need to verify that this map respects the group action. In other
words, we want to see that φ(a + b) = φ(a)φ(b) since the operation of Z/6Z is
addition while the operation of (Z/7Z)× is multiplication. That’s just saying
that 3a+b ≡ 3a3b (mod 7), which is true.

Example 1.3.4 (Primitive roots)
More generally, for any prime p, there exists an element g ∈ (Z/pZ)× called a
primitive root modulo p such that 1, g, g2, . . . , gp−2 are all diﬀerent modulo p. One
can show by copying the above proof that

Z/(p − 1)Z ∼= (Z/pZ)× for all primes p.

The example above was the special case p = 7 and g = 3.

Exercise 1.3.5. Assuming the existence of primitive roots, establish the isomorphism

Z/(p − 1)Z ∼= (Z/pZ)× as above.
It’s not hard to see that ∼= is an equivalence relation (why?). Moreover, because we
really only care about the structure of groups, we’ll usually consider two groups to be
the same when they are isomorphic. So phrases such as “ﬁnd all groups” really mean
“ﬁnd all groups up to isomorphism”.

48

Napkin, by Evan Chen (v1.5.20190718)

§1.4 Orders of groups, and Lagrange’s theorem

Prototypical example for this section: (Z/pZ)×.

As is typical in math, we use the word “order” for way too many things. In groups,

there are two notions of order.

Deﬁnition 1.4.1. The order of a group G is the number of elements of G. We denote
this by |G|. Note that the order may not be ﬁnite, as in Z. We say G is a ﬁnite group
just to mean that |G| is ﬁnite.

Example 1.4.2 (Orders of groups)
For a prime p, |(Z/pZ)×| = p − 1. In other words, the order of (Z/pZ)× is p − 1. As
another example, the order of the symmetric group Sn is |Sn| = n! and the order of
the dihedral group D2n is 2n.

Deﬁnition 1.4.3. The order of an element g ∈ G is the smallest positive integer n
such that gn = 1G, or ∞ if no such n exists. We denote this by ord g.

Example 1.4.4 (Examples of orders)
The order of −1 in Q× is 2, while the order of 1 in Z is inﬁnite.

Question 1.4.5. Find the order of each of the six elements of Z/6Z, the cyclic group on
six elements. (See Example 1.1.8 if you’ve forgotten what Z/6Z means.)

Example 1.4.6 (Primitive roots)
If you know olympiad number theory, this coincides with the deﬁnition of an order
of a residue mod p. That’s why we use the term “order” there as well. In particular,
a primitive root is precisely an element g ∈ (Z/pZ)× such that ord g = p − 1.

You might also know that if xn ≡ 1 (mod p), then the order of x (mod p) must divide n.
The same is true in a general group for exactly the same reason.

Fact 1.4.7. If gn = 1G then ord g divides n.

Also, you can show that any element of a ﬁnite group has a ﬁnite order. The proof is
just an olympiad-style pigeonhole argument. Consider the inﬁnite sequence 1G, g, g2, . . . ,
and ﬁnd two elements that are the same.
Fact 1.4.8. Let G be a ﬁnite group. For any g ∈ G, ord g is ﬁnite.

What’s the last property of (Z/pZ)× that you know from olympiad math? We have
Fermat’s little theorem: for any a ∈ (Z/pZ)×, we have ap−1 ≡ 1 (mod p). This is no
coincidence: exactly the same thing is true in a more general setting.

Theorem 1.4.9 (Lagrange’s theorem for orders)
Let G be any ﬁnite group. Then x|G| = 1G for any x ∈ G.

Keep this result in mind! We’ll prove it later in the generality of Theorem 3.4.1.

1 Groups

§1.5 Subgroups

49

Prototypical example for this section: SLn(R) is a subgroup of GLn(R).

Earlier we saw that GLn(R), the n × n matrices with nonzero determinant, formed a
group under matrix multiplication. But we also saw that a subset of GLn(R), namely
SLn(R), also formed a group with the same operation. For that reason we say that
SLn(R) is a subgroup of GLn(R). And this deﬁnition generalizes in exactly the way you
expect.

Deﬁnition 1.5.1. Let G = (G, (cid:63)) be a group. A subgroup of G is exactly what you
would expect it to be: a group H = (H, (cid:63)) where H is a subset of G. It’s a proper
subgroup if H (cid:54)= G.

Remark 1.5.2 — To specify a group G, I needed to tell you both what the set G
was and the operation (cid:63) was. But to specify a subgroup H of a given group G, I
only need to tell you who its elements are: the operation of H is just inherited from
the operation of G.

Example 1.5.3 (Examples of subgroups)
(a) 2Z is a subgroup of Z, which is isomorphic to Z itself!

(b) Consider again Sn, the symmetric group on n elements. Let T be the set of
permutations τ : {1, . . . , n} → {1, . . . , n} for which τ (n) = n. Then T is a
subgroup of Sn; in fact, it is isomorphic to Sn−1.

(c) Consider the group G× H (Example 1.1.15) and the elements {(g, 1H ) | g ∈ G}.
This is a subgroup of G × H (why?). In fact, it is isomorphic to G by the
isomorphism (g, 1H ) (cid:55)→ g.

Example 1.5.4 (Stupid examples of subgroups)
For any group G, the trivial group {1G} and the entire group G are subgroups of G.

Next is an especially important example that we’ll talk about more in later chap-

ters.

Example 1.5.5 (Subgroup generated by an element)
Let x be an element of a group G. Consider the set

This is also a subgroup of G, called the subgroup generated by x.

(cid:104)x(cid:105) =(cid:8). . . , x−2, x−1, 1, x, x2, . . .(cid:9) .

Exercise 1.5.6. If ord x = 2015, what is the above subgroup equal to? What if ord x = ∞?

Finally, we present some non-examples of subgroups.

50

Napkin, by Evan Chen (v1.5.20190718)

Example 1.5.7 (Non-examples of subgroups)
Consider the group Z = (Z, +).
(a) The set {0, 1, 2, . . .} is not a subgroup of Z because it does not contain inverses.
(b) The set {n3 | n ∈ Z} = {. . . ,−8,−1, 0, 1, 8, . . .} is not a subgroup because it is

not closed under addition; the sum of two cubes is not in general a cube.

(c) The empty set ∅ is not a subgroup of Z because it lacks an identity element.

§1.6 Groups of small orders

Just for fun, here is a list of all groups of order less than or equal to ten (up to isomorphism,
of course).

1. The only group of order 1 is the trivial group.

2. The only group of order 2 is Z/2Z.

3. The only group of order 3 is Z/3Z.

4. The only groups of order 4 are

 Z/4Z, the cyclic group on four elements,
 Z/2Z × Z/2Z, called the Klein Four Group.

5. The only group of order 5 is Z/5Z.

6. The groups of order six are

 Z/6Z, the cyclic group on six elements.
 S3, the permutation group of three elements. This is the ﬁrst non-abelian

group.

Some of you might wonder where Z/2Z × Z/3Z is. All I have to say is: Chinese
remainder theorem!

You might wonder where D6 is in this list. It’s actually isomorphic to S3.

7. The only group of order 7 is Z/7Z.

8. The groups of order eight are more numerous.
 Z/8Z, the cyclic group on eight elements.
 Z/4Z × Z/2Z.
 Z/2Z × Z/2Z × Z/2Z.
 D8, the dihedral group with eight elements, which is not abelian.
 A non-abelian group Q8, called the quaternion group. It consists of eight

elements ±1, ±i, ±j, ±k with i2 = j2 = k2 = ijk = −1.

9. The groups of order nine are

 Z/9Z, the cyclic group on nine elements.
 Z/3Z × Z/3Z.

1 Groups

51

10. The groups of order 10 are

 Z/10Z ∼= Z/5Z × Z/2Z (again Chinese remainder theorem).
 D10, the dihedral group with 10 elements. This group is non-abelian.

§1.7 Unimportant long digression

A common question is: why these axioms? For example, why associative but not
commutative? This answer will likely not make sense until later, but here are some
comments that may help.

One general heuristic is: Whenever you deﬁne a new type of general object, there’s
always a balancing act going on. On the one hand, you want to include enough constraints
that your objects are “nice”. On the other hand, if you include too many constraints,
then your deﬁnition applies to too few objects.

So, for example, we include “associative” because that makes our lives easier and most
operations we run into are associative. In particular, associativity is required for the
inverse of an element to necessarily be unique. However we don’t include “commutative”,
because examples below show that there are lots of non-abelian groups we care about.
(But we introduce another name “abelian” because we still want to keep track of it.)

Another comment: a good motivation for the inverse axioms is that you get a large
amount of symmetry. The set of positive integers with addition is not a group, for
example, because you can’t subtract 6 from 3: some elements are “larger” than others.
By requiring an inverse element to exist, you get rid of this issue. (You also need identity
for this; it’s hard to deﬁne inverses without it.)

Even more abstruse comment: Problem 1F† shows that groups are actually shadows
of the so-called symmetric groups (deﬁned later, also called permutation groups). This
makes rigorous the notion that “groups are very symmetric”.

§1.8 A few harder problems to think about

Problem 1A. What is the joke in the following ﬁgure? (Source: [Ge].)

52

Napkin, by Evan Chen (v1.5.20190718)

Problem 1B. Prove Lagrange’s theorem for orders in the special case that G is a ﬁnite
abelian group.

Problem 1C. Show that D6 ∼= S3 but D24 (cid:54)∼= S4.
Problem 1D(cid:63). Let p be a prime. Show that the only group of order p is Z/pZ.

Problem 1E (A hint for Cayley’s theorem). Find a subgroup of H of S8 which is
isomorphic to D8, and write the isomorphism explicitly.

Problem 1F†. Let G be a ﬁnite group.1 Show that there exists a positive integer n
such that

(a) (Cayley’s theorem) G is isomorphic to some subgroup of the symmetric group Sn.

(b) (Representation Theory) G is isomorphic to some subgroup of the general linear

group GLn(R). (This is the group of invertible n × n matrices.)

Problem 1G (IMO SL 2005 C5). There are n markers, each with one side white and
the other side black. In the beginning, these n markers are aligned in a row so that their
white sides are all up. In each step, if possible, we choose a marker whose white side is
up (but not one of the outermost markers), remove it, and reverse the closest marker to
the left of it and also reverse the closest marker to the right of it.

Prove that if n ≡ 1 (mod 3) it’s impossible to reach a state with only two markers

remaining. (In fact the converse is true as well.)

Problem 1H. Let p be a prime and F1 = F2 = 1, Fn+2 = Fn+1 + Fn be the Fibonacci
sequence. Show that F2p(p2−1) is divisible by p.

1In other words, permutation groups can be arbitrarily weird. I remember being highly unsettled by

this theorem when I ﬁrst heard of it, but in hindsight it is not so surprising.

2 Metric spaces

At the time of writing, I’m convinced that metric topology is the morally correct way
to motivate point-set topology as well as to generalize normal calculus.1 So here is my
best attempt.

The concept of a metric space is very “concrete”, and lends itself easily to visualization.
Hence throughout this chapter you should draw lots of pictures as you learn about new
objects, like convergent sequences, open sets, closed sets, and so on.

§2.1 Deﬁnition and examples of metric spaces

Prototypical example for this section: R2, with the Euclidean metric.

Deﬁnition 2.1.1. A metric space is a pair (M, d) consisting of a set of points M and
a metric d : M × M → R≥0. The distance function must obey:

 For any x, y ∈ M , we have d(x, y) = d(y, x); i.e. d is symmetric.
 The function d must be positive deﬁnite which means that d(x, y) ≥ 0 with

equality if and only if x = y.

 The function d should satisfy the triangle inequality: for all x, y, z ∈ M ,

d(x, z) + d(z, y) ≥ d(x, y).

Abuse of Notation 2.1.2. Just like with groups, we will abbreviate (M, d) as just M .

Example 2.1.3 (Metric spaces of R)
(a) The real line R is a metric space under the metric d(x, y) = |x − y|.
(b) The interval [0, 1] is also a metric space with the same distance function.

(c) In fact, any subset S of R can be made into a metric space in this way.

Example 2.1.4 (Metric spaces of R2)
(a) We can make R2 into a metric space by imposing the Euclidean distance function

d ((x1, y1), (x2, y2)) =(cid:112)(x1 − x2)2 + (y1 − y2)2.

(b) Just like with the ﬁrst example, any subset of R2 also becomes a metric space
after we inherit it. The unit disk, unit circle, and the unit square [0, 1]2 are
special cases.

1Also, “metric” is a fun word to say.

53

54

Napkin, by Evan Chen (v1.5.20190718)

Example 2.1.5 (Taxicab on R2)
It is also possible to place the taxicab distance on R2:

d ((x1, y1), (x2, y2)) = |x1 − x2| + |y1 − y2| .

For now, we will use the more natural Euclidean metric.

Example 2.1.6 (Metric spaces of Rn)
We can generalize the above examples easily. Let n be a positive integer.

(a) We let Rn be the metric space whose points are points in n-dimensional Euclidean

space, and whose metric is the Euclidean metric

d ((a1, . . . , an) , (b1, . . . , bn)) =(cid:112)(a1 − b1)2 + ··· + (an − bn)2.

This is the n-dimensional Euclidean space.

(b) The open unit ball Bn is the subset of Rn consisting of those points (x1, . . . , xn)

such that x2

1 + ··· + x2

n < 1.

1 + ··· + x2

(c) The unit sphere Sn−1 is the subset of Rn consisting of those points (x1, . . . , xn)
such that x2
n = 1, with the inherited metric. (The superscript n − 1
indicates that Sn−1 is an n − 1 dimensional space, even though it lives in n-
dimensional space.) For example, S1 ⊆ R2 is the unit circle, whose distance
between two points is the length of the chord joining them. You can also think
of it as the “boundary” of the unit ball Bn.

Example 2.1.7 (Function space)
We can let M be the space of integrable functions f : [0, 1] → R and deﬁne the
metric by d(f, g) =(cid:82) 1

0 |f − g| dx.

Here is a slightly more pathological example.

Example 2.1.8 (Discrete space)
Let S be any set of points (either ﬁnite or inﬁnite). We can make S into a discrete
space by declaring

d(x, y) =(cid:40)1

0

if x (cid:54)= y
if x = y.

If |S| = 4 you might think of this space as the vertices of a regular tetrahedron,
living in R3. But for larger S it’s not so easy to visualize. . .

Example 2.1.9 (Graphs are metric spaces)
Any connected simple graph G can be made into a metric space by deﬁning the
distance between two vertices to be the graph-theoretic distance between them. (The
discrete metric is the special case when G is the complete graph on S.)

2 Metric spaces

55

Question 2.1.10. Check the conditions of a metric space for the metrics on the discrete
space and for the connected graph.

Abuse of Notation 2.1.11. From now on, we will refer to Rn with the Euclidean
metric by just Rn. Moreover, if we wish to take the metric space for a subset S ⊆ Rn
with the inherited metric, we will just write S.

§2.2 Convergence in metric spaces

Prototypical example for this section: The sequence 1

n (for n = 1, 2, . . . ) in R.

Since we can talk about the distance between two points, we can talk about what it
means for a sequence of points to converge. This is the same as the typical epsilon-delta
deﬁnition, with absolute values replaced by the distance function.

Deﬁnition 2.2.1. Let (xn)n≥1 be a sequence of points in a metric space M . We say
that xn converges to x if the following condition holds: for all ε > 0, there is an integer
N (depending on ε) such that d(xn, x) < ε for each n ≥ N . This is written

or more verbosely as

xn → x

xn = x.

lim
n→∞

We say that a sequence converges in M if it converges to a point in M .

You should check that this deﬁnition coincides with your intuitive notion of “converges”.

Abuse of Notation 2.2.2. If the parent space M is understood, we will allow ourselves
to abbreviate “converges in M ” to just “converges”. However, keep in mind that
convergence is deﬁned relative to the parent space; the “limit” of the space must actually
be a point in M for a sequence to converge.

Example 2.2.3

Consider the sequence x1 = 1, x2 = 1.4, x3 = 1.41, x4 = 1.414, . . . .
(a) If we view this as a sequence in R, it converges to √2.
(b) However, even though each xi is in Q, this sequence does NOT converge when

we view it as a sequence in Q!

Question 2.2.4. What are the convergent sequences in a discrete metric space?

x1x2x3x4x5x6x7x8x9x56

Napkin, by Evan Chen (v1.5.20190718)

§2.3 Continuous maps

In calculus you were also told (or have at least heard) of what it means for a function to
be continuous. Probably something like

A function f : R → R is continuous at a point p ∈ R if for every ε > 0 there
exists a δ > 0 such that |x − p| < δ =⇒ |f (x) − f (p)| < ε.

Question 2.3.1. Can you guess what the corresponding deﬁnition for metric spaces is?

All we have do is replace the absolute values with the more general distance functions:

this gives us a deﬁnition of continuity for any function M → N .
Deﬁnition 2.3.2. Let M = (M, dM ) and N = (N, dN ) be metric spaces. A function
f : M → N is continuous at a point p ∈ M if for every ε > 0 there exists a δ > 0 such
that

dM (x, p) < δ =⇒ dN (f (x), f (p)) < ε.

Moreover, the entire function f is continuous if it is continuous at every point p ∈ M .
Notice that, just like in our deﬁnition of an isomorphism of a group, we use the metric

of M for one condition and the metric of N for the other condition.

This generalization is nice because it tells us immediately how we could carry over
continuity arguments in R to more general spaces like C. Nonetheless, this deﬁnition is
kind of cumbersome to work with, because it makes extensive use of the real numbers
(epsilons and deltas). Here is an equivalent condition.

Theorem 2.3.3 (Sequential continuity)
A function f : M → N of metric spaces is continuous at a point p ∈ M if and only if
the following property holds: if x1, x2, . . . is a sequence in M converging to p, then
the sequence f (x1), f (x2), . . . in N converges to f (p).

Proof. One direction is not too hard:

Exercise 2.3.4. Show that ε-δ continuity implies sequential continuity at each point.

Conversely, we will prove if f is not ε-δ continuous at p then it does not preserve
convergence.

If f is not continuous at p, then there is a “bad” ε > 0, which we now consider ﬁxed.
So for each choice of δ = 1/n, there should be some point xn which is within δ of p, but
which is mapped more than ε away from f (p). But then the sequence xn converges to p,
and f (xn) is always at least ε away from f (p), contradiction.

Example application showcasing the niceness of sequential continuity:

Proposition 2.3.5 (Composition of continuous functions is continuous)
Let f : M → N and g : N → L be continuous maps of metric spaces. Then their
composition g ◦ f is continuous.

Proof. Dead simple with sequences: Let p ∈ M be arbitrary and let xn → p in M . Then
f (xn) → f (p) in N and g(f (xn)) → g(f (p)) in L, QED.

2 Metric spaces

57

Question 2.3.6. Let M be any metric space and D a discrete space. When is a map
f : D → M continuous?

§2.4 Homeomorphisms

Prototypical example for this section: The unit circle S1 is homeomorphic to the boundary
of the unit square.

When do we consider two groups to be the same? Answer:

if there’s a structure-
preserving map between them which is also a bijection. For metric spaces, we do exactly
the same thing, but replace “structure-preserving” with “continuous”.

Deﬁnition 2.4.1. Let M and N be metric spaces. A function f : M → N is a home-
omorphism if it is a bijection, and both f : M → N and its inverse f−1 : N → M are
continuous. We say M and N are homeomorphic.

Needless to say, homeomorphism is an equivalence relation.
You might be surprised that we require f−1 to also be continuous. Here’s the reason:
you can show that if φ is an isomorphism of groups, then φ−1 also preserves the group
operation, hence φ−1 is itself an isomorphism. The same is not true for continuous
bijections, which is why we need the new condition.

Example 2.4.2 (Homeomorphism (cid:54)= continuous bijection)
(a) There is a continuous bijection from [0, 1) to the circle, but it has no continuous

inverse.

(b) Let M be a discrete space with size |R|. Then there is a continuous function

M → R which certainly has no continuous inverse.

Note that this is the topologist’s deﬁnition of “same” – homeomorphisms are “continu-

ous deformations”. Here are some examples.

Example 2.4.3 (Examples of homeomorphisms)

(a) Any space M is homeomorphic to itself through the identity map.

(b) The old saying: a doughnut (torus) is homeomorphic to a coﬀee cup. (Look this

up if you haven’t heard of it.)

(c) The unit circle S1 is homeomorphic to the boundary of the unit square. Here’s

one bijection between them, after an appropriate scaling:

58

Napkin, by Evan Chen (v1.5.20190718)

Example 2.4.4 (Metrics on the unit circle)
It may have seemed strange that our metric function on S1 was the one inherited
from R2, meaning the distance between two points on the circle was deﬁned to be
the length of the chord. Wouldn’t it have made more sense to use the circumference
of the smaller arc joining the two points?

In fact, it doesn’t matter: if we consider S1 with the “chord” metric and the “arc”
metric, we get two homeomorphic spaces, as the map between them is continuous.

The same goes for Sn−1 for general n.

Example 2.4.5 (Homeomorphisms really don’t preserve size)
Surprisingly, the open interval (−1, 1) is homeomorphic to the real line R! One
bijection is given by

with the inverse being given by t (cid:55)→ 2
former is “bounded” while the latter is “unbounded”.

This might come as a surprise, since (−1, 1) doesn’t look that much like R; the

x (cid:55)→ tan(π/2x)
π arctan(t).

§2.5 Extended example/deﬁnition: product metric

Prototypical example for this section: R × R is R2.

Here is an extended example which will occur later on. Let M = (M, dM ) and
N = (N, dN ) be metric spaces (say, M = N = R). Our goal is to deﬁne a metric space
on M × N .
points M × N :

Let pi = (xi, yi) ∈ M × N for i = 1, 2. Consider the following metrics on the set of

dmax(p1, p2) := max{dM (x1, x2), dN (y1, y2)}

dEuclid(p1, p2) :=(cid:112)dM (x1, x2)2 + dN (y1, y2)2

dtaxicab (p1, p2) := dM (x1, x2) + dN (y1, y2).

All of these are good candidates. We are about to see it doesn’t matter which one we
use:

Exercise 2.5.1. Verify that

dmax(p1, p2) ≤ dEuclid(p1, p2) ≤ dtaxicab(p1, p2) ≤ 2dmax(p1, p2).

Use this to show that the metric spaces we obtain by imposing any of the three metrics are
homeomorphic, with the homeomorphism being just the identity map.

Deﬁnition 2.5.2. Hence we will usually simply refer to the metric on M × N , called
the product metric. It will not be important which of the three metrics we select.

Example 2.5.3 (R2)
If M = N = R, we get R2, the Euclidean plane. The metric dEuclid is the one we
started with, but using either of the other two metric works ﬁne as well.

2 Metric spaces

59

The product metric plays well with convergence of sequences.

Proposition 2.5.4 (Convergence in the product metric is by component)
We have (xn, yn) → (x, y) if and only if xn → x and yn → y.

Proof. We have dmax ((x, y), (xn, yn)) = max{dM (x, xn), dN (y, yn)} and the latter ap-
proaches zero as n → ∞ if and only if dM (x, xn) → 0 and dN (y, yn) → 0.

That’s see an application of this:

Proposition 2.5.5 (Addition and multiplication are continuous)
The addition and multiplication maps are continuous maps R × R → R.

Proof. The fact that + : R × R → R is continuous is the easier one, so we prove only
multiplication for concreteness (as the details of the proof will not matter). For any n
we have

xnyn = (x + (xn − x)) (y + (yn − y))

= xy + y(xn − x) + x(yn − y) + (xn − x)(yn − y) =⇒ |xnyn − xy|
≤ |y||xn − x| + |x||yn − y| + |xn − x||yn − y| .

As n → ∞, all three terms on the right-hand side tend to zero.
Problem 2C covers the other two operations, subtraction and division. The upshot of this
is that, since compositions are also continuous, most of your naturally arising real-valued
functions will automatically be continuous as well. For example, the function 3x
x2+1 will
be a continuous function from R → R, since it can be obtained by composing +, ×, ÷.

§2.6 Open sets

Prototypical example for this section: The open disk x2 + y2 < r2 in R2.

Continuity is really about what happens “locally”: how a function behaves “close to a
certain point p”. One way to capture this notion of “closeness” is to use metrics as we’ve
done above. In this way we can deﬁne an r-neighborhood of a point.

Deﬁnition 2.6.1. Let M be a metric space. For each real number r > 0 and point
p ∈ M , we deﬁne

The set Mr(p) is called an r-neighborhood of p.

Mr(p) := {x ∈ M : d(x, p) < r} .

We can rephrase convergence more succinctly in terms of r-neighborhoods. Speciﬁcally,
a sequence (xn) converges to x if for every r-neighborhood of x, all terms of xn eventually
stay within that r-neighborhood.

Let’s try to do the same with functions.

MpMr(p)r60

Napkin, by Evan Chen (v1.5.20190718)

Question 2.6.2. In terms of r-neighborhoods, what does it mean for a function f : M → N
to be continuous at a point p ∈ M ?
Essentially, we require that the pre-image of every ε-neighborhood has the property

that some δ-neighborhood exists inside it. This motivates:
Deﬁnition 2.6.3. A set U ⊆ M is open in M if for each p ∈ U , some r-neighborhood
of p is contained inside U . In other words, there exists r > 0 such that Mr(p) ⊆ U .
Abuse of Notation 2.6.4. Note that a set being open is deﬁned relative to the parent
space M . However, if M is understood we can abbreviate “open in M ” to just “open”.

Figure 2.1: The set of points x2 + y2 < 1 in R2 is open in R2.

Example 2.6.5 (Examples of open sets)

(a) Any r-neighborhood of a point is open.

(b) Open intervals of R are open in R, hence the name! This is the prototypical

example to keep in mind.

(c) The open unit ball Bn is open in Rn for the same reason.

(d) In particular, the open interval (0, 1) is open in R. However, if we embed it in

R2, it is no longer open!

(e) The empty set ∅ and the whole set of points M are open in M .

Example 2.6.6 (Non-examples of open sets)
(a) The closed interval [0, 1] is not open in R. There is no ε-neighborhood of the

point 0 which is contained in [0, 1].

(b) The unit circle S1 is not open in R2.

Question 2.6.7. What are the open sets of the discrete space?

Here are two quite important properties of open sets.

Proposition 2.6.8 (Intersections and unions of open sets)

(a) The intersection of ﬁnitely many open sets is open.

(b) The union of open sets is open, even if there are inﬁnitely many.

px2+y2<12 Metric spaces

61

Question 2.6.9. Convince yourself this is true.

Exercise 2.6.10. Exhibit an inﬁnite collection of open sets in R whose intersection is the
set {0}. This implies that inﬁnite intersections of open sets are not necessarily open.

The whole upshot of this is:

Theorem 2.6.11 (Open set condition)
A function f : M → N of metric spaces is continuous if and only if the pre-image of
every open set in N is open in M .

Proof. I’ll just do one direction. . .

Exercise 2.6.12. Show that δ-ε continuity follows from the open set continuity.

Now assume f is continuous. First, suppose V is an open subset of the metric space N ;
let U = f pre(V ). Pick x ∈ U , so y = f (x) ∈ V ; we want an open neighborhood of x
inside U .

As V is open, there is some small ε-neighborhood around y which is contained inside V .
By continuity of f , we can ﬁnd a δ such that the δ-neighborhood of x gets mapped by f
into the ε-neighborhood in N , which in particular lives inside V . Thus the δ-neighborhood
lives in U , as desired.

§2.7 Closed sets

Prototypical example for this section: The closed unit disk x2 + y2 ≤ r2 in R2.

It would be criminal for me to talk about open sets without talking about closed sets.

The name “closed” comes from the deﬁnition in a metric space.
Deﬁnition 2.7.1. Let M be a metric space. A subset S ⊆ M is closed in M if the
following property holds: let x1, x2, . . . be a sequence of points in S and suppose that xn
converges to x in M . Then x ∈ S as well.
Abuse of Notation 2.7.2. Same caveat: we abbreviate “closed in M ” to just “closed”
if the parent space M is understood.

Here’s another way to phrase it. The limit points of a subset S ⊆ M are deﬁned by

lim S := {p ∈ M : ∃(xn) ∈ S such that xn → p} .

Thus S is closed if and only if S = lim S.

NyεVfMxδU=fpre(V)62

Napkin, by Evan Chen (v1.5.20190718)

Exercise 2.7.3. Prove that lim S is closed even if S isn’t closed. (Draw a picture.)

For this reason, lim S is also called the closure of S in M , and denoted S. It is simply
the smallest closed set which contains S.

Example 2.7.4 (Examples of closed sets)
(a) The empty set ∅ is closed in M for vacuous reasons: there are no sequences of

points with elements in ∅.

(b) The entire space M is closed in M for tautological reasons. (Verify this!)

(c) The closed interval [0, 1] in R is closed in R, hence the name. Like with open

sets, this is the prototypical example of a closed set to keep in mind!

(d) In fact, the closed interval [0, 1] is even closed in R2.

Example 2.7.5 (Non-examples of closed sets)
Let S = (0, 1) denote the open interval. Then S is not closed in R because the
sequence of points

1
8

, . . .

1
2

,

1
4

,

converges to 0 ∈ R, but 0 /∈ (0, 1).

I should now warn you about a confusing part of this terminology. Firstly, “most”

sets are neither open nor closed.

Example 2.7.6 (A set neither open nor closed)
The half-open interval [0, 1) is neither open nor closed in R.

Secondly, it’s also possible for a set to be both open and closed; this will be
discussed in Chapter 7.

The reason for the opposing terms is the following theorem:

Theorem 2.7.7 (Closed sets are complements of open sets)
Let M be a metric space, and S ⊆ M any subset. Then the following are equivalent:

 The set S is closed in M .

 The complement M \ S is open in M .

Exercise 2.7.8 (Great). Prove this theorem! You’ll want to draw a picture to make it clear
what’s happening: for example, you might take M = R2 and S to be the closed unit disk.

§2.8 A few harder problems to think about

Problem 2A. Let M = (M, d) be a metric space. Show that

d : M × M → R

2 Metric spaces

63

is itself a continuous function (where M × M is equipped with the product metric).
Problem 2B. Are Q and N homeomorphic subspaces of R?

Problem 2C (Continuity of arithmetic continued). Show that subtraction is a continuous
map − : R × R → R, and division is a continuous map ÷ : R × R>0 → R.
Problem 2D. Exhibit a function f : R → R such that f is continuous at x ∈ R if and
only if x = 0.
Problem 2E. Prove that a function f : R → R which is strictly increasing must be
continuous at some point.

II

Basic Abstract Algebra

Part II: Contents

3 Homomorphisms and quotient groups

. . . . . . . . . . . . . . . . . . . . . . . . .
3.1 Generators and group presentations
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2 Homomorphisms
3.3 Cosets and modding out . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
(Optional) Proof of Lagrange’s theorem . . . . . . . . . . . . . . . . . . . . . . .
3.4
3.5 Eliminating the homomorphism . . . . . . . . . . . . . . . . . . . . . . . . . . .
(Digression) The ﬁrst isomorphism theorem . . . . . . . . . . . . . . . . . . . . . .
3.6
. . . . . . . . . . . . . . . . . . . . . . . .
3.7 A few harder problems to think about

4 Rings and ideals

4.1 Some motivational metaphors about rings vs groups . . . . . . . . . . . . . . . . . .
(Optional) Pedagogical notes on motivation . . . . . . . . . . . . . . . . . . . . . .
4.2
4.3 Deﬁnition and examples of rings . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.4 Homomorphisms
Ideals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.5
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.6 Generating ideals
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.7 Principal ideal domains
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.8 Noetherian rings
. . . . . . . . . . . . . . . . . . . . . . . .
4.9 A few harder problems to think about

5 Flavors of rings

Integral domains

5.1 Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.2
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.3 Prime ideals
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.4 Maximal ideals
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.5 Field of fractions
. . . . . . . . . . . . . . . . . . . . . . . .
5.6 Unique factorization domains (UFD’s)
. . . . . . . . . . . . . . . . . . . . . . . .
5.7 A few harder problems to think about

67
67
68
70
73
73
75
76

79
79
79
79
82
83
84
86
87
88

91
91
92
93
94
95
96
97

3 Homomorphisms and quotient groups

§3.1 Generators and group presentations

Prototypical example for this section: D2n =(cid:10)r, s | rn = s2 = 1(cid:11)

Let G be a group. Recall that for some element x ∈ G, we could consider the subgroup

(cid:8). . . , x−2, x−1, 1, x, x2, . . .(cid:9)

of G. Here’s a more pictorial version of what we did: put x in a box, seal it tightly,
and shake vigorously. Using just the element x, we get a pretty explosion that
produces the subgroup above.

What happens if we put two elements x, y in the box? Among the elements that get

produced are things like

xyxyx,

x2y9x−5y3,

y−2015,

. . .

Essentially, I can create any ﬁnite product of x, y, x−1, y−1. This leads us to deﬁne:
Deﬁnition 3.1.1. Let S be a subset of G. The subgroup generated by S, denoted (cid:104)S(cid:105),
is the set of elements which can be written as a ﬁnite product of elements in S (and their
inverses). If (cid:104)S(cid:105) = G then we say S is a set of generators for G, as the elements of S
together create all of G.

Exercise 3.1.2. Why is the condition “and their inverses” not necessary if G is a ﬁnite
group? (As usual, assume Lagrange’s theorem.)

Example 3.1.3 (Z is the inﬁnite cyclic group)
Consider 1 as an element of Z = (Z, +). We see (cid:104)1(cid:105) = Z, meaning {1} generates
Z. It’s important that −1, the inverse of 1 is also allowed: we need it to write all
integers as the sum of 1 and −1.

This gives us an idea for a way to try and express groups compactly. Why not just

write down a list of generators for the groups? For example, we could write

Z ∼= (cid:104)a(cid:105)

meaning that Z is just the group generated by one element.

There’s one issue: the generators usually satisfy certain properties. For example,
consider Z/100Z. It’s also generated by a single element x, but this x has the additional
property that x100 = 1. This motivates us to write

Z/100Z =(cid:10)x | x100 = 1(cid:11) .

I’m sure you can see where this is going. All we have to do is specify a set of generators
and relations between the generators, and say that two elements are equal if and only if
you can get from one to the other using relations. Such an expression is appropriately
called a group presentation.

67

68

Napkin, by Evan Chen (v1.5.20190718)

Example 3.1.4 (Dihedral group)
The dihedral group of order 2n has a presentation

D2n =(cid:10)r, s | rn = s2 = 1, rs = sr−1(cid:11) .

Thus each element of D2n can be written uniquely in the form rα or srα, where
α = 0, 1, . . . , n − 1.

Example 3.1.5 (Klein four group)
The Klein four group, isomorphic to Z/2Z × Z/2Z, is given by the presentation

(cid:10)a, b | a2 = b2 = 1, ab = ba(cid:11) .

Example 3.1.6 (Free group)
The free group on n elements is the group whose presentation has n generators
and no relations at all. It is denoted Fn, so

Fn = (cid:104)x1, x2, . . . , xn(cid:105) .

In other words, F2 = (cid:104)a, b(cid:105) is the set of strings formed by appending ﬁnitely many
copies of a, b, a−1, b−1 together.

Question 3.1.7. Notice that F1 ∼= Z.

Abuse of Notation 3.1.8. One might unfortunately notice that “subgroup generated
by a and b” has exactly the same notation as the free group (cid:104)a, b(cid:105). We’ll try to be clear
based on context which one we mean.

Presentations are nice because they provide a compact way to write down groups.

They do have some shortcomings, though.1

Example 3.1.9 (Presentations can look very diﬀerent)
The same group can have very diﬀerent presentations. For instance consider

D2n =(cid:10)x, y | x2 = y2 = 1, (xy)n = 1.(cid:11) .

(To see why this is equivalent, set x = s, y = rs.)

§3.2 Homomorphisms

Prototypical example for this section: The “mod out by 100” map, Z → Z/100Z.

How can groups talk to each other?

1Actually, determining whether two elements of a presentation are equal is undecidable. In fact, it is

undecidable to even determine if a group is ﬁnite from its presentation.

3 Homomorphisms and quotient groups

69

Two groups are “the same” if we can write an isomorphism between them. And as we
saw, two metric spaces are “the same” if we can write a homeomorphism between them.
But what’s the group analogy of a continuous map? We simply drop the “bijection”
condition.
Deﬁnition 3.2.1. Let G = (G, (cid:63)) and H = (H,∗) be groups. A group homomor-
phism is a map φ : G → H such that for any g1, g2 ∈ G we have

φ(g1 (cid:63) g2) = φ(g1) ∗ φ(g2).

Example 3.2.2 (Examples of homomorphisms)
Let G and H be groups.

(a) Any isomorphism G → H is a homomorphism. In particular, the identity map

G → G is a homomorphism.

(b) The trivial homomorphism G → H sends everything to 1H .
(c) There is a homomorphism from Z to Z/100Z by sending each integer to its

residue modulo 100.

(d) There is a homomorphism from Z to itself by x (cid:55)→ 10x which is injective but not

surjective.

(e) There is a homomorphism from Sn to Sn+1 by “embedding”: every permutation
on {1, . . . , n} can be thought of as a permutation on {1, . . . , n + 1} if we simply
let n + 1 be a ﬁxed point.

(f) A homomorphism φ : D12 → D6 is given by s12 (cid:55)→ s6 and r12 (cid:55)→ r6.
(g) Specifying a homomorphism Z → G is the same as specifying just the image of

the element 1 ∈ Z. Why?

The last two examples illustrates something: suppose we have a presentation of G. To
specify a homomorphism G → H, we only have to specify where each generator of G
goes, in such a way that the relations are all satisﬁed.
Important remark: the right way to think about an isomorphism is as a “bijective

homomorphism”. To be explicit,

Exercise 3.2.3. Show that G ∼= H if and only if there exist homomorphisms φ : G → H
and ψ : H → G such that φ ◦ ψ = idH and ψ ◦ φ = idG.

So the deﬁnitions of homeomorphism of metric spaces and isomorphism of groups are
not too diﬀerent.

Some obvious properties of homomorphisms follow.

Fact 3.2.4. Let φ : G → H be a homomorphism. Then φ(1G) = 1H and φ(g−1) = φ(g)−1.
Proof. Boring, and I’m sure you could do it yourself if you wanted to.

Now let me deﬁne a very important property of a homomorphism.

Deﬁnition 3.2.5. The kernel of a homomorphism φ : G → H is deﬁned by

It is a subgroup of G (in particular, 1G ∈ ker φ for obvious reasons).

ker φ := {g ∈ G : φ(g) = 1H} .

70

Napkin, by Evan Chen (v1.5.20190718)

Question 3.2.6. Verify that ker φ is in fact a subgroup of G.

We also have the following important fact, which we also encourage the reader to
verify.

Proposition 3.2.7 (Kernel determines injectivity)
The map φ is injective if and only if ker φ = {1G}.

To make this concrete, let’s compute the kernel of each of our examples.

Example 3.2.8 (Examples of kernels)
(a) The kernel of any isomorphism G → H is trivial, since an isomorphism is

injective. In particular, the kernel of the identity map G → G is {1G}.

(b) The kernel of the trivial homomorphism G → H (by g (cid:55)→ 1H ) is all of G.
(c) The kernel of the homomorphism Z → Z/100Z by n (cid:55)→ n is precisely

100Z = {. . . ,−200,−100, 0, 100, 200, . . .}.

(d) The kernel of the map Z → Z by x (cid:55)→ 10x is trivial: {0}.
(e) There is a homomorphism from Sn to Sn+1 by “embedding”, but it also has

trivial kernel because it is injective.

(f) A homomorphism φ : D12 → D6 is given by s12 (cid:55)→ s6 and r12 (cid:55)→ r6. You can

check that

(g) Exercise below.

ker φ =(cid:8)1, r3

12(cid:9) ∼= Z/2Z.

Exercise 3.2.9. Fix any g ∈ G. Suppose we have a homomorphism Z → G by n (cid:55)→ gn.
What is the kernel?

Question 3.2.10. Show that for any homomorphism φ : G → H, the image φimg(G) is a
subgroup of H. Hence, we’ll be especially interested in the case where φ is surjective.

§3.3 Cosets and modding out
Prototypical example for this section: Modding out by n: Z/(n · Z) ∼= Z/nZ.

The next few sections are a bit dense. If this exposition doesn’t work for you, try [Go11].
Let G and Q be groups, and suppose there exists a surjective homomorphism

φ : G (cid:16) Q.

In other words, if φ is injective then φ : G → Q is a bijection, and hence an isomorphism.
But suppose we’re not so lucky and ker φ is bigger than just {1G}. What is the correct
interpretation of a more general homomorphism?

3 Homomorphisms and quotient groups

71

Let’s look at the special case where φ : Z → Z/100Z is “modding out by 100”. We

already saw that the kernel of this map is

ker φ = 100Z = {. . . ,−200,−100, 0, 100, 200, . . .} .

Recall now that ker φ is a subgroup of G. What this means is that φ is indiﬀerent to
the subgroup 100Z of Z:

φ(15) = φ(2000 + 15) = φ(−300 + 15) = φ(700 + 15) = . . . .

So Z/100Z is what we get when we “mod out by 100”. Cool.

In other words, let G be a group and φ : G (cid:16) Q be a surjective homomorphism with

kernel N ⊆ G.

We claim that Q should be thought of as the quotient of G by N .

To formalize this, we will deﬁne a so-called quotient group G/N in terms of G and N
only (without referencing Q) which will be naturally isomorphic to Q.

For motivation, let’s give a concrete description of Q using just φ and G. Continuing

our previous example, let N = 100Z be our subgroup of G. Consider the sets

N = {. . . ,−200,−100, 0, 100, 200, . . .}
1 + N = {. . . ,−199,−99, 1, 101, 201, . . .}
2 + N = {. . . ,−198,−98, 2, 102, 202, . . .}

...

99 + N = {. . . ,−101,−1, 99, 199, 299, . . .} .

The elements of each set all have the same image when we apply φ, and moreover any two
elements in diﬀerent sets have diﬀerent images. Then the main idea is to notice that

We can think of Q as the group whose elements are the sets above.

Thus, given φ we deﬁne an equivalence relation ∼N on G by saying x ∼N y for
φ(x) = φ(y). This ∼N divides G into several equivalence classes in G which are in
obvious bijection with Q, as above. Now we claim that we can write these equivalence
classes very explicitly.

Exercise 3.3.1. Show that x ∼N y if and only if x = yn for some n ∈ N (in the mod
100 example, this means they “diﬀer by some multiple of 100”). Thus for any g ∈ G, the
equivalence class of ∼N which contains g is given explicitly by

gN := {gn | n ∈ N} .

Here’s the word that describes the types of sets we’re running into now.

Deﬁnition 3.3.2. Let H be any subgroup of G (not necessarily the kernel of some
homomorphism). A set of the form gH is called a left coset of H.

Remark 3.3.3 — Although the notation might not suggest it, keep in mind that g1N
is often equal to g2N even if g1 (cid:54)= g2. In the “mod 100” example, 3 + N = 103 + N .
In other words, these cosets are sets.
This means that if I write “let gH be a coset” without telling you what g is, you
can’t ﬁgure out which g I chose from just the coset itself. If you don’t believe me,

72

Napkin, by Evan Chen (v1.5.20190718)

here’s an example of what I mean:

x + 100Z = {. . . ,−97, 3, 103, 203, . . .} =⇒ x = ?.

There’s no reason to think I picked x = 3. (I actually picked x = −13597.)

Remark 3.3.4 — Given cosets g1H and g2H, you can check that the map x (cid:55)→
g2g−1
1 x is a bijection between them. So actually, all cosets have the same cardinality.

So, long story short,

Elements of the group Q are naturally identiﬁed with left cosets of G.

In practice, people often still prefer to picture elements of Q as single points (for example

it’s easier to think of Z/2Z as {0, 1} rather than(cid:8){. . . ,−2, 0, 2, . . .},{. . . ,−1, 1, 3, . . .}(cid:9)).

If you like this picture, then you might then draw G as a bunch of equally tall ﬁbers (the
cosets), which are then “collapsed” onto Q.

Now that we’ve done this, we can give an intrinsic deﬁnition for the quotient group

we alluded to earlier.

Deﬁnition 3.3.5. A subgroup N of G is called normal if it is the kernel of some
homomorphism. We write this as N (cid:69) G.
Deﬁnition 3.3.6. Let N (cid:69) G. Then the quotient group, denoted G/N (and read “G
mod N ”), is the group deﬁned as follows.

 The elements of G/N will be the left cosets of N .

 We want to deﬁne the product of two cosets C1 and C2 in G/N . Recall that the
cosets are in bijection with elements of Q. So let q1 be the value associated to the
coset C1, and q2 the one for C2. Then we can take the product to be the coset
corresponding to q1q2.
Quite importantly, we can also do this in terms of representatives of the
cosets. Let g1 ∈ C1 and g2 ∈ C2, so C1 = g1N and C2 = g2N . Then C1 · C2

⇐=⇐=⇐=⇐=⇐=⇐=GQ3 Homomorphisms and quotient groups

73

should be the coset which contains g1g2. This is the same as the above deﬁnition
since φ(g1g2) = φ(g1)φ(g2) = q1q2; all we’ve done is deﬁne the product in terms of
elements of G, rather than values in H.

Using the gN notation, and with Remark 3.3.3 in mind, we can write this even
more succinctly:

(g1N ) · (g2N ) := (g1g2)N.

And now you know why the integers modulo n are often written Z/nZ!

Question 3.3.7. Take a moment to digest the above deﬁnition.

By the way we’ve built it, the resulting group G/N is isomorphic to Q. In a sense we
think of G/N as “G modulo the condition that n = 1 for all n ∈ N ”.

§3.4 (Optional) Proof of Lagrange’s theorem

As an aside, with the language of cosets we can now show Lagrange’s theorem in the
general case.

Theorem 3.4.1 (Lagrange’s theorem)
Let G be a ﬁnite group, and let H be any subgroup. Then |H| divides |G|.

The proof is very simple: note that the cosets of H all have the same size and form
a partition of G (even when H is not necessarily normal). Hence if n is the number of
cosets, then n · |H| = |G|.

Question 3.4.2. Conclude that x|G| = 1 by taking H = (cid:104)x(cid:105) ⊆ G.

Remark 3.4.3 — It should be mentioned at this point that in general, if G is a
ﬁnite group and N is normal, then |G/N| = |G|/|N|.

§3.5 Eliminating the homomorphism

Prototypical example for this section: Again Z/nZ ∼= Z/nZ.

Let’s look at the last deﬁnition of G/N we provided. The short version is:

 The elements of G/N are cosets gN , which you can think of as equivalence classes

of a relation ∼N (where g1 ∼N g2 if g1 = g2n for some n ∈ N ).

 Given cosets g1N and g2N the group operation is

g1N · g2N := (g1g2)N.

Question: where do we actually use the fact that N is normal? We don’t talk about φ or
Q anywhere in this deﬁnition.

The answer is in Remark 3.3.3. The group operation takes in two cosets, so it doesn’t
know what g1 and g2 are. But behind the scenes, the normal condition guarantees
that the group operation can pick any g1 and g2 it wants and still end up

74

Napkin, by Evan Chen (v1.5.20190718)

with the same coset. If we didn’t have this property, then it would be hard to deﬁne
the product of two cosets C1 and C2 because it might make a diﬀerence which g1 ∈ C1
and g2 ∈ C2 we picked. The fact that N came from a homomorphism meant we could
pick any representatives g1 and g2 of the cosets we wanted, because they all had the
same φ-value.

We want some conditions which force this to be true without referencing φ at all.
Suppose φ : G → K is a homomorphism of groups with H = ker φ. Aside from the fact
H is a group, we can get an “obvious” property:

Question 3.5.1. Show that if h ∈ H, g ∈ G, then ghg−1 ∈ H. (Check φ(ghg−1) = 1H .)

Example 3.5.2 (Example of a non-normal subgroup)

Let D12 =(cid:10)r, s | r6 = s2 = 1, rs = sr−1(cid:11). Consider the subgroup of order two H =

{1, s} and notice that

rsr−1 = r(sr−1) = r(rs) = r2s /∈ H.

Hence H is not normal, and cannot be the kernel of any homomorphism.

Well, duh – so what? Amazingly it turns out that that this is the suﬃcient condition we
want. Speciﬁcally, it makes the nice “coset multiplication” we wanted work out.

Remark 3.5.3 (For math contest enthusiasts) — This coincidence is really a lot like
functional equations at the IMO. We all know that normal subgroups H satisfy
ghg−1 ∈ H; the surprise is that from the latter seemingly weaker condition, we can
deduce H is normal.

Thus we have a new criterion for “normal” subgroups which does not make any external

references to φ.

Theorem 3.5.4 (Algebraic condition for normal subgroups)

Let H be a subgroup of G. Then the following are equivalent:

 H (cid:69) G.
 For every g ∈ G and h ∈ H, ghg−1 ∈ H.

Proof. We already showed one direction.

For the other direction, we need to build a homomorphism with kernel H. So we
simply deﬁne the group G/H as the cosets. To put a group operation, we need to verify:
Claim 3.5.5. If g(cid:48)1 ∼H g1 and g(cid:48)2 ∼H g2 then g(cid:48)1g(cid:48)2 ∼H g1g2.
Proof. Boring algebraic manipulation (again functional equation style). Let g(cid:48)1 = g1h1
and g(cid:48)2 = g2h2, so we want to show that g1h1g2h2 ∼H g1g2. Since H has the property,
g−1
2 h1g2 is some element of H, say h3. Thus h1g2 = g2h3, and the left-hand side becomes
(cid:4)
g1g2(h3h2), which is ﬁne since h3h2 ∈ H.

With that settled we can just deﬁne the product of two cosets (of normal subgroups)

by

(g1H) · (g2H) = (g1g2)H.

3 Homomorphisms and quotient groups

75

Thus the claim above shows that this multiplication is well-deﬁned (this veriﬁcation is
the “content” of the theorem). So G/H is indeed a group! Moreover there is an obvious
“projection” homomorphism G → G/H (with kernel H), by g (cid:55)→ gH.

Example 3.5.6 (Modding out in the product group)
Consider again the product group G × H. Earlier we identiﬁed a subgroup

G(cid:48) = {(g, 1H ) | g ∈ G} ∼= G.
You can easily see that G(cid:48) (cid:69) G × H. (Easy calculation.)

Moreover, just as the notation would imply, you can check that

(G × H)/(G(cid:48)) ∼= H.

Indeed, we have (g, h) ∼G(cid:48) (1G, h) for all g ∈ G and h ∈ H.

Example 3.5.7 (Another explicit computation)
Let φ : D8 → Z/4Z be deﬁned by

r (cid:55)→ 2,

s (cid:55)→ 2.

The kernel of this map is N = {1, r2, sr, sr3}.

We can do a quick computation of all the elements of D8 to get

φ(1) = φ(r2) = φ(sr) = φ(sr3) = 0 and φ(r) = φ(r3) = φ(s) = φ(sr2) = 2.

The two relevant ﬁbers are

φpre(0) = 1N = r2N = srN = sr3N = {1, r2, sr, sr3}

and

φpre(2) = rN = r3N = sN = sr2N = {r, r3, s, sr2}.

So we see that |D8/N| = 2 is a group of order two, or Z/2Z. Indeed, the image of φ
is

(cid:8)0, 2(cid:9) ∼= Z/2Z.

Question 3.5.8. Suppose G is abelian. Why does it follow that any subgroup of G is
normal?

Finally here’s some food for thought: suppose one has a group presentation for a group
G that uses n generators. Can you write it as a quotient of the form Fn/N , where N is
a normal subgroup of Fn?

§3.6 (Digression) The ﬁrst isomorphism theorem

One quick word about what other sources usually say.

76

Napkin, by Evan Chen (v1.5.20190718)

Most textbooks actually deﬁne normal using the ghg−1 ∈ H property. Then they

deﬁne G/H for normal H in the way I did above, using the coset deﬁnition

(g1H) · (g2H) = g1g2H.

Using purely algebraic manipulations (like I did) this is well-deﬁned, and so now you
have this group G/H or something. The underlying homomorphism isn’t mentioned at
all, or is just mentioned in passing.

I think this is incredibly dumb. The normal condition looks like it gets pulled out of
thin air and no one has any clue what’s going on, because no one has any clue what a
normal subgroup actually should look like.

Other sources like to also write the so-called ﬁrst isomorphism theorem.2 It goes like

this.

Theorem 3.6.1 (First isomorphism theorem)
Let φ : G → H be a homomorphism. Then G/ ker φ is isomorphic to φimg(G).

To me, this is just a clumsier way of stating the same idea.

About the only merit this claim has is that if φ is injective, then the image φimg(G)
is an isomorphic copy of G inside the group H. (Try to see this directly!) This is a
pattern we’ll often see in other branches of mathematics: whenever we have an injective
structure-preserving map, often the image of this map will be some “copy” of G. (Here
“structure” refers to the group multiplication, but we’ll see some more other examples of
“types of objects” later!)

In that sense an injective homomorphism φ : G (cid:44)→ H is an embedding of G into H.

§3.7 A few harder problems to think about

Problem 3A (18.701 at MIT). Determine all groups G for which the map φ : G → G
deﬁned by

φ(g) = g2

is a homomorphism.

Problem 3B. Consider the dihedral group G = D10.

(a) Is H = (cid:104)r(cid:105) a normal subgroup of G? If so, compute G/H up to isomorphism.
(b) Is H = (cid:104)s(cid:105) a normal subgroup of G? If so, compute G/H up to isomorphism.
Problem 3C. Does S4 have a normal subgroup of order 3?

Problem 3D. Let G and H be ﬁnite groups, where |G| = 1000 and |H| = 999. Show
that a homomorphism G → H must be trivial.
Problem 3E. Let C× denote the nonzero complex numbers under multiplication. Show
that there are ﬁve homomorphisms Z/5Z → C× but only two homomorphisms D10 → C×,
even though Z/5Z is a subgroup of D10.

Problem 3F. Find a non-abelian group G such that every subgroup of G is normal.
(These groups are called Hamiltonian.)

3 Homomorphisms and quotient groups

77

Problem 3G (PRIMES entrance exam, 2017). Let G be a group with presentation
given by

G =(cid:10)a, b, c | ab = c2a4, bc = ca6, ac = ca8, c2018 = b2019(cid:11) .

Determine the order of G.

Problem 3H (Homophony group). The homophony group (of English) is the group
with 26 generators a, b, . . . , z and one relation for every pair of English words which
sound the same. For example knight = night (and hence k = 1). Prove that the group
is trivial.

2 There is a second and third isomorphism theorem. But four years after learning about them, I still

don’t know what they are. So I’m guessing they weren’t very important.

4 Rings and ideals

§4.1 Some motivational metaphors about rings vs groups

In this chapter we’ll introduce the notion of a commutative ring R. It is a larger
structure than a group: it will have two operations addition and multiplication, rather
than just one. We will then immediately deﬁne a ring homomorphism R → S between
pairs of rings.
This time, instead of having normal subgroups H (cid:69) G, rings will instead have subsets
I ⊆ R called ideals, which are not themselves rings but satisfy some niceness conditions.
We will then show how you to deﬁne R/I, in analogy to G/H as before. Finally, like
with groups, we will talk a bit about how to generate ideals.

Here is a possibly helpful table of analogies to help you keep track:

Group

Ring

Notation
Operations
Commutativity
for us, always
Sub-structure
(not discussed)
Homomorphism grp hom. G → H ring hom. R → S
Kernel
Quotient

normal subgroup

R
+, ×

ideal
R/I

G
·

only if abelian

subgroup

G/H

§4.2 (Optional) Pedagogical notes on motivation

I wrote most of these examples with a number theoretic eye in mind; thus if you liked
elementary number theory, a lot of your intuition will carry over. Basically, we’ll try to
generalize properties of the ring Z to any abelian structure in which we can also multiply.
That’s why, for example, you can talk about “irreducible polynomials in Q[x]” in the
same way you can talk about “primes in Z”, or about “factoring polynomials modulo p”
in the same way we can talk “unique factorization in Z”. Even if you only care about Z
(say, you’re a number theorist), this has a lot of value: I assure you that trying to solve
xn + yn = zn (for n > 2) requires going into a ring other than Z!

Thus for all the sections that follow, keep Z in mind as your prototype.
I mention this here because commutative algebra is also closely tied to algebraic
geometry. Lots of the ideas in commutative algebra have nice “geometric” interpretations
that motivate the deﬁnitions, and these connections are explored in the corresponding
part later. So, I want to admit outright that this is not the only good way (perhaps not
even the most natural one) of motivating what is to follow.

§4.3 Deﬁnition and examples of rings

Prototypical example for this section: Z all the way! Also R[x] and various ﬁelds.

Well, I guess I’ll deﬁne a ring1.

1Or, according to some authors, a “ring with identity”; some authors don’t require rings to have

multiplicative identity. For us, “ring” always means “ring with 1”.

79

80

Napkin, by Evan Chen (v1.5.20190718)

Deﬁnition 4.3.1. A ring is a triple (R, +,×), the two operations usually called addition
and multiplication, such that

(i) (R, +) is an abelian group, with identity 0R, or just 0.

(ii) × is an associative, binary operation on R with some identity, written 1R or just 1.
(iii) Multiplication distributes over addition.

The ring R is commutative if × is commutative.
Abuse of Notation 4.3.2. As usual, we will abbreviate (R, +,×) to just R.
Abuse of Notation 4.3.3. For simplicity, assume all rings are commutative for the
rest of this chapter. We’ll run into some noncommutative rings eventually, but for such
rings we won’t need the full theory of this chapter anyways.

These deﬁnitions are just here for completeness. The examples are much more impor-

tant.

Example 4.3.4 (Typical rings and ﬁelds)
(a) The sets Z, Q, R and C are all rings with the usual addition and multiplication.

(b) The integers modulo n are also a ring with the usual addition and multiplication.

We also denote it by Z/nZ.

Here is also a trivial example.

Deﬁnition 4.3.5. The zero ring is the ring R with a single element. We denote the
zero ring by 0. A ring is nontrivial if it is not the zero ring.

Exercise 4.3.6 (Comedic). Show that a ring is nontrivial if and only if 0R (cid:54)= 1R.

Since I’ve deﬁned this structure, I may as well state the obligatory facts about it.

Fact 4.3.7. For any ring R and r ∈ R, r · 0R = 0R. Moreover, r · (−1R) = −r.

Here are some more examples of rings.

Example 4.3.8 (Product ring)
Given two rings R and S the product ring, denoted R × S, is deﬁned as ordered
pairs (r, s) with both operations done component-wise. For example, the Chinese
remainder theorem says that

Z/15Z ∼= Z/3Z × Z/5Z
with the isomorphism n mod 15 (cid:55)→ (n mod 3, n mod 5).

Remark 4.3.9 — Equivalently, we can deﬁne R × S as the abelian group R ⊕ S,
and endow it with the multiplication where r · s = 0 for r ∈ R, s ∈ S.

4 Rings and ideals

81

Question 4.3.10. Which (r, s) is the identity element of the product ring R × S?

Example 4.3.11 (Polynomial ring)
Given any ring R, the polynomial ring R[x] is deﬁned as the set of polynomials
with coeﬃcients in R:

R[x] =(cid:8)anxn + an−1xn−1 + ··· + a0 | a0, . . . , an ∈ R(cid:9) .

This is pronounced “R adjoin x”. Addition and multiplication are done exactly in
the way you would expect.

Remark 4.3.12 (Digression on division) — Happily, polynomial division also does
if p(x) ∈ R[x] and p(a) = 0, then (x − a)q(x) = p(x) for some
what we expect:
polynomial q. Proof: just do polynomial long division. With that, note the caveat
that

has four roots 1, 3, 5, 7 in Z/8Z.

x2 − 1 ≡ (x − 1)(x + 1)

(mod 8)

The problem is that 2 · 4 = 0 even though 2 and 4 are not zero; we call 2 and 4
zero divisors for that reason. In an integral domain (a ring without zero divisors),
this pathology goes away, and just about everything you know about polynomials
carries over. (I’ll say this all again next section.)

Example 4.3.13 (Multi-variable polynomial ring)
We can consider polynomials in n variables with coeﬃcients in R, denoted R[x1, . . . , xn].
(We can even adjoin inﬁnitely many x’s if we like!)

Example 4.3.14 (Gaussian integers are a ring)
The Gaussian integers are the set of complex numbers with integer real and
imaginary parts, that is

Z[i] = {a + bi | a, b ∈ Z} .

Abuse of Notation 4.3.15 (Liberal use of adjoinment). Extremely careful readers
will detect some abuse in notation here. Z[i] should oﬃcially be “integer-coeﬃcient
polynomials in i”. However, as i2 = −1, a polynomial in i is just the same as a Gaussian
integer. So not a big deal.

Example 4.3.16 (Cube root of 2)
As another example:

Z[ 3√2] =(cid:110)a + b 3√2 + c 3√4 | a, b, c ∈ Z(cid:111) .

82

Napkin, by Evan Chen (v1.5.20190718)

§4.4 Homomorphisms

Prototypical example for this section: 5Z is an ideal of Z.

This section is going to go briskly – it’s the obvious generalization of all the stuﬀ we

did with quotient groups.2

First, we deﬁne a homomorphism and isomorphism.

Deﬁnition 4.4.1. Let R = (R, +R,×R) and S = (S, +S,×S) be rings. A ring homo-
morphism is a map φ : R → S such that
(i) φ(x +R y) = φ(x) +S φ(y) for each x, y ∈ R.
(ii) φ(x ×R y) = φ(x) ×S φ(y) for each x, y ∈ R.
(iii) φ(1R) = 1S.

If φ is a bijection then φ is an isomorphism and we say that rings R and S are
isomorphic.

Just what you would expect. The only surprise is that we also demand φ(1R) to go
to 1S. This condition is not extraneous: consider the map Z → Z called “multiply by
zero”.

Example 4.4.2 (Examples of homomorphisms)

(a) The identity map, as always.
(b) The map Z → Z/5Z modding out by 5.
(c) The map R[x] → R by p(x) (cid:55)→ p(0) by taking the constant term.
(d) For any ring R, there is a trivial ring homomorphism R → 0.

Example 4.4.3 (Non-examples of homomorphisms)
Because we require 1R to 1S, some maps that you might have thought were homo-
morphisms will fail.
(a) The map Z → Z by x (cid:55)→ 2x is not a ring homomomorphism. Aside from the fact

it sends 1 to 2, it also does not preserve multiplication.

(b) If S is a nontrivial ring, the map R → S by x (cid:55)→ 0 is not a ring homomorphism,

even though it preserves multiplication.

(c) There is no ring homomorphism Z/2016Z → Z at all.
In particular, whereas for groups G and H there was always a trivial group homo-
morphism sending everything in G to 1H , this is not the case for rings.

2I once found an abstract algebra textbook which teaches rings before groups. At the time I didn’t
understand why, but now I think I get it – modding out by things in commutative rings is far more
natural, and you can start talking about all the various ﬂavors of rings and ﬁelds. You also have (in
my opinion) more vivid ﬁrst examples for rings than for groups. I actually sympathize a lot with this
approach — maybe I’ll convert Napkin to follow it one day.

4 Rings and ideals

§4.5 Ideals

83

Prototypical example for this section: The multiples of 5 are an ideal of Z.

Now, just like we were able to mod out by groups, we’d also like to deﬁne quotient

rings. So once again,

Deﬁnition 4.5.1. The kernel of a ring homomorphism φ : R → S, denoted ker φ, is the
set of r ∈ R such that φ(r) = 0.

In group theory, we were able to characterize the “normal” subgroups by a few obviously
necessary conditions (namely, gHg−1 = H). We can do the same thing for rings, and it’s
in fact easier because our operations are commutative.

First, note two obvious facts:

 If φ(x) = φ(y) = 0, then φ(x + y) = 0 as well. So ker φ should be closed under

addition.

 If φ(x) = 0, then for any r ∈ R we have φ(rx) = φ(r)φ(x) = 0 too. So for x ∈ ker φ

and any r ∈ R, we have rx ∈ ker φ.

A (nonempty) subset I ⊆ R is called an ideal if it satisﬁes these properties. That is,
Deﬁnition 4.5.2. A nonempty subset I ⊆ R is an ideal if it is closed under addition,
and for each x ∈ I, rx ∈ I for all r ∈ R. It is proper if I (cid:54)= R.

Note that in the second condition, r need not be in I! So this is stronger than just

saying I is closed under multiplication.

Remark 4.5.3 — If R is not commutative, we also need the condition xr ∈ I. That
is, the ideal is two-sided : it absorbs multiplication from both the left and the right.
But since most of our rings are commutative we needn’t worry with this distinction.

Example 4.5.4 (Prototypical example of an ideal)
Consider the set I = 5Z = {. . . ,−10,−5, 0, 5, 10, . . .} as an ideal in Z. We indeed
see I is the kernel of the “take mod 5” homomorphism:

Z (cid:16) Z/5Z.

It’s clearly closed under addition, but it absorbs multiplication from all elements of
Z: given 15 ∈ I, 999 ∈ Z, we get 15 · 999 ∈ I.

Proposition 4.5.5 (Proper ideal ⇐⇒ no units)
Let R be a ring and I ⊆ R an ideal. Then I is proper if and only if it contains no
units of R.

Question 4.5.6. Prove this.

Now we claim that these conditions are suﬃcient. More explicitly,

84

Napkin, by Evan Chen (v1.5.20190718)

Theorem 4.5.7 (Ring analog of normal subgroups)
Let R be a ring and I (cid:40) R. Then I is the kernel of some homomorphism if and only
if it’s an ideal.

Proof. It’s quite similar to the proof for the normal subgroup thing, and you might try
it yourself as an exercise.

Obviously the conditions are necessary. To see they’re suﬃcient, we deﬁne a ring by

“cosets”

S = {r + I | r ∈ R} .

These are the equivalence where we say r1 ∼ r2 if r1 − r2 ∈ I (think of this as taking
“mod I”). To see that these form a ring, we just have to check that the addition and
multiplication we put on them is well-deﬁned. Speciﬁcally, we want to check that if
r1 ∼ s1 and r2 ∼ s2, then r1 + r2 ∼ s1 + s2 and r1r2 ∼ s1s2. We actually already did the
ﬁrst part – just think of R and S as abelian groups, forgetting for the moment that we
can multiply. The multiplication is more interesting.

Exercise 4.5.8 (Recommended). Show that if r1 ∼ s1 and r2 ∼ s2, then r1r2 ∼ s1s2. You
will need to use the fact that I absorbs multiplication from any elements of R, not just
those in I.

Anyways, since this addition and multiplication is well-deﬁned there is now a surjective
homomorphism R → S with kernel exactly I.
Deﬁnition 4.5.9. Given an ideal I, we deﬁne as above the quotient ring

It’s the ring of these equivalence classes. This ring is pronounced “R mod I”.

R/I := {r + I | r ∈ R} .

Example 4.5.10 (Z/5Z)
The integers modulo 5 formed by “modding out additively by 5” are the Z/5Z we
have already met.

But here’s an important point: just as we don’t actually think of Z/5Z as consisting of
k + 5Z for k = 0, . . . , 4, we also don’t really want to think about R/I as elements r + I.
The better way to think about it is

R/I is the result when we declare that elements of I are all zero; that is,
we “mod out by elements of I”.

For example, modding out by 5Z means that we consider all elements in Z divisible by 5
to be zero. This gives you the usual modular arithmetic!

§4.6 Generating ideals

Prototypical example for this section: In Z, the ideals are all of the form (n).

Let’s give you some practice with ideals.

4 Rings and ideals

85

Exercise 4.6.1. Show that the only ideals of Z are precisely those sets of the form nZ,
where n is an integer.

Thus, while ideals of ﬁelds are not terribly interesting, ideals of Z look eerily like

elements of Z. Let’s make this more precise.

Deﬁnition 4.6.2. Let R be a ring. The ideal generated by a set of elements
x1, . . . , xn ∈ R is denoted by I = (x1, x2, . . . , xn) and given by

I = {r1x1 + ··· + rnxn | ri ∈ R} .

One can think of this as “the smallest ideal containing all the xi”.

The analogy of putting the {xi} in a sealed box and shaking vigorously kind of works

here too.

Remark 4.6.3 (Linear algebra digression) — If you know linear algebra, you can
summarize this as: an ideal is an R-module. The ideal (x1, . . . , xn) is the submodule
spanned by x1, . . . , xn.

In particular, if I = (x) then I consists of exactly the “multiples of x”, i.e. numbers of

the form rx for r ∈ R.

Remark 4.6.4 — We can also apply this deﬁnition to inﬁnite generating sets, as
long as only ﬁnitely many of the ri are not zero (since inﬁnite sums don’t make sense
in general).

Example 4.6.5 (Examples of generated ideals)
(a) As (n) = nZ for all ∈ Z, every ideal in Z is of the form (n).
(b) In Z[i], we have (5) = {5a + 5bi | a, b ∈ Z}.
(c) In Z[x], the ideal (x) consists of polynomials with zero constant terms.

(d) In Z[x, y], the ideal (x, y) again consists of polynomials with zero constant terms.

(e) In Z[x], the ideal (x, 5) consists of polynomials whose constant term is divisible

by 5.

Question 4.6.6. Please check that the set I = {r1x1 + ··· + rnxn | ri ∈ R} is indeed always
an ideal (closed under addition, and absorbs multiplication).

Now suppose I = (x1, . . . , xn). What does R/I look like? According to what I said at
the end of the last section, it’s what happens when we “mod out” by each of the elements
xi. For example. . .

86

Napkin, by Evan Chen (v1.5.20190718)

Example 4.6.7 (Modding out by generated ideals)
(a) Let R = Z and I = (5). Then R/I is literally Z/5Z, or the “integers modulo 5”:

it is the result of declaring 5 = 0.

(b) Let R = Z[x] and I = (x). Then R/I means we send x to zero; hence R/I ∼= Z

as given any polynomial p(x) ∈ R, we simply get its constant term.

(c) Let R = Z[x] again and now let I = (x − 3). Then R/I should be thought of
as the quotient when x − 3 ≡ 0, that is, x ≡ 3. So given a polynomial p(x) its
image after we mod out should be thought of as p(3). Again R/I ∼= Z, but in a
diﬀerent way.

(d) Finally, let I = (x − 3, 5). Then R/I not only sends x to three, but also 5 to

zero. So given p ∈ R, we get p(3) (mod 5). Then R/I ∼= Z/5Z.

Remark 4.6.8 (Mod notation) — By the way, given an ideal I of a ring R, it’s
totally legit to write

to mean that x − y ∈ I. Everything you learned about modular arithmetic carries
over.

x ≡ y

(mod I)

§4.7 Principal ideal domains

Prototypical example for this section: Z is a PID, Z[x] is not. C[x] is a PID, C[x, y] is
not.

What happens if we put multiple generators in an ideal, like (10, 15) ⊆ Z? Well, we

have by deﬁnition that (10, 15) is given as a set by

(10, 15) := {10x + 15y | x, y ∈ Z} .

If you’re good at number theory you’ll instantly recognize that this as just 5Z = (5).
Surprise! In Z, the ideal (a, b) is exactly gcd(a, b)Z. And that’s exactly the reason you
often see the GCD of two numbers denoted (a, b).

We call such an ideal (one generated by a single element) a principal ideal. So, in Z,

every ideal is principal. But the same is not true in more general rings.

Example 4.7.1 (A non-principal ideal)
In Z[x], I = (x, 2015) is not a principal ideal. For if I = (f ) for some polynomial
f ∈ I then f divides x and 2015. This can only occur if f = ±1, but then I contains
a unit.

A ring with the property that all its ideals are principal is called a principal ideal ring.
We like this property because they eﬀectively let us take the “greatest common factor”
in a similar way as the GCD in Z.

In practice, we actually usually care about so-called principal ideal domains
(PID’s). But we haven’t deﬁned what a domain is yet. Nonetheless, all the exam-
ples below are actually PID’s, so we will go ahead and use this word for now, and tell
you what the additional condition is in the next chapter.

4 Rings and ideals

87

Example 4.7.2 (Examples of PID’s)
As just said, for now you should just verify that these are principal ideal rings, even
though we are using the word PID.

(a) As we saw, Z is a PID.

(b) As we also saw, Z[x] is not a PID, since I = (x, 2015) for example is not principal.

(c) It turns out that for a ﬁeld k the ring k[x] is always a PID. For example, Q[x],

R[x], C[x] are PID’s.
If you want to try and prove this, ﬁrst prove an analog of Bezout’s lemma, which
implies the result.

(d) C[x, y] is not a PID, because (x, y) is not principal.

§4.8 Noetherian rings

Prototypical example for this section: Z[x1, x2, . . . ] is not Noetherian, but most reasonable
rings are. In particular polynomial rings are. (Equivalently, only weirdos care about
non-Noetherian rings).

If it’s too much to ask that an ideal is generated by one element, perhaps we can
at least ask that our ideals are generated by ﬁnitely many elements. Unfortunately, in
certain weird rings this is also not the case.

Example 4.8.1 (Non-Noetherian ring)
Consider the ring R = Z[x1, x2, x3, . . . ] which has inﬁnitely many free variables.
Then the ideal I = (x1, x2, . . . ) ⊆ R cannot be written with a ﬁnite generating set.

Nonetheless, most “sane” rings we work in do have the property that their ideals are
ﬁnitely generated. We now name such rings and give two equivalent deﬁnitions:

Proposition 4.8.2 (The equvialent deﬁnitions of a Noetherian ring)

For a ring R, the following are equivalent:

(a) Every ideal I of R is ﬁnitely generated (i.e. can be written with a ﬁnite generating

set).

(b) There does not exist an inﬁnite ascending chain of ideals

I1 (cid:40) I2 (cid:40) I3 (cid:40) . . . .

The absence of such chains is often called the ascending chain condition.

Such rings are called Noetherian.

88

Napkin, by Evan Chen (v1.5.20190718)

Example 4.8.3 (Non-Noetherian ring breaks ACC)
In the ring R = Z[x1, x2, x3, . . . ] we have an inﬁnite ascending chain

(x1) (cid:40) (x1, x2) (cid:40) (x1, x2, x3) (cid:40) . . . .

From the example, you can kind of see why the proposition is true: from an inﬁnitely
generated ideal you can extract an ascending chain by throwing elements in one at a
time. I’ll leave the proof to you if you want to do it.3

Question 4.8.4. Why are ﬁelds Noetherian? Why are PID’s (such as Z) Noetherian?

This leaves the question: is our prototypical non-example of a PID, Z[x], a Noethe-
rian ring? The answer is a glorious yes, according to the celebrated Hilbert basis
theorem.

Theorem 4.8.5 (Hilbert basis theorem)

Given a Noetherian ring R, the ring R[x] is also Noetherian. Thus by induction,
R[x1, x2, . . . , xn] is Noetherian for any integer n.

The proof of this theorem is really olympiad ﬂavored, so I couldn’t possibly spoil it – I’ve
left it as a problem at the end of this chapter.

Noetherian rings really shine in algebraic geometry, and it’s a bit hard for me to
motivate them right now, other than to just say “almost all rings you’ll ever care about
are Noetherian”. Please bear with me!

§4.9 A few harder problems to think about

Problem 4A. The ring R = R[x]/(x2 + 1) is one that you’ve seen before. What is its
name?

Problem 4B. Show that C[x]/(x2 − x) ∼= C × C.
Problem 4C. In the ring Z, let I = (2016) and J = (30). Show that I ∩ J is an ideal of
Z and compute its elements.
Problem 4D(cid:63). Let R be a ring and I an ideal. Find an inclusion-preserving bijection
between

 ideals of R/I, and

 ideals of R which contain I.

Problem 4E. Let R be a ring.
(a) Prove that there is exactly one ring homomorphism Z → R.
(b) Prove that the number of ring homomorphisms Z[x] → R is equal to the number of

elements of R.

Problem 4F. Prove the Hilbert basis theorem, Theorem 4.8.5.

3On the other hand, every undergraduate class in this topic I’ve seen makes you do it as homework.

Admittedly I haven’t gone to that many such classes.

4 Rings and ideals

89

Problem 4G (USA Team Selection Test 2016). Let Fp denote the integers modulo a
ﬁxed prime number p. Deﬁne Ψ : Fp[x] → Fp[x] by
aixi(cid:33) =

aixpi

Ψ(cid:32) n(cid:88)i=0

n(cid:88)i=0

.

Let S denote the image of Ψ.

(a) Show that S is a ring with addition given by polynomial addition, and multiplication

given by function composition.

(b) Prove that Ψ : Fp[x] → S is then a ring isomorphism.
Problem 4H. Let A ⊆ B ⊆ C be rings. Suppose C is a ﬁnitely generated A-module.
Does it follow that B is a ﬁnitely generated A-module?

5 Flavors of rings

We continue our exploration of rings by considering some nice-ness properties that
rings or ideals can satisfy, which will be valuable later on. As before, number theory is
interlaced as motivation. I guess I can tell you at the outset what the completed table is
going to look like, so you know what to expect.

Ring noun
PID

Ideal adjective
principal

Relation
R is a PID ⇐⇒ R is an integral domain,
Noetherian ring ﬁnitely generated R is Noetherian ⇐⇒ every I is ﬁn. gen.
R/I is a ﬁeld ⇐⇒ I is maximal
ﬁeld
R/I is an integral domain ⇐⇒ I is prime
integral domain

and every I is principal

maximal
prime

§5.1 Fields

Prototypical example for this section: Q is a ﬁeld, but Z is not.

As you might already know, if the multiplication is invertible, then we call the ring a

ﬁeld. To be explicit, let me write the relevant deﬁnitions.
Deﬁnition 5.1.1. A unit of a ring R is an element u ∈ R which is invertible: for some
x ∈ R we have ux = 1R.

Example 5.1.2 (Examples of units)
(a) The units of Z are ±1, because these are the only things which “divide 1” (which

is the reason for the name “unit”).

(b) On the other hand, in Q everything is a unit (except 0). For example, 3

5 is a

unit since 3

5 · 5

3 = 1.

(c) The Gaussian integers Z[i] have four units: ±1 and ±i.

Deﬁnition 5.1.3. A nontrivial (commutative) ring is a ﬁeld when all its nonzero
elements are units.

Colloquially, we say that

A ﬁeld is a structure where you can add, subtract, multiply, and divide.

Depending on context, they are often denoted either k, K, F .

Example 5.1.4 (First examples of ﬁelds)
(a) Q, R, C are ﬁelds, since the notion 1

c makes sense in them.

(b) If p is a prime, then Z/pZ is a ﬁeld, which we denote will usually denote by Fp.

The trivial ring 0 is not considered a ﬁeld, since we require ﬁelds to be nontrivial..

91

92

Napkin, by Evan Chen (v1.5.20190718)

Exercise 5.1.5 (Mandatory: ﬁelds have two ideals). If K is a ﬁeld, show that K has exactly
two ideals. What are they?

§5.2 Integral domains

Prototypical example for this section: Z is an integral domain.

Now it would be nice if we could still conclude the zero product property: if ab = 0
if b (cid:54)= 0, then we can
then either a = 0 or b = 0. If our ring is a ﬁeld, this is true:
multiply by b−1 to get a = 0. But many other rings we consider like Z and Z[x] also have
this property, despite not being full-ﬂedged ﬁelds.

Not for all rings though: in Z/15Z,

3 · 5 ≡ 0

(mod 15).

If a, b (cid:54)= 0 but ab = 0 then we say a and b are zero divisors of the ring R. So we give a
name to such rings.

Deﬁnition 5.2.1. A nontrivial ring with no zero divisors is called an integral domain.1

Question 5.2.2. Show that a ﬁeld is an integral domain.

Exercise 5.2.3 (Cancellation in integral domains). Suppose ac = bc in an integral domain,

and c (cid:54)= 0. Show that that a = b. (There is no c−1 to multiply by, so you have to use the

deﬁnition.)

Example 5.2.4 (Examples of integral domains)
Every ﬁeld is an integral domain, so all the previous examples apply. In addition:

(a) Z is an integral domain, but it is not a ﬁeld.

(b) R[x] is not a ﬁeld, since there is no polynomial P (x) with xP (x) = 1. However,
R[x] is an integral domain, because if P (x)Q(x) = 0 then one of P or Q is zero.

(c) Z[x] is also an example of an integral domain. In fact, R[x] is an integral domain

for any integral domain R (why?).

(d) Z/nZ is a ﬁeld (hence integral domain) exactly when n is prime. When n is not

prime, it is a ring but not an integral domain.

The trivial ring 0 is not considered an integral domain.

At this point, we go ahead and say:

Deﬁnition 5.2.5. An integral domain where all ideals are principal is called a principal
ideal domain (PID).

The ring Z/6Z is an example of a ring which is a principal ideal ring, but not an
integral domain. As we alluded to earlier, we will never really use “principal ideal ring”
in any real way: we typically will want to strengthen it to PID.

1Some authors abbreviate this to “domain”, notably Artin.

5 Flavors of rings

§5.3 Prime ideals

93

Prototypical example for this section: (5) is a prime ideal of Z.

We know that every integer can be factored (up to sign) as a unique product of primes;
for example 15 = 3 · 5 and −10 = −2 · 5. You might remember the proof involves the
so-called B´ezout’s lemma, which essentially says that (a, b) = (gcd(a, b)); in other words
we’ve carefully used the fact that Z is a PID.

It turns out that for general rings, the situation is not as nice as factoring elements

because most rings are not PID’s. The classic example of something going wrong is

6 = 2 · 3 =(cid:0)1 − √−5(cid:1)(cid:0)1 + √−5(cid:1)

in Z[√−5]. Nonetheless, we can sidestep the issue and talk about factoring ideals:
somehow the example 10 = 2 · 5 should be (10) = (2) · (5), which says “every multiple of
10 is the product of a multiple of 2 and a multiple of 5”. I’d have to tell you then how to
multiply two ideals, which I do in the chapter on unique factorization.

Let’s at least ﬁgure out what primes are. In Z, we have that p (cid:54)= 1 is prime if whenever

p | xy, either p | x or p | y. We port over this deﬁnition to our world of ideals.
Deﬁnition 5.3.1. A proper ideal I (cid:40) R is a prime ideal if whenever xy ∈ I, either
x ∈ I or y ∈ I.

The condition that I is proper is analogous to the fact that we don’t consider 1 to be

a prime number.

Example 5.3.2 (Examples and non-examples of prime ideals)
(a) The ideal (7) of Z is prime.
(b) The ideal (8) of Z is not prime, since 2 · 4 = 8.
(c) The ideal (x) of Z[x] is prime.
(d) The ideal (x2) of Z[x] is not prime, since x · x = x2.
(e) The ideal (3, x) of Z[x] is prime. This is actually easiest to see using Theorem 5.3.5

below.

(f) The ideal (5) = 5Z + 5iZ of Z[i] is not prime, since the elements 3 + i and 3 − i

have product 10 ∈ (5), yet neither is itself in (5).

Remark 5.3.3 — Ideals have the nice property that they get rid of “sign issues”.
For example, in Z, do we consider −3 to be a prime? When phrased with ideals,
this annoyance goes away: (−3) = (3). More generally, for a ring R, talking about
ideals lets us ignore multiplication by a unit. (Note that −1 is a unit in Z.)

Exercise 5.3.4. What do you call a ring R for which the zero ideal (0) is prime?

We also have:

Theorem 5.3.5 (Prime ideal ⇐⇒ quotient is integral domain)
An ideal I is prime if and only if R/I is an integral domain.

94

Napkin, by Evan Chen (v1.5.20190718)

Exercise 5.3.6 (Mandatory). Convince yourself the theorem is true; it is just deﬁnition
chasing. (A possible start is to consider R = Z and I = (15).)

I now must regrettably inform you that unique factorization is still not true even with
the notion of a “prime” ideal (though again I haven’t told you how to multiply two
ideals yet). But it will become true with some additional assumptions that will arise in
algebraic number theory (relevant buzzword: Dedekind domain).

§5.4 Maximal ideals

Prototypical example for this section: The ideal (x, 5) is maximal in Z[x], by quotient-ing.

Here’s another ﬂavor of an ideal.

Deﬁnition 5.4.1. A proper ideal I of a ring R is maximal if it is not contained in any
other proper ideal.

Example 5.4.2 (Examples of maximal ideals)
(a) The ideal I = (7) of Z is maximal, because if an ideal J contains 7 and an

element n not in I it must contain gcd(7, n) = 1, and hence J = Z.

(b) The ideal (x) is not maximal in Z[x], because it’s contained in (x, 5) (among

others).

(c) On the other hand, (x, 5) is indeed maximal in Z[x]. This is actually easiest to

verify using Theorem 5.4.4 below.

(d) Also, (x) is maximal in C[x], again appealing to Theorem 5.4.4 below.

Exercise 5.4.3. What do you call a ring R for which the zero ideal (0) is maximal?

There’s an analogous theorem to the one for prime ideals.

Theorem 5.4.4 (I maximal ⇐⇒ R/I ﬁeld)
An ideal I is maximal if and only if R/I is a ﬁeld.

Proof. A ring is a ﬁeld if and only if (0) is the only maximal ideal. So this follows by
Problem 4D(cid:63).

Corollary 5.4.5 (Maximal ideals are prime)

If I is a maximal ideal of a ring R, then I is prime.

Proof. If I is maximal, then R/I is a ﬁeld, hence an integral domain, so I is prime.

In practice, because modding out by generated ideals is pretty convenient, this is a

very eﬃcient way to check whether an ideal is maximal.

5 Flavors of rings

95

Example 5.4.6 (Modding out in Z[x])
(a) This instantly implies that (x, 5) is a maximal ideal in Z[x], because if we mod

out by x and 5 in Z[x], we just get F5, which is a ﬁeld.

(b) On the other hand, modding out by just x gives Z, which is an integral domain

but not a ﬁeld; that’s why (x) is prime but not maximal.

As we saw, any maximal ideal is prime. But now note that Z has the special property
that all of its nonzero prime ideals are also maximal.
It’s with this condition and
a few other minor conditions that you get a so-called Dedekind domain where prime
factorization of ideals does work. More on that later.

§5.5 Field of fractions

Prototypical example for this section: Frac(Z) = Q.

As long as we are here, we take the time to introduce a useful construction that turns

any integral domain into a ﬁeld.

Deﬁnition 5.5.1. Given an integral domain R, we deﬁne its ﬁeld of fractions or
fraction ﬁeld Frac(R) as follows: it consists of elements a/b, where a, b ∈ R and b (cid:54)= 0.
We set a/b ∼ c/d if and only if bc = ad. Addition and multiplication is deﬁned by

a
b

+

a
b ·

c
d
c
d

=

=

ad + bc

bd

ab
cd

.

In fact everything you know about Q basically carries over by analogy. You can prove
if you want that this indeed a ﬁeld, but considering how comfortable we are that Q is
well-deﬁned, I wouldn’t worry about it. . .

Deﬁnition 5.5.2. Let k be a ﬁeld. We deﬁne k(x) = Frac(k[x]) (read “k of x”), and
call it the ﬁeld of rational functions.

Example 5.5.3 (Examples of fraction ﬁelds)
(a) By deﬁnition, Frac(Z) = Q.

(b) The ﬁeld R(x) consists of rational functions in x:

R(x) =(cid:26) f (x)

g(x) | f, g ∈ R[x](cid:27) .

For example,

2x

x2−3 might be a typical element.

96

Napkin, by Evan Chen (v1.5.20190718)

Example 5.5.4 (Gaussian rationals)
Just like we deﬁned Z[i] by abusing notation, we can also write Q(i) = Frac(Z[i]).
Oﬃcially, it should consist of

Q(i) =(cid:26) f (i)

g(i) | g(i) (cid:54)= 0(cid:27)

for polynomials f and g with rational coeﬃcients. But since i2 = −1 this just leads
to

Q(i) =(cid:26) a + bi

c + di | a, b, c, d ∈ Q, (c, d) (cid:54)= (0, 0)(cid:27) .

And since

1

c+di = c−di

c2+d2 we end up with

Q(i) = {a + bi | a, b ∈ Q} .

§5.6 Unique factorization domains (UFD’s)

Prototypical example for this section: Z and polynomial rings in general.

Here is one stray deﬁnition that will be important for those with a number-theoretic
inclination. Over the positive integers, we have a fundamental theorem of arithmetic,
stating that every integer is uniquely the product of prime numbers.

We can even make an analogous statement in Z or Z[i], if we allow representations like
6 = (−2)(−3) and so on. The trick is that we only consider everything up to units; so
6 = (−2)(−3) = 2 · 3 are considered the same.

The general deﬁnition goes as follows.

Deﬁnition 5.6.1. A nonzero non-unit of an integral domain R is irreducible if cannot
be written as the product of two non-units.

An integral domain R is a unique factorization domain if every nonzero non-unit
of R can be written as the product of irreducible elements, which is unique up to
multiplication by units.

Question 5.6.2. Verify that Z is a UFD.

5 Flavors of rings

97

Example 5.6.3 (Examples of UFD’s)

(a) Fields are a “degenerate” example of UFD’s: every nonzero element is a unit, so

there is nothing to check.

(b) Z is a UFD. The irreducible elements are p and −p, for example 5 or −17.
(c) Q[x] is a UFD: polynomials with rational coeﬃcients can be uniquely factored,
up to scaling by constants (as the units of Q[x] are just the rational numbers).

(d) Z[x] is a UFD.

(e) The Gaussian integers Z[i] turns out to be a UFD too (and this will be proved

in the chapters on algebraic number theory).

(f) Z[√−5] is the classic non-example of a UFD: one may write
6 = 2 · 3 =(cid:0)1 − √−5(cid:1)(cid:0)1 + √−5(cid:1)

but each of 2, 3, 1 ± √−5 is irreducible. (It turns out the right way to ﬁx this is

by considering prime ideals instead, and this is one big motivation for Part XIII.)

(g) Theorem we won’t prove: every PID is a UFD.

(h) Theorem we won’t prove: if R is a UFD, so is R[x] (and hence by induction so

is R[x, y], R[x, y, z], . . . ).

§5.7 A few harder problems to think about

Not olympiad problems, but again the spirit is very close to what you might see in an
olympiad.

Problem 5A. Consider the ring

Is it a ﬁeld?

Q[√2] =(cid:110)a + b√2 | a, b ∈ Q(cid:111) .

Problem 5B (Homomorphisms from ﬁelds are injective). Let K be a ﬁeld and R a ring.
Prove that any homomorphism ψ : K → R is injective.2
Problem 5C(cid:63) (Pre-image of prime ideals). Suppose φ : R → S is a ring homomorphism,
and I ⊆ S is a prime ideal. Prove that φpre(I) is prime as well.
Problem 5D(cid:63). Let R be an integral domain with ﬁnitely many elements. Prove that R
is a ﬁeld.

Problem 5E(cid:63) (Krull’s theorem). Let R be a ring and J a proper ideal.

(a) Prove that if R is Noetherian, then J is contained in a maximal ideal I.

(b) Use Zorn’s lemma (Chapter 81) to prove the result even if R isn’t Noetherian.
Problem 5F (Spec k[x]). Describe the prime ideals of C[x] and R[x].
Problem 5G. Prove that any nonzero prime ideal of Z[√2] is also a maximal ideal.

2Note that ψ cannot be the zero map for us, since we require ψ(1K ) = 1R. You sometimes ﬁnd diﬀerent

statements in the literature.

III

Basic Topology

Part III: Contents

6 Properties of metric spaces

101
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
6.1 Boundedness
6.2 Completeness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
6.3 Let the buyer beware
6.4 Subspaces, and (inb4) a confusing linguistic point . . . . . . . . . . . . . . . . . . . 104
. . . . . . . . . . . . . . . . . . . . . . . . 105
6.5 A few harder problems to think about

7 Topological spaces

107
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
7.1 Forgetting the metric
7.2 Re-deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
7.3 Hausdorﬀ spaces
7.4 Subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
7.5 Connected spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
7.6 Path-connected spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
. . . . . . . . . . . . . . . . . . . . . . . 112
7.7 Homotopy and simply connected spaces
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
7.8 Bases of spaces
. . . . . . . . . . . . . . . . . . . . . . . . 115
7.9 A few harder problems to think about

8 Compactness

117
. . . . . . . . . . . . . . . . . . . . . . . . . 117
8.1 Deﬁnition of sequential compactness
8.2 Criteria for compactness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
. . . . . . . . . . . . . . . . . . . . . . . . . . . 119
8.3 Compactness using open covers
8.4 Applications of compactness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
. . . . . . . . . . . . . . . . 123
8.5
. . . . . . . . . . . . . . . . . . . . . . . . 124
8.6 A few harder problems to think about

(Optional) Equivalence of formulations of compactness

6 Properties of metric spaces

At the end of the last chapter on metric spaces, we introduced two adjectives “open”
and “closed”. These are important because they’ll grow up to be the deﬁnition for a
general topological space, once we graduate from metric spaces.

To move forward, we provide a couple niceness adjectives that applies to entire metric
spaces, rather than just a set relative to a parent space. They are “(totally) bounded” and
“complete”. These adjectives are speciﬁc to metric spaces, but will grow up to become
the notion of compactness, which is, in the words of [Pu02], “the single most important
concept in real analysis”. At the end of the chapter, we will know enough to realize that
something is amiss with our deﬁnition of homeomorphism, and this will serve as the
starting point for the next chapter, when we deﬁne fully general topological spaces.

§6.1 Boundedness

Prototypical example for this section: [0, 1] is bounded but R is not.

Here is one notion of how to prevent a metric space from being a bit too large.

Deﬁnition 6.1.1. A metric space M is bounded if there is a constant D such that
d(p, q) ≤ D for all p, q ∈ M .

You can change the order of the quantiﬁers:

Proposition 6.1.2 (Boundedness with radii instead of diameters)
A metric space M is bounded if and only if for every point p ∈ M , there is a radius
R (possibly depending on p) such that d(p, q) ≤ R for all q ∈ M .

Exercise 6.1.3. Use the triangle inequality to show these are equivalent. (The names
“radius” and “diameter” are a big hint!)

Example 6.1.4 (Examples of bounded spaces)

(a) Finite intervals like [0, 1] and (a, b) are bounded.

(b) The unit square [0, 1]2 is bounded.
(c) Rn is not bounded for any n ≥ 1.
(d) A discrete space on an inﬁnite set is bounded.

(e) N is not bounded, despite being homeomorphic to the discrete space!

The fact that a discrete space on an inﬁnite set is “bounded” might be upsetting to

you, so here is a somewhat stronger condition you can use:

Deﬁnition 6.1.5. A metric space is totally bounded if for any ε > 0, we can cover
M with ﬁnitely many ε-neighborhoods.

101

102

Napkin, by Evan Chen (v1.5.20190718)

For example, if ε = 1/2, you can cover [0, 1]2 by ε-neighborhoods.

Exercise 6.1.6. Show that “totally bounded” implies “bounded”.

Example 6.1.7 (Examples of totally bounded spaces)
(a) A subset of Rn is bounded if and only if it is totally bounded.

This is for Euclidean geometry reasons: for example in R2 if I can cover a set by
a single disk of radius 2, then I can certainly cover it by ﬁnitely many disks of
radius 1/2. (We won’t prove this rigorously.)

(b) So for example [0, 1] or [0, 2] × [0, 3] is totally bounded.
(c) In contrast, a discrete space on an inﬁnite set is not totally bounded.

§6.2 Completeness

Prototypical example for this section: R is complete, but Q and (0, 1) are not.

So far we can only talk about sequences converging if they have a limit. But consider

the sequence

It converges to √2 in R, of course. But it fails to converge in Q; there is no rational

x1 = 1, x2 = 1.4, x3 = 1.41, x4 = 1.414, . . . .

number this sequence converges to. And so somehow, if we didn’t know about the
existence of R, we would have no idea that the sequence (xn) is “approaching” something.
That seems to be a shame. Let’s set up a new deﬁnition to describe these sequences
whose terms get close to each other, even if they don’t approach any particular point
in the space. Thus, we only want to mention the given points in the deﬁnition.

Deﬁnition 6.2.1. Let x1, x2, . . . be a sequence which lives in a metric space M =
(M, dM ). We say the sequence is Cauchy if for any ε > 0, we have

for all suﬃciently large m and n.

dM (xm, xn) < ε

Question 6.2.2. Show that a sequence which converges is automatically Cauchy. (Draw a
picture.)

Now we can deﬁne:

Deﬁnition 6.2.3. A metric space M is complete if every Cauchy sequence converges.

6 Properties of metric spaces

103

Example 6.2.4 (Examples of complete spaces)
(a) R is complete. (Depending on your deﬁnition of R, this either follows by deﬁnition,

or requires some work. We won’t go through this here.)

(b) The discrete space is complete, as the only Cauchy sequences are eventually

constant.

(c) The closed interval [0, 1] is complete.

(d) Rn is complete as well. (You’re welcome to prove this by induction on n.)

Example 6.2.5 (Non-examples of complete spaces)
(a) The rationals Q are not complete.

(b) The open interval (0, 1) is not complete, as the sequence 0.9, 0.99, 0.999, 0.9999,

. . . is Cauchy but does not converge.

So, metric spaces need not be complete, like Q. But we certainly would like them to

be complete, and in light of the following theorem this is not unreasonable.

Theorem 6.2.6 (Completion)

Every metric space can be “completed”, i.e. made into a complete space by adding
in some points.

We won’t need this construction at all, so it’s left as Problem 6C†.

Example 6.2.7 (Q completes to R)
The completion of Q is R (in fact, this is often taken as the deﬁnition of R).

§6.3 Let the buyer beware

There is something suspicious about both these notions: neither are preserved under
homeomorphism!

Example 6.3.1 (Something ﬁshy is going on here)
Let M = (0, 1) and N = R. As we saw much earlier M and N are homeomorphic.
However:

 (0, 1) is totally bounded, but not complete.

 R is complete, but not bounded.

This is the ﬁrst hint of something going awry with the metric. As we progress further
into our study of topology, we will see that in fact open and closed sets (which we
motivated by using the metric) are the notion that will really shine later on. I insist on
introducing the metric ﬁrst so that the standard pictures of open and closed sets make
sense, but eventually it becomes time to remove the training wheels.

104

Napkin, by Evan Chen (v1.5.20190718)

§6.4 Subspaces, and (inb4) a confusing linguistic point

Prototypical example for this section: A circle is obtained as a subspace of R2.

As we’ve already been doing implicitly in examples, we’ll now say:

Deﬁnition 6.4.1. Every subset S ⊆ M is a metric space in its own right, by re-using
the distance function on M . We say that S is a subspace of M .

For example, we saw that the circle S1 is just a subspace of R2.
It thus becomes important to distinguish between

(i) “absolute” adjectives like “complete” or “bounded”, which can be applied to

both spaces, and hence even to subsets of spaces (by taking a subspace), and

(ii) “relative” adjectives like “open (in M )” and “closed (in M )”, which make sense

only relative to a space, even though people are often sloppy and omit them.

So “[0, 1] is complete” makes sense, as does “[0, 1] is a complete subset of R”, which we
take to mean “[0, 1] is a complete as a subspace of R”. This is since “complete” is an
absolute adjective.

But here are some examples of ways in which relative adjectives require a little more

care:

 Consider the sequence 1, 1.4, 1.41, 1.414, . . . . Viewed as a sequence in R, it
converges to √2. But if viewed as a sequence in Q, this sequence does not converge!

Similarly, the sequence 0.9, 0.99, 0.999, 0.9999 does not converge in the space (0, 1),
although it does converge in [0, 1].

The fact that these sequences fail to converge even though they “ought to” is weird
and bad, and was why we deﬁned complete spaces to begin with.

 In general, it makes no sense to ask a question like “is [0, 1] open?”. The questions
“is [0, 1] open in R?” and “is [0, 1] open in [0, 1]?” do make sense, however. The
answer to the ﬁrst question is “no” but the answer to the second question is “yes”;
indeed, every space is open in itself. Similarly, [0, 1
2 ) is an open set in the space
M = [0, 1] because it is the ball in M of radius 1

2 centered at 0.

 Dually, it doesn’t make sense to ask “is [0, 1] closed”? It is closed in R and in itself

(but every space is closed in itself, anyways).

To make sure you understand the above, here are two exercises to help you practice

relative adjectives.

Exercise 6.4.2. Let M be a complete metric space and let S ⊆ M . Prove that S is complete
if and only if it is closed in M . In particular, [0, 1] is complete.

Exercise 6.4.3. Let M = [0, 1]∪ (2, 3). Show that [0, 1] and (2, 3) are both open and closed
in M .

This illustrates a third point: a nontrivial set can be both open and closed1 As we’ll
see in Chapter 7, this implies the space is disconnected; i.e. the only examples look quite
like the one we’ve given above.

1Which always gets made fun of.

6 Properties of metric spaces

105

§6.5 A few harder problems to think about

Problem 6A† (Banach ﬁxed point theorem). Let M = (M, d) be a complete metric
space. Suppose T : M → M is a continuous map such that for any p, q ∈ M ,

d (T (p), T (q)) < 0.999d(p, q).

(We call T a contraction.) Show that T has a unique ﬁxed point.

Problem 6B (Henning Makholm, on math.SE). We let M and N denote the metric
spaces obtained by equipping R with the following two metrics:

dM (x, y) = min{1,|x − y|}
dN (x, y) = |ex − ey| .

(a) Fill in the following 2 × 3 table with “yes” or “no” for each cell.
Complete? Bounded? Totally bounded?

M
N

(b) Are M and N homeomorphic?

Problem 6C† (Completion of a metric space). Let M be a metric space. Construct
a complete metric space M such that M is a subspace of M , and every open set of M
contains a point of M (meaning M is dense in M ).

Problem 6D. Prove that Q is not homeomorphic to any complete metric space.

7 Topological spaces

In Chapter 2 we introduced the notion of space by describing metrics on them. This
gives you a lot examples, and nice intuition, and tells you how you should draw pictures
of open and closed sets.

However, moving forward, it will be useful to begin thinking about topological spaces
in terms of just their open sets. (One motivation is that our ﬁshy Example 6.3.1 shows
that in some ways the notion of homeomorphism really wants to be phrased in terms of
open sets, not in terms of the metric.) As we are going to see, the open sets manage to
actually retain nearly all the information we need, but are simpler.1 This will be done in
just a few sections, and after that we will start describing more adjectives that we can
apply to topological (and hence metric) spaces.

The most important topological notion is missing from this chapter: that of a compact

space. It is so important that I have dedicated a separate chapter just for it.

Quick note for those who care: the adjectives “Hausdorﬀ”, “connected”, and later

“compact” are all absolute adjectives.

§7.1 Forgetting the metric

Recall Theorem 2.6.11:

A function f : M → N of metric spaces is continuous if and only if the
pre-image of every open set in N is open in M .

Despite us having deﬁned this in the context of metric spaces, this nicely doesn’t refer to
the metric at all, only the open sets. As alluded to at the start of this chapter, this is
a great motivation for how we can forgot about the fact that we had a metric to begin
with, and rather start with the open sets instead.

Deﬁnition 7.1.1. A topological space is a pair (X,T ), where X is a set of points,
and T is the topology, which consists of several subsets of X, called the open sets of
X. The topology must obey the following axioms.

 ∅ and X are both in T .
 Finite intersections of open sets are also in T .
 Arbitrary unions (possibly inﬁnite) of open sets are also in T .
So this time, the open sets are given. Rather than deﬁning a metric and getting open

sets from the metric, we instead start from just the open sets.

Abuse of Notation 7.1.2. We abbreviate (X,T ) by just X, leaving the topology T
implicit. (Do you see a pattern here?)

1The reason I adamantly introduce metric spaces ﬁrst is because I think otherwise the examples make

much less sense.

107

108

Napkin, by Evan Chen (v1.5.20190718)

Example 7.1.3 (Examples of topologies)
(a) Given a metric space M , we can let T be the open sets in the metric sense. The

point is that the axioms are satisﬁed.

(b) In particular, discrete space is a topological space in which every set is open.

(Why?)

(c) Given X, we can let T = {∅, X}, the opposite extreme of the discrete space.

Now we can port over our metric deﬁnitions.

Deﬁnition 7.1.4. An open neighborhood2 of a point x ∈ X is an open set U which
contains x (see ﬁgure).

Abuse of Notation 7.1.5. Just to be perfectly clear: by an “open neighborhood” I
mean any open set containing x. But by an “r-neighborhood” I always mean the points
with distance less than r from x, and so I can only use this term if my space is a metric
space.

§7.2 Re-deﬁnitions

Now that we’ve deﬁned a topological space, for nearly all of our metric notions we can
write down as the deﬁnition the one that required only open sets (which will of course
agree with our old deﬁnitions when we have a metric space).

§7.2.i Continuity

Here was our motivating example, continuity:

Deﬁnition 7.2.1. We say function f : X → Y of topological spaces is continuous at a
point p ∈ X if the pre-image of any open neighborhood of f p is an open neighborhood of
p. The function is continuous if it is continuous at every point.

Thus homeomorphisms carries over: a bijection which is continuous in both directions.

Deﬁnition 7.2.2. A homeomorphism of topological spaces (X, τX ) and (Y, τY ) is a
bijection f : X → Y which induces a bijection from τX to τY : i.e. the bijection preserves
open sets.

2In literature, a “neighborhood” refers to a set which contains some open set around x. We will not use

this term, and exclusively refer to “open neighborhoods”.

XxU7 Topological spaces

109

Question 7.2.3. Show that this is equivalent to f and its inverse both being continuous.

Therefore, any property deﬁned only in terms of open sets is preserved by homeomorphism.
Such a property is called a topological property. However, the later adjectives we
deﬁne (“connected”, “Hausdorﬀ”, “compact”) will all be deﬁned only in terms of the
open sets, so they will be.

§7.2.ii Closed sets

We saw last time there were two equivalent deﬁnitions for closed sets, but one of them
relies only on open sets, and we use it:

Deﬁnition 7.2.4. In a general topological space X, we say that S ⊆ X is closed in X
if the complement X \ S is open in X.
If S ⊆ X is any set, the closure of S, denoted S, is deﬁned as the smallest closed set
containing S.

Thus for general topological spaces, open and closed sets carry the same information,
and it is entirely a matter of taste whether we deﬁne everything in terms of open sets or
closed sets. In particular, you can translate axioms and properties of open sets to closed
ones:

Question 7.2.5. Show that the (possibly inﬁnite) intersection of closed sets is closed while
the union of ﬁnitely many closed sets is closed. (Look at complements.)

Exercise 7.2.6. Show that a function is continuous if and only if the pre-image of every
closed set is closed.

Mathematicians seem to have agreed that they like open sets better.

§7.2.iii Properties that don’t carry over

Not everything works:

Remark 7.2.7 (Complete and (totally) bounded are metric properties) — The two
metric properties we have seen, “complete” and “(totally) bounded”, are not topolog-
ical spaces. They rely on a metric, so as written we cannot apply them to topological
spaces. One might hope that maybe, there is some alternate deﬁnition (like we saw
for “continuous function”) that is just open-set based. But Example 6.3.1 showing
(0, 1) ∼= R tells us that it is hopeless.

Remark 7.2.8 (Sequences don’t work well) — You could also try to port over the
notion of sequences and convergent sequences. However, this turns out to break a
lot of desirable properties. Therefore I won’t bother to do so, and thus if we are
discussing sequences you should assume that we are working with a metric space.

§7.3 Hausdorﬀ spaces

Prototypical example for this section: Every space that’s not the Zariski topology (deﬁned
much later).

110

Napkin, by Evan Chen (v1.5.20190718)

As you might have guessed, there exist topological spaces which cannot be realized
as metric spaces (in other words, are not metrizable). One example is just to take
X = {a, b, c} and the topology τX = {∅,{a, b, c}}. This topology is fairly “stupid”: it
can’t tell apart any of the points a, b, c! But any metric space can tell its points apart
(because d(x, y) > 0 when x (cid:54)= y).
We’ll see less trivial examples later, but for now we want to add a little more sanity
condition onto our spaces. There is a whole hierarchy of such axioms, labelled Tn for
integers n (with n = 0 being the weakest and n = 6 the strongest); these axioms are
called separation axioms.

By far the most common hypothesis is the T2 axiom, which bears a special name.

Deﬁnition 7.3.1. A topological space X is Hausdorﬀ if for any two disjoint points p
and q in X, there exists an open neighborhood U of p and an open neighborhood V of q
such that

U ∩ V = ∅.

In other words, around any two distinct points we should be able to draw disjoint open

neighborhoods. Here’s a picture to go with above, but not much going on.

Question 7.3.2. Show that all metric spaces are Hausdorﬀ.

I just want to deﬁne this here so that I can use this word later. In any case, basically

any space we will encounter other than the Zariski topology is Hausdorﬀ.

§7.4 Subspaces

Prototypical example for this section: S1 is a subspace of R2.

One can also take subspaces of general topological spaces.

Deﬁnition 7.4.1. Given a topological space X, and a subset S ⊆ X, we can make S
into a topological space by declaring that the open subsets of S are U ∩ S for open
U ⊆ X. This is called the subspace topology.

So for example, if we view S1 as a subspace of R2, then any open arc is an open set,

because you can view it as the intersection of an open disk with S1.

Needless to say, for metric spaces it doesn’t matter which of these deﬁnitions I choose.

(Proving this turns out to be surprisingly annoying, so I won’t do so.)

pqS1R27 Topological spaces

§7.5 Connected spaces

111

Prototypical example for this section: [0, 1] ∪ [2, 3] is disconnected.

Even in metric spaces, it is possible for a set to be both open and closed.

Deﬁnition 7.5.1. A subset S of a topological space X is clopen if it is both closed and
open in X. (Equivalently, both S and its complement are open.)

For example ∅ and the entire space are examples of clopen sets. In fact, the presence
of a nontrivial clopen set other than these two leads to a so-called disconnected space.

Question 7.5.2. Show that a space X has a nontrivial clopen set (one other than ∅ and
X) if and only if X can be written as a disjoint union of two nonempty open sets.

We say X is disconnected if there are nontrivial clopen sets, and connected otherwise.

To see why this should be a reasonable deﬁnition, it might help to solve Problem 7A†.

Example 7.5.3 (Disconnected and connected spaces)

(a) The metric space

{(x, y) | x2 + y2 ≤ 1} ∪ {(x, y) | (x − 4)2 + y2 ≤ 1} ⊆ R2

is disconnected (it consists of two disks).

(b) The space [0, 1]∪ [2, 3] is disconnected: it consists of two segments, each of which

is a clopen set.

(c) A discrete space on more than one point is disconnected, since every set is clopen

in the discrete space.

(d) Convince yourself that the set

is a clopen subset of Q. Hence Q is disconnected too – it has gaps.

(cid:8)x ∈ Q : x2 < 2014(cid:9)

(e) [0, 1] is connected.

§7.6 Path-connected spaces

Prototypical example for this section: Walking around in C.

A stronger and perhaps more intuitive notion of a connected space is a path-connected

space. The short description: “walk around in the space”.

Deﬁnition 7.6.1. A path in the space X is a continuous function

Its endpoints are the two points γ(0) and γ(1).

γ : [0, 1] → X.

You can think of [0, 1] as measuring “time”, and so we’ll often write γ(t) for t ∈ [0, 1]

(with t standing for “time”). Here’s a picture of a path.

112

Napkin, by Evan Chen (v1.5.20190718)

Question 7.6.2. Why does this agree with your intuitive notion of what a “path” is?

Deﬁnition 7.6.3. A space X is path-connected if any two points in it are connected
by some path.

Exercise 7.6.4 (Path-connected implies connected). Let X = U (cid:116) V be a disconnected
space. Show that there is no path from a point of U to point V . (If γ : [0, 1] → X, then we
get [0, 1] = γpre(U ) (cid:116) γpre(V ), but [0, 1] is connected.)

Example 7.6.5 (Examples of path-connected spaces)

 R2 is path-connected, since we can “connect” any two points with a straight

line.

 The unit circle S1 is path-connected, since we can just draw the major or

minor arc to connect two points.

§7.7 Homotopy and simply connected spaces

Prototypical example for this section: C and C \ {0}.

Now let’s motivate the idea of homotopy. Consider the example of the complex plane
C (which you can think of just as R2) with two points p and q. There’s a whole bunch of
paths from p to q but somehow they’re not very diﬀerent from one another. If I told you
“walk from p to q” you wouldn’t have too many questions.

So we’re living happily in C until a meteor strikes the origin, blowing it out of existence.
Then suddenly to get from p to q, people might tell you two diﬀerent things: “go left
around the meteor” or “go right around the meteor”.

Xγ(0)γ(1)γCpq7 Topological spaces

113

So what’s happening? In the ﬁrst picture, the red, green, and blue paths somehow all
looked the same: if you imagine them as pieces of elastic string pinned down at p and q,
you can stretch each one to any other one.

But in the second picture, you can’t move the red string to match with the blue string:

there’s a meteor in the way. The paths are actually diﬀerent.3

The formal notion we’ll use to capture this is homotopy equivalence. We want to write
a deﬁnition such that in the ﬁrst picture, the three paths are all homotopic, but the two
paths in the second picture are somehow not homotopic. And the idea is just continuous
deformation.

Deﬁnition 7.7.1. Let α and β be paths in X whose endpoints coincide. A (path)
homotopy from α to β is a continuous function F : [0, 1]2 → X, which we’ll write Fs(t)
for s, t ∈ [0, 1], such that

F0(t) = α(t) and F1(t) = β(t) for all t ∈ [0, 1]

and moreover

α(0) = β(0) = Fs(0) and α(1) = β(1) = Fs(1) for all s ∈ [0, 1].

If a path homotopy exists, we say α and β are path homotopic and write α (cid:39) β.

Abuse of Notation 7.7.2. While I strictly should say “path homotopy” to describe
this relation between two paths, I will shorten this to just “homotopy” instead. Similarly
I will shorten “path homotopic” to “homotopic”.

Animated picture: https://commons.wikimedia.org/wiki/File:HomotopySmall.gif.

Needless to say, (cid:39) is an equivalence relation.

What this deﬁnition is doing is taking α and “continuously deforming” it to β, while
keeping the endpoints ﬁxed. Note that for each particular s, Fs is itself a function. So s
represents time as we deform α to β: it goes from 0 to 1, starting at α and ending at β.

3If you know about winding numbers, you might feel this is familiar. We’ll talk more about this in the

chapter on the fundamental group.

C\{0}pq114

Napkin, by Evan Chen (v1.5.20190718)

Question 7.7.3. Convince yourself the above deﬁnition is right. What goes wrong when
the meteor strikes?

So now I can tell you what makes C special:

Deﬁnition 7.7.4. A space X is simply connected if it’s path-connected and for any
points p and q, all paths from p to q are homotopic.

That’s why you don’t ask questions when walking from p to q in C: there’s really only

one way to walk. Hence the term “simply” connected.

Question 7.7.5. Convince yourself that Rn is simply connected for all n.

§7.8 Bases of spaces

Prototypical example for this section: R has a basis of open intervals, and R2 has a basis
of open disks.

You might have noticed that the open sets of R are a little annoying to describe: the

prototypical example of an open set is (0, 1), but there are other open sets like

(0, 1) ∪(cid:18)1,

3

2(cid:19) ∪(cid:18)2,

7

3(cid:19) ∪ (2014, 2015).

Question 7.8.1. Check this is an open set.

But okay, this isn’t that diﬀerent. All I’ve done is taken a bunch of my prototypes and

threw a bunch of ∪ signs at it. And that’s the idea behind a basis.
Deﬁnition 7.8.2. A basis for a topological space X is a subset B of the open sets such
that every open set in X is a union of some (possibly inﬁnite) number of elements in B.

And all we’re doing is saying

Example 7.8.3 (Basis of R)
The open intervals form a basis of R.

In fact, more generally we have:

CpqF0=αF0.25F0.5F0.75F1=β7 Topological spaces

115

Theorem 7.8.4 (Basis of metric spaces)

The r-neighborhoods form a basis of any metric space M .

Proof. Kind of silly – given an open set U draw an rp-neighborhood Up contained entirely

inside U . Then(cid:83)p Up is contained in U and covers every point inside it.

Hence, an open set in R2 is nothing more than a union of a bunch of open disks, and
so on. The point is that in a metric space, the only open sets you really ever have to
worry too much about are the r-neighborhoods.

§7.9 A few harder problems to think about

Problem 7A†. Let X be a topological space. Show that there exists a nonconstant
continuous function X → {0, 1} if and only if X is disconnected (here {0, 1} is given the
discrete topology).
Problem 7B(cid:63). Let X and Y be topological spaces and let f : X → Y be a continuous
function.

(a) Show that if X is connected then so is f img(X).

(b) Show that if X is path-connected then so is f img(X).

Problem 7C (Hausdorﬀ implies T1 axiom). Let X be a Hausdorﬀ topological space.
Prove that for any point p ∈ X the set {p} is closed.
Problem 7D ([Pu02], Exercise 2.56). Let M be a metric space with more than one point
but at most countably inﬁnitely many points. Show that M is disconnected.

Problem 7E (Furstenberg). We declare a subset of Z to be open if it’s the union
(possibly empty or inﬁnite) of arithmetic sequences {a + nd | n ∈ Z}, where a and d are
positive integers.

(a) Verify this forms a topology on Z, called the evenly spaced integer topology.

(b) Prove there are inﬁnitely many primes by considering(cid:83)p pZ for primes p.
Problem 7F. Prove that the evenly spaced integer topology on Z is metrizable. In
other words, show that one can impose a metric d : Z2 → R which makes Z into a metric
space whose open sets are those described above.
Problem 7G. We know that any open set U ⊆ R is a union of open intervals (allowing
±∞ as endpoints). One can show that it’s actually possible to write U as the union of
pairwise disjoint open intervals.4 Prove that there exists such a disjoint union with at
most countably many intervals in it.

4You are invited to try and prove this, but I personally found the proof quite boring.

8 Compactness

One of the most important notions of topological spaces is that of compactness. It
generalizes the notion of “closed and bounded” in Euclidean space to any topological
space (e.g. see Problem 8F†).

For metric spaces, there are two equivalent ways of formulating compactness:

 A “natural” deﬁnition using sequences, called sequential compactness.

 A less natural deﬁnition using open covers.

As I alluded to earlier, sequences in metric spaces are super nice, but sequences in general
topological spaces suck (to the point where I didn’t bother to deﬁne convergence of
general sequences). So it’s the second deﬁnition that will be used for general spaces.

§8.1 Deﬁnition of sequential compactness

Prototypical example for this section: [0, 1] is compact, but (0, 1) is not.

To emphasize, compactness is one of the best possible properties that a metric space

can have.

Deﬁnition 8.1.1. A subsequence of an inﬁnite sequence x1, x2, . . . is exactly what it
sounds like: a sequence xi1, xi2, . . . where i1 < i2 < ··· are positive integers. Note that
the sequence is required to be inﬁnite.

Another way to think about this is “selecting inﬁnitely many terms” or “deleting some

terms” of the sequence, depending on whether your glass is half empty or half full.

Deﬁnition 8.1.2. A metric space M is sequentially compact if every sequence has a
subsequence which converges.

This time, let me give some non-examples before the examples.

Example 8.1.3 (Non-examples of compact metric spaces)
(a) The space R is not compact: consider the sequence 1, 2, 3, 4, . . . . Any subsequence

explodes, hence R cannot possibly be compact.

(b) More generally, if a space is not bounded it cannot be compact. (You can prove

this if you want.)

(c) The open interval (0, 1) is bounded but not compact: consider the sequence
4 , . . . . No subsequence can converge to a point in (0, 1) because the sequence

3 , 1

1

2 , 1
“converges to 0”.

(d) More generally, any space which is not complete cannot be compact.

Now for the examples!

117

118

Napkin, by Evan Chen (v1.5.20190718)

Question 8.1.4. Show that a ﬁnite set is compact. (Pigeonhole Principle.)

Example 8.1.5 (Examples of compact spaces)
Here are some more examples of compact spaces. I’ll prove they’re compact in just a
moment; for now just convince yourself they are.

(a) [0, 1] is compact. Convince yourself of this! Imagine having a large number of

dots in the unit interval. . .

(b) The surface of a sphere, S2 =(cid:8)(x, y, z) | x2 + y2 + z2 = 1(cid:9) is compact.
(c) The unit ball B2 =(cid:8)(x, y) | x2 + y2 ≤ 1(cid:9) is compact.

(d) The Hawaiian earring living in R2 is compact: it consists of mutually tangent

circles of radius 1

n for each n, as in Figure 8.1.

Figure 8.1: Hawaiian Earring.

To aid in generating more examples, we remark:

Proposition 8.1.6 (Closed subsets of compacts)

Closed subsets of sequentially compact sets are compact.

Question 8.1.7. Prove this. (It should follow easily from deﬁnitions.)

We need to do a bit more work for these examples, which we do in the next section.

§8.2 Criteria for compactness

Theorem 8.2.1 (Tychonoﬀ’s theorem)
If X and Y are compact spaces, then so is X × Y .

Proof. Problem 8E.

We also have:

8 Compactness

119

Theorem 8.2.2 (The interval is compact)

[0, 1] is compact.

Proof. Killed by Problem 8F†; however, here is a sketch of a direct proof. Split [0, 1] into
[0, 1
2 ] ∪ [ 1
2 , 1]. By Pigeonhole, inﬁnitely many terms of the sequence lie in the left half
(say); let x1 be the ﬁrst one and then keep only the terms in the left half after x1. Now
split [0, 1
2 ]. Again, by Pigeonhole, inﬁnitely many terms fall in some
half; pick one of them, call it x2. Rinse and repeat. In this way we generate a sequence
x1, x2, . . . which is Cauchy, implying that it converges since [0, 1] is complete.

2 ] into [0, 1

4 ] ∪ [ 1

4 , 1

Now we can prove the main theorem about Euclidean space: in Rn, compactness is

equivalent to being “closed and bounded”.

Theorem 8.2.3 (Bolzano-Weierstraß)
A subset of Rn is compact if and only if it closed and bounded.

Question 8.2.4. Why does this imply the spaces in our examples are compact?

Proof. Well, look at a closed and bounded S ⊆ Rn. Since it’s bounded, it lives inside
some box [a1, b1] × [a2, b2] × ··· × [an, bn]. By Tychonoﬀ’s theorem, since each [ai, bi] is
compact the entire box is. Since S is a closed subset of this compact box, we’re done.

One really has to work in Rn for this to be true! In other spaces, this criterion can

easily fail.

Example 8.2.5 (Closed and bounded but not compact)
Let S = {s1, s2, . . .} be any inﬁnite set equipped with the discrete metric. Then S
is closed (since all convergent sequences are constant sequences) and S is bounded
(all points are a distance 1 from each other) but it’s certainly not compact since the
sequence s1, s2, . . . doesn’t converge.

The Bolzano-Weierstrass theorem, which is Problem 8F†, tells you exactly which sets

are compact in metric spaces in a geometric way.

§8.3 Compactness using open covers

Prototypical example for this section: [0, 1] is compact.

There’s a second related notion of compactness which I’ll now deﬁne. The following

deﬁnitions might appear very unmotivated, but bear with me.

Deﬁnition 8.3.1. An open cover of a topological space X is a collection of open sets
{Uα} (possibly inﬁnite or uncountable) which cover it: every point in X lies in at least
one of the Uα, so that

Such a cover is called an open cover.

A subcover is exactly what it sounds like: it takes only some of the Uα, while ensuring

that X remains covered.

X =(cid:91) Uα.

120

Some art:

Napkin, by Evan Chen (v1.5.20190718)

Deﬁnition 8.3.2. A topological space X is quasicompact if every open cover has a
ﬁnite subcover. It is compact if it is also Hausdorﬀ.

Remark 8.3.3 — The “Hausdorﬀ” hypothesis that I snuck in is a sanity condition
which is not worth worrying about unless you’re working on the algebraic geometry
chapters, since all the spaces you will deal with are Hausdorﬀ. (In fact, some authors
don’t even bother to include it.) For example all metric spaces are Hausdorﬀ and
thus this condition can be safely ignored if you are working with metric spaces.

What does this mean? Here’s an example:

Example 8.3.4 (Example of a ﬁnite subcover)
Suppose we cover the unit square M = [0, 1]2 by putting an open disk of diameter
1 centered at every point (trimming any overﬂow). This is clearly an open cover
because, well, every point lies in many of the open sets, and in particular is the
center of one.

But this is way overkill – we only need about four of these circles to cover the

whole square. That’s what is meant by a “ﬁnite subcover”.

Why do we care? Because of this:

Theorem 8.3.5 (Sequentially compact ⇐⇒ compact)
A metric space M is sequentially compact if and only if it is compact.

XX=SαUα8 Compactness

121

We defer the proof to the last section.

This gives us the motivation we wanted for our deﬁnition. Sequential compactness was
a condition that made sense. The open-cover deﬁnition looked strange, but it turned out
to be equivalent. But we now prefer it, because we have seen that whenever possible we
want to resort to open-set-only based deﬁnitions: so that e.g. they are preserved under
homeomorphism.

Example 8.3.6 (An example of non-compactness)
The space X = [0, 1) is not compact in either sense. We can already see it is not
sequentially compact, because it is not even complete (look at xn = 1 − 1
n ). To see
it is not compact under the covering deﬁnition, consider the sets

Um =(cid:20)0, 1 −

1

m + 1(cid:19)

for m = 1, 2, . . . . Then X =(cid:83) Ui; hence the Ui are indeed a cover. But no ﬁnite

collection of the Ui’s will cover X.

Question 8.3.7. Convince yourself that [0, 1] is compact; this is a little less intuitive than
it being sequentially compact.

Abuse of Notation 8.3.8. Thus, we’ll never call a metric space “sequentially compact”
again — we’ll just say “compact”. (Indeed, I kind of already did this in the previous few
sections.)

§8.4 Applications of compactness

Compactness lets us reduce inﬁnite open covers to ﬁnite ones. Actually, it lets us do this
even if the open covers are blithely stupid. Very often one takes an open cover consisting
of an open neighborhood of x ∈ X for every single point x in the space; this is a huge
number of open sets, and yet compactness lets us reduce to a ﬁnite set.

To give an example of a typical usage:

Proposition 8.4.1 (Compact =⇒ totally bounded)
Let M be compact. Then M is totally bounded.

Proof using covers. For every point p ∈ M , take an ε-neighborhood of p, say Up. These
cover M for the horrendously stupid reason that each point p is at the very least covered
by its open neighborhood Up. Compactness then lets us take a ﬁnite subcover.

Next, an important result about maps between compact spaces.

Theorem 8.4.2 (Images of compacts are compact)
Let f : X → Y be a continuous function, where X is compact. Then the image

is compact.

f img(X) ⊆ Y

122

Napkin, by Evan Chen (v1.5.20190718)

Proof using covers. Take any open cover {Vα} in Y of f img(X). By continuity of f , it
pulls back to an open cover {Uα} of X. Thus some ﬁnite subcover of this covers X. The
corresponding V ’s cover f img(X).

Question 8.4.3. Give another proof using the sequential deﬁnitions of continuity and
compactness. (This is even easier.)

Some nice corollaries of this:

Corollary 8.4.4 (Extreme value theorem)
Let X be compact and consider a continuous function f : X → R. Then f achieves
a maximum value at some point, i.e. there is a point p ∈ X such that f (p) ≥ f (q)
for any other q ∈ X.

Corollary 8.4.5 (Intermediate value theorem)
Consider a continuous function f : [0, 1] → R. Then the image of f is of the form
[a, b] for some real numbers a ≤ b.

Sketch of Proof. The point is that the image of f is compact in R, and hence closed
and bounded. You can convince yourself that the closed sets are just unions of closed
intervals. That implies the extreme value theorem.

When X = [0, 1], the image is also connected, so there should only be one closed
interval in f img([0, 1]). Since the image is bounded, we then know it’s of the form [a, b].
(To give a full proof, you would use the so-called least upper bound property, but that’s a
little involved for a bedtime story; also, I think R is boring.)

Example 8.4.6 (1/x)
The compactness hypothesis is really important here. Otherwise, consider the
function

(0, 1) → R by

x (cid:55)→

1
x

.

This function (which you plot as a hyperbola) is not bounded; essentially, you can
see graphically that the issue is we can’t extend it to a function on [0, 1] because it
explodes near x = 0.

One last application:

if M is a compact metric space, then continuous functions

f : M → N are continuous in an especially “nice” way:
Deﬁnition 8.4.7. A function f : M → N of metric spaces is called uniformly contin-
uous if for any ε > 0, there exists a δ > 0 (depending only on ε) such that whenever
dM (x, y) < δ we also have dN (f x, f y) < ε.

The name means that for ε > 0, we need a δ that works for every point of M .

8 Compactness

123

Example 8.4.8 (Uniform continuity)
(a) The functions R to R of the form x (cid:55)→ ax + b are all uniformly continuous, since

one can always take δ = ε/|a| (or δ = 1 if a = 0).

(b) Actually, it is true that a diﬀerentiable function R → R with a bounded derivative
is uniformly continuous. (The converse is false for the reason that uniformly
continuous doesn’t imply diﬀerentiable at all.)

(c) The function f : R → R by x (cid:55)→ x2 is not uniformly continuous, since for large
x, tiny δ changes to x lead to fairly large changes in x2. (If you like, you can try
to prove this formally now.)
Think f (2017.01) − f (2017) > 40; even when δ = 0.01, one can still cause large
changes in f .

(d) However, when restricted to (0, 1) or [0, 1] the function x (cid:55)→ x2 becomes uniformly

continuous. (For ε > 0 one can now pick for example δ = min{1, ε}/3.)

(e) The function (0, 1) → R by x (cid:55)→ 1/x is not uniformly continuous (same reason

as before).

Now, as promised:

Proposition 8.4.9 (Continuous on compact =⇒ uniformly continuous)
If M is compact and f : M → N is continuous, then f is uniformly continuous.

Proof using sequences. Fix ε > 0, and assume for contradiction that for every δ = 1/k
there exists points xk and yk within δ of each other but with images ε > 0 apart. By
compactness, take a convergent subsequence xik → p. Then yik → p as well, since the
xk’s and yk’s are close to each other. So both sequences f (xik ) and f (yik ) should converge
to f (p) by sequential continuity, but this can’t be true since the two sequences are always
ε apart.

§8.5 (Optional) Equivalence of formulations of compactness

We will prove that:

Theorem 8.5.1 (Heine-Borel for general metric spaces)

For a metric space M , the following are equivalent:

(i) Every sequence has a convergent subsequence,

(ii) The space M is complete and totally bounded, and

(iii) Every open cover has a ﬁnite subcover.

We leave the proof that (i) ⇐⇒ (ii) as Problem 8F†; the idea of the proof is much in
the spirit of Theorem 8.2.2.

Proof that (i) and (ii) =⇒ (iii). We prove the following lemma, which is interesting in
its own right.

124

Napkin, by Evan Chen (v1.5.20190718)

Lemma 8.5.2 (Lebesgue number lemma)
Let M be a compact metric space and {Uα} an open cover. Then there exists a
real number δ > 0, called a Lebesgue number for that covering, such that the
δ-neighborhood of any point p lies entirely in some Uα.

Proof of lemma. Assume for contradiction that for every δ = 1/k there is a point xk ∈ M
such that its 1/k-neighborhood isn’t contained in any Uα. In this way we construct a
sequence x1, x2, . . . ; thus we’re allowed to take a subsequence which converges to some
x. Then for every ε > 0 we can ﬁnd an integer n such that d(xn, x) + 1/n < ε; thus
the ε-neighborhood at x isn’t contained in any Uα for every ε > 0. This is impossible,
(cid:4)
because we assumed x was covered by some open set.

Now, take a Lebesgue number δ for the covering. Since M is totally bounded, ﬁnitely
many δ-neighborhoods cover the space, so ﬁnitely many Uα do as well.

Proof that (iii) =⇒ (ii). One step is immediate:

Question 8.5.3. Show that the covering condition =⇒ totally bounded.

The tricky part is showing M is complete. Assume for contradiction it isn’t and thus
that the sequence (xk) is Cauchy, but it doesn’t converge to any particular point.

Question 8.5.4. Show that this implies for each p ∈ M , there is an εp-neighborhood Up
which contains at most ﬁnitely many of the points of the sequence (xk). (You will have to
use the fact that xk (cid:54)→ p and (xk) is Cauchy.)

Now if we consider M =(cid:83)p Up we get a ﬁnite subcover of these open neighborhoods; but

this ﬁnite subcover can only cover ﬁnitely many points of the sequence, by contradiction.

§8.6 A few harder problems to think about

The later problems are pretty hard; some have the ﬂavor of IMO 3/6-style constructions.
It’s important to draw lots of pictures so one can tell what’s happening. Of these
Problem 8F† is deﬁnitely my favorite.

Problem 8A. Show that [0, 1] and (0, 1) are not homeomorphic.

Problem 8B. Let X be a topological space with the discrete topology. Under what
conditions is X compact?

Problem 8C (The coﬁnite topology is quasicompact only). We let X be an inﬁnite
set and equip it with the coﬁnite topology: the open sets are the empty set and
complements of ﬁnite sets. This makes X into a topological space. Show that X is
quasicompact but not Hausdorﬀ.

Problem 8D (Cantor’s intersection theorem). Let X be a compact topological space,
and suppose

is an inﬁnite sequence of nested nonempty closed subsets. Show that(cid:84)n≥0 Kn (cid:54)= ∅.

X = K0 ⊇ K1 ⊇ K2 ⊇ . . .

8 Compactness

125

Problem 8E (Tychonoﬀ’s theorem). Let X and Y be compact metric spaces. Show
that X × Y is compact. (This is also true for general topological spaces, but the proof is
surprisingly hard, and we haven’t even deﬁned X × Y in general yet.)
Problem 8F† (Bolzano-Weierstraß theorem for general metric spaces). Prove that a
metric space M is sequentially compact if and only if it is complete and totally bounded.

Problem 8G (Almost Arzel`a-Ascoli theorem). Let f1, f2, . . . : [0, 1] → [−100, 100] be
an equicontinuous sequence of functions, meaning

∀ε > 0 ∃δ > 0 ∀n ∀x, y

(|x − y| < δ =⇒ |fn(x) − fn(y)| < ε)

Show that we can extract a subsequence fi1, fi2, . . . of these functions such that for every
x ∈ [0, 1], the sequence fi1(x), fi2(x), . . . converges.
Problem 8H. In this problem a “circle” refers to the boundary of a disk with nonzero
radius.

(a) Is it possible to partition the plane R2 into disjoint circles?

(b) From the plane R2 we delete two distinct points p and q. Is it possible to partition

the remaining points into disjoint circles?

IV

Linear Algebra

Part IV: Contents

9 Vector spaces

131
. . . . . . . . . . . . . . . . . . . . . . . . . . 131
9.1 The deﬁnitions of a ring and ﬁeld
9.2 Modules and vector spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
9.3 Direct sums . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
. . . . . . . . . . . . . . . . . . . . . . . . 135
9.4 Linear independence, spans, and basis
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
9.5 Linear maps
9.6 What is a matrix? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
9.7 Subspaces and picking convenient bases . . . . . . . . . . . . . . . . . . . . . . . . 140
9.8 A cute application: Lagrange interpolation . . . . . . . . . . . . . . . . . . . . . . 142
(Digression) Arrays of numbers are evil . . . . . . . . . . . . . . . . . . . . . . . . 143
9.9
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
9.10 A word on general modules
. . . . . . . . . . . . . . . . . . . . . . . . 145
9.11 A few harder problems to think about

10 Eigen-things

147
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147
10.1 Why you should care
10.2 Warning on assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
10.3 Eigenvectors and eigenvalues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
10.4 The Jordan form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
10.5 Nilpotent maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
10.6 Reducing to the nilpotent case . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152
10.7 (Optional) Proof of nilpotent Jordan . . . . . . . . . . . . . . . . . . . . . . . . . 153
10.8 Algebraic and geometric multiplicity . . . . . . . . . . . . . . . . . . . . . . . . . 154
. . . . . . . . . . . . . . . . . . . . . . . . 155
10.9 A few harder problems to think about

11 Dual space and trace

157
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
11.1 Tensor product
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
11.2 Dual space
11.3 V ∨ ⊗ W gives matrices from V to W . . . . . . . . . . . . . . . . . . . . . . . . . 161
11.4 The trace . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
. . . . . . . . . . . . . . . . . . . . . . . . 163
11.5 A few harder problems to think about

12 Determinant

165
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
12.1 Wedge product
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
12.2 The determinant
12.3 Characteristic polynomials, and Cayley-Hamilton . . . . . . . . . . . . . . . . . . . 169
. . . . . . . . . . . . . . . . . . . . . . . . 171
12.4 A few harder problems to think about

13 Inner product spaces

173
13.1 The inner product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176
13.2 Norms
13.3 Orthogonality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177
13.4 Hilbert spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
. . . . . . . . . . . . . . . . . . . . . . . . 180
13.5 A few harder problems to think about

14 Bonus: Fourier analysis

181
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181
14.1 Synopsis
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 181
14.2 A reminder on Hilbert spaces
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182
14.3 Common examples
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 186
14.4 Summary, and another teaser
14.5 Parseval and friends . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186
14.6 Application: Basel problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
14.7 Application: Arrow’s Impossibility Theorem . . . . . . . . . . . . . . . . . . . . . 188
. . . . . . . . . . . . . . . . . . . . . . . . 190
14.8 A few harder problems to think about

15 Duals, adjoint, and transposes

191
15.1 Dual of a map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191

TABLE OF CONTENTS

129

15.2 Identifying with the dual space . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192
. . . . . . . . . . . . . . . . . . . . . . . . . . 193
15.3 The adjoint (conjugate transpose)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195
15.4 Eigenvalues of normal maps
. . . . . . . . . . . . . . . . . . . . . . . . 196
15.5 A few harder problems to think about

9 Vector spaces

This is a pretty light chapter. The point of it is to just deﬁne what a vector space and

a basis are. These are intuitive concepts that you may already know.

§9.1 The deﬁnitions of a ring and ﬁeld

Prototypical example for this section: Z, R, and C are rings; the latter two are ﬁelds.
I’ll very informally deﬁne a ring/ﬁeld here, in case you skipped the earlier chapter.

 A ring is a structure with a commutative addition and multiplication, as well as
subtraction, like Z. It also has an additive identity 0 and multiplicative identity 1.

 If the multiplication is invertible like in R or C, (meaning 1

x makes sense for any

x (cid:54)= 0), then the ring is called a ﬁeld.

In fact, if you replace “ﬁeld” by “R” everywhere in what follows, you probably won’t
lose much. It’s customary to use the letter R for rings, and k or K for ﬁelds.
Finally, in case you skipped the chapter on groups, I should also mention:

 An additive abelian group is a structure with a commutative addition, as well
as subtraction, plus an additive identity 0. It doesn’t have to have multiplication.
A good example is R3 (with addition componentwise).

§9.2 Modules and vector spaces

Prototypical example for this section: Polynomials of degree at most n.

You intuitively know already that Rn is a “vector space”: its elements can be added
together, and there’s some scaling by real numbers. Let’s develop this more generally.

Fix a commutative ring R. Then informally,

An R-module is any structure where you can add two elements and scale
by elements of R.

Moreover, a vector space is just a module whose commanding ring is actually a ﬁeld.
I’ll give you the full deﬁnition in a moment, but ﬁrst, examples. . .

Example 9.2.1 (Quadratic polynomials, aka my favorite example)
My favorite example of an R-vector space is the set of polynomials of degree at most
two, namely

(cid:8)ax2 + bx + c | a, b, c ∈ R(cid:9) .

Indeed, you can add any two quadratics, and multiply by constants. You can’t
multiply two quadratics to get a quadratic, but that’s irrelevant – in a vector space
there need not be a notion of multiplying two vectors together.

In a sense we’ll deﬁne later, this vector space has dimension 3 (as expected!).

131

132

Napkin, by Evan Chen (v1.5.20190718)

Example 9.2.2 (All polynomials)
The set of all polynomials with real coeﬃcients is an R-vector space, because you
can add any two polynomials and scale by constants.

Example 9.2.3 (Euclidean space)

(a) The complex numbers

{a + bi | a, b ∈ R}

form a real vector space. As we’ll see later, it has “dimension 2”.

(b) The real numbers R form a real vector space of dimension 1.

(c) The set of 3D vectors

{(x, y, z) | x, y, z ∈ R}

forms a real vector space, because you can add any two triples component-wise.
Again, we’ll later explain why it has “dimension 3”.

Example 9.2.4 (More examples of vector spaces)

(a) The set

Q[√2] =(cid:110)a + b√2 | a, b ∈ Q(cid:111)

has a structure of a Q-vector space in the obvious fashion: one can add any two
elements, and scale by rational numbers. (It is not a real vector space – why?)

(b) The set

{(x, y, z) | x + y + z = 0 and x, y, z ∈ R}

is a 2-dimensional real vector space.

(c) The set of all functions f : R → R is also a real vector space (since the notions

f + g and c · f both make sense for c ∈ R).

Now let me write the actual rules for how this multiplication behaves.

Deﬁnition 9.2.5. Let R be a commutative ring. An R-module is an additive abelian
group M = (M, +) equipped with a left multiplication by elements of R. This multipli-
cation must satisfy the following properties for every r1, r2 ∈ R and m ∈ M :
(i) r1 · (r2 · m) = (r1r2) · m.
(ii) Multiplication is distributive, meaning

(r1 + r2) · m = r1 · m + r2 · m and r · (m1 + m2) = r · m1 + r · m2.

(iii) 1R · m = m.
(iv) 0R · m = 0M . (This is actually extraneous; one can deduce it from the ﬁrst three.)
If R is a ﬁeld we say M is an R-vector space; its elements are called vectors and the
members of R are called scalars.

9 Vector spaces

133

Abuse of Notation 9.2.6. In the above, we’re using the same symbol + for the addition
of M and the addition of R. Sorry about that, but it’s kind of hard to avoid, and the
point of the axioms is that these additions should be related. I’ll try to remember to put
r · m for the multiplication of the module and just r1r2 for the multiplication of R.

Question 9.2.7. In Example 9.2.1, I was careful to say “degree at most 2” instead of
“degree 2”. What’s the reason for this? In other words, why is

not an R-vector space?

(cid:8)ax2 + bx + c | a, b, c ∈ R, a (cid:54)= 0(cid:9)

A couple less intuitive but somewhat important examples. . .

Example 9.2.8 (Abelian groups are Z-modules)
(Skip this example if you’re not comfortable with groups.)

(a) The example of real polynomials

(cid:8)ax2 + bx + c | a, b, c ∈ R(cid:9)

is also a Z-module! Indeed, we can add any two such polynomials, and we can
scale them by integers.

(b) The set of integers modulo 100, say Z/100Z, is a Z-module as well. Can you see

how?

(c) In fact, any abelian group G = (G, +) is a Z-module. The multiplication can be

deﬁned by

for n ≥ 0. (Here −g is the additive inverse of g.)

(cid:124)

(cid:125)

n · g = g + ··· + g

(−n) · g = n · (−g)

n times

(cid:123)(cid:122)

Example 9.2.9 (Every ring is its own module)
(a) R can be thought of as an R-vector space over itself. Can you see why?

(b) By the same reasoning, we see that any commutative ring R can be thought of

as an R-module over itself.

§9.3 Direct sums

Prototypical example for this section: {ax2 + bx + c} = R ⊕ xR ⊕ x2R, and R2 is the sum
of its axes.

Let’s return to Example 9.2.1, and consider

V =(cid:8)ax2 + bx + c | a, b, c ∈ R(cid:9) .

Even though I haven’t told you what a dimension is, you can probably see that this
vector space “should have” dimension 3. We’ll get to that in a moment.

134

Napkin, by Evan Chen (v1.5.20190718)

The other thing you may have noticed is that somehow the x2, x and 1 terms don’t
“talk to each other”. They’re totally unrelated. In other words, we can consider the three
sets

x2R :=(cid:8)ax2 | a ∈ R(cid:9)
xR := {bx | b ∈ R}
R := {c | c ∈ R} .

In an obvious way, each of these can be thought of as a “copy” of R.

Then V quite literally consists of the “sums of these sets”. Speciﬁcally, every element
of V can be written uniquely as the sum of one element from each of these sets. This
motivates us to write

The notion which captures this formally is the direct sum.

V = x2R ⊕ xR ⊕ R.

Deﬁnition 9.3.1. Let M be an R-module. Let M1 and M2 be subsets of M which are
themselves R-modules. Then we write M = M1 ⊕ M2 and say M is a direct sum of M1
and M2 if every element from M can be written uniquely as the sum of an element from
M1 and M2.

Example 9.3.2 (Euclidean plane)
Take the vector space R2 = {(x, y) | x ∈ R, y ∈ R}. We can consider it as a direct
sum of its x-axis and y-axis:

X = {(x, 0) | x ∈ R} and Y = {(0, y) | y ∈ R} .

Then R2 = X ⊕ Y .

This gives us a “top-down” way to break down modules into some disconnected

components.

By applying this idea in reverse, we can also construct new vector spaces as follows. In
a very unfortunate accident, the two names and notations for technically distinct things
are exactly the same.

Deﬁnition 9.3.3. Let M and N be R-modules. We deﬁne the direct sum M ⊕ N to
be the R-module whose elements are pairs (m, n) ∈ M × N . The operations are given by

(m1, n1) + (m2, n2) = (m1 + m2, n1 + n2).

and

r · (m, n) = (r · m, r · n).

For example, while we technically wrote R2 = X ⊕ Y , since each of X and Y is just a
copy of R, we may as well have written R2 ∼= R ⊕ R.
Abuse of Notation 9.3.4. The above illustrates an abuse of notation in the way we
write a direct sum. The symbol ⊕ has two meanings.

 If V is a given space and W1 and W2 are subspaces, then V = W1 ⊕ W2 means

that “V splits as a direct sum W1 ⊕ W2” in the way we deﬁned above.

9 Vector spaces

135

 If W1 and W2 are two unrelated spaces, then W1 ⊕ W2 is deﬁned as the vector

space whose elements are pairs (w1, w2) ∈ W1 × W2.

You can see that these deﬁnitions “kind of” coincide.

In this way, you can see that V should be isomorphic to R ⊕ R ⊕ R; we had V =
x2R ⊕ xR ⊕ R, but the 1, x, x2 don’t really talk to each other and each of the summands
is really just a copy of R at heart.

Deﬁnition 9.3.5. We can also deﬁne, for every positive integer n, the module

M⊕n := M ⊕ M ⊕ ··· ⊕ M
(cid:125)

(cid:123)(cid:122)

n times

(cid:124)

.

§9.4 Linear independence, spans, and basis

Prototypical example for this section: (cid:8)1, x, x2(cid:9) is a basis of(cid:8)ax2 + bx + c | a, b, c ∈ R(cid:9).

The idea of a basis, the topic of this section, gives us another way to capture the notion

that

V =(cid:8)ax2 + bx + c | a, b, c ∈ R(cid:9)

is just sums of copies of {1, x, x2}. This section should be very intuitive, if technical. If
you can’t see why the theorems here “should” be true, you’re doing it wrong.
Let M be an R-module now. We deﬁne three very classical notions that you likely are

already familiar with. If not, fall upon your notion of Euclidean space or V above.

Deﬁnition 9.4.1. A linear combination of some vectors v1, . . . , vn is a sum of the
form r1v1 + ··· + rnvn, where r1, . . . , rn ∈ R. The linear combination is called trivial if
r1 = r2 = ··· = rn = 0R, and nontrivial otherwise.
Deﬁnition 9.4.2. Consider a ﬁnite set of vectors v1, . . . , vn in a module M .

 It is called linearly independent if there is no nontrivial linear combination with
value 0M . (Observe that 0M = 0 · v1 + 0 · v2 + ··· + 0 · vn is always true – the
assertion is that there is no other way to express 0M in this form.)

 It is called a generating set if every v ∈ M can be written as a linear combination

of the {vi}. If M is a vector space we say it is spanning instead.

 It is called a basis (plural bases) if every v ∈ M can be written uniquely as a

linear combination of the {vi}.

The same deﬁnitions apply for an inﬁnite set, with the proviso that all sums must be
ﬁnite.

So by deﬁnition, (cid:8)1, x, x2(cid:9) is a basis for V .
It’s not the only one: {2, x, x2} and
{x + 4, x − 2, x2 + x} are other examples of bases, though not as natural. However, the
set S = {3 + x2, x + 1, 5 + 2x + x2} is not a basis; it fails for two reasons:
 Note that 0 = (3 + x2) + 2(x + 1) − (5 + 2x + x2). So the set S is not linearly

independent.

 It’s not possible to write x2 as a sum of elements of S. So S fails to be spanning.

With these new terms, we can just say a basis is a linearly independent and spanning set.

136

Napkin, by Evan Chen (v1.5.20190718)

Example 9.4.3 (More example of bases)

(a) Regard Q[√2] =(cid:8)a + b√2 | a, b ∈ Q(cid:9) as a Q-vector space. Then {1,√2} is a

basis.

(b) If V is the set of all real polynomials, there is an inﬁnite basis {1, x, x2, . . .}. The
condition that we only use ﬁnitely many terms just says that the polynomials
must have ﬁnite degree (which is good).

(c) Let V = {(x, y, z) | x + y + z = 0 and x, y, z ∈ R}. Then we expect there to be
a basis of size 2, but unlike previous examples there is no immediately “obvious”
choice. Some working examples include:

 (1,−1, 0) and (1, 0,−1),
 (0, 1,−1) and (1, 0,−1),
 (5, 3,−8) and (2,−1,−1).

Exercise 9.4.4. Show that a set of vectors is a basis if and only if it is linearly independent
and spanning. (Think about the polynomial example if you get stuck.)

Now we state a few results which assert that bases in vector spaces behave as nicely as

possible.

Theorem 9.4.5 (Maximality and minimality of bases)
Let V be a vector space over some ﬁeld k and take e1, . . . , en ∈ V . The following
are equivalent:

(a) The ei form a basis.

(b) The ei are spanning, but no proper subset is spanning.

(c) The ei are linearly independent, but adding any other element of V makes them

not linearly independent.

Remark 9.4.6 — If we replace V by a general module M over a commutative ring
R, then (a) =⇒ (b) and (a) =⇒ (c) but not conversely.

Proof. Straightforward, do it yourself if you like. The key point to notice is that you
need to divide by scalars for the converse direction, hence V is required to be a vector
space instead of just a module for the implications (b) =⇒ (a) and (c) =⇒ (a).

Theorem 9.4.7 (Dimension theorem for vector spaces)

If a vector space V has a ﬁnite basis, then every other basis has the same number of
elements.

Proof. We prove something stronger: Assume v1, . . . , vn is a spanning set while w1, . . . , wm
is linearly independent. We claim that n ≥ m.

9 Vector spaces

137

Question 9.4.8. Show that this claim is enough to imply the theorem.

Let A0 = {v1, . . . , vn} be the spanning set. Throw in w1: by the spanning condition,

w1 = c1v1 + ··· + cnvn. There’s some nonzero coeﬃcient, say cn. Thus

vn =

1
cn

w1 −

c1
cn

v1 −

c2
cn

v2 − . . . .

Thus A1 = {v1, . . . , vn−1, w1} is spanning. Now do the same thing, throwing in w2, and
deleting some element of the vi as before to get A2; the condition that the wi are linearly
independent ensures that some vi coeﬃcient must always not be zero. Since we can
eventually get to Am, we have n ≥ m.

Remark 9.4.9 (Generalizations) —

 The theorem is true for an inﬁnite basis
as well if we interpret “the number of elements” as “cardinality”. This is
confusing on a ﬁrst read through, so we won’t elaborate.

 In fact, this is true for modules over any commutative ring. Interestingly, the
proof for the general case proceeds by reducing to the case of a vector space.

The dimension theorem, true to its name, lets us deﬁne the dimension of a vector
space as the size of any ﬁnite basis, if one exists. When it does exist we say V is
ﬁnite-dimensional. So for example,

has dimension three, because(cid:8)1, x, x2(cid:9) is a basis. That’s not the only basis: we could as

well have written

V =(cid:8)ax2 + bx + c | a, b, c ∈ R(cid:9)
(cid:8)a(x2 − 4x) + b(x + 2) + c | a, b, c ∈ R(cid:9)

and gotten the exact same vector space. But the beauty of the theorem is that no matter
how we try to contrive the generating set, we always will get exactly three elements.
That’s why it makes sense to say V has dimension three.

On the other hand, the set of all polynomials R[x] is inﬁnite-dimensional (which should

be intuitively clear).

A basis e1, . . . , en of V is really cool because it means that to specify v ∈ V , I just
have to specify a1, . . . , an ∈ k, and then let v = a1e1 + ··· + anen. You can even think of
v as just (a1, . . . , an). To put it another way, if V is a k-vector space we always have

V = e1k ⊕ e2k ⊕ ··· ⊕ enk.

§9.5 Linear maps

Prototypical example for this section: Evaluation of {ax2 + bx + c} at x = 3.

We’ve seen homomorphisms and continuous maps. Now we’re about to see linear maps,

the structure preserving maps between vector spaces. Can you guess the deﬁnition?

Deﬁnition 9.5.1. Let V and W be vector spaces over the same ﬁeld k. A linear map
is a map T : V → W such that:
(i) We have T (v1 + v2) = T (v1) + T (v2) for any v1, v2 ∈ V .1
1In group language, T is a homomorphism (V, +) → (W, +).

138

Napkin, by Evan Chen (v1.5.20190718)

(ii) For any a ∈ k and v ∈ V , T (a · v) = a · T (v).
If this map is a bijection (equivalently, if it has an inverse), it is an isomorphism. We
then say V and W are isomorphic vector spaces and write V ∼= W .

Example 9.5.2 (Examples of linear maps)

(a) For any vector spaces V and W there is a trivial linear map sending everything

to 0W ∈ W .

(b) For any vector space V , there is the identity isomorphism id : V → V .
(c) The map R3 → R by (a, b, c) (cid:55)→ 4a + 2b + c is a linear map.
(d) Let V be the set of real polynomials of degree at most 2. The map R3 → V by

(a, b, c) (cid:55)→ ax2 + bx + c is an isomorphism.

(e) Let V be the set of real polynomials of degree at most 2. The map V → R by
ax2 + bx + c (cid:55)→ 9a + 3b + c is a linear map, which can be described as “evaluation
at 3”.

is a linear map.

(f) Let W be the set of functions R → R. The evaluation map W → R by f (cid:55)→ f (0)
(g) There is a map of Q-vector spaces Q[√2] → Q[√2] called “multiply by √2”; this
map sends a + b√2 (cid:55)→ 2b + a√2. This map is an isomorphism, because it has an
inverse “multiply by 1/√2”.

In the expression T (a · v) = a · T (v), note that the ﬁrst · is the multiplication of V and
the second · is the multiplication of W . Note that this notion of isomorphism really only
cares about the size of the basis:

Proposition 9.5.3 (n-dimensional vector spaces are isomorphic)
If V is an n-dimensional vector space, then V ∼= k⊕n.

Question 9.5.4. Let e1, . . . , en be a basis for V . What is the isomorphism? (Your ﬁrst
guess is probably right.)

Remark 9.5.5 — You could technically say that all ﬁnite-dimensional vector spaces
are just k⊕n and that no other space is worth caring about. But this seems kind of
rude. Spaces often are more than just triples: ax2 + bx + c is a polynomial, and so
it has some “essence” to it that you’d lose if you just compressed it into (a, b, c).

Moreover, a lot of spaces, like the set of vectors (x, y, z) with x + y + z = 0, do not
have an obvious choice of basis. Thus to cast such a space into k⊕n would require
you to make arbitrary decisions.

§9.6 What is a matrix?

Now I get to tell you what a matrix is: it’s a way of writing a linear map in terms of
bases.

9 Vector spaces

139

Suppose we have a ﬁnite-dimensional vector space V with basis e1, . . . , em and a vector
space W with basis w1, . . . , wn. I also have a map T : V → W and I want to tell you
what T is. It would be awfully inconsiderate of me to try and tell you what T (v) is at
every point v. In fact, I only have to tell you what T (e1), . . . , T (em) are, because from
there you can work out T (a1e1 + ··· + amem) for yourself:

T (a1e1 + ··· + amem) = a1T (e1) + ··· + amT (em).
Since the ei are a basis, that tells you all you need to know about T .

Example 9.6.1 (Extending linear maps)

Let V =(cid:8)ax2 + bx + c | a, b, c ∈ R(cid:9). Then T (ax2 + bx + c) = aT (x2) + bT (x) + cT (1).

Now I can even be more concrete. I could tell you what T (e1) is, but seeing as I have
a basis of W , I can actually just tell you what T (e1) is in terms of this basis. Speciﬁcally,
there are unique a11, a21, . . . , an1 ∈ k such that

T (e1) = a11w1 + a21w2 + ··· + an1wn.

So rather than telling you the value of T (e1) in some abstract space W , I could just tell
you what a11, a21, . . . , an1 were. Then I’d just repeat this for T (e2), T (e3), all the way
up to T (em), and that would tell you everything you need to know about T .

That’s where the matrix T comes from! It’s just a concise way of writing down all mn

numbers I need to tell you. To be explicit, the matrix for T is deﬁned as the array

|
|

T =
(cid:124)
=

a11 a12
a21 a22
...
...
an1 an2

m columns

|
|

(cid:123)(cid:122)

. . .
. . .
. . .
. . .

T (e1) T (e2)

. . . T (em)

(cid:41)n rows

|
|


(cid:125)

a1m
a2m
...
anm

.



Example 9.6.2 (An example of a matrix)
Here is a concrete example in terms of a basis. Let V = R3 with basis e1, e2, e3 and
let W = R2 with basis w1, w2. If I have T : V → W then uniquely determined by
three values, for example:

T (e1) = 4w1 + 7w2
T (e2) = 2w1 + 3w2
T (e3) = w1

The columns then correspond to T (e1), T (e2), T (e3):

T =(cid:20)4 2 1
7 3 0(cid:21)

140

Napkin, by Evan Chen (v1.5.20190718)

Example 9.6.3 (An example of a matrix after choosing a basis)

most 2. We ﬁx the basis 1, x, x2 for it.

We again let V =(cid:8)ax2 + bx + c(cid:9) be the vector space of polynomials of degree at
Consider the “evaluation at 3” map, a map V → R. We pick 1 as the basis element

of the RHS; then we can write it as a 1 × 3 matrix

with the columns corresponding to T (1), T (x), T (x2).

(cid:2)1 3 9(cid:3)

From here you can actually work out for yourself what it means to multiply two
matrices. Suppose we have picked a basis for three spaces U , V , W . Given maps
T : U → V and S : V → W , we can consider their composition S ◦ T , i.e.

U T−→ V S−→ W.

Matrix multiplication is deﬁned exactly so that the matrix ST is the same thing we get
from interpreting the composed function S ◦ T as a matrix.

Exercise 9.6.4. Check this for yourself! For a concrete example let R2 T
−→ R2 by
T (e1) = 2e1 + 3e2 and T (e2) = 4e1 + 5e2, S(e1) = 6e1 + 7e2 and S(e2) = 8e1 + 9e2. Compute
S(T (e1)) and S(T (e2)) and see how it compares to multiplying the matrices associated to S
and T .

−→ R2 S

In particular, since function composition is associative, it follows that matrix multiplica-
tion is as well. To drive this point home,

A matrix is the laziest possible way to specify a linear map from V to W .

This means you can deﬁne concepts like the determinant or the trace of a matrix both
in terms of an “intrinsic” map T : V → W and in terms of the entries of the matrix.
Since the map T itself doesn’t refer to any basis, the abstract deﬁnition will imply that
the numerical deﬁnition doesn’t depend on the choice of a basis.

§9.7 Subspaces and picking convenient bases

Prototypical example for this section: Any two linearly independent vectors in R3.

Deﬁnition 9.7.1. Let M be a left R-module. A submodule N of M is a module N
such that every element of N is also an element of M . If M is a vector space then N is
called a subspace.

Example 9.7.2 (Kernels)
The kernel of a map T : V → W is the set of v ∈ V such that T (v) = 0W . It is a
subspace of V , since it’s closed under addition and scaling (why?).

9 Vector spaces

141

Example 9.7.3 (Spans)
Let V be a vector space and v1, . . . , vm be any vectors of V . The span of these
vectors is deﬁned as the set

{a1v1 + ··· + amvm | a1, . . . , am ∈ k} .

Note that it is a subspace of V as well!

Question 9.7.4. Why is 0V an element of each of the above examples? In general, why
must any subspace contain 0V ?

Subspaces behave nicely with respect to bases.

Theorem 9.7.5 (Basis completion)
Let V be an n-dimensional space, and V (cid:48) a subspace of V . Then

(a) V (cid:48) is also ﬁnite-dimensional.

(b) If e1, . . . , em is a basis of V (cid:48), then there exist em+1, . . . , en in V such that

e1, . . . , en is a basis of V .

Proof. Omitted, since it is intuitive and the proof is not that enlightening. (However, we
will use this result repeatedly later on, so do take the time to internalize it now.)

A very common use case is picking a convenient basis for a map T .

Theorem 9.7.6 (Picking a basis for linear maps)
Let T : V → W be a map of ﬁnite-dimensional vector spaces, with n = dim V ,
m = dim W . Then there exists a basis v1, . . . , vn of V and a basis w1, . . . , wm of W ,
as well as a nonnegative integer k, such that

T (vi) =(cid:40)wi

if i ≤ k
0W if i > k.

Moreover dim im T = k and dim ker T = n − k.

Sketch of Proof. You might like to try this one yourself before reading on: it’s a repeated
application of Theorem 9.7.5.

Let ker T have dimension n− k. We can pick vk+1, . . . , vn a basis of ker T . Then extend
it to a basis v1, . . . , vn of V . The map T is injective over the span of v1, . . . , vk (since
only 0V is in the kernel) so its images in W are linearly independent. Setting wi = T (vi)
for each i, we get some linearly independent set in W . Then extend it again to a basis of
W .

This theorem is super important, not only because of applications but also because it
will give you the right picture in your head of how a linear map is supposed to look. I’ll
even draw a cartoon of it to make sure you remember:

142

Napkin, by Evan Chen (v1.5.20190718)

kernel while sending V (cid:48) to an isomorphic copy in W .

In particular, for T : V → W , one can write V = ker T ⊕ V (cid:48), so that T annihilates its
A corollary of this (which you should have expected anyways) is the so called rank-

nullity theorem, which is just the analog of the ﬁrst isomorphism theorem.

Theorem 9.7.7 (Rank-nullity theorem)
Let V and W be ﬁnite-dimensional vector spaces. If T : V → W , then

dim V = dim ker T + dim im T.

Question 9.7.8. Conclude the rank-nullity theorem from Theorem 9.7.6.

§9.8 A cute application: Lagrange interpolation

Here’s a cute application2 of linear algebra to a theorem from high school.

Theorem 9.8.1 (Lagrange interpolation)

Let x1, . . . , xn+1 be distinct real numbers and y1, . . . , yn+1 any real numbers. Then
there exists a unique polynomial P of degree at most n such that

for every i.

P (xi) = yi

When n = 1 for example, this loosely says there is a unique line joining two points.

Proof. The idea is to consider the vector space V of polynomials with degree at most n,
as well as the vector space W = Rn+1.

2Source: Communicated to me by Joe Harris at the ﬁrst Harvard-MIT Undergraduate Math Symposium.

Ve1e2...ekek+1ek+2...enWf1f2...fkfk+1fk+2fk+3...fmT000imTkerT9 Vector spaces

143

Question 9.8.2. Check that dim V = n + 1 = dim W . This is easiest to do if you pick
a basis for V , but you can then immediately forgot about the basis once you ﬁnish this
exercise.

Then consider the linear map T : V → W given by

P (cid:55)→ (P (x1), . . . , P (xn+1)) .

This is indeed a linear map because, well, T (P + Q) = T (P ) + T (Q) and T (cP ) = cT (P ).
if P ∈ ker T , then P (x1) = ··· = P (xn+1) = 0, but
It also happens to be injective:
deg P ≤ n and so P can only be the zero polynomial.
So T is an injective map between vector spaces of the same dimension. Thus it is
actually a bijection, which is exactly what we wanted.

§9.9 (Digression) Arrays of numbers are evil

As I’ll stress repeatedly, a matrix represents a linear map between two vector spaces.
Writing it in the form of an m × n matrix is merely a very convenient way to see the
map concretely. But it obfuscates the fact that this map is, well, a map, not an array of
numbers.

If you took high school precalculus, you’ll see everything done in terms of matrices.
To any typical high school student, a matrix is an array of numbers. No one is sure what
exactly these numbers represent, but they’re told how to magically multiply these arrays
to get more arrays. They’re told that the matrix

1 0 . . .
0 1 . . .
...
. . .
0 0 . . .

...



0
0
...
1



is an “identity matrix”, because when you multiply by another matrix it doesn’t change.
Then they’re told that the determinant is some magical combination of these numbers
formed by this weird multiplication rule. No one knows what this determinant does,
other than the fact that det(AB) = det A det B, and something about areas and row
operations and Cramer’s rule.

Then you go into linear algebra in college, and you do more magic with these arrays of

numbers. You’re told that two matrices T1 and T2 are similar if

T2 = ST1S−1

for some invertible matrix S. You’re told that the trace of a matrix Tr T is the sum of
the diagonal entries. Somehow this doesn’t change if you look at a similar matrix, but
you’re not sure why. Then you deﬁne the characteristic polynomial as

pT (X) = det(XI − T ).

Somehow this also doesn’t change if you take a similar matrix, but now you really don’t
know why. And then you have the Cayley-Hamilton theorem in all its black magic:
pT (T ) is the zero map. Out of curiosity you Google the proof, and you ﬁnd some ad-hoc
procedure which still leaves you with no idea why it’s true.

This is terrible. What’s so special about T2 = ST1S−1? Only if you know that the
matrices are linear maps does this make sense: T2 is just T1 rewritten with a diﬀerent
choice of basis.

144

Napkin, by Evan Chen (v1.5.20190718)

I really want to push the opposite view. Linear algebra is the study of linear maps,
but it is taught as the study of arrays of numbers, and no one knows what these numbers
mean. And for a good reason: the numbers are meaningless. They are a highly convenient
way of encoding the matrix, but they are not the main objects of study, any more than
the dates of events are the main objects of study in history.

The other huge downside is that people get the impression that the only (real) vector
space in existence is R⊕n. As explained in Remark 9.5.5, while you can work this way if
you’re a soulless robot, it’s very unnatural for humans to do so.

When I took Math 55a as a freshman at Harvard, I got the exact opposite treatment:
we did all of linear algebra without writing down a single matrix. During all this time I
was quite confused. What’s wrong with a basis? I didn’t appreciate until later that this
approach was the morally correct way to treat the subject: it made it clear what was
happening.

Throughout the Napkin, I’ve tried to strike a balance between these two approaches,
using matrices when appropriate to illustrate the maps and to simplify proofs, but
ultimately writing theorems and deﬁnitions in their morally correct form. I hope that
this has both the advantage of giving the “right” deﬁnitions while being concrete enough
to be digested. But I would just like to say for the record that, if I had to pick between
the high school approach and the 55a approach, I would pick 55a in a heartbeat.

§9.10 A word on general modules
Prototypical example for this section: Z[√2] is a Z-module of rank two.

I focused mostly on vector ﬁelds (aka modules over a ﬁeld) in this chapter for simplicity,
so I want to make a few remarks about modules over a general commutative ring R
before concluding.

Firstly, recall that for general modules, we say “generating set” instead of “spanning

set”. Shrug.

The main issue with rings is that our key theorem Theorem 9.4.5 fails in spectacular
ways. For example, consider Z as a Z-module over itself. Then {2} is linearly independent,
but it cannot be extended to a basis. Similarly, {2, 3} is spanning, but one cannot cut it
down to a basis. You can see why deﬁning dimension is going to be diﬃcult.

Nonetheless, there are still analogs of some of the deﬁnitions above.

Deﬁnition 9.10.1. An R-module M is called ﬁnitely generated if it has a ﬁnite
generating set.

Deﬁnition 9.10.2. An R-module M is called free if it has a basis. As said before, the
analogue of the dimension theorem holds, and we use the word rank to denote the size
of the basis. As before, there’s an isomorphism M ∼= R⊕n where n is the rank.

Example 9.10.3 (An example of a Z-module)
The Z-module

has a basis {1,√2}, so we say it is a free Z-module of rank 2.

Z[√2] =(cid:110)a + b√2 | a, b ∈ Z(cid:111)

Abuse of Notation 9.10.4 (Notation for groups). Recall that an abelian group can be
viewed a Z-module (and in fact vice-versa!), so we can (and will) apply these words to

9 Vector spaces

145

abelian groups. We’ll use the notation G ⊕ H for two abelian groups G and H for their
Cartesian product, emphasizing the fact that G and H are abelian. This will happen
when we study algebraic number theory and homology groups.

§9.11 A few harder problems to think about

General hint: Theorem 9.7.6 will be your best friend for many of these problems.

Problem 9A†. Let V and W be ﬁnite-dimensional vector spaces with nonzero dimen-
sion, and consider linear maps T : V → W . Complete the following table by writing
“sometimes”, “always”, or “never” for each entry.

T injective T surjective T isomorphism

If dim V > dim W . . .
If dim V = dim W . . .
If dim V < dim W . . .

Problem 9B† (Equal dimension vector spaces are usually isomorphisms). Let V and
W be ﬁnite-dimensional vector spaces with dim V = dim W . Prove that for a map
T : V → W , the following are equivalent:

 T is injective,

 T is surjective,

 T is bijective.

Problem 9C (Multiplication by √5). Let V = Q[√5] =(cid:8)a + b√5(cid:9) be a two-dimensional
Q-vector space, and ﬁx the basis {1,√5} for it. Write down the 2× 2 matrix with rational
coeﬃcients that corresponds to multiplication by √5.
Problem 9D (Multivariable Lagrange interpolation). Let S ⊂ Z2 be a set of n lattice
points. Prove that there exists a nonzero two-variable polynomial p with real coeﬃcients,
of degree at most √2n, such that p(x, y) = 0 for every (x, y) ∈ S.

Problem 9E (Putnam 2003). Do there exist polynomials a(x), b(x) , c(y), d(y) such
that

1 + xy + (xy)2 = a(x)c(y) + b(x)d(y)

holds identically?

Problem 9F (TSTST 2014). Let P (x) and Q(x) be arbitrary polynomials with real
coeﬃcients, and let d be the degree of P (x). Assume that P (x) is not the zero polynomial.
Prove that there exist polynomials A(x) and B(x) such that

(i) Both A and B have degree at most d/2,

(ii) At most one of A and B is the zero polynomial,

(iii) P divides A + Q · B.
Problem 9G(cid:63) (Idempotents are projection maps). Let P : V → V be a linear map,
where V is a vector space (not necessarily ﬁnite-dimensional). Suppose P is idempotent,
meaning P (P (v)) = P (v) for each v ∈ V , or equivalently P is the identity on its image.
Prove that

Thus we can think of P as projection onto the subspace im P .

V = ker P ⊕ im P.

146

Napkin, by Evan Chen (v1.5.20190718)

Problem 9H(cid:63). Let V be a ﬁnite dimensional vector space. Let T : V → V be a linear
map, and let T n : V → V denote T applied n times. Prove that there exists an integer
N such that

V = ker T N ⊕ im T N .

10 Eigen-things

This chapter will develop the theory of eigenvalues and eigenvectors, the so-called

“Jordan canonical form”, and from it the characteristic polynomial.

§10.1 Why you should care

We know that a square matrix T is really just a linear map from V to V . What’s the
simplest type of linear map? It would just be multiplication by some scalar λ, which
would have associated matrix (in any basis!)

T =
T =

λ1
0
...
0

0
λ 0 . . .
0
0 λ . . .
...
...
. . .
0 0 . . . λ

...

.



0
λ2
...
0

0
. . .
0
. . .
...
. . .
. . . λn

.



That’s perhaps too simple, though. If we had a ﬁxed basis e1, . . . , en then another very
“simple” operation would just be scaling each basis element ei by λi, i.e. a diagonal
matrix of the form

These maps are more general. Indeed, you can, for example, compute T 100 in a heartbeat:
the map sends e1 → λ100

1 e1. (Try doing that with an arbitrary n × n matrix.)

Of course, most linear maps are probably not that nice. Or are they?

Example 10.1.1 (Getting lucky)
Let V be some two-dimensional vector space with e1 and e2 as basis elements. Let’s
consider a map T : V → V by e1 (cid:55)→ 2e1 and e2 (cid:55)→ e1 + 3e2, which you can even write
concretely as

T =(cid:20)2 1
0 3(cid:21)

in basis e1, e2.

This doesn’t look anywhere as nice until we realize we can rewrite it as

e1 (cid:55)→ 2e1

e1 + e2 (cid:55)→ 3(e1 + e2).

So suppose we change to the basis e1 and e1 + e2. Thus in the new basis,

T =(cid:20)2 0
0 3(cid:21)

in basis e1, e1 + e2.

So our completely random-looking map, under a suitable change of basis, looks like
the very nice maps we described before!

147

148

Napkin, by Evan Chen (v1.5.20190718)

In this chapter, we will be making our luck, and we will see that our better understanding
of matrices gives us the right way to think about this.

§10.2 Warning on assumptions

Most theorems in this chapter only work for

 ﬁnite-dimensional vector spaces V ,

 over a ﬁeld k which is algebraically closed.

On the other hand, the deﬁnitions work ﬁne without these assumptions.

§10.3 Eigenvectors and eigenvalues

Let k be a ﬁeld and V a vector space over it. In the above example, we saw that there
were two very nice vectors, e1 and e1 + e2, for which V did something very simple.
Naturally, these vectors have a name.
Deﬁnition 10.3.1. Let T : V → V and v ∈ V a nonzero vector. We say that v is an
eigenvector if T (v) = λv for some λ ∈ k (possibly zero, but remember v (cid:54)= 0). The
value λ is called an eigenvalue of T .
We will sometimes abbreviate “v is an eigenvector with eigenvalue λ” to just “v is a

λ-eigenvector”.

Of course, no mention to a basis anywhere.

Example 10.3.2 (An example of an eigenvector and eigenvalue)

0 3(cid:21).
Consider the example earlier with T =(cid:20)2 1

(a) Note that e1 and e1 + e2 are 2-eigenvectors and 3-eigenvectors.

(b) Of course, 5e1 is also an 2-eigenvector.

(c) And, 7e1 + 7e2 is also a 3-eigenvector.

So you can quickly see the following observation.

Question 10.3.3. Show that the λ-eigenvectors, together with {0} form a subspace.

Deﬁnition 10.3.4. For any λ, we deﬁne the λ-eigenspace as the set of λ-eigenvectors
together with 0.

This lets us state succinctly that “2 is an eigenvalue of T with one-dimensional eigenspace
spanned by e1”.

Unfortunately, it’s not exactly true that eigenvalues always exist.

Example 10.3.5 (Eigenvalues need not exist)
Let V = R2 and let T be the map which rotates a vector by 90◦ around the origin.
Then T (v) is not a multiple of v for any v ∈ V , other than the trivial v = 0.
However, it is true if we replace k with an algebraically closed ﬁeld 1

1A ﬁeld is algebraically closed if all its polynomials have roots, the archetypal example being C.

10 Eigen-things

149

Theorem 10.3.6 (Eigenvalues always exist over algebraically closed ﬁelds)
Suppose k is an algebraically closed ﬁeld. Let V be a ﬁnite dimensional k-vector
space. Then if T : V → V is a linear map, there exists an eigenvalue λ ∈ k.

Proof. (From [Ax97]) The idea behind this proof is to consider “polynomials” in T . For
example, 2T 2 − 4T + 5 would be shorthand for 2T (T (v)) − 4T (v) + 5v. In this way we
can consider “polynomials” P (T ); this lets us tie in the “algebraically closed” condition.
These polynomials behave nicely:

Question 10.3.7. Show that P (T ) + Q(T ) = (P + Q)(T ) and P (T ) ◦ Q(T ) = (P · Q)(T ).

Let n = dim V < ∞ and ﬁx any nonzero vector v ∈ V , and consider vectors v, T (v),
. . . , T n(v). There are n + 1 of them, so they must be linearly dependent for dimension
reasons; thus there is a nonzero polynomial P such that P (T ) is zero when applied to
v. WLOG suppose P is a monic polynomial, and thus P (z) = (z − r1) . . . (z − rm) say.
Then we get

0 = (T − r1id) ◦ (T − r2id) ◦ ··· ◦ (T − rmid)(v)

which means at least one of T − riid is not injective, i.e. has a nontrivial kernel, which is
the same as an eigenvector.

So in general we like to consider algebraically closed ﬁelds. This is not a big loss: any
real matrix can be interpreted as a complex matrix whose entries just happen to be real,
for example.

§10.4 The Jordan form

So that you know exactly where I’m going, here’s the main theorem.

Deﬁnition 10.4.1. A Jordan block is an n × n matrix of the following shape:

In other words, it has λ on the diagonal, and 1 above it. We allow n = 1, so(cid:2)λ(cid:3) is a

Jordan block.

Theorem 10.4.2 (Jordan canonical form)
Let T : V → V be a linear map of ﬁnite-dimensional vector spaces over an alge-
braically closed ﬁeld k. Then we can choose a basis of V such that the matrix T is
“block-diagonal” with each block being a Jordan block.

Such a matrix is said to be in Jordan form. This form is unique up to rearranging

the order of the blocks.

0 0
λ 1 0 0 . . .
0 0
0 λ 1 0 . . .
0 0
0 0 λ 1 . . .
0 0
0 0 0 λ . . .
...
...
...
. . .
0 0 0 0 . . . λ 1
0 λ
0 0 0 0 . . .

...

...

...



.



150

Napkin, by Evan Chen (v1.5.20190718)

As an example, this means the matrix should look something like:

1
λ1

λ2

λ3
0
0

1
λ3
0

0
1
λ3

λ1
0





. . .

λm 1
0
λm

Question 10.4.3. Check that diagonal matrices are the special case when each block is
1 × 1.

What does this mean? Basically, it means our dream is almost true. What happens is

that V can get broken down as a direct sum

V = J1 ⊕ J2 ⊕ ··· ⊕ Jm

and T acts on each of these subspaces independently. These subspaces correspond to the
blocks in the matrix above. In the simplest case, dim Ji = 1, so Ji has a basis element e
for which T (e) = λie; in other words, we just have a simple eigenvalue. But on occasion,
the situation is not quite so simple, and we have a block of size greater than 1; this leads
to 1’s just above the diagonals.

I’ll explain later how to interpret the 1’s, when I make up the word descending staircase.
For now, you should note that even if dim Ji ≥ 2, we still have a basis element which is
an eigenvector with element λi.

Example 10.4.4 (A concrete example of Jordan form)
Let T : k6 → k6 and suppose T is given by the matrix
3 0 0 0 0 0
0 7 0 0 0 0
0 0 2 1 0 0
0 0 0 2 0 0
0 0 0 0 5 0
0 0 0 0 0 3

T =



.



Reading the matrix, we can compute all the eigenvectors and eigenvalues: they are

T (e1) = 3e1
T (e2) = 7e2
T (e3) = 2e3
T (e5) = 5e5
T (e6) = 3e6.

The element e4, on the other hand, is not an eigenvalue. For T (e4) = e3 + 2e4.

10 Eigen-things

§10.5 Nilpotent maps

151

Bear with me for a moment. First, deﬁne:
Deﬁnition 10.5.1. A map T : V → V is nilpotent if T m is the zero map for some
integer m. (Here T m means “T applied m times”.)

What’s an example of a nilpotent map?

Example 10.5.2 (The “descending staircase”)
Let V = k3 have basis e1, e2, e3. Then the map T which sends

is nilpotent, since T (e1) = T 2(e2) = T 3(e3) = 0, and hence T 3(v) = 0 for all v ∈ V .

e3 (cid:55)→ e2 (cid:55)→ e1 (cid:55)→ 0

The 3 × 3 descending staircase has matrix representation

T =

0 1 0
0 0 1
0 0 0

 .

You’ll notice this is a Jordan block.

Exercise 10.5.3. Show that the descending staircase above has 0 as its only eigenvalue.

That’s a pretty nice example. As another example, we can have multiple such stair-

cases.

Example 10.5.4 (Double staircase)
Let V = k⊕5 have basis e1, e2, e3, e4, e5. Then the map

e3 (cid:55)→ e2 (cid:55)→ e1 (cid:55)→ 0 and e5 (cid:55)→ e4 (cid:55)→ 0

is nilpotent.

Picture, with some zeros omitted for emphasis:

0 1 0
0 0 1
0 0 0

T =



0 1
0 0

You can see this isn’t really that diﬀerent from the previous example; it’s just the same
idea repeated multiple times. And in fact we now claim that all nilpotent maps have
essentially that form.

Theorem 10.5.5 (Nilpotent Jordan)

Let V be a ﬁnite-dimensional vector space over an algebraically closed ﬁeld k. Let
i=1 Vi where each Vi has

T : V → V be a nilpotent map. Then we can write V =(cid:76)m
a basis of the form vi, T (vi), . . . , T dim Vi−1(vi) for some vi ∈ Vi.

Hence:

152

Napkin, by Evan Chen (v1.5.20190718)

Every nilpotent map can be viewed as independent staircases.

Each chain vi, T (vi), T (T (vi)), . . . is just one staircase. The proof is given later, but ﬁrst
let me point out where this is going.

Here’s the punch line. Let’s take the double staircase again. Expressing it as a matrix

gives, say

Then we can compute

S =

0 1 0
0 0 1
0 0 0



.



0 1
0 0

λ 1 0
0 λ 1
0 0 λ

S + λid =



.



λ 1
0 λ

It’s a bunch of λ Jordan blocks! This gives us a plan to proceed: we need to break V into
a bunch of subspaces such that T − λid is nilpotent over each subspace. Then Nilpotent
Jordan will ﬁnish the job.

§10.6 Reducing to the nilpotent case

Deﬁnition 10.6.1. Let T : V → V . A subspace W ⊆ V is called T -invariant if
T (w) ∈ W for any w ∈ W . In this way, T can be thought of as a map W → W .
In this way, the Jordan form is a decomposition of V into invariant subspaces.
Now I’m going to be cheap, and deﬁne:

Deﬁnition 10.6.2. A map T : V → V is called indecomposable if it’s impossible to
write V = W1 ⊕ W2 where both W1 and W2 are nontrivial T -invariant spaces.

Picture of a decomposable map:

W1

0 0 0
0 0 0

W2



0 0
0 0
0 0



As you might expect, we can break a space apart into “indecomposable” parts.

Proposition 10.6.3 (Invariant subspace decomposition)
Let V be a ﬁnite-dimensional vector space. Given any map T : V → V , we can write

V = V1 ⊕ V2 ⊕ ··· ⊕ Vm

where each Vi is T -invariant, and for any i the map T : Vi → Vi is indecomposable.

10 Eigen-things

153

Proof. Same as the proof that every integer is the product of primes.
If V is not
indecomposable, we are done. Otherwise, by deﬁnition write V = W1 ⊕ W2 and then
repeat on each of W1 and W2.

Incredibly, with just that we’re almost done! Consider a decomposition as above,
so that T : V1 → V1 is an indecomposable map. Then T has an eigenvalue λ1, so let
S = T − λ1id; hence ker S (cid:54)= {0}.

Question 10.6.4. Show that V1 is also S-invariant, so we can consider S : V1 → V1.

By Problem 9H(cid:63), we have

V1 = ker SN ⊕ im SN

for some N . But we assumed T was indecomposable, so this can only happen if
im SN = {0} and ker SN = V (since ker SN contains our eigenvector). Hence S is
nilpotent, so it’s a collection of staircases. In fact, since T is indecomposable, there is
only one staircase. Hence V1 is a Jordan block, as desired.

§10.7 (Optional) Proof of nilpotent Jordan

The proof is just induction on dim V . Assume dim V ≥ 1, and let W = T img(V ) be the
image of V . Since T is nilpotent, we must have W (cid:40) V . Moreover, if W = {0} (i.e. T is
the zero map) then we’re already done. So assume {0} (cid:40) W (cid:40) V .
By the inductive hypothesis, we can select a good basis of W :
B(cid:48) =(cid:110)T (v1), T (T (v1)), T (T (T (v1))), . . .
T (v(cid:96)), T (T (v(cid:96))), T (T (T (v(cid:96)))), . . .(cid:111)

T (v2), T (T (v2)), T (T (T (v2))), . . .
. . . ,

for some T (vi) ∈ W (here we have taken advantage of the fact that each element of W is
itself of the form T (v) for some v).
Also, note that there are exactly (cid:96) elements of B(cid:48) which are in ker T (namely the last
element of each of the (cid:96) staircases). We can thus complete it to a basis v(cid:96)+1, . . . , vm
(where m = dim ker T ). (In other words, the last element of each staircase plus the m − (cid:96)
new ones are a basis for ker T .)

Now consider

B =(cid:110)v1, T (v1), T (T (v1)), T (T (T (v1))), . . .

v2, T (v2), T (T (v2)), T (T (T (v2))), . . .
. . . ,
v(cid:96), T (v(cid:96)), T (T (v(cid:96))), T (T (T (v(cid:96)))), . . .

v(cid:96)+1, v(cid:96)+2, . . . , vm(cid:111).

Question 10.7.1. Check that there are exactly (cid:96) + dim W + (dim ker T − (cid:96)) = dim V
elements.

154

Napkin, by Evan Chen (v1.5.20190718)

Exercise 10.7.2. Show that all the elements are linearly independent. (Assume for contra-
diction there is some linear dependence, then take T of both sides.)

Hence B is a basis of the desired form.

§10.8 Algebraic and geometric multiplicity

Prototypical example for this section: The matrix T below.

This is some convenient notation: let’s consider the matrix in Jordan form

7 1
0 7

T =



.



9

7 1 0
0 7 1
0 0 7

We focus on the eigenvalue 7, which appears multiple times, so it is certainly “repeated”.
However, there a two diﬀerent senses in which you could say it is repeated.

 Algebraic: You could say it is repeated ﬁve times, because it appears ﬁve times on

the diagonal.

 Geometric: You could say it really only appears two times: because there are only

two eigenvectors with eigenvalue 7, namely e1 and e4.
Indeed, the vector e2 for example has T (e2) = 7e2 + e1, so it’s not really an
eigenvector! If you apply T − 7id to e2 twice though, you do get zero.

Question 10.8.1. In this example, how many times do you need to apply T − 7id to e6 to
get zero?

Both these notions are valid, so we will name both. To preserve generality, we ﬁrst state
the “intrinsic” deﬁnition.
Deﬁnition 10.8.2. Let T : V → V be a linear map and λ a scalar.

 The geometric multiplicity of λ is the dimension dim Vλ of the λ-eigenspace.
 Deﬁne the generalized eigenspace V λ to be the subspace of v for which (T −
λid)n(v) = 0 for some n ≥ 1. The algebraic multiplicity of λ is the dimension
dim V λ.

(Silly edge case: we allow “multiplicity zero” if λ is not an eigenvalue at all.)

However in practice you should just count the Jordan blocks.

Example 10.8.3 (An example of eigenspaces via Jordan form)
Retain the matrix T mentioned earlier and let λ = 7.

 The eigenspace Vλ has basis e1 and e4, so the geometric multiplicity is 2.

 The generalized eigenspace V λ has basis e1, e2, e4, e5, e6 so the algebraic

multiplicity is 5.

To be completely explicit, here is how you think of these in practice:

10 Eigen-things

155

Proposition 10.8.4 (Geometric and algebraic multiplicity vs Jordan blocks)
Assume T : V → V is a linear map of ﬁnite-dimensional vector spaces, written in
Jordan form. Let λ be a scalar. Then

 The geometric multiplicity of λ is the number of Jordan blocks with eigenvalue

λ; the eigenspace has one basis element per Jordan block.

 The algebraic multiplicity of λ is the sum of the dimensions of the Jordan
blocks with eigenvalue λ; the eigenspace is the direct sum of the subspaces
corresponding to those blocks.

Question 10.8.5. Show that the geometric multiplicity is always less than or equal to the
algebraic multiplicity.

This actually gives us a tentative deﬁnition:

 The trace is the sum of the eigenvalues, counted with algebraic multiplicity.

 The determinant is the product of the eigenvalues, counted with algebraic multi-

plicity.

This deﬁnition is okay, but it has the disadvantage of requiring the ground ﬁeld to be alge-
braically closed. It is also not the deﬁnition that is easiest to work with computationally.
The next two chapters will give us a better deﬁnition.

§10.9 A few harder problems to think about

Problem 10A (Sum of algebraic multiplicities). Given a 2018-dimensional complex
vector space V and a map T : V → V , what is the sum of the algebraic multiplicities of
all eigenvalues of T ?
Problem 10B (The word “diagonalizable”). A linear map T : V → V (where dim V is
ﬁnite) is said to be diagonalizable if it has a basis e1, . . . , en such that each ei is an
eigenvector.

(a) Explain the name “diagonalizable”.

(b) Suppose we are working over an algebraically closed ﬁeld. Then show that that T
is diagonalizable if and only if for any λ, the geometric multiplicity of λ equals the
algebraic multiplicity of λ.

Problem 10C (Switcharoo). Let V be the C-vector space with basis e1 and e2. The
map T : V → V sends T (e1) = e2 and T (e2) = e1. Determine the eigenspaces of T .
Problem 10D (Writing a polynomial backwards). Deﬁne the complex vector space

V of polynomials with degree at most 2, say V =(cid:8)ax2 + bx + c | a, b, c ∈ C(cid:9). Deﬁne

T : V → V by

T (ax2 + bx + c) = cx2 + bx + a.

Determine the eigenspaces of T .

Problem 10E (Diﬀerentiation of polynomials). Let V = R[x] be the real vector space
of all real polynomials. Note that d
dx : V → V is a linear map (for example it sends x3 to
3x2). Which real numbers are eigenvalues of this map?

156

Napkin, by Evan Chen (v1.5.20190718)

Problem 10F (Diﬀerentiation of functions). Let V be the real vector space of all
inﬁnitely diﬀerentiable functions R → R. Note that d
dx : V → V is a linear map (for
example it sends cos x to − sin x). Which real numbers are eigenvalues of this map?

11 Dual space and trace

You may have learned in high school that given a matrix

(cid:20)a c
b d(cid:21)

the trace is the sum along the diagonals a + d and the determinant is ad − bc. But we
know that a matrix is somehow just encoding a linear map using a choice of basis. Why
would these random formulas somehow not depend on the choice of a basis?

In this chapter, we are going to give an intrinsic deﬁnition of Tr T , where T : V → V
and dim V < ∞. This will give a coordinate-free deﬁnition which will in particular imply
the trace a + d doesn’t change if we take a diﬀerent basis.
In doing so, we will introduce two new constructions: the tensor product V ⊗ W (which
is a sort of product of two spaces, with dimension dim V · dim W ) and the dual space V ∨,
which is the set of linear maps V → k (a k-vector space). Later on, when we upgrade
from a vector space V to an inner product space V ∨, we will see that the dual space
gives a nice interpretation of the “transpose” of a matrix. You’ll already see some of that
come through here.

The trace is only deﬁned for ﬁnite-dimensional vector spaces, so if you want you can
restrict your attention to ﬁnite-dimensional vector spaces for this chapter. (On the other
hand we do not need the ground ﬁeld to be algebraically closed.)

The next chapter will then do the same for the determinant.

§11.1 Tensor product

Prototypical example for this section: R[x] ⊗ R[y] = R[x, y].

We know that dim(V ⊕ W ) = dim V + dim W , even though as sets V ⊕ W looks like
V × W . What if we wanted a real “product” of spaces, with multiplication of dimensions?
For example, let’s pull out my favorite example of a real vector space, namely

Here’s another space, a little smaller:

V =(cid:8)ax2 + bx + c | a, b, c ∈ R(cid:9) .
W = {dy + e | d, e ∈ R} .

If we take the direct sum, then we would get some rather unnatural vector space of
dimension ﬁve (whose elements can be thought of as pairs (ax2 + bx + c, dy + e)). But
suppose we want a vector space whose elements are products of polynomials in V and W ;
it would contain elements like 4x2y + 5xy + y + 3. In particular, the basis would be

and thus have dimension six.

(cid:8)x2y, x2, xy, x, y, 1(cid:9)

For this we resort to the tensor product. It does exactly this, except that the “multipli-
cation” is done by a scary1 symbol ⊗: think of it as a “wall” that separates the elements
between the two vector spaces. For example, the above example might be written as

1Seriously, ⊗ looks terrifying to non-mathematicians, and even to many math undergraduates.

4x2 ⊗ y + 5x ⊗ y + 1 ⊗ y + 3 ⊗ 1.

157

158

Napkin, by Evan Chen (v1.5.20190718)

(This should be read as (4x2 ⊗ y) + (5x ⊗ y) + . . . ; addition comes after ⊗.) Of course
there should be no distinction between writing 4x2 ⊗ y and x2 ⊗ 4y or even 2x2 ⊗ 2y.
While we want to keep the x and y separate, the scalars should be free to ﬂoat around.
Of course, there’s no need to do everything in terms of just the monomials. We are

free to write

If you like, you can expand this as

(x + 1) ⊗ (y + 1).

x ⊗ y + 1 ⊗ y + x ⊗ 1 + 1 ⊗ 1.

Same thing. The point is that we can take any two of our polynomials and artiﬁcially
“tensor” them together.

The deﬁnition of the tensor product does exactly this, and nothing else.2

Deﬁnition 11.1.1. Let V and W be vector spaces over the same ﬁeld k. The tensor
product V ⊗k W is the abelian group generated by elements of the form v ⊗ w, subject
to relations

(v1 + v2) ⊗ w = v1 ⊗ w + v2 ⊗ w
v ⊗ (w1 + w2) = v ⊗ w1 + v ⊗ w2

(c · v) ⊗ w = v ⊗ (c · w).

As a vector space, its action is given by c · (v ⊗ w) = (c · v) ⊗ w = v ⊗ (c · w).

Here’s another way to phrase the same idea. We deﬁne a pure tensor as an element
of the form v ⊗ w for v ∈ V and w ∈ W . But we let the ⊗ wall be “permeable” in the
sense that

(c · v) ⊗ w = v ⊗ (c · w) = c · (v ⊗ w)

and we let multiplication and addition distribute as we expect. Then V ⊗ W consists of
sums of pure tensors.

Example 11.1.2 (Inﬁnite-dimensional example of tensor product: two-variable
polynomials)

Although it’s not relevant to this chapter, this deﬁnition works equally well with
inﬁnite-dimensional vector spaces. The best example might be

R[x] ⊗R R[y] = R[x, y].

That is, the tensor product of polynomials in x with real polynomials in y turns out
to just be two-variable polynomials R[x, y].

Remark 11.1.3 (Warning on sums of pure tensors) — Remember the elements of
V ⊗k W really are sums of these pure tensors! If you liked the previous example,
this fact has a nice interpretation — not every polynomial in R[x, y] = R[x] ⊗R R[y]
factors as a polyonmial in x times a polynomial in y (i.e. as pure tensors f (x)⊗ g(y)).
But they all can be written as sums of pure tensors xa ⊗ yb.

2I’ll only deﬁne this for vector spaces for simplicity. The deﬁnition for modules over a commutative ring

R is exactly the same.

11 Dual space and trace

159

As the example we gave suggested, the basis of V ⊗k W is literally the “product”
of the bases of V and W . In particular, this fulﬁlls our desire that dim(V ⊗k W ) =
dim V · dim W .

Proposition 11.1.4 (Basis of V ⊗ W )
Let V and W be ﬁnite-dimensional k-vector spaces. If e1, . . . , em is a basis of V
and f1, . . . , fn is a basis of W , then the basis of V ⊗k W is precisely ei ⊗ fj, where
i = 1, . . . , m and j = 1, . . . , n.

Proof. Omitted; it’s easy at least to see that this basis is spanning.

Example 11.1.5 (Explicit computation)
Let V have basis e1, e2 and W have basis f1, f2. Let v = 3e1 + 4e2 ∈ V and
w = 5f1 + 6f2 ∈ W . Let’s write v ⊗ w in this basis for V ⊗k W :

v ⊗ w = (3e1 + 4e2) ⊗ (5f1 + 6f2)

= (3e1) ⊗ (5f1) + (4e2) ⊗ (5f1) + (3e1) ⊗ (6f2) + (4e2) ⊗ (6f2)
= 15(e1 ⊗ f1) + 20(e2 ⊗ f1) + 18(e1 ⊗ f2) + 24(e2 ⊗ f2).

So you can see why tensor products are a nice “product” to consider if we’re really
interested in V × W in a way that’s more intimate than just a direct sum.

Abuse of Notation 11.1.6. Moving forward, we’ll almost always abbreviate ⊗k to just
⊗, since k is usually clear.

Remark 11.1.7 — Observe that to deﬁne a linear map V ⊗ W → X, I only have to
say what happens to each pure tensor v ⊗ w, since the pure tensors generate V ⊗ W .
But again, keep in mind that V ⊗ W consists of sums of these pure tensors! In other
words, V ⊗ W is generated by pure tensors.

Remark 11.1.8 — Much like the Cartesian product A × B of sets, you can tensor
together any two vector spaces V and W over the same ﬁeld k; the relationship
between V and W is completely irrelevant. One can think of the ⊗ as a “wall”
through which one can pass scalars in k, but otherwise keeps the elements of V and
W separated. Thus, ⊗ is content-agnostic.
This also means that even if V and W have some relation to each other, the
tensor product doesn’t remember this. So for example v ⊗ 1 (cid:54)= 1 ⊗ v, just like
(g, 1G) (cid:54)= (1G, g) in the group G × G.

§11.2 Dual space

Prototypical example for this section: Rotate a column matrix by 90 degrees.

Consider the following vector space:

160

Napkin, by Evan Chen (v1.5.20190718)

Example 11.2.1 (Functions from R3 → R)
The set of real functions f (x, y, z) is an inﬁnite-dimensional real vector space. Indeed,
we can add two functions to get f + g, and we can think of functions like 2f .

This is a terrifyingly large vector space, but you can do some reasonable reductions. For
example, you can restrict your attention to just the linear maps from R3 to R.

That’s exactly what we’re about to do. This deﬁnition might seem strange at ﬁrst,

but bear with me.

Deﬁnition 11.2.2. Let V be a k-vector space. Then V ∨, the dual space of V , is
deﬁned as the vector space whose elements are linear maps from V to k.

The addition and multiplication are pointwise: it’s the same notation we use when we
write cf + g to mean c · f (x) + g(x). The dual space itself is less easy to think about.
Let’s try to ﬁnd a basis for V ∨. First, here is a very concrete interpretation of the
vector space. Suppose for example V = R3. We can think of elements of V as column
matrices, like

2
5
9

v =
 ∈ V.
f =(cid:2)3 4 5(cid:3) ∈ V ∨.
 = 71.
f (v) =(cid:2)3 4 5(cid:3)

2
5
9

Then a linear map f : V → k can be interpreted as a row matrix :

Then

More precisely: to specify a linear map V → k, I only have to tell you where
each basis element of V goes. In the above example, f sends e1 to 3, e2 to 4, and e3
to 5. So f sends

2e1 + 5e2 + 9e3 (cid:55)→ 2 · 3 + 5 · 4 + 9 · 5 = 71.

Let’s make all this precise.

Proposition 11.2.3 (The dual basis for V ∨)
Let V be a ﬁnite-dimensional vector space with basis e1, . . . , en. For each i consider
the function e∨i

: V → k by

e∨i (ej) =(cid:40)1

0

i = j
i (cid:54)= j.

In more humane terms, e∨i (v) gives the coeﬃcient of ei in v.

Then e∨1 , e∨2 , . . . , e∨n is a basis of V ∨.

11 Dual space and trace

161

Example 11.2.4 (Explicit example of element in V ∨)
In this notation, f = 3e∨1 + 4e∨2 + 5e∨3 . Do you see why the “sum” notation works as
expected here? Indeed

f (e1) = (3e∨1 + 4e∨2 + 5e∨3 )(e1)

= 3e∨1 (e1) + 4e∨2 (e1) + 5e∨3 (e1)
= 3 · 1 + 4 · 0 + 5 · 0 = 3.

That’s exactly what we wanted.

You might be inclined to point out that V ∼= V ∨ at this point, since there’s an obvious
isomorphism ei (cid:55)→ e∨i . You might call it “rotating the column matrix by 90◦”. The
issue is that this isomorphism depends very much on which basis you choose: if I pick a
diﬀerent basis, then the isomorphism will be intrinsically diﬀerent.

It is true that V and V ∨ are isomorphic for ﬁnite-dimensional V , but you should
already know that any two k-vector spaces of the same dimension are isomorphic. In
light of this, the fact that V ∼= V ∨ is not especially impressive.

§11.3 V ∨ ⊗ W gives matrices from V to W

Goal of this section:

If V and W are ﬁnite-dimensional k-vector spaces then V ∨⊗ W represents
linear maps V → W .
Here’s the intuition. If V is three-dimensional and W is ﬁve-dimensional, then we can
think of the maps V → W as a 5 × 3 array of numbers. We want to think of these maps
as a vector space: (since one can add or scale matrices). So it had better be a vector
space with dimension 15, but just saying “k⊕15” is not really that satisfying (what is the
basis?).

To do better, we consider the tensor product
V ∨ ⊗ W

which somehow is a product of maps out of V and the target space W . We claim that
this is in fact the space we want: i.e. there is a natural bijection between elements
of V ∨ ⊗ W and linear maps from V to W .
First, how do we interpret an element of V ∨ ⊗ W as a map V → W ? For concreteness,
suppose V has a basis e1, e2, e3, and W has a basis f1, f2, f3, f4, f5. Consider an element
of V ∨ ⊗ W , say

e∨1 ⊗ (f2 + 2f4) + 4e∨2 ⊗ f5.

We want to interpret this element as a function V → W : so given a v ∈ V , we want to
output an element of W . There’s really only one way to do this: feed in v ∈ V into the
V ∨ guys on the left. That is, take the map

v (cid:55)→ e∨1 (v) · (f2 + 2f4) + 4e∨2 (v) · f5 ∈ W.

So, there’s a natural way to interpret any element ξ1 ⊗ w1 + ··· + ξm ⊗ wm ∈ V ∨ ⊗ W as
a linear map V → W . The claim is that in fact, every linear map V → W has a unique
such interpretation.

First, for notational convenience,

162

Napkin, by Evan Chen (v1.5.20190718)

Deﬁnition 11.3.1. Let Hom(V, W ) denote the set of linear maps from V to W (which
one can interpret as matrices which send V to W ), viewed as a vector space over k. (The
“Hom” stands for homomorphism.)

Question 11.3.2. Identify Hom(V, k) by name.

We can now write down something that’s more true generally.

Theorem 11.3.3 (V ∨ ⊗ W ⇐⇒ linear maps V → W )
Let V and W be ﬁnite-dimensional vector spaces. We described a map

Ψ : V ∨ ⊗ W → Hom(V, W )
by sending ξ1 ⊗ w1 + ··· + ξm ⊗ wm to the linear map

v (cid:55)→ ξ1(v)w1 + ··· + ξm(v)wm.

Then Ψ is an isomorphism of vector spaces, i.e. every linear map V → W can be
uniquely represented as an element of V ∨ ⊗ W in this way.

The above is perhaps a bit dense, so here is a concrete example.

Example 11.3.4 (Explicit example)
Let V = R2 and take a basis e1, e2 of V . Then deﬁne T : V → V by

3 4(cid:21) .
T =(cid:20)1 2

Then Ψ sends T to

T (cid:55)→ e∨1 ⊗ e1 + 2e∨2 ⊗ e1 + 3e∨1 ⊗ e2 + 4e∨2 ⊗ e2 ∈ V ∨ ⊗ V.

The beauty is that the Ψ deﬁnition is basis-free; thus even if we change the basis,
although the above expression will look completely diﬀerent, the actual element in
V ∨ ⊗ V doesn’t change.

Despite this, we’ll indulge ourselves in using coordinates for the proof.

Proof of Theorem 11.3.3. This looks intimidating, but it’s actually not diﬃcult. We
proceed in two steps:

1. First, we check that Ψ is surjective; every linear map has at least one representation
of this form. To see this, take any T : V → W . Suppose V has basis e1, e2, e3 and
that T (e1) = w1, T (e2) = w2 and T (e3) = w3. Then the element

e∨1 ⊗ w1 + e∨2 ⊗ w2 + e∨3 ⊗ w3

works, as it is contrived to agree with T on the basis elements ei.

2. So it suﬃces to check now that dim V ∨ ⊗ W = dim Hom(V, W ). Certainly, V ∨ ⊗ W
has dimension dim V ·dim W . But by viewing Hom(V, W ) as dim V ·dim W matrices,
we see that it too has dimension dim V · dim W .

11 Dual space and trace

163

So there is a natural isomorphism V ∨ ⊗ W ∼= Hom(V, W ). While we did use a basis
liberally in the proof that it works, this doesn’t change the fact that the isomorphism
is “God-given”, depending only on the spirit of V and W itself and not which basis we
choose to express the vector spaces in.

§11.4 The trace

We are now ready to give the deﬁnition of a trace. Recall that a square matrix T can be
thought of as a map T : V → V . According to the above theorem,

Hom(V, V ) ∼= V ∨ ⊗ V

so every map V → V can be thought of as an element of V ∨ ⊗ V . But we can also deﬁne
an evaluation map ev : V ∨ ⊗ V → k by “collapsing” each pure tensor: f ⊗ v (cid:55)→ f (v). So
this gives us a composed map

Hom(V, V ) ∼=- V ∨ ⊗ V

ev - k.

This result is called the trace of a matrix T .

Example 11.4.1 (Example of a trace)
Continuing the previous example,

Tr T = e∨1 (e1) + 2e∨2 (e1) + 3e∨1 (e2) + 4e∨2 (e2) = 1 + 0 + 0 + 4 = 5.

And that is why the trace is the sum of the diagonal entries.

§11.5 A few harder problems to think about

Problem 11A (Trace is sum of eigenvalues). Let V be an n-dimensional vector space
over an algebraically closed ﬁeld k. Let T : V → V be a linear map with λ1, λ2, . . . , λn
(counted with algebraic multiplicity). Show that Tr T = λ1 + ··· + λn.
Problem 11B† (Product of traces). Let T : V → V and S : W → W be linear maps of
ﬁnite-dimensional vector spaces V and W . Consider T ⊗ S : V ⊗ W → V ⊗ W . Prove
that

Tr(T ⊗ S) = Tr(T ) Tr(S).

Problem 11C† (Traces kind of commute). Let T : V → W and S : W → V be linear
maps between ﬁnite-dimensional vector spaces V and W . Show that

Tr(T ◦ S) = Tr(S ◦ T ).

Problem 11D (Putnam 1988). Let V be an n-dimensional vector space. Let T : V → V
be a linear map and suppose there exists n + 1 eigenvectors, any n of which are linearly
independent. Does it follow that T is a scalar multiple of the identity?

12 Determinant

The goal of this chapter is to give the basis-free deﬁnition of the determinant: that is,
we’re going to deﬁne det T for T : V → V without making reference to the encoding for
T . This will make it obvious the determinant of a matrix do not depend on the choice
of basis, and that several properties are vacuously true (e.g. that the determinant is
multiplicative).

The determinant is only deﬁned for ﬁnite-dimensional vector spaces, so if you want
you can restrict your attention to ﬁnite-dimensional vector spaces for this chapter. On
the other hand we do not need the ground ﬁeld to be algebraically closed.

§12.1 Wedge product

Prototypical example for this section: Λ2(R2) gives parallelograms.

We’re now going to deﬁne something called the wedge product. It will look at ﬁrst like

the tensor product V ⊗ V , but we’ll have one extra relation.
with any n.

For simplicity, I’ll ﬁrst deﬁne the wedge product Λ2(V ). But we will later replace 2

Deﬁnition 12.1.1. Let V be a k-vector space. The 2-wedge product Λ2(V ) is the
abelian group generated by elements of the form v ∧ w (where v, w ∈ V ), subject to the
same relations

(v1 + v2) ∧ w = v1 ∧ w + v2 ∧ w
v ∧ (w1 + w2) = v ∧ w1 + v ∧ w2

(c · v) ∧ w = v ∧ (c · w)

plus two additional relations:

v ∧ v = 0

and v ∧ w = −w ∧ v.

As a vector space, its action is given by c · (v ∧ w) = (c · v) ∧ w = v ∧ (c · w).

Exercise 12.1.2. Show that the condition v ∧ w = −(w ∧ v) is actually extraneous: you
can derive it from the fact that v ∧ v = 0. (Hint: expand (v + w) ∧ (v + w) = 0.)

This looks almost exactly the same as the deﬁnition for a tensor product, with two
subtle diﬀerences. The ﬁrst is that we only have V now, rather than V and W as with
the tensor product1 Secondly, there is a new mysterious relation

v ∧ v = 0 =⇒ v ∧ w = −(w ∧ v).

What’s that doing there? It seems kind of weird.

I’ll give you a hint.

1So maybe the wedge product might be more accurately called the “wedge power”!

165

166

Napkin, by Evan Chen (v1.5.20190718)

Example 12.1.3 (Wedge product explicit computation)
Let V = R2, and let v = ae1 + be2, w = ce1 + de2. Now let’s compute v ∧ w in Λ2(V ).

v ∧ w = (ae1 + be2) ∧ (ce1 + de2)

= ac(e1 ∧ e1) + bd(e2 ∧ e2) + ad(e1 ∧ e2) + bc(e2 ∧ e1)
= ad(e1 ∧ e2) + bc(e2 ∧ e1)
= (ad − bc)(e1 ∧ e2).

What is ad − bc? You might already recognize it:
 You might know that the area of the parallelogram formed by v and w is ad − bc.

 You might recognize it as the determinant of(cid:20)a c

b d(cid:21). In fact, you might even know

that the determinant is meant to interpret hypervolumes.

This is absolutely no coincidence. The wedge product is designed to interpret signed
areas. That is, v ∧ w is meant to interpret the area of the parallelogram formed by v and
w. You can see why the condition (cv) ∧ w = v ∧ (cw) would make sense now. And now
of course you know why v ∧ v ought to be zero: it’s an area zero parallelogram!
The miracle of wedge products is that the only additional condition we need to
add to the tensor product axioms is that v ∧ w = −(w ∧ v). Then suddenly, the wedge
will do all our work of interpreting volumes for us.

In analog to earlier:

Proposition 12.1.4 (Basis of Λ2(V ))
Let V be a vector space with basis e1, . . . , en. Then a basis of Λ2(V ) is

where i < j. Hence Λ2(V ) has dimension(cid:0)n
2(cid:1).

ei ∧ ej

Proof. Surprisingly slippery, and also omitted. (You can derive it from the corresponding
theorem on tensor products.)

Now I have the courage to deﬁne a multi-dimensional wedge product. It’s just the

same thing with more wedges.

Deﬁnition 12.1.5. Let V be a vector space and m a positive integer. The space Λm(V )
is generated by wedges of the form

v1 ∧ v2 ∧ ··· ∧ vm

0v=ae1+be2w=ce1+de2v+wad−bc12 Determinant

subject to relations

167

··· ∧ (v1 + v2) ∧ . . . = (··· ∧ v1 ∧ . . . ) + (··· ∧ v2 ∧ . . . )
··· ∧ (cv1) ∧ v2 ∧ . . . = ··· ∧ v1 ∧ (cv2) ∧ . . .

··· ∧ v ∧ v ∧ . . . = 0
··· ∧ v ∧ w ∧ . . . = −(··· ∧ w ∧ v ∧ . . . )

As a vector space

c · (v1 ∧ v2 ∧ ··· ∧ vm) = (cv1) ∧ v2 ∧ ··· ∧ vm = v1 ∧ (cv2) ∧ ··· ∧ vm = . . . .

This deﬁnition is pretty wordy, but in English the three conditions say

 We should be able to add products like before,

 You can put constants onto any of the m components (as is directly pointed out in

the “vector space” action), and

 Switching any two adjacent wedges negates the whole wedge.

So this is the natural generalization of Λ2(V ). You can convince yourself that any element
of the form

should still be zero.

Just like e1∧e2 was a basis earlier, we can ﬁnd the basis for general m and n.

··· ∧ v ∧ ··· ∧ v ∧ . . .

Proposition 12.1.6 (Basis of the wedge product)
Let V be a vector space with basis e1, . . . , en. A basis for Λm(V ) consists of the
elements

ei1 ∧ ei2 ∧ ··· ∧ eim

where

Hence Λm(V ) has dimension(cid:0) n
m(cid:1).

1 ≤ i1 < i2 < ··· < im ≤ n.

Sketch of proof. We knew earlier that ei1 ⊗ ··· ⊗ eim was a basis for the tensor product.
Here we have the additional property that (a) if two basis elements re-appear then the
whole thing becomes zero, thus we should assume the i’s are all distinct; and (b) we
can shuﬄe around elements, and so we arbitrarily decide to put the basis elements in
increasing order.

§12.2 The determinant

Prototypical example for this section: (ae1 + be2) ∧ (ce1 + de2) = (ad − bc)e1 ∧ e2.

Now we’re ready to deﬁne the determinant. Suppose T : V → V is a square matrix.

We claim that the map Λm(V ) → Λm(V ) given on wedges by

v1 ∧ v2 ∧ ··· ∧ vm (cid:55)→ T (v1) ∧ T (v2) ∧ ··· ∧ T (vm)

and extending linearly to all of Λm(V ) is a linear map. (You can check this yourself if
you like.) We call that map Λm(T ).

168

Napkin, by Evan Chen (v1.5.20190718)

Example 12.2.1 (Example of Λm(T ))
In V = R4 with standard basis e1, e2, e3, e4, let T (e1) = e2, T (e2) = 2e3, T (e3) = e3
and T (e4) = 2e2 + e3. Then, for example, Λ2(T ) sends

e1 ∧ e2 + e3 ∧ e4 (cid:55)→ T (e1) ∧ T (e2) + T (e3) ∧ T (e4)

= e2 ∧ 2e3 + e3 ∧ (2e2 + e3)
= 2(e2 ∧ e3 + e3 ∧ e2)
= 0.

Now here’s something interesting. Suppose V has dimension n, and let m = n. Then

Λn(V ) has dimension(cid:0)n

n(cid:1) = 1 — it’s a one dimensional space! Hence Λn(V ) ∼= k.

So Λn(T ) can be thought of as a linear map from k to k. But we know that a linear
map from k to k is just multiplication by a constant. Hence Λn(T ) is multiplication by
some constant.

Deﬁnition 12.2.2. Let T : V → V , where V is an n-dimensional vector space. Then
Λn(T ) is multiplication by a constant c; we deﬁne the determinant of T as c = det T .

Example 12.2.3 (The determinant of a 2 × 2 matrix)
Let V = R2 again with basis e1 and e2. Let

b d(cid:21) .
T =(cid:20)a c

In other words, T (e1) = ae1 + be2 and T (e2) = ce1 + de2.

Now let’s consider Λ2(V ). It has a basis e1 ∧ e2. Now Λ2(T ) sends it to

e1 ∧ e2

Λ2(T )

(cid:55)−−−−→ T (e1) ∧ T (e2) = (ae1 + be2) ∧ (ce1 + de2) = (ad − bc)(e1 ∧ e2).

So Λ2(T ) : Λ2(V ) → Λ2(V ) is multiplication by det T = ad − bc, because it sent
e1 ∧ e2 to (ad − bc)(e1 ∧ e2).

And that is the deﬁnition of a determinant. Once again, since we deﬁned it in terms of
Λn(T ), this deﬁnition is totally independent of the choice of basis. In other words, the
determinant can be deﬁned based on T : V → V alone without any reference to matrices.

Question 12.2.4. Why does Λn(S ◦ T ) = Λn(S) ◦ Λn(T )?

In this way, we also get

det(S ◦ T ) = det(S) det(T )

for free. The general nasty formula for a determinant in terms of the matrix also follows
from our work, and is just a generalization of the work we did for n = 2. Simply write
out

(a11e1 + a21e2 + . . . ) ∧ ··· ∧ (a1ne1 + a2ne2 + ··· + annen)

and do a full expansion.

12 Determinant

169

Exercise 12.2.5. Convince yourself this gives the right answer. (For example, expand this
for n = 3.)

§12.3 Characteristic polynomials, and Cayley-Hamilton

Let’s connect with the theory of eigenvalues. Take a map T : V → V , where V is
n-dimensional over an algebraically closed ﬁeld, and suppose its eigenvalues are λ1, λ2,
. . . , λn (with repetition). Then the characteristic polynomial is given by

pT (X) = (X − λ1)(X − λ2) . . . (X − λn).

Note that if we’ve written T in Jordan form, that is,

T =

λ1
0
0
...
0



∗
λ2
0
...
0

0
∗
λ3
...
0

0
. . .
0
. . .
0
. . .
...
. . .
. . . λn



(here each ∗ is either 0 or 1), then we can hack together the deﬁnition
. . .
. . .
. . .
. . .
. . . X − λn

pT (X) := det (X · idn − T ) = det

X − λ1

X − λ2

X − λ3

∗
0
...
0



0
0
...
0

0
∗
...
0

0
0
0
...

.



The latter deﬁnition is what you’ll see in most linear algebra books because it lets
you deﬁne the characteristic polynomial without mentioning the word “eigenvalue” (i.e.
entirely in terms of arrays of numbers). I’ll admit it does have the merit that it means
that given any matrix, it’s easy to compute the characteristic polynomial and hence
compute the eigenvalues; but I still think the deﬁnition should be done in terms of
eigenvalues to begin with. For instance the determinant deﬁnition obscures the following
theorem, which is actually a completely triviality.

Theorem 12.3.1 (Cayley-Hamilton)
Let T : V → V be a map of ﬁnite-dimensional vector spaces over an algebraically
closed ﬁeld. Then for any T : V → V , the map pT (T ) is the zero map.

Here, by pT (T ) we mean that if

then

pT (X) = X n + cn−1X n−1 + ··· + c0

pT (T ) = T n + cn−1T n−1 + ··· + c1T + c0I

is the zero map, where T k denotes T applied k times. We saw this concept already when
we proved that T had at least one nonzero eigenvector.

170

Napkin, by Evan Chen (v1.5.20190718)

Example 12.3.2 (Example of Cayley-Hamilton using determinant deﬁnition)

Suppose T =(cid:20)1 2
3 4(cid:21). Using the determinant deﬁnition of characteristic polynomial,
we ﬁnd that pT (X) = (X − 1)(X − 4) − (−2)(−3) = X 2 − 5X − 2. Indeed, you can
verify that

T 2 − 5T − 2 =(cid:20) 7

10

15 22(cid:21) − 5 ·(cid:20)1 2

3 4(cid:21) − 2 ·(cid:20)1 0

0 1(cid:21) =(cid:20)0 0
0 0(cid:21) .

If you deﬁne pT without the word eigenvalue, and adopt the evil view that matrices are
arrays of numbers, then this looks like a complete miracle. (Indeed, just look at the
terrible proofs on Wikipedia.)

But if you use the abstract viewpoint of T as a linear map, then the theorem is almost

obvious:

Proof of Cayley-Hamilton. Suppose we write V in Jordan normal form as

where Ji has eigenvalue λi and dimension di. By deﬁnition,

V = J1 ⊕ ··· ⊕ Jm

pT (T ) = (T − λ1)d1(T − λ2)d2 . . . (T − λm)dm.

By deﬁnition, (T − λ1)d1 is the zero map on J1. So pT (T ) is zero on J1. Similarly it’s
zero on each of the other Ji’s — end of story.

Remark 12.3.3 (Tensoring up) — The Cayley-Hamilton theorem holds without the
hypothesis that k is algebraically closed: because for example any real matrix can
be regarded as a matrix with complex coeﬃcients (a trick we’ve mentioned before).
I’ll brieﬂy hint at how you can use tensor products to formalize this idea.

Let’s take the space V = R3, with basis e1, e2, e3. Thus objects in V are of the
form r1e1 + r2e2 + r3e3 where r1, r2, r3 are real numbers. We want to consider
essentially the same vector space, but with complex coeﬃcients zi rather than real
coeﬃcients ri.

So here’s what we do: view C as a R-vector space (with basis {1, i}, say) and

consider the complexiﬁcation

Then you can check that our elements are actually of the form

VC := C ⊗R V.

z1 ⊗ e1 + z2 ⊗ e2 + z3 ⊗ e3.

Here, the tensor product is over R, so we have z ⊗ rei = (zr) ⊗ ei for r ∈ R. Then
VC can be thought as a three-dimensional vector space over C, with basis 1 ⊗ ei for
i ∈ {1, 2, 3}. In this way, the tensor product lets us formalize the idea that we “fuse
on” complex coeﬃcients.
If T : V → W is a map, then TC : VC → WC is just the map z ⊗ v (cid:55)→ z ⊗ T (v).
You’ll see this written sometimes as TC = id ⊗ T . One can then apply theorems to
TC and try to deduce the corresponding results on T .

12 Determinant

171

§12.4 A few harder problems to think about

Problem 12A (Column operations). Show that for any real numbers xij (here 1 ≤
i, j ≤ n) we have

det

x11 x12
x21 x22
...
...
xn1 xn2

. . . x1n
. . . x2n
...
. . .
. . . xnn



= det

x11 + cx12 x12
x21 + cx22 x22
...
xn1 + cxn2 xn2

...

. . . x1n
. . . x2n
...
. . .
. . . xnn

.



Problem 12B (Determinant is product of eigenvalues). Let V be an n-dimensional
vector space over an algebraically closed ﬁeld k. Let T : V → V be a linear map with λ1,
λ2, . . . , λn (counted with algebraic multiplicity). Show that det T = λ1 . . . λn.

Problem 12C (Exponential matrix). Let X be n × n matrix with complex coeﬃcients.
We deﬁne the exponential map by

exp(X) = 1 + X +

X 2
2!

+

X 3
3!

+ . . .

(take it for granted that this converges to some n × n matrix). Prove that

det(exp(X)) = eTr X .

Problem 12D (Extension to Problem 9B†). Let T : V → V be a map of ﬁnite-
dimensional vector spaces. Prove that T is an isomorphism if and only if det T (cid:54)= 0.
Problem 12E (Based on Sweden 2010). A herd of 1000 cows of nonzero weight is given.
Prove that we can remove one cow such that the remaining 999 cows cannot be split into
two halves of equal weights.

Problem 12F (Putnam 2015). Deﬁne S to be the set of real matrices(cid:0) a b
a, b, c, d form an arithmetic progression in that order. Find all M ∈ S such that for
some integer k > 1, M k ∈ S.
Problem 12G. Let V be a ﬁnite-dimensional vector space over K and T : V → V .
Show that

c d(cid:1) such that

det(a · idV − T ) =

adim V −n · (−1)n Tr (Λn(T ))

dim V(cid:88)n=0

where the trace is taking by viewing Λn(T ) : Λn(V ) → Λn(V ).

13 Inner product spaces

It will often turn out that our vector spaces which look more like Rn not only have the
notion of addition, but also a notion of orthogonality and the notion of distance. All this
is achieved by endowing the vector space with a so-called inner form, which you likely
already know as the “dot product” for Rn. Indeed, in Rn you already know that

 v · w = 0 if and only if v and w are perpendicular, and
 |v|2 = v · v.

The purpose is to quickly set up this structure in full generality. Some highlights of the
chapter:

 We’ll see that the high school “dot product” formulation is actually very natural:

it falls out from the two axioms we listed above. If you ever wondered why(cid:80) aibi

behaves as nicely as it does, now you’ll know.

 We show how the inner form can be used to make V into a metric space, giving it

more geometric structure.

 A few chapters later, we’ll identify V ∼= V ∨ in a way that wasn’t possible before,
and as a corollary deduce the nice result that symmetric matrices with real entries
always have real eigenvalues.

Throughout this chapter, all vector spaces are over C or R, unless otherwise speciﬁed.
We’ll generally prefer working over C instead of R since, like we saw in the chapter on
spectral theory, C is algebraically closed in a nice way that R is not. Every real matrix
can be thought of as a matrix with complex entries anyways.

§13.1 The inner product

Prototypical example for this section: Dot product in Rn.

§13.1.i For real numbers: bilinear forms

First, let’s deﬁne the inner form for real spaces. Rather than the notation v · w it is most
customary to use (cid:104)v, w(cid:105) for general vector spaces.
Deﬁnition 13.1.1. Let V be a real vector space. A real inner form1 is a function

(cid:104)•,•(cid:105) : V × V → R

which satisﬁes the following properties:

 The form is symmetric: for any v, w ∈ V we have

(cid:104)v, w(cid:105) = (cid:104)w, v(cid:105) .

Of course, one would expect this property from a product.

1Other names include “inner product”, “dot product”, “positive deﬁnite nondegenerate symmetric

bilinear form”, . . .

173

174

Napkin, by Evan Chen (v1.5.20190718)

 The form is bilinear, or linear in both arguments, meaning that (cid:104)−, v(cid:105) and

(cid:104)v,−(cid:105) are linear functions for any ﬁxed v. Spelled explicitly this means that

(cid:104)cx, v(cid:105) = c(cid:104)x, v(cid:105)

(cid:104)x + y, v(cid:105) = (cid:104)x, v(cid:105) + (cid:104)y, v(cid:105) .

and similarly if v was on the left. This is often summarized by the single equation
(cid:104)cx + y, z(cid:105) = c(cid:104)x, z(cid:105) + (cid:104)y, z(cid:105).

 The form is positive deﬁnite, meaning (cid:104)v, v(cid:105) ≥ 0 is a nonnegative real number,

and equality takes place only if v = 0V .

Exercise 13.1.2. Show that linearity in the ﬁrst argument plus symmetry already gives
you linearity in the second argument, so we could edit the above deﬁnition by only requiring
(cid:104)−, v(cid:105) to be linear.

Example 13.1.3 (Rn)
As we already know, one can deﬁne the inner form on Rn as follows. Let e1 =
(1, 0, . . . , 0), e2 = (0, 1, . . . , 0), . . . , en = (0, . . . , 0, 1) be the usual basis. Then we let

(cid:104)a1e1 + ··· + anen, b1e1 + ··· + bnen(cid:105) := a1b1 + ··· + anbn.

It’s easy to see this is bilinear (symmetric and linear in both arguments). To see it
is positive deﬁnite, note that if ai = bi then the dot product is a2
n, which
is zero exactly when all ai are zero.

1 + ··· + a2

§13.1.ii For complex numbers: sesquilinear forms

The deﬁnition for a complex product space is similar, but has one diﬀerence: rather than
symmetry we instead have conjugate symmetry meaning (cid:104)v, w(cid:105) = (cid:104)w, v(cid:105). Thus, while we
still have linearity in the ﬁrst argument, we actually have a diﬀerent linearity for the
second argument. To be explicit:

Deﬁnition 13.1.4. Let V be a complex vector space. A complex inner product is a
function

which satisﬁes the following properties:

(cid:104)•,•(cid:105) : V × V → C

 The form has conjugate symmetry, which means that for any v, w ∈ V we have

(cid:104)v, w(cid:105) = (cid:104)w, v(cid:105).

 The form is sesquilinear (the name means “one-and-a-half linear”). This means

that:

– The form is linear in the ﬁrst argument, so again we have

(cid:104)x + y, v(cid:105) = (cid:104)x, v(cid:105) + (cid:104)y, v(cid:105)

(cid:104)cx, v(cid:105) = c(cid:104)x, v(cid:105) .

Again this is often abbreviated to the single line (cid:104)cx + y, v(cid:105) = c(cid:104)x, v(cid:105) + (cid:104)y, v(cid:105)
in the literature.

13 Inner product spaces

175

– However, it is now anti-linear in the second argument: for any complex

number c and vectors x and y we have

(cid:104)v, x + y(cid:105) = (cid:104)v, x(cid:105) + (cid:104)v, y(cid:105)

(cid:104)v, cx(cid:105) = c(cid:104)v, x(cid:105) .

Note the appearance of the complex conjugate c, which is new! Again, we can
abbreviate this to just (cid:104)v, cx + y(cid:105) = (cid:104)v, x(cid:105) + c(cid:104)v, y(cid:105) if we only want to write
one equation.

 The form is positive deﬁnite, meaning (cid:104)v, v(cid:105) is a nonnegative real number, and

equals zero exactly when v = 0V .

Exercise 13.1.5. Show that anti-linearity follows from conjugate symmetry plus linearity
in the ﬁrst argument.

Example 13.1.6 (Cn)
The dot product in Cn is deﬁned as follows: let e1, e2, . . . , en be the standard basis.
For complex numbers wi, zi we set

(cid:104)w1e1 + ··· + wnen, z1e1 + ··· + znen(cid:105) := w1z1 + ··· + wnzn.

Question 13.1.7. Check that the above is in fact a complex inner form.

§13.1.iii Inner product space

It’ll be useful to treat both types of spaces simultaneously:

Deﬁnition 13.1.8. An inner product space is either a real vector space equipped
with a real inner form, or a complex vector space equipped with a complex inner form.
A linear map between inner product spaces is a map between the underlying vector

spaces (we do not require any compatibility with the inner form).

Remark 13.1.9 (Why sesquilinear?) — The above example explains one reason why
we want to satisfy conjugate symmetry rather than just symmetry. If we had tried

to deﬁne the dot product as(cid:80) wizi, then we would have lost the condition of being
positive deﬁnite, because there is no guarantee that (cid:104)v, v(cid:105) =(cid:80) z2
i will even be a real
number at all. On the other hand, with conjugate symmetry we actually enforce
(cid:104)v, v(cid:105) = (cid:104)v, v(cid:105), i.e. (cid:104)v, v(cid:105) ∈ R for every v.
Let’s make this point a bit more forcefully. Suppose we tried to put a bilinear
form (cid:104)−,−(cid:105), on a complex vector space V . Let e be any vector with (cid:104)e, e(cid:105) = 1 (a
unit vector). Then we would instead get (cid:104)ie, ie(cid:105) = −(cid:104)e, e(cid:105) = −1; this is a vector
with length −1, which is not okay! That’s why it is important that, when we have a
complex inner product space, our form is sesquilinear, not bilinear.

Now that we have a dot product, we can talk both about the norm and orthogonality.

176

§13.2 Norms

Napkin, by Evan Chen (v1.5.20190718)

Prototypical example for this section: Rn becomes its usual Euclidean space with the
vector norm.

The inner form equips our vector space with a notion of distance, which we call the

norm.

Deﬁnition 13.2.1. Let V be an inner product space. The norm of v ∈ V is deﬁned by

(cid:107)v(cid:107) =(cid:112)(cid:104)v, v(cid:105).

This deﬁnition makes sense because we assumed our form to be positive deﬁnite, so (cid:104)v, v(cid:105)
is a nonnegative real number.

Example 13.2.2 (Rn and Cn are normed vector spaces)
When V = Rn or V = Cn with the standard dot product norm, then the norm of v
corresponds to the absolute value that we are used to.

Our goal now is to prove that

With the metric d(v, w) = (cid:107)v − w(cid:107), V becomes a metric space.

Question 13.2.3. Verify that d(v, w) = 0 if and only if v = w.

So we just have to establish the triangle inequality. Let’s now prove something we all
know and love, which will be a stepping stone later:

Lemma 13.2.4 (Cauchy-Schwarz)
Let V be an inner product space. For any v, w ∈ V we have

|(cid:104)v, w(cid:105)| ≤ (cid:107)v(cid:107)(cid:107)w(cid:107)

with equality if and only if v and w are linearly dependent.

Proof. The theorem is immediate if (cid:104)v, w(cid:105) = 0. It is also immediate if (cid:107)v(cid:107)(cid:107)w(cid:107) = 0, since
then one of v or w is the zero vector. So henceforth we assume all these quantities are
nonzero (as we need to divide by them later).

The key to the proof is to think about the equality case: we’ll use the inequality

(cid:104)cv − w, cv − w(cid:105) ≥ 0. Deferring the choice of c until later, we compute

0 ≤ (cid:104)cv − w, cv − w(cid:105)
= (cid:104)cv, cv(cid:105) − (cid:104)cv, w(cid:105) − (cid:104)w, cv(cid:105) + (cid:104)w, w(cid:105)
= |c|2 (cid:104)v, v(cid:105) − c(cid:104)v, w(cid:105) − c(cid:104)w, v(cid:105) + (cid:104)w, w(cid:105)
= |c|2 (cid:107)v(cid:107)2 + (cid:107)w(cid:107)2 − c(cid:104)v, w(cid:105) − c(cid:104)v, w(cid:105)

2 Re [c(cid:104)v, w(cid:105)] ≤ |c|2 (cid:107)v(cid:107)2 + (cid:107)w(cid:107)2

13 Inner product spaces

177

At this point, a good choice of c is

since then

c = (cid:107)w(cid:107)

(cid:107)v(cid:107) · |(cid:104)v, w(cid:105)|
(cid:104)v, w(cid:105)

c(cid:104)v, w(cid:105) = (cid:107)w(cid:107)
(cid:107)v(cid:107) |(cid:104)v, w(cid:105)| ∈ R
|c| = (cid:107)w(cid:107)
(cid:107)v(cid:107)

whence the inequality becomes

2(cid:107)w(cid:107)
(cid:107)v(cid:107) |(cid:104)v, w(cid:105)| ≤ 2(cid:107)w(cid:107)2
|(cid:104)v, w(cid:105)| ≤ (cid:107)v(cid:107)(cid:107)w(cid:107) .

Thus:

Theorem 13.2.5 (Triangle inequality)

We always have

with equality if and only if v and w are linearly dependent.

(cid:107)v(cid:107) + (cid:107)w(cid:107) ≥ (cid:107)v + w(cid:107)

Exercise 13.2.6. Prove this by squaring both sides, and applying Cauchy-Schwarz.

In this way, our vector space now has a topological structure of a metric space.

§13.3 Orthogonality

Prototypical example for this section: Still Rn!

Our next goal is to give the geometric notion of “perpendicular”. The deﬁnition is

easy enough:

Deﬁnition 13.3.1. Two nonzero vectors v and w in an inner product space are orthog-
onal if (cid:104)v, w(cid:105) = 0.

As we expect from our geometric intuition in Rn, this implies independence:

Lemma 13.3.2 (Orthogonal vectors are independent)
Any set of pairwise orthogonal vectors v1, v2, . . . , vn, with (cid:107)vi(cid:107) (cid:54)= 0 for each i, is
linearly independent.

Proof. Consider a dependence

a1v1 + ··· + anvn = 0

for ai in R or C. Then

Hence a1 = 0, since we assumed (cid:107)v1(cid:107) (cid:54)= 0. Similarly a2 = ··· = am = 0.

0V =(cid:68)v1,(cid:88) aivi(cid:69) = a1 (cid:107)vi(cid:107)2 .

178

Napkin, by Evan Chen (v1.5.20190718)

In light of this, we can now consider a stronger condition on our bases:

Deﬁnition 13.3.3. An orthonormal basis of a ﬁnite-dimensional inner product space
V is a basis e1, . . . , en such that (cid:107)ei(cid:107) = 1 for every i and (cid:104)ei, ej(cid:105) = 0 for any i (cid:54)= j.

Example 13.3.4 (Rn and Cn have standard bases)
In Rn and Cn equipped with the standard dot product, the standard basis e1, . . . ,
en is also orthonormal.

This is no loss of generality:

Theorem 13.3.5 (Gram-Schmidt)

Let V be a ﬁnite-dimensional inner product space. Then it has an orthonormal
basis.

Sketch of Proof. One constructs the orthonormal basis explicitly from any basis e1, . . . ,
en of V . Deﬁne proju(v) = (cid:104)v,u(cid:105)
(cid:104)u,u(cid:105)

u. Then recursively deﬁne

u1 = e1
u2 = e2 − proju1(e2)
u3 = e3 − proju1(e3) − proju2(e3)

...

un = en − proju1(en) − ··· − projun−1(en).

One can show the ui are pairwise orthogonal and not zero.

Thus, we can generally assume our bases are orthonormal.

Worth remarking:

Example 13.3.6 (The dot product is the “only” inner form)
Let V be a ﬁnite-dimensional inner product space, and consider any orthonormal
basis e1, . . . , en. Then we have that

(cid:104)a1e1 + ··· + anen, b1e1 + ··· + bnen(cid:105) =

n(cid:88)i,j=1

aibj (cid:104)ei, ej(cid:105) =

aibi

n(cid:88)i=1

owing to the fact that the {ei} are orthonormal.

And now you know why the dot product expression is so ubiquitous.

§13.4 Hilbert spaces

In algebra we are usually scared of inﬁnity, and so when we deﬁned a basis of a vanilla
vector space many chapters ago, we only allowed ﬁnite linear combinations. However, if
we have an inner product space, then it is a metric space and we can sometimes actually
talk about convergence.

Here is how it goes:

13 Inner product spaces

179

Deﬁnition 13.4.1. A Hilbert space is a inner product space V , such that the corre-
sponding metric space is complete.

In that case, it will now often make sense to take inﬁnite linear combinations, because
we can look at the sequence of partial sums and let it converge. Here is how we might do
it. Let’s suppose we have e1, e2, . . . an inﬁnite sequence of vectors with norm 1 and which
are pairwise orthogonal. Suppose c1, c2, . . . , is a sequence of real or complex numbers.
Then consider the sequence

v1 = c1e1
v2 = c1e1 + c2e2
v3 = c1e1 + c2e2 + c3e3

...

Proposition 13.4.2 (Convergence criteria in a Hilbert space)

The sequence (vi) deﬁned above converges if and only if(cid:80)|ci|2 < ∞.

Proof. This will make more sense if you read Chapter 26, so you could skip this proof
if you haven’t read the chapter. The sequence vi converges if and only if it is Cauchy,
meaning that when i < j,

(cid:107)vj − vi(cid:107)2 = |ci|2 + ··· + |cj−1|2

tends to zero as i and j get large. This is equivalent to the sequence sn = |c1|2 +··· +|cn|2
being Cauchy.
Since R is complete, sn is Cauchy if and only if it converges. Since sn consists of
nonnegative real numbers, that is if and only if sn is bounded, or(cid:80)i |ci|2 < ∞.

Thus, when we have a Hilbert space, we change our deﬁnition slightly:

Deﬁnition 13.4.3. A orthonormal basis for a Hilbert space V is a (possibly inﬁnite)
sequence e1, e2, . . . , of vectors such that

 (cid:104)ei, e1(cid:105) = 1 for all i,
 (cid:104)ei, ej(cid:105) = 0 for i (cid:54)= j, i.e. the vectors are pairwise orthogonal
 every element of V can be expressed uniquely as an inﬁnite linear combination

(cid:88)i
where(cid:80)i |ci|2 < ∞, as described above.

ciei

That’s the oﬃcial deﬁnition, anyways. (Note that if dim V < ∞, this agrees with our
usual deﬁnition, since then there are only ﬁnitely many ei.) But for our purposes you
can mostly not worry about it and instead think:

A Hilbert space is an inner product space whose basis requires inﬁnite
linear combinations, not just ﬁnite ones.

The technical condition(cid:80)|ci|2 < ∞ is exactly the one which ensures the inﬁnite sum

makes sense.

180

Napkin, by Evan Chen (v1.5.20190718)

§13.5 A few harder problems to think about

Problem 13A (Pythagorean theorem). Show that if (cid:104)v, w(cid:105) = 0 in an inner product
space, then (cid:107)v(cid:107)2 + (cid:107)w(cid:107)2 = (cid:107)v + w(cid:107)2.
Problem 13B(cid:63) (Finite-dimensional =⇒ Hilbert). Show that a ﬁnite-dimensional inner
product space is a Hilbert space.

Problem 13C (Taiwan IMO camp). In a town there are n people and k clubs. Each
club has an odd number of members, and any two clubs have an even number of common
members. Prove that k ≤ n.
Problem 13D(cid:63) (Inner product structure of tensors). Let V and W be ﬁnite-dimensional
inner product spaces over k, where k is either R or C.

(a) Find a canonical way to make V ⊗k W into an inner product space too.
(b) Let e1, . . . , en be an orthonormal basis of V and f1, . . . , fm be an orthonormal basis

of W . What’s an orthonormal basis of V ⊗ W ?

Problem 13E (Putnam 2014). Let n be a positive integer. What is the largest k for
which there exist n × n matrices M1, . . . , Mk and N1, . . . , Nk with real entries such that
for all i and j, the matrix product MiNj has a zero entry somewhere on its diagonal if
and only if i (cid:54)= j?
Problem 13F (Sequence space). Consider the space (cid:96)2 of inﬁnite sequences of real

numbers a = (a1, a2, . . . ) satisfying(cid:80)i a2

i < ∞. We equip it with the dot product

(cid:104)a, b(cid:105) =(cid:88)i

aibi.

Is this a Hilbert space? If so, identify a Hilbert basis.

14 Bonus: Fourier analysis

Now that we’ve worked hard to deﬁne abstract inner product spaces, I want to give an

(optional) application: how to set up Fourier analysis correctly, using this language.

For fun, I also prove a form of Arrow’s Impossibility Theorem using binary Fourier

analysis.

In what follows, we let T = R/Z denote the “circle group”, thought of as the additive
group of “real numbers modulo 1”. There is a canonical map e : T → C sending T to the
complex unit circle, given by

e(θ) = exp(2πiθ).

§14.1 Synopsis

Suppose we have a domain Z and are interested in functions f : Z → C. Naturally, the
set of such functions form a complex vector space. We like to equip the set of such
functions with an positive deﬁnite inner product.

The idea of Fourier analysis is to then select an orthonormal basis for this set of
functions, say (eξ)ξ, which we call the characters; the indexing ξ are called frequencies.
In that case, since we have a basis, every function f : Z → C becomes a sum

f (x) =(cid:88)ξ (cid:98)f (ξ)eξ

where (cid:98)f (ξ) are complex coeﬃcients of the basis; appropriately we call (cid:98)f the Fourier

coeﬃcients. The variable x ∈ Z is referred to as the physical variable. This is generally
good because the characters are deliberately chosen to be nice “symmetric” functions,
like sine or cosine waves or other periodic functions. Thus we decompose an arbitrarily
complicated function into a sum on nice ones.

§14.2 A reminder on Hilbert spaces

For convenience, we record a few facts about orthonormal bases.

Proposition 14.2.1 (Facts about orthonormal bases)

Let V be a complex Hilbert space with inner form (cid:104)−,−(cid:105) and suppose x =(cid:80)ξ aξeξ
and y =(cid:80)ξ bξeξ where eξ are an orthonormal basis. Then

|aξ|2

(cid:104)x, x(cid:105) =(cid:88)ξ
(cid:104)x, y(cid:105) =(cid:88)ξ

aξ = (cid:104)x, eξ(cid:105)

aξbξ.

181

182

Napkin, by Evan Chen (v1.5.20190718)

Exercise 14.2.2. Prove all of these. (You don’t need any of the preceding section, it’s only
there to motivate the notation with lots of scary ξ’s.)

In what follows, most of the examples will be of ﬁnite-dimensional inner product spaces
(which are thus Hilbert spaces), but the example of “square-integrable functions” will
actually be an inﬁnite dimensional example. Fortunately, as I alluded to earlier, this is
no cause for alarm and you can mostly close your eyes and not worry about inﬁnity.

§14.3 Common examples

§14.3.i Binary Fourier analysis on {±1}n
Let Z = {±1}n for some positive integer n, so we are considering functions f (x1, . . . , xn)
accepting binary values. Then the functions Z → C form a 2n-dimensional vector space
CZ, and we endow it with the inner form
1

In particular,

(cid:104)f, g(cid:105) =

f (x)g(x).

(cid:104)f, f(cid:105) =

|f (x)|2

2n(cid:88)x∈Z
2n(cid:88)x∈Z

1

is the average of the squares; this establishes also that (cid:104)−,−(cid:105) is positive deﬁnite.

In that case, the multilinear polynomials form a basis of CZ, that is the polynomials

χS(x1, . . . , xn) =(cid:89)s∈S

xs.

Exercise 14.3.1. Show that they’re actually orthonormal under (cid:104)−,−(cid:105). This proves they
form a basis, since there are 2n of them.

Thus our frequency set is actually the subsets S ⊆ {1, . . . , n}. Thus, we have a decompo-
sition

f = (cid:88)S⊆{1,...,n}(cid:98)f (S)χS.

Example 14.3.2 (An example of binary Fourier analysis)
Let n = 2. Then binary functions {±1}2 → C have a basis given by the four
polynomials

1,

x1,

x2,

x1x2.

For example, consider the function f which is 1 at (1, 1) and 0 elsewhere. Then we
can put

f (x1, x2) =

=

(1 + x1 + x2 + x1x2) .

x1 + 1

x2 + 1

·

2

2

1
4

So the Fourier coeﬃcients are (cid:98)f (S) = 1
This notion is useful in particular for binary functions f : {±1}n → {±1}; for these
functions (and products thereof), we always have (cid:104)f, f(cid:105) = 1.
It is worth noting that the frequency ∅ plays a special role:

4 for each of the four S’s.

14 Bonus: Fourier analysis

183

Exercise 14.3.3. Show that

1

|Z|(cid:88)x∈Z

(cid:98)f (∅) =

f (x).

§14.3.ii Fourier analysis on ﬁnite groups Z
This time, suppose we have a ﬁnite abelian group Z, and consider functions Z → C; this
is a |Z|-dimensional vector space. The inner product is the same as before:

(cid:104)f, g(cid:105) =

1

|Z|(cid:88)x∈Z

f (x)g(x).

To proceed, we’ll need to be able to multiply two elements of Z. This is a bit of a
nuisance since it actually won’t really matter what map I pick, so I’ll move briskly; feel
free to skip most or all of the remaining paragraph.

Deﬁnition 14.3.4. We select a symmetric non-degenerate bilinear form

satisfying the following properties:

· : Z × Z → T

 ξ · (x1 + x2) = ξ · x1 + ξ · x2 and (ξ1 + ξ2) · x = ξ1 · x + ξ2 · x (this is the word
“bilinear”)

 · is symmetric,
 For any ξ (cid:54)= 0, there is an x with ξ · x (cid:54)= 0 (this is the word “nondegenerate”).

Example 14.3.5 (The form on Z/nZ)
If Z = Z/nZ then ξ · x = (ξx)/n satisﬁes the above.

In general, it turns out ﬁnite abelian groups decompose as the sum of cyclic groups (see
Section 18.1), which makes it relatively easy to ﬁnd such a ·; but as I said the choice
won’t matter, so let’s move on.

Now for the fun part: deﬁning the characters.

Proposition 14.3.6 (eξ are orthonormal)
For each ξ ∈ Z we deﬁne the character

eξ(x) = e(ξ · x).

The |Z| characters form an orthonormal basis of the space of functions Z → |CC.

Proof. I recommend skipping this one, but it is:

(cid:10)eξ, eξ(cid:48)(cid:11) =

=

=

1

1

|Z|(cid:88)x∈Z
|Z|(cid:88)x∈Z
|Z|(cid:88)x∈Z

1

e(ξ · x)e(ξ(cid:48) · x)

e(ξ · x)e(−ξ(cid:48) · x)
e(cid:0)(ξ − ξ(cid:48)) · x(cid:1) .

184

Napkin, by Evan Chen (v1.5.20190718)

In this way, the set of frequencies is also Z, but the ξ ∈ Z play very diﬀerent roles from

the “physical” x ∈ Z. Here is an example which might be enlightening.

Example 14.3.7 (Cube roots of unity ﬁlter)
Suppose Z = Z/3Z, with the inner form given by ξ · x = (ξx)/3. Let ω = exp( 2
be a primitive cube root of unity. Note that

3 πi)

eξ(x) =

1
ωx
ω2x

ξ = 0
ξ = 1
ξ = 2.

Then given f : Z → C with f (0) = a, f (1) = b, f (2) = c, we obtain
a + ωb + ω2c

a + ω2b + ωc

a + b + c

f (x) =

3

· 1 +

3

· ωx +

3

· ω2x.

In this way we derive that the transforms are

a + b + c

3

a + ω2b + ωc

3

a + ωb + ω2c

3

.

(cid:98)f (0) =
(cid:98)f (1) =
(cid:98)f (2) =

Exercise 14.3.8. Show that in analogy to (cid:98)f (∅) for binary Fourier analysis, we now have

1

f (x).

|Z|(cid:88)x∈Z

(cid:98)f (0) =

Olympiad contestants may recognize the previous example as a “roots of unity ﬁlter”,
which is exactly the point. For concreteness, suppose one wants to compute

In that case, we can consider the function

(cid:18)1000
0 (cid:19) +(cid:18)1000

3 (cid:19) + ··· +(cid:18)1000
999(cid:19).
w : Z/3 → C.

such that w(0) = 1 but w(1) = w(2) = 0. By abuse of notation we will also think of w as

a function w : Z (cid:16) Z/3 → C. Then the sum in question is

(cid:88)n (cid:18)1000

n (cid:19)w(n) =(cid:88)n (cid:18)1000

n (cid:19) (cid:88)k=0,1,2(cid:98)w(k)ωkn
= (cid:88)k=0,1,2(cid:98)w(k)(cid:88)n (cid:18)1000
n (cid:19)ωkn
= (cid:88)k=0,1,2(cid:98)w(k)(1 + ωk)n.

3 , and we have evaluated the desired
sum. More generally, we can take any periodic weight w and use Fourier analysis in order
to interchange the order of summation.

In our situation, we have (cid:98)w(0) = (cid:98)w(1) = (cid:98)w(2) = 1

14 Bonus: Fourier analysis

185

Example 14.3.9 (Binary Fourier analysis)
Suppose Z = {±1}n, viewed as an abelian group under pointwise multiplication
hence isomorphic to (Z/2Z)⊕n. Assume we pick the dot product deﬁned by

ξ · x =

1

2(cid:88)i

ξixi

where ξ = (ξ1, . . . , ξn) and x = (x1, . . . , xn).

Thus Fourier analysis on a ﬁnite group Z subsumes binary Fourier analysis.

We claim this coincides with the ﬁrst example we gave. Indeed, let S ⊆ {1, . . . , n}
and let ξ ∈ {±1}n which is −1 at positions in S, and +1 at positions not in S. Then
the character χS form the previous example coincides with the character eξ in the
new notation. In particular, (cid:98)f (S) = (cid:98)f (ξ).
§14.3.iii Fourier series for functions L2([−π, π])
This is the most famous one, and hence the one you’ve heard of.
Deﬁnition 14.3.10. The space L2([−π, π]) consists of all functions f : [−π, π] → C such
that the integral(cid:82)[−π,π] |f (x)|2 dx exists and is ﬁnite, modulo the relation that a function
which is zero “almost everywhere” is considered to equal zero.1

It is made into an inner product space according to

(cid:104)f, g(cid:105) =

1

2π(cid:90)[−π,π]

f (x)g(x).

It turns out (we won’t prove) that this is an (inﬁnite-dimensional) Hilbert space!
Now, the beauty of Fourier analysis is that this space has a great basis:

Theorem 14.3.11 (The classical Fourier basis)

For each integer n, deﬁne

en(x) = exp(inx).

Then en form an orthonormal basis of the Hilbert space L2([−π, π]).

Thus this time the frequency set Z is inﬁnite, and we have

f (x) =(cid:88)n (cid:98)f (n) exp(inx)

almost everywhere

for coeﬃcients (cid:98)f (n) with(cid:80)n(cid:12)(cid:12)(cid:12)(cid:98)f (n)(cid:12)(cid:12)(cid:12)

2

call this a Fourier series to reﬂect the fact that the index is n ∈ Z.

< ∞. Since the frequency set is indexed by Z, we

1We won’t deﬁne this, yet, as it won’t matter to us for now. But we will elaborate more on this in the

parts on measure theory.
There is one point at which this is relevant. Often we require that the function f satisﬁes
f (−π) = f (π), so that f becomes a periodic function, and we can think of it as f : T → C. This
makes no essential diﬀerence since we merely change the value at one point.

186

Napkin, by Evan Chen (v1.5.20190718)

Exercise 14.3.12. Show once again

1

2π(cid:90)[−π,π]

f (x).

(cid:98)f (0) =

§14.4 Summary, and another teaser

We summarize our various ﬂavors of Fourier analysis in the following table.

Physical var Frequency var
{±1}n

Type
Binary
ξ ∈ Z, choice of ·
Finite group Z
Fourier series T or [−π, π] n ∈ Z
Z/nZ
Discrete

Subsets S ⊆ {1, . . . , n} (cid:81)s∈S xs

e(ξ · x)
exp(inx)
e(ξx/n)

Z/nZ

Basis functions

I snuck in a fourth row with Z = Z/nZ, but it’s a special case of the second row, so no
cause for alarm.

Alluding to the future, I want to hint at how Chapter 38 starts. Each one of these
is really a statement about how functions from G → C can be expressed in terms of
functions (cid:98)G → C, for some “dual” (cid:98)G. In that sense, we could rewrite the above table as:

Characters

Name
Binary
Finite group Z

Domain G Dual (cid:98)G
S ⊆ {1, . . . , n} (cid:81)s∈S xs
{±1}n
ξ ∈ (cid:98)Z ∼= Z
Fourier series T ∼= [−π, π] n ∈ Z
ξ ∈ Z/nZ

e(iξ · x)
exp(inx)
e(ξx/n)

Discrete

Z/nZ

It will turn out that in general we can say something about many diﬀerent domains G,
once we know what it means to integrate a measure. This is the so-called Pontryagin
duality; and it is discussed as a follow-up bonus in Chapter 38.

§14.5 Parseval and friends

Here is a fun section in which you get to learn a lot of big names quickly. Basically, we
can take each of the three results from Proposition 14.2.1, translate it into the context of
our Fourier analysis (for which we have an orthonormal basis of the Hilbert space), and
get a big-name result.

Corollary 14.5.1 (Parseval theorem)
Let f : Z → C, where Z is a ﬁnite abelian group. Then
|f (x)|2.

1

(cid:88)ξ

|(cid:98)f (ξ)|2 =

|Z|(cid:88)x∈Z

Similarly, if f : [−π, π] → C is square-integrable then its Fourier series satisﬁes

(cid:88)n

|(cid:98)f (n)|2 =

1

2π(cid:90)[−π,π] |f (x)|2.

Proof. Recall that (cid:104)f, f(cid:105) is equal to the square sum of the coeﬃcients.

14 Bonus: Fourier analysis

187

Corollary 14.5.2 (Fourier inversion formula)
Let f : Z → C, where Z is a ﬁnite abelian group. Then
f (x)eξ(x).

1

Similarly, if f : [−π, π] → C is square-integrable then its Fourier series is given by

|Z|(cid:88)x∈Z
(cid:98)f (ξ) =
2π(cid:90)[−π,π]

1

(cid:98)f (n) =

f (x) exp(−inx).

Proof. Recall that in an orthonormal basis (eξ)ξ, the coeﬃcient of eξ in f is (cid:104)f, eξ(cid:105).

Question 14.5.3. What happens when ξ = 0 above?

Corollary 14.5.4 (Plancherel theorem)
Let f : Z → C, where Z is a ﬁnite abelian group. Then

Similarly, if f : [−π, π] → C is square-integrable then

(cid:104)f, g(cid:105) =(cid:88)ξ∈Z (cid:98)f (ξ)(cid:98)g(ξ).
(cid:104)f, g(cid:105) =(cid:88)n (cid:98)f (ξ)(cid:98)g(ξ).

Question 14.5.5. Prove this one in one line (like before).

§14.6 Application: Basel problem

One cute application about Fourier analysis on L2([−π, π]) is that you can get some
otherwise hard-to-compute sums, as long as you are willing to use a little calculus.

Here is the classical one:

Theorem 14.6.1 (Basel problem)

We have

1
n2 =

π2
6

.

(cid:88)n≥1

The proof is to consider the identity function f (x) = x, which is certainly square-integrable.
Then by Parseval, we have

2

(cid:88)n∈Z(cid:12)(cid:12)(cid:12)(cid:98)f (n)(cid:12)(cid:12)(cid:12)

= (cid:104)f, f(cid:105)

1

2π(cid:90)[−π,π] |f (x)|2 dx.

188

Napkin, by Evan Chen (v1.5.20190718)

A calculus computation gives

x2 dx =

π2
3

.

1

2π(cid:90)[−π,π]
(cid:98)f (0) =(cid:90)[−π,π]

f (x) dx =(cid:90)[−π,π]

x dx = 0.

On the other hand, we will now compute all Fourier coeﬃcients. We have already that

For n (cid:54)= 0, we have by deﬁnition (or “Fourier inversion formula”, if you want to use big
words) the formula

1

=

(cid:98)f (n) = (cid:104)f, exp(inx)(cid:105)
2π(cid:90)[−π,π]
2π(cid:90)[−π,π]

=

1

x · exp(inx) dx

x exp(−inx) dx.

The anti-derivative is equal to 1
calculation gives that

n2 exp(−inx)(1 + inx), which thus with some more

So

implying the result.

(cid:98)f (n) =
(cid:88)n (cid:12)(cid:12)(cid:12)(cid:98)f (n)(cid:12)(cid:12)(cid:12)

2

n

i.

(−1)n
= 2(cid:88)n≥1

1
n2

§14.7 Application: Arrow’s Impossibility Theorem

As an application of binary Fourier analysis, we now prove a form of Arrow’s theorem.
Consider n voters voting among 3 candidates A, B, C. Each voter speciﬁes a tuple

vi = (xi, yi, zi) ∈ {±1}3 as follows:

 xi = 1 if person i ranks A ahead of B, and xi = −1 otherwise.
 yi = 1 if person i ranks B ahead of C, and yi = −1 otherwise.
 zi = 1 if person i ranks C ahead of A, and zi = −1 otherwise.

Tacitly, we only consider 3! = 6 possibilities for vi: we forbid “paradoxical” votes of the
form xi = yi = zi by assuming that people’s votes are consistent (meaning the preferences
are transitive).

Then, we can consider a voting mechanism

f : {±1}n → {±1}
g : {±1}n → {±1}
h : {±1}n → {±1}

such that

 f (x•) is the global preference of A vs. B,

14 Bonus: Fourier analysis

189

 g(y•) is the global preference of B vs. C,
 and h(z•) is the global preference of C vs. A.

We’d like to avoid situations where the global preference (f (x•), g(y•), h(z•)) is itself
paradoxical.

In fact, we will prove the following theorem:

Theorem 14.7.1 (Arrow Impossibility Theorem)
Assume that (f, g, h) always avoids paradoxical outcomes, and assume Ef = Eg =
Eh = 0. Then (f, g, h) is either a dictatorship or anti-dictatorship: there exists a
“dictator” k such that

f (x•) = ±xk,
where all three signs coincide.

g(y•) = ±yk,

h(z•) = ±zk

The assumption Ef = Eg = Eh = 0 provides symmetry (and e.g. excludes the possibility
that f , g, h are constant functions which ignore voter input). Unlike the usual Arrow the-
orem, we do not assume that f (+1, . . . , +1) = +1 (hence possibility of anti-dictatorship).

Proof. Suppose the voters each randomly select one of the 3! = 6 possible consistent
votes. In Problem 14B it is shown that the exact probability of a paradoxical outcome
for any functions f , g, h is given exactly by

Assume for contradiction this equals 0. Then, we derive

1
4

+

1

4 (cid:88)S⊆{1,...,n}(cid:18)−
−(cid:18)−

1 = (cid:88)S⊆{1,...,n}

1

3(cid:19)|S|(cid:16)(cid:98)f (S)(cid:98)g(S) +(cid:98)g(S)(cid:98)h(S) +(cid:98)h(S)(cid:98)f (S)(cid:17) .
3(cid:19)|S|(cid:16)(cid:98)f (S)(cid:98)g(S) +(cid:98)g(S)(cid:98)h(S) +(cid:98)h(S)(cid:98)f (S)(cid:17) .

1

But now we can just use weak inequalities. We have (cid:98)f (∅) = Ef = 0 and similarly for
(cid:98)g and(cid:98)h, so we restrict attention to |S| ≥ 1. We then combine the famous inequality
|ab + bc + ca| ≤ a2 + b2 + c2 (which is true across all real numbers) to deduce that

1

3(cid:19)|S|(cid:16)(cid:98)f (S)(cid:98)g(S) +(cid:98)g(S)(cid:98)h(S) +(cid:98)h(S)(cid:98)f (S)(cid:17)
−(cid:18)−
1 = (cid:88)S⊆{1,...,n}
3(cid:19)|S|(cid:16)(cid:98)f (S)2 +(cid:98)g(S)2 +(cid:98)h(S)2(cid:17)
≤ (cid:88)S⊆{1,...,n}(cid:18) 1
≤ (cid:88)S⊆{1,...,n}(cid:18) 1
3(cid:19)1(cid:16)(cid:98)f (S)2 +(cid:98)g(S)2 +(cid:98)h(S)2(cid:17)

=

(1 + 1 + 1) = 1.

1
3

with the last step by Parseval. So all inequalities must be sharp, and in particular (cid:98)f ,(cid:98)g,
(cid:98)h are supported on one-element sets, i.e. they are linear in inputs. As f , g, h are ±1

valued, each f , g, h is itself either a dictator or anti-dictator function. Since (f, g, h) is
always consistent, this implies the ﬁnal result.

190

Napkin, by Evan Chen (v1.5.20190718)

§14.8 A few harder problems to think about

Problem 14A (For calculus fans). Prove that

1
n4 =

π4
90

.

(cid:88)n≥1

Problem 14B. Let f, g, h : {±1}n → {±1} be any three functions. For each i, we
randomly select (xi, yi, zi) ∈ {±1}3 subject to the constraint that not all are equal (hence,
choosing among 23 − 2 = 6 possibilities). Prove that the probability that

f (x1, . . . , xn) = g(y1, . . . , yn) = h(z1, . . . , zn)

is given by the formula

1
4

+

1

4 (cid:88)S⊆{1,...,n}(cid:18)−

1

3(cid:19)|S|(cid:16)(cid:98)f (S)(cid:98)g(S) +(cid:98)g(S)(cid:98)h(S) +(cid:98)h(S)(cid:98)f (S)(cid:17)

15 Duals, adjoint, and transposes

This chapter is dedicated to the basis-free interpretation of the transpose and conjugate
transpose of a matrix. Poster corollary: we will see that symmetric matrices with real
coeﬃcients are diagonalizable have real eigenvalues.

§15.1 Dual of a map

Prototypical example for this section: The example below.

We go ahead and now deﬁne a notion that will grow up to be the transpose of a matrix.

Deﬁnition 15.1.1. Let V and W be vector spaces. Suppose T : V → W is a linear map.
Then we actually get a map

T ∨ : W ∨ → V ∨

f (cid:55)→ f ◦ T.

This map is called the dual map.

Example 15.1.2 (Example of a dual map)
Work over R. Let’s consider V with basis e1, e2, e3 and W with basis f1, f2. Suppose
that

T (e1) = f1 + 2f2
T (e2) = 3f1 + 4f2
T (e3) = 5f1 + 6f2.

Now consider V ∨ with its dual basis e∨1 , e∨2 , e∨3 and W ∨ with its dual basis f∨1 , f∨2 .
Let’s compute T ∨(f∨1 ) = f∨1 ◦ T : it is given by

f∨1 (T (ae1 + be2 + ce3)) = f∨1 ((a + 3b + 5c)f1 + (2a + 4b + 6c)f2)

= a + 3b + 5c.

So accordingly we can write

Similarly,

T ∨(f∨1 ) = e∨1 + 3e∨2 + 5e∨3

T ∨(f∨2 ) = 2e∨1 + 4e∨2 + 6e∨3 .

This determines T ∨ completely.

If we write the matrices for T and T ∨ in terms of our basis, we now see that

T =(cid:20)1 3 5
2 4 6(cid:21)

and T ∨ =

1 2
3 4
5 6

 .

191

192

Napkin, by Evan Chen (v1.5.20190718)

So in our selected basis, we ﬁnd that the matrices are transposes: mirror images of
each other over the diagonal.

Of course, this should work in general.

Theorem 15.1.3 (Transpose interpretation of T ∨)
Let V and W be ﬁnite-dimensional k-vector spaces. Then, for any T : V → W , the
following two matrices are transposes:

 The matrix for T : V → W expressed in the basis (ei), (fj).
 The matrix for T ∨ : W ∨ → V ∨ expressed in the basis (f∨j ), (e∨i ).

Proof. The (i, j)th entry of the matrix T corresponds to the coeﬃcient of fj in T (ei),
which corresponds to the coeﬃcient of e∨i

in f∨j ◦ T .

The nice part of this is that the deﬁnition of T ∨ is basis-free. So it means that if we start
with any linear map T , and then pick whichever basis we feel like, then T and T ∨ will
still be transposes.

§15.2 Identifying with the dual space

For the rest of this chapter, though, we’ll now bring inner products into the picture.

Earlier I complained that there was no natural isomorphism V ∼= V ∨. But in fact,
given an inner form we can actually make such an identiﬁcation: that is we can naturally
associate every linear map ξ : V → k with a vector v ∈ V .
To see how we might do this, suppose V = R3 for now with an orthonormal basis
e1, e2, e3. How might we use the inner product to represent a map from V → R? For
example, take ξ ∈ V ∨ by ξ(e1) = 3, ξ(e2) = 4 and ξ(e3) = 5. Actually, I claim that

ξ(v) = (cid:104)v, 3e1 + 4e2 + 5e3(cid:105)

for every v.

Question 15.2.1. Check this.

And this works beautifully in the real case.

Theorem 15.2.2 (V ∼= V ∨ for real inner form)
Let V be a ﬁnite-dimensional real inner product space and V ∨ its dual. Then the
map V → V ∨ by

v (cid:55)→ (cid:104)−, v(cid:105) ∈ V ∨

is an isomorphism of real vector spaces.

Proof. It suﬃces to show that the map is injective and surjective.

 Injective: suppose (cid:104)v1, v(cid:105) = (cid:104)v2, v(cid:105) for every vector v ∈ V . This means (cid:104)v1 − v2, v(cid:105) =
0 for every vector v ∈ V . This can only happen if v1 − v2 = 0; for example, take
v = v1 − v2 and use positive deﬁniteness.

15 Duals, adjoint, and transposes

193

 Surjective: take an orthonormal basis e1, . . . en and let e∨1 , . . . , e∨n be the dual basis

on V ∨. Then e1 maps to e∨1 , et cetera.

Actually, since we already know dim V = dim V ∨ we only had to prove one of the above.
As a matter of personal taste, I ﬁnd the proof of injectivity more elegant, and the proof
of surjectivity more enlightening, so I included both. Thus

If a real inner product space V is given an inner form, then V and V ∨ are
canonically isomorphic.

Unfortunately, things go awry if V is complex. Here is the results:

Theorem 15.2.3 (V versus V ∨ for complex inner forms)
Let V be a ﬁnite-dimensional complex inner product space and V ∨ its dual. Then
the map V → V ∨ by

v (cid:55)→ (cid:104)−, v(cid:105) ∈ V ∨

is a bijection of sets.

Wait, what? Well, the proof above shows that it is both injective and surjective, but why
is it not an isomorphism? The answer is that it is not a linear map: since the form is
sesquilinear we have for example

iv (cid:55)→ (cid:104)−, iv(cid:105) = −i(cid:104)−, v(cid:105)

which has introduced a minus sign! In fact, it is an anti-linear map, in the sense we
deﬁned before.

Eager readers might try to ﬁx this by deﬁning the isomorphism v (cid:55)→ (cid:104)v,−(cid:105) instead.
However, this also fails, because the right-hand side is not even an element of V ∨: it is
an “anti-linear”, not linear.

And so we are stuck. Fortunately, we will only need the “bijection” result for what
follows, so we can continue on anyways. (If you want to ﬁx this, Problem 15D gives a
way to do so.)

§15.3 The adjoint (conjugate transpose)

We will see that, as a result of the ﬂipping above, the conjugate transpose is actually
the better concept for inner product spaces: since it can be deﬁned using only the inner
product with making mention to dual spaces at all.

Deﬁnition 15.3.1. Let V and W be ﬁnite-dimensional inner product spaces, and let
T : W → V . The adjoint or conjugate transpose of T , denoted T † : W → V , is
deﬁned as follows: for every vector w ∈ W , we let T †(w) ∈ V be the unique vector with

(cid:68)v, T †(w)(cid:69)V

= (cid:104)T (v), w(cid:105)W

for every v ∈ V .

Some immediate remarks about this deﬁnition:
 This is well-deﬁned, because (cid:104)T (−), w(cid:105)W is some function in V ∨, and hence by the

bijection earlier it should be uniquely of the form (cid:104)−, v(cid:105) for some v ∈ V .

194

Napkin, by Evan Chen (v1.5.20190718)

 The niceness of this deﬁnition is that it doesn’t make reference to any basis or even

V ∨, so it is the “right” deﬁnition for a inner product space.

 By symmetry, of course, we also have(cid:10)T †(v), w(cid:11) = (cid:104)v, T (w)(cid:105).

Example 15.3.2 (Example of an adjoint map)
We’ll work over C, so the conjugates are more visible. Let’s consider V with
orthonormal basis e1, e2, e3 and W with orthonormal basis f1, f2. We put

T (e1) = if1 + 2f2
T (e2) = 3f1 + 4f2
T (e3) = 5f1 + 6if2.

We compute T †(f1). It is the unique vector x ∈ V such that

(cid:104)v, x(cid:105)V = (cid:104)T (v), f1(cid:105)W

for any v ∈ V . If we expand v = ae1 + be2 + ce3 the above equality becomes

(cid:104)ae1 + be2 + ce3, x(cid:105)V = (cid:104)T (ae1 + be2 + ce3), f1(cid:105)W

= ia + 3b + 5c.

However, since x is in the second argument, this means we actually want to take

so that the sesquilinearity will conjugate the i.

T †(f1) = −ie1 + 3e2 + 5e3

The pattern continues, though we remind the reader that we need the basis to be

orthonormal to proceed.

Theorem 15.3.3 (Adjoints are conjugate transposes)
Fix an orthonormal basis of a ﬁnite-dimensional inner product V . If we write T
as a matrix in this basis, then the matrix T † (in the same basis) is the conjugate
transpose of that of T .

Proof. One-line version: take v and w to be basis elements, and this falls right out.

Full proof: let

T =

a11
...
an1

. . .
. . .
. . .

a1n
...
ann



in this basis e1, . . . , en. Then, letting w = ei and v = ej we deduce that

(cid:68)ei, T †(ej)(cid:69) = (cid:104)T (ei), ej(cid:105) = aji =⇒ (cid:68)T †(ej), ei(cid:69) = aji

for any i, which is enough to deduce the result.

15 Duals, adjoint, and transposes

195

§15.4 Eigenvalues of normal maps

We now come to the advertised theorem. Restrict to the situation where T : V → V .
You see, the world would be a very beautiful place if it turned out that we could pick a
basis of eigenvectors that was also orthonormal. This is of course far too much to hope
for; even without the orthonormal condition, we saw that Jordan form could still have
1’s oﬀ the diagonal.

However, it turns out that there is a complete characterization of exactly when our

overzealous dream is true.

Deﬁnition 15.4.1. We say a linear map T (from a ﬁnite-dimensional inner product
space to itself) is normal if T T † = T †T .

We say a complex T is self-adjoint or Hermitian if T = T †; i.e. as a matrix in any
orthonormal basis, T is its own conjugate transpose. For real T we say “self-adjoint”,
“Hermitian” or symmetric.

Theorem 15.4.2 (Normal ⇐⇒ diagonalizable with orthonormal basis)
Let V be a ﬁnite-dimensional complex inner product space. A linear map T : V → V
is normal if and only if one can pick an orthonormal basis of eigenvectors.

Exercise 15.4.3. Show that if there exists such an orthonormal basis then T : V → V is
normal, by writing T as a diagonal matrix in that basis.

Proof. This is long, and maybe should be omitted on a ﬁrst reading.
orthonormal basis of eigenvectors, this result is immediate.

If T has an

Now assume T is normal. We ﬁrst prove T is diagonalizable; this is the hard part.

Claim 15.4.4. If T is normal, then ker T = ker T r = ker T † for r ≥ 1. (Here T r is T
applied r times.)

Proof of Claim. Let S = T † ◦ T , which is self-adjoint. We ﬁrst note that S is Hermitian
and ker S = ker T . To see it’s Hermitian, note (cid:104)Sv, w(cid:105) = (cid:104)T v, T w(cid:105) = (cid:104)v, Sw(cid:105). Taking
v = w also implies ker S ⊆ ker T (and hence equality since obviously ker T ⊆ ker S).
First, since we have(cid:10)Sr(v), Sr−2(v)(cid:11) =(cid:10)Sr−1(v), Sr−1(v)(cid:11), an induction shows that
ker S = ker Sr for r ≥ 1. Now, since T is normal, we have Sr = (T †)r ◦ T r, and thus we
have the inclusion

ker T ⊆ ker T r ⊆ ker Sr = ker S = ker T

where the last equality follows from the ﬁrst claim. Thus in fact ker T = ker T r.

Finally, to show equality with ker T † we

(cid:104)T v, T v(cid:105) =(cid:68)v, T †T v(cid:69)
=(cid:68)v, T T †v(cid:69)
=(cid:68)T †v, T †v(cid:69) .

(cid:4)

Now consider the given T , and any λ.

196

Napkin, by Evan Chen (v1.5.20190718)

Question 15.4.5. Show that (T − λid)† = T †

− λid. Thus if T is normal, so is T − λid.

In particular, for any eigenvalue λ of T , we ﬁnd that ker(T − λid) = ker(T − λid)r. This
implies that all the Jordan blocks of T have size 1; i.e. that T is in fact diagonalizable.
Finally, we conclude that the eigenvectors of T and T † match, and the eigenvalues are
complex conjugates.

So, diagonalize T . We just need to show that if v and w are eigenvectors of T with
distinct eigenvalues, then they are orthogonal.
(We can use Gram-Schmidt on any
eigenvalue that appears multiple times.) To do this, suppose T (v) = λv and T (w) = µw
(thus T †(w) = µw). Then

λ(cid:104)v, w(cid:105) = (cid:104)λv, w(cid:105) = (cid:104)T v, w(cid:105) =(cid:68)v, T †(w)(cid:69) = (cid:104)v, µw(cid:105) = µ(cid:104)v, w(cid:105) .

Since λ (cid:54)= µ, we conclude (cid:104)v, w(cid:105) = 0.
This means that not only can we write

. . .
λ2
...
0

. . .
0
. . .
0
...
. . .
. . . λn



T =

λ1
0
...
0

but moreover that the basis associated with this matrix happens to be orthonormal
vectors.

As a corollary:

Theorem 15.4.6 (Hermitian matrices have real eigenvalues)

A Hermitian matrix T is diagonalizable, and all its eigenvalues are real.

Proof. Obviously Hermitian =⇒ normal, so write it in the orthonormal basis of
eigenvectors. To see that the eigenvalues are real, note that T = T † means λi = λi for
every i.

§15.5 A few harder problems to think about

Problem 15A(cid:63) (Double dual). Let V be a ﬁnite-dimensional vector space. Prove that

V → (V ∨)∨
v (cid:55)→ (ξ (cid:55)→ ξ(v))

gives an isomorphism. (This is signiﬁcant because the isomorphism is canonical, and in
particular does not depend on the choice of basis. So this is more impressive.)
Problem 15B (Fundamental theorem of linear algebra). Let T : V → W be a map of
ﬁnite-dimensional k-vector spaces. Prove that

dim im T = dim im T ∨ = dim V − dim ker T = dim W − dim ker T ∨.

Problem 15C† (Row rank is column rank). A m× n matrix M of real numbers is given.
The column rank of M is the dimension of the span in Rm of its n column vectors. The
row rank of M is the dimension of the span in Rn of its m row vectors. Prove that the
row rank and column rank are equal.

15 Duals, adjoint, and transposes

197

Problem 15D (The complex conjugate spaces). Let V = (V, +,·) be a complex vector
space. Deﬁne the complex conjugate vector space, denoted V = (V, +,∗) by changing
just the multiplication:

Show that for any sesquilinear form on V , if V is ﬁnite-dimensional, then

c ∗ v = c · V.

V → V ∨
v (cid:55)→ (cid:104)−, v(cid:105)

is an isomorphism of complex vector spaces.
Problem 15E (T † vs T ∨). Let V and W be real inner product spaces and let T : V → W
be an inner product. Show that the following diagram commutes:

W

∼=

W ∨

T †

T ∨

V

∼=

V ∨

Here the isomorphisms are v (cid:55)→ (cid:104)−, v(cid:105). Thus, for real inner product spaces, T † is just T ∨
with the duals eliminated (by Theorem 15.2.2).

Problem 15F (Polynomial criteria for normality). Let V be a complex inner product
space and let T : V → V be a linear map. Show that T is normal if and only if there is a
polynomial1 p ∈ C[t] such that

T † = p(T ).

1Here, p(T ) is meant in the same composition sense as in Cayley-Hamilton.

V

More on Groups

Part V: Contents

16 Group actions overkill AIME problems

201
16.1 Deﬁnition of a group action . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202
16.2 Stabilizers and orbits
16.3 Burnside’s lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
16.4 Conjugation of elements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204
. . . . . . . . . . . . . . . . . . . . . . . . 205
16.5 A few harder problems to think about

17 Find all groups

207
17.1 Sylow theorems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207
17.2 (Optional) Proving Sylow’s theorem . . . . . . . . . . . . . . . . . . . . . . . . . 208
17.3 (Optional) Simple groups and Jordan-H¨older . . . . . . . . . . . . . . . . . . . . . 210
. . . . . . . . . . . . . . . . . . . . . . . . 211
17.4 A few harder problems to think about

18 The PID structure theorem

213
. . . . . . . . . . . . . . . . . . . . . . . . . . 213
18.1 Finitely generated abelian groups
18.2 Some ring theory prerequisites . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214
18.3 The structure theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
18.4 Reduction to maps of free R-modules . . . . . . . . . . . . . . . . . . . . . . . . . 216
18.5 Smith normal form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217
. . . . . . . . . . . . . . . . . . . . . . . . 219
18.6 A few harder problems to think about

16 Group actions overkill AIME

problems

Consider this problem from the 1996 AIME:

(AIME 1996) Two of the squares of a 7 × 7 checkerboard are painted yellow,
and the rest are painted green. Two color schemes are equivalent if one can
be obtained from the other by applying a rotation in the plane of the board.
How many inequivalent color schemes are possible?

2(cid:1) possible colorings of the board.
What’s happening here? Let X be the set of the(cid:0)49
What’s the natural interpretation of “rotation”? Answer: the group Z/4Z =(cid:10)r | r4 = 1(cid:11)

somehow “acts” on this set X by sending one state x ∈ X to another state r · x, which is
just x rotated by 90◦. Intuitively we’re just saying that two conﬁgurations are the same
if they can be reached from one another by this “action”.

We can make all of this precise using the idea of a group action.

§16.1 Deﬁnition of a group action

Prototypical example for this section: The AIME problem.

Deﬁnition 16.1.1. Let X be a set and G a group. A group action is a binary operation
· : G × X → X which lets a g ∈ G send an x ∈ X to g · x. It satisﬁes the axioms

 (g1g2) · x = g1 · (g2 · x) for any g1, g2 ∈ G for all x ∈ X.
 1G · x = x for any x ∈ X.
Here are some more examples of group actions.

Example 16.1.2 (Examples of group actions)
Let G = (G, (cid:63)) be a group. Here are some examples of group actions.
(a) The group Z/4Z can act on the set of ways to color a 7 × 7 board either yellow
(b) The group Z/4Z =(cid:10)r | r4 = 1(cid:11) acts on the xy-plane R2 as follows: r · (x, y) =
(y,−x). In other words, it’s a rotation by 90◦.

or green.

(c) The dihedral group D2n acts on the set of ways to color the vertices of an n-gon.

(d) The group Sn acts on X = {1, 2, . . . , n} by applying the permutation σ: σ · x :=

σ(x).

(e) The group G can act on itself (i.e. X = G) by left multiplication: put g·g(cid:48) := g(cid:63)g(cid:48).

201

202

Napkin, by Evan Chen (v1.5.20190718)

§16.2 Stabilizers and orbits

Prototypical example for this section: Again the AIME problem.

Given a group action G on X, we can deﬁne an equivalence relation ∼ on X as follows:
x ∼ y if x = g · y for some g ∈ G. For example, in the AIME problem, ∼ means “one
can be obtained from the other by a rotation”.

Question 16.2.1. Why is this an equivalence relation?

In that case, the AIME problem wants the number of equivalence classes under ∼. So
let’s give these equivalence classes a name: orbits. We usually denote orbits by O.

As usual, orbits carve out X into equivalence classes.

It turns out that a very closely related concept is:

Deﬁnition 16.2.2. The stabilizer of a point x ∈ X, denoted StabG(x), is the set of
g ∈ G which ﬁx x; in other words

StabG(x) := {g ∈ G | g · x = x} .

Example 16.2.3
Consider the AIME problem again, with X the possible set of states (again G = Z/4Z).
Let x be the conﬁguration where two opposite corners are colored yellow. Evidently
1G ﬁxes x, but so does the 180◦ rotation r2. But r and r3 do not preserve x, so
StabG(x) = {1, r2} ∼= Z/2Z.

Question 16.2.4. Why is StabG(x) a subgroup of G?

Once we realize the stabilizer is a group, this leads us to what I privately call the

“fundamental theorem of how big an orbit is”.

Theorem 16.2.5 (Orbit-stabilizer theorem)
Let O be an orbit, and pick any x ∈ O. Let S = StabG(x) be a subgroup of G.
There is a natural bijection between O and left cosets. In particular,

In particular, the stabilizers of each x ∈ O have the same size.

|O||S| = |G| .

XO1O2O316 Group actions overkill AIME problems

203

Proof. The point is that every coset gS just speciﬁes an element of O, namely g · x. The
fact that S is a stabilizer implies that it is irrelevant which representative we pick.

Since the |O| cosets partition G, each of size |S|, we obtain the second result.

§16.3 Burnside’s lemma

Now for the crux of this chapter: a way to count the number of orbits.

Theorem 16.3.1 (Burnside’s lemma)

Let G act on a set X. The number of orbits of the action is equal to

1

|G|(cid:88)g∈G

|FixPt g|

where FixPt g is the set of points x ∈ X such that g · x = x.

The proof is deferred as a bonus problem, since it has a very olympiad-ﬂavored solution. As
usual, this lemma was not actually proven by Burnside; Cauchy got there ﬁrst, and thus it
is sometimes called the lemma that is not Burnside’s. Example application:

Example 16.3.2 (AIME 1996)
Two of the squares of a 7 × 7 checkerboard are painted yellow, and the rest are painted green. Two
color schemes are equivalent if one can be obtained from the other by applying a rotation in the

plane of the board. How many inequivalent color schemes are possible?

We know that G = Z/4Z acts on the set X of(cid:0)49
we can compute FixPt g explicitly for each g ∈ Z/4Z.
 If g = 1G, then every coloring is ﬁxed, for a count of(cid:0)49

 If g = r2 there are exactly 24 coloring schemes ﬁxed by g: this occurs when the
two squares are reﬂections across the center, which means they are preserved
under a 180◦ rotation.

2(cid:1) possible coloring schemes. Now

2(cid:1) = 1176.

 If g = r or g = r3, then there are no ﬁxed coloring schemes.

As |G| = 4, the average is

1176 + 24 + 0 + 0

4

= 300.

O⊆XS⊆G◦◦◦◦◦g204

Napkin, by Evan Chen (v1.5.20190718)

Exercise 16.3.3 (MathCounts Chapter Target Round). A circular spinner has seven sections
of equal size, each of which is colored either red or blue. Two colorings are considered the
same if one can be rotated to yield the other. In how many ways can the spinner be colored?
(Answer: 20)

Consult [Ma13b] for some more examples of “hands-on” applications.

§16.4 Conjugation of elements

Prototypical example for this section: In Sn, conjugacy classes are “cycle types”.

A particularly common type of action is the so-called conjugation. We let G act on

itself as follows:

g : h (cid:55)→ ghg−1.

You might think this deﬁnition is a little artiﬁcial. Who cares about the element ghg−1?
Let me try to convince you this deﬁnition is not so unnatural.

Example 16.4.1 (Conjugacy in Sn)
is πσπ−1 related to σ? To
Let G = S5, and ﬁx a π ∈ S5. Here’s the question:
illustrate this, I’ll write out a completely random example of a permutation σ ∈ S5.

If σ =

1 (cid:55)→ 3
2 (cid:55)→ 1
3 (cid:55)→ 5
4 (cid:55)→ 2
5 (cid:55)→ 4

then

πσπ−1 =

π(1)
π(2)
π(3)
π(4)
π(5)

(cid:55)→ π(3)
(cid:55)→ π(1)
(cid:55)→ π(5)
(cid:55)→ π(2)
(cid:55)→ π(4)

Thus our ﬁxed π doesn’t really change the structure of σ at all: it just “renames”
each of the elements 1, 2, 3, 4, 5 to π(1), π(2), π(3), π(4), π(5).

But wait, you say. That’s just a very particular type of group behaving nicely under
conjugation. Why does this mean anything more generally? All I have to say is: remember
Cayley’s theorem! (This was Problem 1F†.)

In any case, we may now deﬁne:

Deﬁnition 16.4.2. The conjugacy classes of a group G are the orbits of G under the
conjugacy action.

Let’s see what the conjugacy classes of Sn are, for example.

16 Group actions overkill AIME problems

205

Example 16.4.3 (Conjugacy classes of Sn correspond to cycle types)
Intuitively, the discussion above says that two elements of Sn should be conjugate if
they have the same “shape”, regardless of what the elements are named. The right
way to make the notion of “shape” rigorous is cycle notation. For example, consider
the permutation

σ1 = (1 3 5)(2 4)

in cycle notation, meaning 1 (cid:55)→ 3 (cid:55)→ 5 (cid:55)→ 1 and 2 (cid:55)→ 4 (cid:55)→ 2. It is conjugate to the
permutation

σ2 = (1 2 3)(4 5)

or any other way of relabeling the elements. So, we could think of σ as having
conjugacy class

More generally, you can show that two elements of Sn are conjugate if and only if
they have the same “shape” under cycle decomposition.

(− − −)(− −).

Question 16.4.4. Show that the number of conjugacy classes of Sn equals the number of
partitions of n.

As long as I’ve put the above picture, I may as well also deﬁne:

Deﬁnition 16.4.5. Let G be a group. The center of G, denoted Z(G), is the set of
elements x ∈ G such that xg = gx for every g ∈ G. More succinctly,

Z(G) := {x ∈ G | gx = xg ∀g ∈ G} .

You can check this is indeed a subgroup of G.

Question 16.4.6. Why is Z(G) normal in G?

Question 16.4.7. What are the conjugacy classes of elements in the center?

A trivial result that gets used enough that I should explicitly call it out:

Corollary 16.4.8 (Conjugacy in abelian groups is trivial)

If G is abelian, then the conjugacy classes all have size one.

§16.5 A few harder problems to think about

Problem 16A (PUMaC 2009 C8). Taotao wants to buy a bracelet consisting of seven
beads, each of which is orange, white or black. (The bracelet can be rotated and reﬂected
in space.) Find the number of possible bracelets.

Problem 16B. Show that two elements in the same conjugacy class have the same
order.

Problem 16C. Prove Burnside’s lemma.

206

Napkin, by Evan Chen (v1.5.20190718)

Problem 16D (The “class equation”). Let G be a ﬁnite group. We deﬁne the central-
izer CG(g) = {x ∈ G | xg = gx} for each g ∈ G. Show that

|G| = |Z(G)| +(cid:88)s∈S

|G|

|CG(s)|

where S ⊆ G is deﬁned as follows: for each conjugacy class C ⊆ G with |C| > 1, we pick
a representative of C and add it to S.

Problem 16E† (Classical). Assume G is a ﬁnite group and p is the smallest prime
dividing its order. Let H be a subgroup of G with |G| /|H| = p. Show that H is normal
in G.

17 Find all groups

The following problem will hopefully never be proposed at the IMO.

Let n be a positive integer and let S = {1, . . . , n}. Find all functions
f : S × S → S such that
(a) f (x, 1) = f (1, x) = x for all x ∈ S.
(b) f (f (x, y), z) = f (x, f (y, z)) for all x, y, z ∈ S.
(c) For every x ∈ S there exists a y ∈ S such that f (x, y) = f (y, x) = 1.

Nonetheless, it’s remarkable how much progress we’ve made on this “problem”. In this
chapter I’ll try to talk about some things we have accomplished.

§17.1 Sylow theorems

Here we present the famous Sylow theorems, some of the most general results we have
about ﬁnite groups.

Theorem 17.1.1 (The Sylow theorems)
Let G be a group of order pnm, where gcd(p, m) = 1 and p is a prime. A Sylow
p-subgroup is a subgroup of order pn. Let np be the number of Sylow p-subgroups
of G. Then

(a) np ≡ 1 (mod p). In particular, np (cid:54)= 0 and a Sylow p-subgroup exists.
(b) np divides m.

(c) Any two Sylow p-subgroups are conjugate subgroups (hence isomorphic).

Sylow’s theorem is really huge for classifying groups; in particular, the conditions
np ≡ 1 (mod p) and np | m can often pin down the value of np to just a few values. Here
are some results which follow from the Sylow theorems.

 A Sylow p-subgroup is normal if and only if np = 1.

 Any group G of order pq, where p < q are primes, must have nq = 1, since nq ≡ 1

(mod q) yet nq | p. Thus G has a normal subgroup of order q.

 Since any abelian group has all subgroups normal, it follows that any abelian group

has exactly one Sylow p-subgroup for every p dividing its order.

 If p (cid:54)= q, the intersection of a Sylow p-subgroup and a Sylow q-subgroup is just
{1G}. That’s because the intersection of any two subgroups is also a subgroup,
and Lagrange’s theorem tells us that its order must divide both a power of p and a
power of q; this can only happen if the subgroup is trivial.

Here’s an example of another “practical” application.

207

208

Napkin, by Evan Chen (v1.5.20190718)

Proposition 17.1.2 (Triple product of primes)
If |G| = pqr is the product of distinct primes, then G must have a normal Sylow
subgroup.

Proof. WLOG, assume p < q < r. Notice that np ≡ 1 (mod p), np|qr and cyclically, and
assume for contradiction that np, nq, nr > 1.
Since nr|pq, we have nr = pq since nr divides neither p nor q as nr ≥ 1 + r > p, q.
Also, np ≥ 1 + p and nq ≥ 1 + q. So we must have at least 1 + p Sylow p-subgroups, at
least 1 + q Sylow q-subgroups, and at least pq Sylow r-subgroups.

But these groups are pretty exclusive.

Question 17.1.3. Take the np + nq + nr Sylow subgroups and consider two of them, say
H1 and H2. Show that |H1 ∩ H2| = 1 as follows: check that H1 ∩ H2 is a subgroup of both
H1 and H2, and then use Lagrange’s theorem.

We claim that there are too many elements now. Indeed, if we count the non-identity

elements contributed by these subgroups, we get

np(p − 1) + nq(q − 1) + nr(r − 1) ≥ (1 + p)(p − 1) + (1 + q)(q − 1) + pq(r − 1) > pqr
which is more elements than G has!

§17.2 (Optional) Proving Sylow’s theorem

The proof of Sylow’s theorem is somewhat involved, and in fact many proofs exist. I’ll
present one below here. It makes extensive use of group actions, so I want to recall a few
facts ﬁrst. If G acts on X, then

 The orbits of the action form a partition of X.

 if O is any orbit, then the orbit-Stabilizer theorem says that

|O| = |G| /|StabG(x)|

for any x ∈ O.

 In particular: suppose in the above that G is a p-group, meaning |G| = pt for some
t. Then either |O| = 1 or p divides |O|. In the case O = {x}, then by deﬁnition, x
is a ﬁxed point of every element of G: we have g · x = x for every x.

Note that when I say x is a ﬁxed point, I mean it is ﬁxed by every element of the group,
i.e. the orbit really has size one. Hence that’s a really strong condition.

§17.2.i Deﬁnitions

Prototypical example for this section: Conjugacy in Sn.

I’ve deﬁned conjugacy of elements previously, but I now need to deﬁne it for groups:

Deﬁnition 17.2.1. Let G be a group, and let X denote the set of subgroups of G. Then
conjugation is the action of G on X that sends

If H and K are subgroups of G such that H = gKg−1 for some g ∈ G (in other words,
they are in the same orbit under this action), then we say they are conjugate subgroups.

H (cid:55)→ gHg−1 =(cid:8)ghg−1 | h ∈ H(cid:9) .

17 Find all groups

209

Because we somehow don’t think of conjugate elements as “that diﬀerent” (for example,

in permutation groups), the following shouldn’t be surprising:

Question 17.2.2. Show that for any subgroup H of a group G, the map H → gHg−1 by
h (cid:55)→ ghg−1 is in fact an isomorphism. This implies that any two conjugate subgroups are

isomorphic.

Deﬁnition 17.2.3. For any subgroup H of G the normalizer of H is deﬁned as

In other words, it is the stabilizer of H under the conjugation action.

NG(H) :=(cid:8)g ∈ G | gHg−1 = H(cid:9) .

We are now ready to present the proof.

§17.2.ii Step 1: Prove that a Sylow p-subgroup exists

What follows is something like the probabilistic method. By considering the set X of
ALL subsets of size pn at once, we can exploit the “deep number theoretic fact” that

|X| =(cid:18)pnm

pn (cid:19) (cid:54)≡ 0

(mod p).

(It’s not actually deep: use Lucas’ theorem.)

Here is the proof.

 Let G act on X by g · X := {gx | x ∈ X}.
 Take an orbit O with size not divisible by p. (This is possible because of our deep
number theoretic fact. Since |X| is nonzero mod p and the orbits partition X, the
claimed orbit must exist.)

 Let S ∈ O, H = StabG(S). Then pn divides |H|, by the orbit-Stabilizer theorem.
 Consider a second action: let H act on S by h · s := hs (we know hs ∈ S since

H = StabG(S)).

 Observe that StabH (s) = {1H}. Then all orbits of the second action must have

size |H|. Thus |H| divides |S| = pn.

 This implies |H| = pn, and we’re done.

§17.2.iii Step 2: Any two Sylow p-subgroups are conjugate

Let P be a Sylow p-subgroup (which exists by the previous step). We now prove that for
any p-group Q, Q ⊆ gP g−1. Note that if Q is also a Sylow p-subgroup, then Q = gP g−1
for size reasons; this implies that any two Sylow subgroups are indeed conjugate.

Let Q act on the set of left cosets of P by left multiplication. Note that

 Q is a p-group, so any orbit has size divisible by p unless it’s 1.

 But the number of left cosets is m, which isn’t divisible by p.

Hence some coset gP is a ﬁxed point for every q, meaning qgP = gP for all q.
Equivalently, qg ∈ gP for all q ∈ Q, so Q ⊆ gP g−1 as desired.

210

Napkin, by Evan Chen (v1.5.20190718)

§17.2.iv Step 3: Showing np ≡ 1 (mod p)
Let S denote the set of all the Sylow p-subgroups. Let P ∈ S be arbitrary.
Question 17.2.4. Why does |S| equal np? (In other words, are you awake?)

Now we can proceed with the proof. Let P act on S by conjugation. Then:

 Because P is a p-group, np (mod p) is the number of ﬁxed points of this action.

Now we claim P is the only ﬁxed point of this action.

 Let Q be any other ﬁxed point, meaning xQx−1 = Q for any x ∈ P .
 Deﬁne the normalizer NG(Q) =(cid:8)g ∈ G | gQg−1 = Q(cid:9). It contains both P and Q.

 Now for the crazy part: apply Step 2 to NG(Q). Since P and Q are Sylow

p-subgroups of it, they must be conjugate.

 Hence P = Q, as desired.

§17.2.v Step 4: np divides m
Since np ≡ 1 (mod p), it suﬃces to show np divides |G|. Let G act on the set of all Sylow
p-groups by conjugation. Step 2 says this action has only one orbit, so the Orbit-Stabilizer
theorem implies np divides |G|.

§17.3 (Optional) Simple groups and Jordan-H¨older
Prototypical example for this section: Decomposition of Z/12Z is 1 (cid:69) Z/2Z (cid:69) Z/4Z (cid:69)
Z/12Z.

Just like every integer breaks down as the product of primes, we can try to break every
group down as a product of “basic” groups. Armed with our idea of quotient groups, the
right notion is this.

Deﬁnition 17.3.1. A simple group is a group with no normal subgroups other than
itself and the trivial group.

Question 17.3.2. For which n is Z/nZ simple? (Hint: remember that Z/nZ is abelian.)

Then we can try to deﬁne what it means to “break down a group”.

Deﬁnition 17.3.3. A composition series of a group G is a sequence of subgroups H0,
H1, . . . , Hn such that

{1} = H0 (cid:69) H1 (cid:69) H2 (cid:69) . . . (cid:69) Hn = G

of maximal length (i.e. n is as large as possible, but all Hi are of course distinct). The
composition factors are the groups H1/H0, H2/H1, . . . , Hn/Hn−1.

You can show that the “maximality” condition implies that the composition factors

are all simple groups.

Let’s say two composition series are equivalent if they have the same composition
factors (up to permutation); in particular they have the same length. Then it turns out
that the following theorem is true.

17 Find all groups

211

Theorem 17.3.4 (Jordan-H¨older)

Every ﬁnite group G admits a unique composition series up to equivalence.

Example 17.3.5 (Fundamental theorem of arithmetic when n = 12)
Let’s consider the group Z/12Z. It’s not hard to check that the possible composition
series are

{1} (cid:69) Z/2Z (cid:69) Z/4Z (cid:69) Z/12Z with factors Z/2Z, Z/2Z, Z/3Z
{1} (cid:69) Z/2Z (cid:69) Z/6Z (cid:69) Z/12Z with factors Z/2Z, Z/3Z, Z/2Z
{1} (cid:69) Z/3Z (cid:69) Z/6Z (cid:69) Z/12Z with factors Z/3Z, Z/2Z, Z/2Z.

These correspond to the factorization 12 = 22 · 3.

This suggests that classifying all ﬁnite simple groups would be great progress, since
every ﬁnite group is somehow a “product” of simple groups; the only issue is that there
are multiple ways of building a group from constituents.

Amazingly, we actually have a full list of simple groups, but the list is really bizarre.

Every ﬁnite simple group falls in one of the following categories:

 Z/pZ for p a prime,

 For n ≥ 5, the subgroup of Sn consisting of “even” permutations.
 A simple group of Lie type (which I won’t explain), and

 Twenty-six “sporadic” groups which do not ﬁt into any nice family.

The two largest of the sporadic groups have cute names. The baby monster group
has order

241 · 313 · 56 · 72 · 11 · 13 · 17 · 19 · 23 · 31 · 47 ≈ 4 · 1033

and the monster group (also “friendly giant”) has order

246 · 320 · 59 · 76 · 112 · 133 · 17 · 19 · 23 · 29 · 31 · 41 · 47 · 59 · 71 ≈ 8 · 1053.

It contains twenty of the sporadic groups as subquotients (including itself), and these
twenty groups are called the “happy family”.

Math is weird.

Question 17.3.6. Show that “ﬁnite simple group of order 2” is redundant in the sense that
any group of order 2 is both ﬁnite and simple.

§17.4 A few harder problems to think about

Problem 17A(cid:63) (Cauchy’s theorem). Let G be a group and let p be a prime dividing
|G|. Prove1 that G has an element of order p.
Problem 17B. Let G be a ﬁnite simple group. Show that |G| (cid:54)= 56.

212

Napkin, by Evan Chen (v1.5.20190718)

Problem 17C (Engel’s PSS?). Consider the set of all words consisting of the letters a
and b. Given such a word, we can change the word either by inserting a word of the form
www, where w is a word, anywhere in the given word, or by deleting such a sequence
from the word. Can we turn the word ab into the word ba?

Problem 17D. Let p be a prime. Show that the only simple group with order pn (for
some positive integer n) is the cyclic group Z/pZ.

1 Cauchy’s theorem can be proved without the Sylow theorems, and in fact can often be used to give

alternate proofs of Sylow.

18 The PID structure theorem

The main point of this chapter is to discuss a classiﬁcation theorem for ﬁnitely generated
abelian groups. This won’t take long to do, and if you like you can read just the ﬁrst
section and then move on.

However, since I’m here, I will go ahead and state the result as a special case of the

much more general structure theorem. Its corollaries include

 All ﬁnite-dimensional vector spaces are k⊕n.

 The classiﬁcation theorem for ﬁnitely generated abelian groups,

 The Jordan decomposition of a matrix from before,

 Another canonical form for a matrix: “Frobenius normal form”.

§18.1 Finitely generated abelian groups

Remark 18.1.1 — We talk about abelian groups in what follows, but really the
morally correct way to think about these structures is as Z-modules.

Deﬁnition 18.1.2. An abelian group G = (G, +) is ﬁnitely generated if it is ﬁnitely
generated as a Z-module. (That is, there exists a ﬁnite collection b1, . . . , bm ∈ G, such
that every x ∈ G can be written in the form c1b1 + ··· + cmbm for some c1, . . . , cm ∈ Z.)

Example 18.1.3 (Examples of ﬁnitely generated abelian groups)
(a) Z is ﬁnitely generated (by 1).

(b) Z/nZ is ﬁnitely generated (by 1).

(c) Z⊕2 is ﬁnitely generated (by two elements (1, 0) and (0, 1)).
(d) Z⊕3 ⊕ Z/9Z ⊕ Z/2016Z is ﬁnitely generated by ﬁve elements.
(e) Z/3Z ⊕ Z/5Z is ﬁnitely generated by two elements.

Exercise 18.1.4. In fact Z/3Z ⊕ Z/5Z is generated by one element. What is it?

You might notice that these examples are not very diverse. That’s because they are
actually the only examples:

Theorem 18.1.5 (Fundamental theorem of ﬁnitely generated abelian groups)

Let G be a ﬁnitely generated abelian group. Then there exists an integer r, prime
powers q1, . . . , qm (not necessarily distinct) such that

G ∼= Z⊕r ⊕ Z/q1Z ⊕ Z/q2Z ⊕ ··· ⊕ Z/qmZ.
This decomposition is unique up to permutation of the Z/qiZ.

213

214

Napkin, by Evan Chen (v1.5.20190718)

Deﬁnition 18.1.6. The rank of a ﬁnitely generated abelian group G is the integer r
above.

Now, we could prove this theorem, but it is more interesting to go for the gold and

state and prove the entire structure theorem.

§18.2 Some ring theory prerequisites

Prototypical example for this section: R = Z.

Before I can state the main theorem, I need to deﬁne a few terms for UFD’s, which

behave much like Z:

Our intuition from the case R = Z basically carries over verbatim.

We don’t even need to deal with prime ideals and can factor elements instead.

Deﬁnition 18.2.1. If R is a UFD, then p ∈ R is a prime element if (p) is a prime
ideal and p (cid:54)= 0. For UFD’s this is equivalent to: if p = xy then either x or y is a unit.
So for example in Z the set of prime elements is {±2,±3,±5, . . .}. Now, since R is a

UFD, every element r factors into a product of prime elements

r = upe1

1 pe2

2 . . . pem
m

Deﬁnition 18.2.2. We say r divides s if s = r(cid:48)r for some r(cid:48) ∈ R. This is written r | s.

Example 18.2.3 (Divisibility in Z)
The number 0 is divisible by every element of Z. All other divisibility as expected.

Question 18.2.4. Show that r | s if and only if the exponent of each prime in r is less than
or equal to the corresponding exponent in s.

Now, the case of interest is the even stronger case when R is a PID:

Proposition 18.2.5 (PID’s are Noetherian UFD’s)

If R is a PID, then it is Noetherian and also a UFD.

Proof. The fact that R is Noetherian is obvious. For R to be a UFD we essentially repeat
the proof for Z, using the fact that (a, b) is principal in order to extract gcd(a, b).

In this case, we have a Chinese remainder theorem for elements.

Theorem 18.2.6 (Chinese remainder theorem for rings)

Let m and n be relatively prime elements, meaning (m) + (n) = (1). Then

R/(mn) ∼= R/m × R/n.

Here the ring product is as deﬁned in Example 4.3.8.

18 The PID structure theorem

215

Proof. This is the same as the proof of the usual Chinese remainder theorem. First, since
(m, n) = (1) we have am + bn = 1 for some a and b. Then we have a map

R/m × R/n → R/(mn) by (r, s) (cid:55)→ r · bn + s · am.

One can check that this map is well-deﬁned and an isomorphism of rings. (Diligent
readers invited to do so.)

Finally, we need to introduce the concept of a Noetherian R-module.

Deﬁnition 18.2.7. An R-module M is Noetherian if it satisﬁes one of the two equiv-
alent conditions:

 Its submodules obey the ascending chain condition: there is no inﬁnite sequence of

modules M1 (cid:40) M2 (cid:40) . . . .

 All submodules of M (including M itself) are ﬁnitely generated.

This generalizes the notion of a Noetherian ring: a Noetherian ring R is one for which

R is Noetherian as an R-module.

Question 18.2.8. Check these two conditions are equivalent. (Copy the proof for rings.)

§18.3 The structure theorem

Our structure theorem takes two forms:

Theorem 18.3.1 (Structure theorem, invariant form)

Let R be a PID and let M be any ﬁnitely generated R-module. Then

M ∼=

m(cid:77)i=1

R/si

for some si satisfying s1 | s2 | ··· | sm.

Corollary 18.3.2 (Structure theorem, primary form)

Let R be a PID and let M be any ﬁnitely generated R-module. Then

M ∼= R⊕r ⊕ R/(q1) ⊕ R/(q2) ⊕ ··· ⊕ R/(qm)

where qi = pei
i

for some prime element pi and integer ei ≥ 1.

Proof of corollary. Factor each si into prime factors (since R is a UFD), then use the
Chinese remainder theorem.

Remark 18.3.3 — In both theorems the decomposition is unique up to permutations
of the summands; good to know, but I won’t prove this.

216

Napkin, by Evan Chen (v1.5.20190718)

§18.4 Reduction to maps of free R-modules

Deﬁnition 18.4.1. A free R-module is a module of the form R⊕n (or more generally,

(cid:76)I R for some indexing set I, just to allow an inﬁnite basis).

The proof of the structure theorem proceeds in two main steps. First, we reduce the
problem to a linear algebra problem involving free R-modules R⊕d. Once that’s done, we
just have to play with matrices; this is done in the next section.

Suppose M is ﬁnitely generated by d elements. Then there is a surjective map of

R-modules

R⊕d (cid:16) M

whose image on the basis of R⊕d are the generators of M . Let K denote the kernel.

We claim that K is ﬁnitely generated as well. To this end we prove that

Lemma 18.4.2 (Direct sum of Noetherian modules is Noetherian)
Let M and N be two Noetherian R-modules. Then the direct sum M ⊕ N is also a
Noetherian R-module.

Proof. It suﬃces to show that if L ⊆ M ⊕ N , then L is ﬁnitely generated. One guess is
that L = P ⊕ Q, where P and Q are the projections of L onto M and N . Unfortunately
this is false (take M = N = Z and L = {(n, n) | n ∈ Z}) so we will have to be more
careful.

Consider the submodules

A = {x ∈ M | (x, 0) ∈ L} ⊆ M
B = {y ∈ N | ∃x ∈ M : (x, y) ∈ L} ⊆ N.

(Note the asymmetry for A and B: the proof doesn’t work otherwise.) Then A is ﬁnitely
generated by a1, . . . , ak, and B is ﬁnitely generated by b1, . . . , b(cid:96). Let xi = (ai, 0) and
let yi = (∗, bi) be elements of L (where the ∗’s are arbitrary things we don’t care about).
Then xi and yi together generate L.

Question 18.4.3. Deduce that for R a PID, R⊕d is Noetherian.

Hence K ⊆ R⊕d is ﬁnitely generated as claimed. So we can ﬁnd another surjective map
R⊕f (cid:16) K. Consequently, we have a composition

--

K

⊂

R⊕f

-

-

R⊕d

T

--

M

Observe that M is the cokernel of the linear map T , i.e. we have that

So it suﬃces to understand the map T well.

M ∼= R⊕d/ im(T ).

18 The PID structure theorem

§18.5 Smith normal form

217

The idea is now that we have reduced our problem to studying linear maps T : R⊕m →
R⊕n, which can be thought of as a generic matrix

T =

a11
...
an1

. . .
. . .
. . .

a1m
...
anm



for a basis e1, . . . , em of R⊕m and f1, . . . , fn of N .

Of course, as you might expect it ought to be possible to change the given basis of T
such that T has a nicer matrix form. We already saw this in Jordan form, where we had
a map T : V → V and changed the basis so that T was “almost diagonal”. This time,
we have two sets of bases we can change, so we would hope to get a diagonal basis, or
even better.

Before proceeding let’s think about how we might edit the matrix: what operations

are permitted? Here are some examples:

 Swapping rows and columns, which just corresponds to re-ordering the basis.

 Adding a multiple of a column to another column. For example, if we add 3 times

the ﬁrst column to the second column, this is equivalent to replacing the basis

(e1, e2, e3, . . . , em) (cid:55)→ (e1, e2 + 3e1, e3, . . . , em).

 Adding a multiple of a row to another row. One can see that adding 3 times the

ﬁrst row to the second row is equivalent to replacing the basis

(f1, f2, f3, . . . , fn) (cid:55)→ (f1 − 3f2, f2, f3, . . . , fn).

More generally,

If A is an invertible n × n matrix we can replace T with AT .

This corresponds to replacing

(f1, . . . , fn) (cid:55)→ (A(f1), . . . , A(fn))

(the “invertible” condition just guarantees the latter is a basis). Of course similarly we
can replace X with XB where B is an invertible m × m matrix; this corresponds to

(e1, . . . , em) (cid:55)→ (B−1(e1), . . . , B−1(em))

Armed with this knowledge, we can now approach:

Theorem 18.5.1 (Smith normal form)
Let R be a PID. Let M = R⊕m and N = R⊕n be free R-modules and let T : M → N
be a linear map. Set k = min{m, n}.
Then we can select a pair of new bases for M and N such that T has only diagonal
entries s1, s2, . . . , sk and s1 | s2 | ··· | sk.

218

Napkin, by Evan Chen (v1.5.20190718)

So if m > n, the matrix should take the form

s1
0
...
0



0
s2
...
0

0
0
. . .
0

0
0
...
sn

. . .
. . .

. . .
. . .

.

0
0
...
0



and similarly when m ≤ n.

Question 18.5.2. Show that Smith normal form implies the structure theorem.

Remark 18.5.3 — Note that this is not a generalization of Jordan form.

 In Jordan form we consider maps T : V → V ; note that the source and target

space are the same, and we are considering one basis for the space V .

 In Smith form the maps T : M → N are between diﬀerent modules, and we

pick two sets of bases (one for M and one for N ).

Example 18.5.4 (Example of Smith normal form)
To give a ﬂavor of the idea of the proof, let’s work through a concrete example with
the Z-matrix

14 30 32(cid:21) .
(cid:20)18 38 48

The GCD of all the entries is 2, and so motivated by this, we perform the Euclidean
algorithm on the left column: subtract the second row from the ﬁrst row, then
three times the ﬁrst row from the second:

14 30 32(cid:21) (cid:55)→(cid:20) 4
(cid:20)18 38 48

2(cid:21) .
14 30 32(cid:21) (cid:55)→(cid:20)4 8 10

2 6

10

8

Now that the GCD of 2 is present, we move it to the upper-left by switching the two
rows, and then kill oﬀ all the entries in the same row/column; since 2 was the GCD
all along, we isolate 2 completely:

(cid:20)4 8 10

2(cid:21) (cid:55)→(cid:20)2 6

4 8 10(cid:21) (cid:55)→(cid:20)2

2 6

0 −4 6(cid:21) (cid:55)→(cid:20)2

0 −4 6(cid:21) .

2

6

2

0

0

This reduces the problem to a 1× 2 matrix. So we just apply the Euclidean algorithm
again there:

0

(cid:20)2
0 −4 6(cid:21) (cid:55)→(cid:20)2

0 −4 2(cid:21) (cid:55)→(cid:20)2 0 0

0 0 2(cid:21) (cid:55)→(cid:20)2 0 0
0 2 0(cid:21) .

0

0

0

Now all we have to do is generalize this proof to work with any PID. It’s intuitively clear
how to do this: the PID condition more or less lets you perform a Euclidean algorithm.

Proof of Smith normal form. Begin with a generic matrix

T =

a11
...
an1

. . .
. . .
. . .

a1m
...
anm



18 The PID structure theorem

219

We want to show, by a series of operations (gradually changing the given basis) that we
can rearrange the matrix into Smith normal form.

Deﬁne gcd(x, y) to be any generator of the principal ideal (x, y).

Claim 18.5.5 (“Euclidean algorithm”). If a and b are entries in the same row or column,
we can change bases to replace a with gcd(a, b) and b with something else.

Proof. We do just the case of columns. By hypothesis, gcd(a, b) = xa + yb for some
x, y ∈ R. We must have (x, y) = (1) now (we’re in a UFD). So there are u and v such
that xu + yv = 1. Then

y

(cid:20) x
−v u(cid:21)(cid:20)a

b(cid:21) =(cid:20) gcd(a, b)
something(cid:21)

and the ﬁrst matrix is invertible (check this!), as desired.

(cid:4)

Let s1 = (aij)i,j be the GCD of all entries. Now by repeatedly applying this algorithm,
we can cause s to appear in the upper left hand corner. Then, we use it to kill oﬀ all the
entries in the ﬁrst row and the ﬁrst column, thus arriving at a matrix

s1
0
0
...
0



0
0
a(cid:48)23
a(cid:48)22
a(cid:48)33
a(cid:48)32
...
...
a(cid:48)m2 a(cid:48)m3

. . .
. . .
. . .
. . .
. . .

0
a(cid:48)2n
a(cid:48)3n
...
a(cid:48)mn

.



Now we repeat the same procedure with this lower-right (m − 1) × (n − 1) matrix, and
so on. This gives the Smith normal form.

With the Smith normal form, we have in the original situation that

and applying the theorem to T completes the proof of the structure theorem.

M ∼= R⊕d/ im T

§18.6 A few harder problems to think about

Now, we can apply our structure theorem!

Problem 18A† (Finite-dimensional vector spaces are all isomorphic). A vector space V
over a ﬁeld k has a ﬁnite spanning set of vectors. Show that V ∼= k⊕n for some n.
Problem 18B† (Frobenius normal form). Let T : V → V where V is a ﬁnite-dimensional
vector space over an arbitrary ﬁeld k (not necessarily algebraically closed). Show that
one can write T as a block-diagonal matrix whose blocks are all of the form

0 ∗
0 ∗
0 ∗
...
...
1 ∗
(View V as a k[x]-module with action x · v = T (v).)

0 0 0 . . .
1 0 0 . . .
0 1 0 . . .
...
. . .
0 0 0 . . .

...

...



.



220

Napkin, by Evan Chen (v1.5.20190718)

Problem 18C† (Jordan normal form). Let T : V → V where V is a ﬁnite-dimensional
vector space over an arbitrary ﬁeld k which is algebraically closed. Prove that T can be
written in Jordan form.

Problem 18D. Find two abelian groups G and H which are not isomorphic, but for
which there are injective homomorphisms G (cid:44)→ H and H (cid:44)→ G.
Solution. Take G = Z/3Z⊕ Z/9Z⊕ Z/9Z⊕ Z/9Z⊕ . . . and H = Z/9Z⊕ Z/9Z⊕ Z/9Z⊕
Z/9Z⊕ . . . . Then there are maps G (cid:44)→ H and H (cid:44)→ G, but the groups are not isomorphic
since e.g. G has an element g ∈ G of order 3 for which there’s no g(cid:48) ∈ G with g = 3g(cid:48).

VI

Representation Theory

Part VI: Contents

19 Representations of algebras

223
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223
19.1 Algebras
19.2 Representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224
19.3 Direct sums . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226
19.4 Irreducible and indecomposable representations . . . . . . . . . . . . . . . . . . . . 227
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 228
19.5 Morphisms of representations
. . . . . . . . . . . . . . . . . . . . . . . . . . . 230
19.6 The representations of Matd(k)
. . . . . . . . . . . . . . . . . . . . . . . . 231
19.7 A few harder problems to think about

20 Semisimple algebras

233
20.1 Schur’s lemma continued . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233
20.2 Density theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234
20.3 Semisimple algebras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236
20.4 Maschke’s theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
20.5 Example: the representations of C[S3]
. . . . . . . . . . . . . . . . . . . . . . . . 237
. . . . . . . . . . . . . . . . . . . . . . . . 238
20.6 A few harder problems to think about

21 Characters

241
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241
21.1 Deﬁnitions
. . . . . . . . . . . . . . . . . . . . . . . 242
21.2 The dual space modulo the commutator
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243
21.3 Orthogonality of characters
21.4 Examples of character tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
. . . . . . . . . . . . . . . . . . . . . . . . 247
21.5 A few harder problems to think about

22 Some applications

249
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249
22.1 Frobenius divisibility
22.2 Burnside’s theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250
22.3 Frobenius determinant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251

19 Representations of algebras

In the 19th century, the word “group” hadn’t been invented yet; all work was done
with subsets of GL(n) or Sn. Only much later was the abstract deﬁnition of a group was
given, an abstract set G which was an object in its own right.

While this abstraction is good for some reasons, it is often also useful to work with
concrete representations. This is the subject of representation theory. Linear algebra is
easier than abstract algebra, so if we can take a group G and represent it concretely as
a set of matrices in GL(n), this makes them easier to study. This is the representation
theory of groups: how can we take a group and represent its elements as matrices?

§19.1 Algebras

Prototypical example for this section: k[x1, . . . , xn] and k[G].

Rather than working directly with groups from the beginning, it will be more convenient
to deal with so-called k-algebras. This setting is more natural and general than that of
groups, so once we develop the theory of algebras well enough, it will be fairly painless
to specialize to the case of groups.

Colloquially,

An associative k-algebra is a possibly noncommutative ring with a copy
of k inside it. It is thus a k-vector space.

I’ll present examples before the deﬁnition:

Example 19.1.1 (Examples of k-Algebras)
Let k be any ﬁeld. The following are examples of k-algebras:

(a) The ﬁeld k itself.

(b) The polynomial ring k[x1, . . . , xn].

(c) The set of n × n matrices with entries in k, which we denote by Matn(k). Note

the multiplication here is not commutative.

(d) The set Mat(V ) of linear operators T : V → V , with multiplication given by the
composition of operators. (Here V is some vector space over k.) This is really
the same as the previous example.

Deﬁnition 19.1.2. Let k be a ﬁeld. A k-algebra A is a possibly noncommutative ring,
equipped with an injective ring homomorphism k (cid:44)→ A (whose image is the “copy of k”).
In particular, 1k (cid:55)→ 1A.
Thus we can consider k as a subset of A, and we then additionally require λ · a = a · λ
for each λ ∈ k and a ∈ A.
If the multiplication operation is also commutative, then we say A is a commutative
algebra.

223

224

Napkin, by Evan Chen (v1.5.20190718)

Deﬁnition 19.1.3. Equivalently, a k-algebra A is a k-vector space which also has an
associative, bilinear multiplication operation (with an identity 1A). The “copy of k” is
obtained by considering elements λ1A for each λ ∈ k (i.e. scaling the identity by the
elements of k, taking advantage of the vector space structure).

Abuse of Notation 19.1.4. Some other authors don’t require A to be associative or
to have an identity, so to them what we have just deﬁned is an “associative algebra with
1”. However, this is needlessly wordy for our purposes.

Example 19.1.5 (Group algebra)
The group algebra k[G] is the k-vector space whose basis elements are the elements
of a group G, and where the product of two basis elements is the group multiplication.
For example, suppose G = Z/2Z = {1G, x}. Then

with multiplication given by

k[G] = {a1G + bx | a, b ∈ k}

(a1G + bx)(c1G + dx) = (ac + bd)1G + (bc + ad)x.

Question 19.1.6. When is k[G] commutative?

The example k[G] is very important, because (as we will soon see) a representation of
the algebra k[G] amounts to a representation of the group G itself.

It is worth mentioning at this point that:

Deﬁnition 19.1.7. A homomorphism of k-algebras A, B is a linear map T : A → B
which respects multiplication (i.e. T (xy) = T (x)T (y)) and which sends 1A to 1B. In
other words, T is both a homomorphism as a ring and as a vector space.

Deﬁnition 19.1.8. Given k-algebras A and B, the direct sum A ⊕ B is deﬁned as
pairs a + b, where addition is done in the obvious way, but we declare ab = 0 for any
a ∈ A and b ∈ B.

Question 19.1.9. Show that 1A + 1B is the multiplicative identity of A ⊕ B.

§19.2 Representations

Prototypical example for this section: k[S3] acting on k⊕3 is my favorite.

Deﬁnition 19.2.1. A representation of a k-algebra A (also a left A-module) is:

(i) A k-vector space V , and

(ii) An action · of A on V : thus, for every a ∈ A we can take v ∈ V and act on it to

get a · v. This satisﬁes the usual axioms:

 (a + b) · v = a · v + b · v, a · (v + w) = a · v + a · w, and (ab) · v = a · (b · v).
 λ · v = λv for λ ∈ k. In particular, 1A · v = v.

19 Representations of algebras

225

Deﬁnition 19.2.2. The action of A can be more succinctly described as saying that
there is a k-algebra homomorphism ρ : A → Mat(V ). (So a · v = ρ(a)(v).) Thus we can
also deﬁne a representation of A as a pair

(V, ρ : A → Mat(V )) .

This is completely analogous to how a group action G on a set X with n elements just
amounts to a group homomorphism G → Sn. From this perspective, what we are really
trying to do is:

If A is an algebra, we are trying to represent the elements of A as matrices.

Abuse of Notation 19.2.3. While a representation is a pair (V, ρ) of both the vector
space V and the action ρ, we frequently will just abbreviate it to “V ”. This is probably
one of the worst abuses I will commit, but everyone else does it and I fear the mob.

Abuse of Notation 19.2.4. Rather than ρ(a)(v) we will just write ρ(a)v.

Example 19.2.5 (Representations of Mat(V ))
(a) Let A = Mat2(R). Then there is a representation (R⊕2, ρ) where a matrix a ∈ A

just acts by a · v = ρ(a)(v) = a(v).

(b) More generally, given a vector space V over any ﬁeld k, there is an obvious

representation of A = Mat(V ) by a · v = ρ(a)(v) = a(v) (since a ∈ Mat(V )).
From the matrix perspective: if A = Mat(V ), then we can just represent A as
matrices over V .

(c) There are other representations of A = Mat2(R). A silly example is the repre-

sentation (R⊕4, ρ) given by

c d(cid:21) (cid:55)→
ρ :(cid:20)a b

a b 0 0
c d 0 0
0 0 a b
0 0 c d

 .

More abstractly, viewing R⊕4 as (R⊕2)⊕ (R⊕2), this is a· (v1, v2) = (a· v1, a· v2).

Example 19.2.6 (Representations of polynomial algebras)

(a) Let A = k. Then a representation of k is just any k-vector space V .

(b) If A = k[x], then a representation (V, ρ) of A amounts to a vector space V plus

the choice of a linear operator T ∈ Mat(V ) (by T = ρ(x)).

(c) If A = k[x]/(x2) then a representation (V, ρ) of A amounts to a vector space V

plus the choice of a linear operator T ∈ Mat(V ) satisfying T 2 = 0.

(d) We can create arbitrary “functional equations” with this pattern. For example,
if A = k[x, y]/(x2 − x + y, y4) then representing A by V amounts to ﬁnding
operators S, T ∈ Mat(V ) satisfying S2 = S − T and T 4 = 0.

226

Napkin, by Evan Chen (v1.5.20190718)

Example 19.2.7 (Representations of groups)
(a) Let A = R[S3]. Then let

V = R⊕3 = {(x, y, z) | x, y, z ∈ R}.

We can let A act on V as follows: given a permutation π ∈ S3, we permute the
corresponding coordinates in V . So for example, if

If π = (1 2) then π · (x, y, z) = (y, x, z).

This extends linearly to let A act on V , by permuting the coordinates.

From the matrix perspective, what we are doing is representing the permutations
in S3 as permutation matrices on k⊕3, like

(1 2) (cid:55)→

0 1 0
1 0 0
0 0 1

 .

(b) More generally, let A = k[G]. Then a representation (V, ρ) of A amounts to a
group homomorphism ψ : G → GL(V ). (In particular, ρ(1G) = idV .) We call
this a group representation of G.

Example 19.2.8 (Regular representation)
Any k-algebra A is a representation (A, ρ) over itself, with a · b = ρ(a)(b) = ab (i.e.
multiplication given by A). This is called the regular representation, denoted
Reg(A).

§19.3 Direct sums

Prototypical example for this section: The example with R[S3] seems best.

Deﬁnition 19.3.1. Let A be k-algebra and let V = (V, ρV ) and W = (W, ρW ) be two
representations of A. Then V ⊕ W is a representation, with action ρ given by

This representation is called the direct sum of V and W .

a · (v, w) = (a · v, a · w).

Example 19.3.2
Earlier we let Mat2(R) act on R⊕4 by

So this is just a direct sum of two two-dimensional representations.

 .

c d(cid:21) (cid:55)→
ρ :(cid:20)a b

a b 0 0
c d 0 0
0 0 a b
0 0 c d

19 Representations of algebras

227

More generally, given representations (V, ρV ) and (W, ρW ) the representation ρ of V ⊕ W
looks like

ρ(a) =(cid:20)ρV (a)

0

0

ρW (a)(cid:21) .

Example 19.3.3 (Representation of Sn decomposes)
Let A = R[S3] again, acting via permutation of coordinates on

V = R⊕3 = {(x, y, z) | x, y, z ∈ R}.

Consider the two subspaces

W1 = {(t, t, t) | t ∈ R}
W2 = {(x, y, z) | x + y + z = 0} .

Note V = W1 ⊕ W2 as vector spaces. But each of W1 and W2 is a subrepresentation
(since the action of A keeps each Wi in place), so V = W1 ⊕ W2 as representations
too.

Direct sums also come up when we play with algebras.

Proposition 19.3.4 (Representations of A ⊕ B are VA ⊕ VB)
Let A and B be k-algebras. Then every representation of A ⊕ B is of the form

where VA and VB are representations of A and B, respectively.

VA ⊕ VB

Sketch of Proof. Let (V, ρ) be a representation of A ⊕ B. For any v ∈ V , ρ(1A + 1B)v =
ρ(1A)v + ρ(1B)v. One can then set VA = {ρ(1A)v | v ∈ V } and VB = {ρ(1B)v | v ∈ V }.
These are disjoint, since if ρ(1A)v = ρ(1B)v(cid:48), we have ρ(1A)v = ρ(1A1A)v = ρ(1A1B)v(cid:48) =
0V , and similarly for the other side.

§19.4 Irreducible and indecomposable representations

Prototypical example for this section: k[S3] decomposes as the sum of two spaces.

One of the goals of representation theory will be to classify all possible representations
of an algebra A. If we want to have a hope of doing this, then we want to discard “silly”
representations such as

c d(cid:21) (cid:55)→
ρ :(cid:20)a b

a b 0 0
c d 0 0
0 0 a b
0 0 c d



and focus our attention instead on “irreducible” representations. This motivates:

Deﬁnition 19.4.1. Let V be a representation of A. A subrepresentation W ⊆ V is
a subspace W with the property that for any a ∈ A and w ∈ W , a · w ∈ W . In other
words, this subspace is invariant under actions by A.

228

Napkin, by Evan Chen (v1.5.20190718)

Thus if V (cid:54)= W1 ⊕ W2 for representations W1, W2 then W1 and W2 are subrepresenta-

tions.

Deﬁnition 19.4.2. If V has no proper subrepresentations then it is irreducible. If
V (cid:54)= W1 ⊕ W2 for proper subrepresentations W1, W2, then we say it is indecomposable.
Deﬁnition 19.4.3. For brevity, an irrep of an algebra/group is a ﬁnite-dimensional
irreducible representation.

Example 19.4.4 (Representation of Sn decomposes)
Let A = R[S3] again, acting via permutation of coordinates on

V = R⊕3 = {(x, y, z) | x, y, z ∈ R}.

Consider again the two subspaces

W1 = {(t, t, t) | t ∈ R}
W2 = {(x, y, z) | x + y + z = 0} .

As we’ve seen, V = W1 ⊕ W2, and thus V is not irreducible. But one can show that
W1 and W2 are irreducible (and hence indecomposable) as follows.

 For W1 it’s obvious, since W1 is one-dimensional.

 For W2, consider any vector w = (a, b, c) with a + b + c = 0 and not all zero.
Then WLOG we can assume a (cid:54)= b (since not all three coordinates are equal).
In that case, (1 2) sends w to w(cid:48) = (b, a, c). Then w and w(cid:48) span W2.

Thus V breaks down completely into irreps.

Unfortunately, if W is a subrepresentation of V , then it is not necessarily the case that
we can ﬁnd a supplementary vector space W (cid:48) such that V = W ⊕ W (cid:48). Put another way,
if V is reducible, we know that it has a subrepresentation, but a decomposition requires
two subrepresentations. Here is a standard counterexample:

Exercise 19.4.5. Let A = R[x], and V = R⊕2 be the representation with action

ρ(x) =(cid:20)1

0

1

1(cid:21) .

Show that the only subrepresentation is W = {(t, 0) | t ∈ R}. So V is not irreducible, but it
is indecomposable.

Here is a slightly more optimistic example, and the “prototypical example” that you

should keep in mind.

Exercise 19.4.6. Let A = Matd(k) and consider the obvious representation k⊕d of A that
we described earlier. Show that it is irreducible. (This is obvious if you understand the
deﬁnitions well enough.)

§19.5 Morphisms of representations

We now proceed to deﬁne the morphisms between representations.

19 Representations of algebras

229

Deﬁnition 19.5.1. Let (V, ρV ) and (W, ρW ) be representations of A. An intertwining
operator, or morphism, is a linear map T : V → W such that

T (a · v) = a · T (v)

for any a ∈ A, v ∈ V . (Note that the ﬁrst · is the action of ρV and the second · is the
action of ρW .) This is exactly what you expect if you think that V and W are “left
A-modules”. If T is invertible, then it is an isomorphism of representations and we say
V ∼= W .

Remark 19.5.2 (For commutative diagram lovers) — The condition T (a· v) = a· T (v)
can be read as saying that

V

ρ1(a) - V

T

?
W

T

?
W

-

ρ2(a)

commutes for any a ∈ A.

Remark 19.5.3 (For category lovers) — A representation is just a “bilinear” functor
from an abelian one-object category {∗} (so Hom(∗,∗) ∼= A) to the abelian category
Vectk. Then an intertwining operator is just a natural transformation.

Here are some examples of intertwining operators.

Example 19.5.4 (Intertwining operators)
(a) For any λ ∈ k, the scalar map T (v) = λv is intertwining.
(b) If W ⊆ V is a subrepresentation, then the inclusion W (cid:44)→ V is an intertwining
(c) The projection map V1 ⊕ V2 (cid:16) V1 is an intertwining operator.
(d) Let V = R⊕2 and represent A = k[x] by (V, ρ) where

operator.

ρ(x) =(cid:20) 0

−1 0(cid:21) .

1

Thus ρ(x) is rotation by 90◦ around the origin. Let T be rotation by 30◦. Then
T : V → V is intertwining (the rotations commute).

Exercise 19.5.5 (Kernel and image are subrepresentations). Let T : V → W be an
intertwining operator.
(a) Show that ker T ⊆ V is a subrepresentation of V .
(b) Show that im T ⊆ W is a subrepresentation of W .

The previous lemma gives us the famous Schur’s lemma.

230

Napkin, by Evan Chen (v1.5.20190718)

Theorem 19.5.6 (Schur’s lemma)
Let V and W be representations of a k-algebra A. Let T : V → W be a nonzero
intertwining operator. Then

(a) If V is irreducible, then T is injective.

(b) If W is irreducible, then T is surjective.

In particular if both V and W are irreducible then T is an isomorphism.

An important special case is if k is algebraically closed: then the only intertwining
operators T : V → V are multiplication by a constant.

Theorem 19.5.7 (Schur’s lemma for algebraically closed ﬁelds)

Let k be an algebraically closed ﬁeld. Let V be an irrep of a k-algebra A. Then any
intertwining operator T : V → V is multiplication by a scalar.

Exercise 19.5.8. Use the fact that T has an eigenvalue λ to deduce this from Schur’s
lemma. (Consider T − λ · idV , and use Schur to deduce it’s zero.)

We have already seen the counterexample of rotation by 90◦ for k = R; this was the same
counterexample we gave to the assertion that all linear maps have eigenvalues.

§19.6 The representations of Matd(k)

To give an example of the kind of progress already possible, we prove:

Theorem 19.6.1 (Representations of Matd(k))
Let k be any ﬁeld, d be a positive integer and let W = k⊕d be the obvious rep-
resentation of A = Matd(k). Then the only ﬁnite-dimensional representations of
Matd(k) are W ⊕n for some positive integer n (up to isomorphism). In particular, it
is irreducible if and only if n = 1.

For concreteness, I’ll just sketch the case d = 2, since the same proof applies verbatim to
other situations. This shows that the examples of representations of Mat2(R) we gave
earlier are the only ones.

As we’ve said this is essentially a functional equation. The algebra A = Mat2(k) has

basis given by four matrices

E1 =(cid:20)1 0
0 0(cid:21) ,

E2 =(cid:20)0 0
0 1(cid:21) ,

E3 =(cid:20)0 1
0 0(cid:21) ,

E4 =(cid:20)0 0
1 0(cid:21)

satisfying relations like E1 + E2 = idA, E2
i = Ei, E1E2 = 0, etc. So let V be a
representation of A, and let Mi = ρ(Ei) for each i; we want to classify the possible
matrices Mi on V satisfying the same functional equations. This is because, for example,

idV = ρ(idA) = ρ(E1 + E2) = M1 + M2.

19 Representations of algebras

231

By the same token M1M3 = M3. Proceeding in a similar way, we can obtain the following
multiplication table:

× M1 M2 M3 M4
M1 M1 0 M3 0
M2 0 M2 0 M4
M3 0 M3 0 M1
M4 M4 0 M2 0

and

M1 + M2 = idV

Note that each Mi is a linear operator V → V ; for all we know, it could have hun-
dreds of entries. Nonetheless, given the multiplication table of the basis Ei we get the
corresponding table for the Mi.

So, in short, the problem is as follows:

Find all vector spaces V and quadruples of matrices Mi satisfying the
multiplication table above.

Let W1 = M img

1

(V ) and W2 = M img

2

(V ) be the images of M1 and M2.

Claim 19.6.2. V = W1 ⊕ W2.
Proof. First, note that for any v ∈ V we have

v = ρ(id)(v) = (M1 + M2)v = M1v + M2v.

Moreover, we have that W1 ∩ W2 = {0}, because if M1v1 = M2v2 then M1v1 =
M1(M1v1) = M1(M2v2) = 0.
Claim 19.6.3. W1 ∼= W2.
Proof. Check that the maps

are well-deﬁned and mutually inverse.

W1 ×M4−−−→ W2

and W2 ×M3−−−→ W1

Now, let e1, . . . , en be basis elements of W1; thus M4e1, . . . , M4en are basis elements
of W2. However, each {ej, M4ej} forms a basis of a subrepresentation isomorphic to
W = k⊕2 (what’s the isomorphism?).
This ﬁnally implies that all representations of A are of the form W ⊕n. In particular,

W is irreducible because there are no representations of smaller dimension at all!

§19.7 A few harder problems to think about

Problem 19A†. Suppose we have one-dimensional representations V1 = (V1, ρ1) and
V2 = (V2, ρ2) of A. Show that V1 ∼= V2 if and only if ρ1(a) and ρ2(a) are multiplication
by the same constant for every a ∈ A.
Problem 19B† (Schur’s lemma for commutative algebras). Let A be a commutative
algebra over an algebraically closed ﬁeld k. Prove that any irrep of A is one-dimensional.

Problem 19C(cid:63). Let (V, ρ) be a representation of A. Then Mat(V ) is a representation
of A with action given by

for T ∈ Mat(V ).

a · T = ρ(a) ◦ T

232

Napkin, by Evan Chen (v1.5.20190718)

(a) Show that ρ : Reg(A) → Mat(V ) is an intertwining operator.
(b) If V is d-dimensional, show that Mat(V ) ∼= V ⊕d as representations of A.
Problem 19D(cid:63). Fix an algebra A. Find all intertwining operators

T : Reg(A) → Reg(A).

Problem 19E. Let (V, ρ) be an indecomposable (not irreducible) representation of an
algebra A. Prove that any intertwining operator T : V → V is either nilpotent or an
isomorphism.
(Note that Theorem 19.5.7 doesn’t apply, since the ﬁeld k may not be algebraically

closed.)

20 Semisimple algebras

In what follows, assume the ﬁeld k is algebraically closed.
Fix an algebra A and suppose you want to study its representations. We have a “direct
sum” operation already. So, much like we pay special attention to prime numbers, we’re
motivated to study irreducible representations and then build all the representations of
A from there.

Unfortunately, we have seen (Exercise 19.4.5) that there exists a representation which
is not irreducible, and yet cannot be broken down as a direct sum (indecomposable).
This is weird and bad, so we want to give a name to representations which are more
well-behaved. We say that a representation is completely reducible if it doesn’t exhibit
this bad behavior.

Even better, we say a ﬁnite-dimensional algebra A is semisimple if all its ﬁnite-
dimensional representations are completely reducible. So when we study ﬁnite-dimensional
representations of semisimple algebras A, they exhibit the good property that we just
have to ﬁgure out what the irreps are, and then piecing them together will give all the
representations of A.

In fact, semisimple algebras A have even nicer properties. The culminating point of

the chapter is when we prove that A is semisimple if and only if A ∼=(cid:76)i Mat(Vi), where

the Vi are the irreps of A (yes, there are only ﬁnitely many!).

§20.1 Schur’s lemma continued

Prototypical example for this section: For V irreducible, Homrep(V ⊕2, V ⊕2) ∼= k⊕4.

Deﬁnition 20.1.1. For an algebra A and representations V and W , we let Homrep(V, W )
be the set of intertwining operators between them. (It is also a k-algebra.)

By Schur’s lemma (since k is algebraically closed, which again, we are taking as a

standing assumption), we already know that if V and W are irreps, then

Homrep(V, W ) ∼=(cid:40)k if V ∼= W
if V (cid:54)∼= W .

0

Can we say anything more? For example, it also tells us that

Homrep(V, V ⊕2) = k⊕2.

The possible maps are v (cid:55)→ (c1v1, c2v2) for some choice of c1, c2 ∈ k.
More generally, suppose V is an irrep and consider Homrep(V ⊕m, V ⊕n). Intertwining
operators are determined completely T : V ⊕m → V ⊕n by the mn choices of compositions

V ⊂

-

V ⊕m

T -

V ⊕n

--

V

where the ﬁrst arrow is inclusion to the ith component of V ⊕m (for 1 ≤ i ≤ m) and the
second arrow is inclusion to the jth component of V ⊕n (for 1 ≤ j ≤ n). However, by
Schur’s lemma on each of these compositions, we know they must be constant.

233

234

Napkin, by Evan Chen (v1.5.20190718)

Thus, Homrep(V ⊕n, V ⊕m) consist of n × m “matrices” of constants, and the map is

provided by

c11
c21
...
cm1



c12
c22
...
cm2

. . .
. . .
. . .
. . .

c1(n−1)
c2(n−1)

...

cm(n−1)

c1n
c1n
...
cmn





v1
v2
...
vn

 ∈ V ⊕n

where the cij ∈ k but vi ∈ V ; note the type mismatch! This is not just a linear map
V ⊕ni → V ⊕mi; rather, the outputs are m linear combinations of the inputs.

More generally, we have:

Theorem 20.1.2 (Schur’s lemma for completely reducible representations)

Let V and W be completely reducible representations, and set V = (cid:76) V ⊕ni
W =(cid:76) V ⊕mi

for integers ni, mi ≥ 0, where each Vi is an irrep. Then

i

i

,

Homrep(V, W ) ∼=(cid:77)i

Matni×mi(k)

meaning that an intertwining operator T : V → W amounts to, for each i, an ni× mi
matrix of constants which gives a map V ⊕ni

.

i → V ⊕mi

i

Corollary 20.1.3 (Subrepresentations of completely reducible representations)

Let V =(cid:76) V ⊕ni
isomorphic to(cid:76) V ⊕mi

i

by the direct sum of inclusion V ⊕mi

i

be completely reducible. Then any subrepresentation W of V is
where mi ≤ ni for each i, and the inclusion W (cid:44)→ V is given

i

(cid:44)→ V ⊕ni

i

, which are ni × mi matrices.

Proof. Apply Schur’s lemma to the inclusion W (cid:44)→ V .

§20.2 Density theorem

We are going to take advantage of the previous result to prove that ﬁnite-dimensional
algebras have ﬁnitely many irreps.

Theorem 20.2.1 (Jacobson density theorem)

Let (V1, ρ1), . . . , (Vr, ρr) be pairwise nonisomorphic ﬁnite-dimensional representations
of A. Then there is a surjective map of vector spaces

r(cid:77)i=1

ρi : A (cid:16) r(cid:77)i=1

Mat(Vi).

The right way to think about this theorem is that

Density is the “Chinese remainder theorem” for irreps of A.

Recall that in number theory, the Chinese remainder theorem tells us that given lots of
“unrelated” congruences, we can ﬁnd a single N which simultaneously satisﬁes them all.

20 Semisimple algebras

235

Similarly, given lots of diﬀerent nonisomorphic representations of A, this means that we
can select a single a ∈ A which induces any tuple (ρ1(a), . . . , ρr(a)) of actions we want —
a surprising result, since even the r = 1 case is not obvious at all!

ρ1(a)= M1 ∈ Mat(V1)

-

-

ρ2(a)= M2 ∈ Mat(V2)

...

a ∈ A

This also gives us the non-obvious corollary

-

ρr(a)

= Mr ∈ Mat(Vr)

Corollary 20.2.2 (Finiteness of number of representations)

Any ﬁnite-dimensional algebra A has at most dim A irreps.

Proof. If Vi are such irreps then A (cid:16) (cid:76)i V ⊕ dim Vi
(cid:80)(dim Vi)2 ≤ dim A.
Proof of density theorem. Let V = V1 ⊕ ··· ⊕ Vr, so A acts on V = (V, ρ) by ρ =(cid:76)i ρi.

Thus by Problem 19C(cid:63), we can instead consider ρ as an intertwining operator

, hence we have the inequality

i

ρ : Reg(A) →

Mat(Vi) ∼=

V ⊕di
i

.

r(cid:77)i=1

r(cid:77)i=1

We will use this instead as it will be easier to work with.

First, we handle the case r = 1. Fix a basis e1, . . . , en of V = V1. Assuming for
contradiction that the map is not surjective. Then there is a map of representations (by
ρ and the isomorphism) Reg(A) → V ⊕n given by a (cid:55)→ (a · e1, . . . , a · en). By hypothesis
is not surjective: its image is a proper subrepresentation of V ⊕n. Assume its image is
isomorphic to V ⊕m for m < n, so by Theorem 20.1.2 there is a matrix of constants X
with

Reg(A)

-

V ⊕n 

X·−

⊃ V ⊕r

a

1A

- (a · e1, . . . , a · en)
- (e1, . . . , en) 

(v1, . . . , vm)

where the two arrows in the top row have the same image; hence the pre-image (v1, . . . , vm)
of (e1, . . . , en) can be found. But since r < n we can ﬁnd constants c1, . . . , cn not all zero
such that X applied to the column vector (c1, . . . , cn) is zero:

. . .

n(cid:88)i=1

cn(cid:3)

ciei =(cid:2)c1

cn(cid:3) X
As for r ≥ 2, the image ρimg(A) is necessarily of the form(cid:76)i V ⊕ri

 =(cid:2)c1

and by the above ri = dim Vi for each i.

e1
...
en

. . .

v1
...
vm

 = 0

contradicting the fact that ei are linearly independent. Hence we conclude the theorem
for r = 1.

(by Corollary 20.1.3)

i

236

Napkin, by Evan Chen (v1.5.20190718)

§20.3 Semisimple algebras

Deﬁnition 20.3.1. A ﬁnite-dimensional algebra A is a semisimple if every ﬁnite-
dimensional representation of A is completely reducible.

Theorem 20.3.2 (Semisimple algebras)

Let A be a ﬁnite-dimensional algebra. Then the following are equivalent:

(i) A ∼=(cid:76)i Matdi(k) for some di.

(ii) A is semisimple.

(iii) Reg(A) is completely reducible.

Proof. (i) =⇒ (ii) follows from Theorem 19.6.1 and Proposition 19.3.4. (ii) =⇒ (iii) is
tautological.

To see (iii) =⇒ (i), we use the following clever trick. Consider

Homrep(Reg(A), Reg(A)).

On one hand, by Problem 19D(cid:63), it is isomorphic to Aop (A with opposite multiplication),
because the only intertwining operators Reg(A) → Reg(A) are those of the form − · a.
. By Theorem 20.1.2,
we have

On the other hand, suppose that we have set Reg(A) =(cid:76)i V ⊕ni

i

Aop ∼= Homrep(Reg(A), Reg(A)) =(cid:77)i

Matni×ni(k).

But Matn(k)op ∼= Matn(k) (just by transposing), so we recover the desired conclusion.

In fact, if we combine the above result with the density theorem (and Corollary 20.2.2),

we obtain:

Theorem 20.3.3 (Sum of squares formula)

For a ﬁnite-dimensional algebra A we have

where the Vi are the irreps of A; equality holds exactly when A is semisimple, in
which case

(cid:88)i
Reg(A) ∼=(cid:77)i

dim(Vi)2 ≤ dim A

Mat(Vi) ∼=(cid:77)I

V ⊕ dim Vi
i

.

Proof. The inequality was already mentioned in Corollary 20.2.2. It is equality if and

only if the map ρ : A →(cid:76)i Mat(Vi) is an isomorphism; this means all Vi are present.
ρ : A →(cid:76)i Mat(Vi) is denoted Rad(A) and is the so-called Jacobson radical of

A; it’s the set of all a ∈ A which act by zero in all irreps of A. The usual deﬁnition
of “semisimple” given in books is that this Jacobson radical is trivial.

Remark 20.3.4 (Digression) — For any ﬁnite-dimensional A, the kernel of the map

20 Semisimple algebras

§20.4 Maschke’s theorem

237

We now prove that the representation theory of groups is as nice as possible.

Theorem 20.4.1 (Maschke’s theorem)

Let G be a ﬁnite group, and k an algebraically closed ﬁeld whose characteristic does
not divide |G|. Then k[G] is semisimple.

This tells us that when studying representations of groups, all representations are
completely reducible.

Proof. Consider any ﬁnite-dimensional representation (V, ρ) of k[G]. Given a proper
subrepresentation W ⊆ V , our goal is to construct a supplementary G-invariant subspace
W (cid:48) which satisﬁes

V = W ⊕ W (cid:48).

This will show that indecomposable ⇐⇒ irreducible, which is enough to show k[G] is
semisimple.
Let π : V → W be any projection of V onto W , meaning π(v) = v ⇐⇒ v ∈ W . We

consider the averaging map P : V → V by

P (v) =

1

|G|(cid:88)g∈G

ρ(g−1) ◦ π ◦ ρ(g).

We’ll use the following properties of the map:

Exercise 20.4.2. Show that the map P satisﬁes:

 For any w ∈ W , P (w) = w.
 For any v ∈ V , P (w) ∈ W .
 The map P : V → V is an intertwining operator.

Thus P is idempotent (it is the identity on its image W ), so by Problem 9G(cid:63) we have
V = ker P ⊕ im P , but both ker P and im P are subrepresentations as desired.

Remark 20.4.3 — In the case where k = C, there is a shorter proof. Suppose
B : V × V → C is an arbitrary bilinear form. Then we can “average” it to obtain a
new bilinear form

(cid:104)v, w(cid:105) :=

B(g · v, g · w).

1

|G|(cid:88)g∈G

The averaged form (cid:104)−,−(cid:105) is G-invariant, in the sense that (cid:104)v, w(cid:105) = (cid:104)g · v, g · w(cid:105).
Then, one sees that if W ⊆ V is a subrepresentation, so is its orthogonal complement
W ⊥. This implies the result.

§20.5 Example: the representations of C[S3]
We compute all irreps of C[S3]. I’ll take for granted right now there are exactly three
such representations (which will be immediate by the ﬁrst theorem in the next chapter:

238

Napkin, by Evan Chen (v1.5.20190718)

we’ll in fact see that the number of representations of G is exactly equal to the number
of conjugacy classes of G).

Given that, if the three representations of have dimension d1, d2, d3 , then we ought

to have

1 + d2
d2

2 + d2

3 = |G| = 6.

From this, combined with some deep arithmetic, we deduce that we should have d1 =
d2 = 1 and d3 = 2 or some permutation.

In fact, we can describe these representations explicitly. First, we deﬁne:

Deﬁnition 20.5.1. Let G be a group. The complex trivial group representation of
a group G is the one-dimensional representation Ctriv = (C, ρ) where g · v = v for all
g ∈ G and v ∈ C (i.e. ρ(g) = id for all g ∈ G).

Remark 20.5.2 (Warning) — The trivial representation of an algebra A doesn’t
make sense for us: we might want to set a·v = v but this isn’t linear in A. (You could
try to force it to work by deleting the condition 1A · v = v from our deﬁnition; then
one can just set a· v = 0. But even then Ctriv would not be the trivial representation
of k[G].)

Then the representations are:
 The one-dimensional Ctriv; each σ ∈ S3 acts by the identity.
 There is a nontrivial one-dimensional representation Csign where the map S3 → C×
is given by sending σ to the sign of σ. Thus in Csign every σ ∈ S3 acts as ±1. Of
course, Ctriv and Csign are not isomorphic (as one-dimensional representations are
never isomorphic unless the constants they act on coincide for all a, as we saw in
Problem 19A†).

 Finally, we have already seen the two-dimensional representation, but now we give
it a name. Deﬁne reﬂ0 to be the representation whose vector space is {(x, y, z) |
x + y + z = 0}, and whose action of S3 on it is permutation of coordinates.

Exercise 20.5.3. Show that reﬂ0 is irreducible, for example by showing directly that
no subspace is invariant under the action of S3.

Thus V is also not isomorphic to the previous two representations.

This implies that these are all the irreps of S3. Note that, if we take the representation
V of S3 on k⊕3, we just get that V = reﬂ0 ⊕Ctriv.

§20.6 A few harder problems to think about

Problem 20A. Find all the irreps of C[Z/nZ].
Problem 20B (Maschke requires |G| ﬁnite). Consider the representation of the group
R on C⊕2 under addition by a homomorphism

R → Mat2(C) by t (cid:55)→(cid:20)1 t
0 1(cid:21) .

Show that this representation is not irreducible, but it is indecomposable.

20 Semisimple algebras

239

Problem 20C. Prove that all irreducible representations of a ﬁnite group are ﬁnite-
dimensional.

Problem 20D. Determine all the complex irreps of D10.

21 Characters

Characters are basically the best thing ever. To every representation V of A we will
attach a so-called character χV : A → k. It will turn out that the characters of irreps of
V will determine the representation V completely. Thus an irrep is just speciﬁed by a
set of dim A numbers.

§21.1 Deﬁnitions

Deﬁnition 21.1.1. Let V = (V, ρ) be a ﬁnite-dimensional representation of A. The
character χV : A → k attached to A is deﬁned χV = Tr◦ρ, i.e.

χV (a) := Tr (ρ(a) : V → V ) .

Since Tr and ρ are additive, this is a k-linear map (but it is not multiplicative). Note

also that χV ⊕W = χV + χW for any representations V and W .
We are especially interested in the case A = k[G], of course. As usual, we just have
to specify χV (g) for each g ∈ S3 to get the whole map k[G] → k. Thus we often
think of χV as a function G → k, called a character of the group G. Here is the case
G = S3:

Example 21.1.2 (Character table of S3)
Let’s consider the three irreps of G = S3 from before. For Ctriv all traces are 1;
for Csign the traces are ±1 depending on sign (obviously, for one-dimensional maps
k → k the trace “is” just the map itself). For reﬂ0 we take a basis (1, 0,−1) and
(0, 1,−1), say, and compute the traces directly in this basis.

χV (g)
Ctriv
Csign
reﬂ0

id (1 2)
1
1
−1
1
0
2

(2 3)
1
−1
0

(3 1)
1
−1
0

(1 2 3)
1
1
−1

(3 2 1)
1
1
−1

The above table is called the character table of the group G. The table above has
certain mysterious properties, which we will prove as the chapter progresses.

(I) The value of χV (g) only depends on the conjugacy class of g.

(II) The number of rows equals the number of conjugacy classes.

(III) The sum of the squares of any row is 6 again!

(IV) The “dot product” of any two rows is zero.

Abuse of Notation 21.1.3. The name “character” for χV : G → k is a bit of a
misnomer. This χV is not multiplicative in any way, as the above example shows: one
can almost think of it as an element of k⊕|G|.

241

242

Napkin, by Evan Chen (v1.5.20190718)

Question 21.1.4. Show that χV (1A) = dim V , so one can read the dimensions of the
representations from the leftmost column of a character table.

§21.2 The dual space modulo the commutator

For any algebra, we ﬁrst observe that since Tr(T S) = Tr(ST ), we have for any V that

This explains observation (I) from earlier:

χV (ab) = χV (ba).

Question 21.2.1. Deduce that if g and h are in the same conjugacy class of a group G,
and V is a representation of C[G], then χ(g) = χ(h).

Now, given our algebra A we deﬁne the commutator [A, A] to be the (two-sided) ideal1
generated by elements of the form xy − yx. Thus [A, A] is contained in the kernel of each
χV .
Deﬁnition 21.2.2. The space A/[A, A] is called the abelianization of A; for brevity we
denote it as Aab. We think of this as “A modulo the relation ab = ba for each a, b ∈ A.”

So we can think of each character χV as an element of (Aab)∨.

Example 21.2.3 (Examples of abelianizations)
(a) If A is commutative, then [A, A] = {0} and Aab = A.
(b) If A = Matk(d), then [A, A] consists exactly of the d × d matrices of trace zero.

(Proof: harmless exercise.) Consequently, Aab is one-dimensional.

(c) Suppose A = k[G]. We claim that dim Aab is equal to the number of conjugacy
classes of A. Indeed, an element of A can be thought of as just an arbitrary
function ξ : G → k. So an element of Aab is a function ξ : G → k such that
ξ(gh) = ξ(hg) for every g, h ∈ G. This is equivalent to functions from conjugacy
classes of G to k.

Theorem 21.2.4 (Character of representations of algebras)

Let A be an algebra over an algebraically closed ﬁeld. Then

(a) Characters of pairwise non-isomorphic irreps are linearly independent as elements

of Aab.

(b) If A is ﬁnite-dimensional and semisimple, then the characters attached to irreps

form a basis of Aab.

In particular, in (b) the number of irreps of A equals dim Aab.

Proof. Part (a) is more or less obvious by the density theorem. Suppose there is a linear
dependence, so that for every a we have

c1χV1(a) + c2χV2(a) + ··· + crχVr (a) = 0

for some integer r.
1This means the ideal consists of sums elements of the form a(xy − yx)b for a, b ∈ A.

21 Characters

243

Question 21.2.5. Deduce that c1 = ··· = cr = 0 from the density theorem.

For part (b), assume there are r irreps we may assume that

A =

r(cid:77)i=1

Mat(Vi)

where V1, . . . , Vr are the irreps of A. Since we have already showed the characters are
linearly independent we need only show that dim(A/[A, A]) = r, which follows from the
observation earlier that each Mat(Vi) has a one-dimensional abelianization.

Since G has dim C[G]ab conjugacy classes, this completes the proof of (II).

§21.3 Orthogonality of characters

Now we specialize to the case of ﬁnite groups G, represented over C.

Deﬁnition 21.3.1. Let Classes(G) denote the set conjugacy classes of G.

If G has r conjugacy classes, then it has r irreps. Each (ﬁnite-dimensional) representa-

tion V , irreducible or not, gives a character χV .
Abuse of Notation 21.3.2. From now on, we will often regard χV as a function G → C
or as a function Classes(G) → C. So for example, we will write both χV (g) (for g ∈ G)
and χV (C) (for a conjugacy class C); the latter just means χV (gC) for any representative
gC ∈ C.
Deﬁnition 21.3.3. Let Funclass(G) denote the set of functions Classes(G) → C viewed
as a vector space over C. We endow it with the inner form

(cid:104)f1, f2(cid:105) =

1

|G|(cid:88)g∈G

f1(g)f2(g).

This is the same “dot product” that we mentioned at the beginning, when we looked
at the character table of S3. We now aim to prove the following orthogonality theorem,
which will imply (III) and (IV) from earlier.

Theorem 21.3.4 (Orthogonality)

For any ﬁnite-dimensional complex representations V and W of G we have

(cid:104)χV , χW(cid:105) = dim Homrep(W, V ).

In particular, if V and W are irreps then

(cid:104)χV , χW(cid:105) =(cid:40)1 V ∼= W

0

otherwise.

Corollary 21.3.5 (Irreps give an orthonormal basis)
The characters associated to irreps form an orthonormal basis of Funclass(G).

244

Napkin, by Evan Chen (v1.5.20190718)

In order to prove this theorem, we have to deﬁne the dual representation and the
tensor representation, which give a natural way to deal with the quantity χV (g)χW (g).

Deﬁnition 21.3.6. Let V = (V, ρ) be a representation of G. The dual representation
V ∨ is the representation on V ∨ with the action of G given as follows: for each ξ ∈ V ∨,
the action of g gives a g · ξ ∈ V ∨ speciﬁed by

v

g·ξ

(cid:55)−−→ ξ(cid:0)ρ(g−1)(v)(cid:1) .

Deﬁnition 21.3.7. Let V = (V, ρV ) and W = (W, ρW ) be group representations of G.
The tensor product of V and W is the group representation on V ⊗ W with the action
of G given on pure tensors by

g · (v ⊗ w) = (ρV (g)(v)) ⊗ (ρW (g)(w))

which extends linearly to deﬁne the action of G on all of V ⊗ W .

Remark 21.3.8 — Warning: the deﬁnition for tensors does not extend to algebras.
We might hope that a · (v ⊗ w) = (a · v) ⊗ (a · w) would work, but this is not even
linear in a ∈ A (what happens if we take a = 2, for example?).

Theorem 21.3.9 (Character traces)
If V and W are ﬁnite-dimensional representations of G, then for any g ∈ G.
(a) χV ⊕W (g) = χV (g) + χW (g).
(b) χV ⊗W (g) = χV (g) · χW .
(c) χV ∨(g) = χV (g).

Proof. Parts (a) and (b) follow from the identities Tr(S ⊕ T ) = Tr(S) + Tr(T ) and
Tr(S ⊗ T ) = Tr(S) Tr(T ). However, part (c) is trickier. As (ρ(g))|G| = ρ(g|G|) = ρ(1G) =
idV by Lagrange’s theorem, we can diagonalize ρ(g), say with eigenvalues λ1, . . . , λn
which are |G|th roots of unity, corresponding to eigenvectors e1, . . . , en. Then we see
that in the basis e∨1 , . . . , e∨n, the action of g on V ∨ has eigenvalues λ−1
n . So

2 , . . . , λ−1

1 , λ−1

χV (g) =

n(cid:88)i=1

λi

and χV ∨(g) =

λ−1
i =

n(cid:88)i=1

λi

n(cid:88)i=1

where the last step follows from the identity |z| = 1 ⇐⇒ z−1 = z.

Remark 21.3.10 (Warning) — The identities (b) and (c) do not extend linearly to
C[G], i.e. it is not true for example that χV (a) = χV (a) if we think of χV as a map
C[G] → C.

Proof of orthogonality relation. The key point is that we can now reduce the sums of
products to just a single character by

χV (g)χW (g) = χV ⊗W ∨(g).

21 Characters

245

So we can rewrite the sum in question as just

(cid:104)χV , χW(cid:105) =

1

|G|(cid:88)g∈G

χV ⊗W ∨(g) = χV ⊗W ∨ 1
|G|(cid:88)g∈G

g .

Let P : V ⊗ W ∨ → V ⊗ W ∨ be the action of 1

|G|(cid:80)g∈G, so we wish to ﬁnd Tr P .

Exercise 21.3.11. Show that P is idempotent. (Compute P ◦ P directly.)

Hence V ⊗ W ∨ = ker P ⊕ im P (by Problem 9G(cid:63)) and im P is subspace of elements which
are ﬁxed under G. From this we deduce that

Tr P = dim im P = dim(cid:8)x ∈ V ⊗ W ∨ | g · x = x ∀g ∈ G(cid:9) .

Now, consider the usual isomorphism V ⊗ W ∨ → Hom(W, V ).

Exercise 21.3.12. Let g ∈ G. Show that under this isomorphism, T ∈ Hom(W, V ) satisﬁes
g · T = T if and only if T (g · w) = g · T (w) for each w ∈ W . (This is just unwinding three or
four deﬁnitions.)

Consequently, χV ⊗W ∨(P ) = Tr P = dim Homrep(W, V ) as desired.

The orthogonality relation gives us a fast and mechanical way to check whether a
ﬁnite-dimensional representation V is irreducible. Namely, compute the traces χV (g) for
each g ∈ G, and then check whether (cid:104)χV , χV (cid:105) = 1. So, for example, we could have seen
the three representations of S3 that we found were irreps directly from the character
table. Thus, we can now eﬃciently verify any time we have a complete set of irreps.

§21.4 Examples of character tables

246

Napkin, by Evan Chen (v1.5.20190718)

Example 21.4.1 (Dihedral group on 10 elements)

Let D10 =(cid:10)r, s | r5 = s2 = 1, rs = sr−1(cid:11). Let ω = exp( 2πi

tations of D10:

5 ). We write four represen-

 Ctriv, all elements of D10 act as the identity.

 Csign, r acts as the identity while s acts by negation.

 V1, which is two-dimensional and given by r (cid:55)→(cid:20)ω
 V2, which is two-dimensional and given by r (cid:55)→(cid:20)ω2

0 ω4(cid:21) and s (cid:55)→(cid:20)0 1
1 0(cid:21).
0 ω3(cid:21) and s (cid:55)→(cid:20)0 1
1 0(cid:21).

0

0

We claim that these four representations are irreducible and pairwise non-isomorphic.
We do so by writing the character table:

1
D10
Ctriv
1
Csign 1
V1
V2

r, r4

r2, r3

1
1

1
1

2 ω + ω4 ω2 + ω3
2 ω2 + ω3 ω + ω4

srk
1
−1
0
0

Then a direct computation shows the orthogonality relations, hence we indeed have
an orthonormal basis. For example, (cid:104)Ctriv, Csign(cid:105) = 1 + 2 · 1 + 2 · 1 + 5 · (−1) = 0.

21 Characters

247

Example 21.4.2 (Character table of S4)
We now have enough machinery to to compute the character table of S4, which has
ﬁve conjugacy classes (corresponding to cycle types id, 2, 3, 4 and 2 + 2). First of all,
we note that it has two one-dimensional representations, Ctriv and Csign, and these
are the only ones (because there are only two homomorphisms S4 → C×). So thus
far we have the table

1 (• •)
S4
Ctriv
1
1
Csign 1
−1

...

(• • •)
1
1

(• • • •)
1
−1

...

(• •)(• •)
1
1

Note the columns represent 1 + 6 + 8 + 6 + 3 = 24 elements.

Now, the remaining three representations have dimensions d1, d2, d3 with

d2
1 + d2

2 + d2

3 = 4! − 2 = 22

which has only (d1, d2, d3) = (2, 3, 3) and permutations. Now, we can take the reﬂ0
representation

{(w, x, y, z) | w + x + y + z = 0}

with basis (1, 0, 0,−1), (0, 1, 0,−1) and (0, 0, 1,−1). This can be geometrically
checked to be irreducible, but we can also do this numerically by computing the
character directly (this is tedious): it comes out to have 3, −1, 0, 1, −1 which indeed
gives norm

(cid:104)χreﬂ0, χreﬂ0(cid:105) =

1

4! 32
(cid:124)(cid:123)(cid:122)(cid:125)id

+ 6 · (−1)2
(cid:124)
(cid:123)(cid:122)
(cid:125)

(• •)

+ 8 · (0)2
(cid:124) (cid:123)(cid:122) (cid:125)
(• • •)

+ 6 · (1)2
(cid:124) (cid:123)(cid:122) (cid:125)
(• • • •)

 = 1.
+ 3 · (−1)2
(cid:125)
(cid:124)
(cid:123)(cid:122)
(• •)(• •)

Note that we can also tensor this with the sign representation, to get another
irreducible representation (since Csign has all traces ±1, the norm doesn’t change).
Finally, we recover the ﬁnal row using orthogonality (which we name C2, for lack of
a better name); hence the completed table is as follows.

S4
Ctriv
Csign
C2
reﬂ0

1 (• •)
1
1
−1
1
0
2
−1
3
reﬂ0 ⊗Csign 3
1

(• • •)
1
1
−1
0
0

(• • • •)
1
−1
0
1
−1

(• •)(• •)
1
1
2
−1
−1

§21.5 A few harder problems to think about

Problem 21A† (Reading decompositions from characters). Let W be a complex rep-
resentation of a ﬁnite group G. Let V1, . . . , Vr be the complex irreps of G and set
ni = (cid:104)χW , χVi(cid:105). Prove that each ni is a positive integer and

W =

V ⊕ni
i

.

r(cid:77)i=1

248

Napkin, by Evan Chen (v1.5.20190718)

Problem 21B. Consider complex representations of G = S4. The representation
reﬂ0 ⊗ reﬂ0 is 9-dimensional, so it is clearly reducible. Compute its decomposition in
terms of the ﬁve irreducible representations.

Problem 21C (Tensoring by one-dimensional irreps). Let V and W be irreps of G, with
dim W = 1. Show that V ⊗ W is irreducible.
Problem 21D (Quaternions). Compute the character table of the quaternion group Q8.

Problem 21E(cid:63) (Second orthogonality formula). Let g and h be elements of a ﬁnite
group G, and let V1, . . . , Vr be the irreps of G. Prove that

χVi(g)χVi(h) =(cid:40)|Z(g)|

0

r(cid:88)i=1

if g and h are conjugates
otherwise.

Here, Z(g) = {x ∈ G : xg = gx} is the center of G.

22 Some applications

With all this setup, we now take the time to develop some nice results which are of

independent interest.

§22.1 Frobenius divisibility

Theorem 22.1.1 (Frobenius divisibility)
Let V be a complex irrep of a ﬁnite group G. Then dim V divides |G|.

The proof of this will require algebraic integers (developed in the algebraic number theory
chapter). Recall that an algebraic integer is a complex number which is the root of a
polynomial with integer coeﬃcients, and that these algebraic integers form a ring Z under
addition and multiplication, and that Z ∩ Q = Z.

First, we prove:

Lemma 22.1.2 (Elements of Z[G] are integral)
Let α ∈ Z[G]. Then there exists a monic polynomial P with integer coeﬃcients such
that P (α) = 0.

Proof. Let Ak be the Z-span of 1, α1, . . . , αk. Since Z[G] is Noetherian, the inclusions
A0 ⊆ A1 ⊆ A2 ⊆ . . . cannot all be strict, hence Ak = Ak+1 for some k, which means
αk+1 can be expressed in terms of lower powers of α.

Proof of Frobenius divisibility. Let C1, . . . , Cm denote the conjugacy classes of G. Then
consider the rational number

|G|
dim V

;

we will show it is an algebraic integer, which will prove the theorem. Observe that we
can rewrite it as

|G|
dim V

= |G|(cid:104)χV , χV (cid:105)

dim V

We split the sum over conjugacy classes, so

=(cid:88)g∈G

χV (g)χV (g)

dim V

.

|G|
dim V

=

m(cid:88)i=1

χV (Ci) · |Ci|χV (Ci)

dim V

.

We claim that for every i,

is an algebraic integer, where

|Ci|χV (Ci)

dim V

=

1

dim V

Tr Ti

Ti := ρ(cid:88)h∈Ci

h .

249

250

Napkin, by Evan Chen (v1.5.20190718)

To see this, note that Ti commutes with elements of G, and hence is an intertwining
operator Ti : V → V . Thus by Schur’s lemma, Ti = λi · idV and Tr T = λi dim V . By
Lemma 22.1.2, λi ∈ Z, as desired.
Now we are done, since χV (Ci) ∈ Z too (it is the sum of conjugates of roots of unity),
so |G|
dim V is the sum of products of algebraic integers, hence itself an algebraic integer.

§22.2 Burnside’s theorem

We now prove a group-theoretic result. This is the famous poster child for representation
theory (in the same way that RSA is the poster child of number theory) because the
result is purely group theoretic.

Recall that a group is simple if it has no normal subgroups.

In fact, we will

prove:

Theorem 22.2.1 (Burnside)
Let G be a nonabelian group of order paqb (where a, b ≥ 0). Then G is not simple.

In what follows p and q will always denote prime numbers.

Lemma 22.2.2 (On gcd(|C|, dim V ) = 1)
Let V = (V, ρ) be an complex irrep of G. Assume C is a conjugacy class of G with
gcd(|C|, dim V ) = 1. Then for any g ∈ C, either

 ρ(g) is multiplication by a scalar, or

 χV (g) = Tr ρ(g) = 0.

Proof. If εi are the n eigenvalues of ρ(g) (which are roots of unity), then from the proof
of Frobenius divisibility we know |C|n χV (g) ∈ Z, thus from gcd(|C|, n) = 1 we get

1
n

χV (g) =

1
n

(ε1 + ··· + εn) ∈ Z.

So this follows readily from a fact from algebraic number theory, namely Problem 46C(cid:63):
either ε1 = ··· = εn (ﬁrst case) or ε1 + ··· + εn = 0 (second case).

Lemma 22.2.3 (Simple groups don’t have prime power conjugacy classes)
Let G be a ﬁnite simple group. Then G cannot have a conjugacy class of order pk
(where k > 0).

Proof. By contradiction. Assume C is such a conjugacy class, and ﬁx any g ∈ C. By the
second orthogonality formula (Problem 21E(cid:63)) applied g and 1G (which are not conjugate
since g (cid:54)= 1G) we have

where Vi are as usual all irreps of G.

dim ViχVi(g) = 0

r(cid:88)i=1

22 Some applications

251

Exercise 22.2.4. Show that there exists a nontrivial irrep V such that p (cid:45) dim V and
χV (g) (cid:54)= 0. (Proceed by contradiction to show that − 1

p ∈ Z if not.)

Let V = (V, ρ) be the irrep mentioned. By the previous lemma, we now know that ρ(g)
acts as a scalar in V .

Now consider the subgroup

H =(cid:10)ab−1 | a, b ∈ C(cid:11) ⊆ G.

We claim this is a nontrivial normal subgroup of G. It is easy to check H is normal, and
since |C| > 1 we have that H is nontrivial. Each element of H acts trivially in G, so
since V is nontrivial and irreducible H (cid:54)= G. This contradicts the assumption that G
was simple.

With this lemma, Burnside’s theorem follows by partitioning the |G| elements of our
group into conjugacy classes. Assume for contradiction G is simple. Each conjugacy
class must have order either 1 (of which there are |Z(G)|) or divisible by pq, but on the
other hand the sum equals |G| = paqb. Consequently, we must have |Z(G)| > 1. But G
is not abelian, hence Z(G) (cid:54)= G, thus the center Z(G) is a nontrivial normal subgroup,
contradicting the assumption that G was simple.

§22.3 Frobenius determinant

We ﬁnish with the following result, the problem that started the branch of representation
theory. Given a ﬁnite group G, we create n variables {xg}g∈G, and an n × n matrix MG
whose (g, h)th entry is xgh.

Example 22.3.1 (Frobenius determinants)

(a) If G = Z/2Z =(cid:10)T | T 2 = 1(cid:11) then the matrix would be

MG =(cid:20)xid xT
xT xid(cid:21) .

Then det MG = (xid − xT )(xid + xT ).

(b) If G = S3, a long computation gives the irreducible factorization of det MG is

(cid:88)σ∈S3

xσ(cid:88)σ∈S3

sign(σ)xσ(cid:16)F(cid:0)xid, x(123), x(321)(cid:1) − F(cid:0)x(12), x(23), x(31)(cid:1)(cid:17)2

where F (a, b, c) = a2 + b2 + c2 − ab − bc − ca; the latter factor is irreducible.

Theorem 22.3.2 (Frobenius determinant)
The polynomial det MG (in |G| variables) factors into a product of irreducible
polynomials such that

(i) The number of polynomials equals the number of conjugacy classes of G, and

(ii) The multiplicity of each polynomial equals its degree.

252

Napkin, by Evan Chen (v1.5.20190718)

You may already be able to guess how the “sum of squares” result is related! (Indeed,
look at deg det MG.)

Legend has it that Dedekind observed this behavior ﬁrst in 1896. He didn’t know how
to prove it in general, so he sent it in a letter to Frobenius, who created representation
theory to solve the problem.

With all the tools we’ve built, it is now fairly straightforward to prove the result.

Proof. Let V = (V, ρ) = Reg(C[G]) and let V1, . . . , Vr be the irreps of G. Let’s consider
the map T : C[G] → C[G] which has matrix MG in the usual basis of C[G], namely

T = T ({xg}) =(cid:88)g∈G

xgρ(g) ∈ Mat(V ).

as before, and so breaking down T over its

Thus we want to examine det T .

But we know that V = (cid:76)r

subspaces we know

i=1 V ⊕ dim Vi

i

det T =

(det(T(cid:22)Vi))dim Vi .

r(cid:89)i=1

So we only have to show two things: the polynomials det TVi are irreducible, and they
are pairwise diﬀerent for diﬀerent i.

Let Vi = (Vi, ρ), and pick k = dim Vi.

 Irreducible: By the density theorem, for any M ∈ Mat(Vi) there exists a particular

choice of complex numbers xg ∈ G such that

M =(cid:88)g∈G

xg · ρi(g) = (T(cid:22)Vi)({xg}).

View ρi(g) as a k × k matrix with complex coeﬃcients. Thus the “generic”
(T(cid:22)Vi)({xg}), viewed as a matrix with polynomial entries, must have linearly
independent entries (or there would be some matrix in Mat(Vi) that we can’t
achieve).

Then, the assertion follows (by a linear variable change) from the simple fact that
the polynomial det(yij)1≤i,j≤m in m2 variables is always irreducible.

 Pairwise distinct: We show that from det T|Vi({xg}) we can read oﬀ the character

χVi, which proves the claim. In fact

Exercise 22.3.3. Pick any basis for Vi. If dim Vi = k, and 1G (cid:54)= g ∈ G, then

χVi (g) is the coeﬃcient of xk−1

g x1G .

Thus, we are done.

VII

Quantum Algorithms

Part VII: Contents

23 Quantum states and measurements

255
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255
23.1 Bra-ket notation
23.2 The state space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256
23.3 Observations
23.4 Entanglement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259
. . . . . . . . . . . . . . . . . . . . . . . . 262
23.5 A few harder problems to think about

24 Quantum circuits

263
24.1 Classical logic gates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
24.2 Reversible classical logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264
24.3 Quantum logic gates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266
24.4 Deutsch-Jozsa algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268
. . . . . . . . . . . . . . . . . . . . . . . . 269
24.5 A few harder problems to think about

25 Shor’s algorithm

271
25.1 The classical (inverse) Fourier transform . . . . . . . . . . . . . . . . . . . . . . . 271
25.2 The quantum Fourier transform . . . . . . . . . . . . . . . . . . . . . . . . . . . 272
25.3 Shor’s algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274

23 Quantum states and measurements

In this chapter we’ll explain how to set up quantum states using linear algebra. This
will allow me to talk about quantum circuits in the next chapter, which will set the stage
for Shor’s algorithm.

I won’t do very much physics (read: none at all). That is, I’ll only state what the
physical reality is in terms of linear algebras, and defer the philosophy of why this is
true to your neighborhood “Philosophy of Quantum Mechanics” class (which is a “social
science” class at MIT!).

§23.1 Bra-ket notation

Physicists have their own notation for vectors: whereas I previously used something like
v, e1, and so on, in this chapter you’ll see the infamous bra-ket notation: a vector will
be denoted by |•(cid:105), where • is some variable name: unlike in math or Python, this can
include numbers, symbols, Unicode characters, whatever you like. This is called a “ket”.
To pay a homage to physicists everywhere, we’ll use this notation for this chapter too.

Abuse of Notation 23.1.1 (For this part, dim H < ∞). In this part on quantum
computation, we’ll use the word “Hilbert space” as deﬁned earlier, but in fact all our
Hilbert spaces will be ﬁnite-dimensional.

If dim H = n, then its orthonormal basis elements are often denoted

|0(cid:105) ,|1(cid:105) , . . . ,|n − 1(cid:105)
(instead of ei) and a generic element of H denoted by

and various other Greek letters.

|ψ(cid:105) ,|φ(cid:105) , . . .

Now for any |ψ(cid:105) ∈ H, we can consider the canonical dual element in H∨ (since H has
an inner form), which we denote by (cid:104)ψ| (a “bra”). For example, if dim H = 2 then we
can write

in an orthonormal basis, in which case

β(cid:21)
|ψ(cid:105) =(cid:20)α
(cid:104)ψ| =(cid:2)α β(cid:3) .

We even can write dot products succinctly in this notation: if |φ(cid:105) =(cid:20)γ

δ(cid:21), then the dot

product of |φ(cid:105) and |ψ(cid:105) is given by

(cid:104)ψ|φ(cid:105) =(cid:2)α β(cid:3)(cid:20)γ

δ(cid:21) = αγ + βδ.

So we will use the notation (cid:104)ψ|φ(cid:105) instead of the more mathematical (cid:104)|ψ(cid:105) ,|φ(cid:105)(cid:105).
In
particular, the squared norm of |ψ(cid:105) is just (cid:104)ψ|ψ(cid:105). Concretely, for dim H = 2 we have
(cid:104)ψ|ψ(cid:105) = |α|2 + |β|2.

255

256

Napkin, by Evan Chen (v1.5.20190718)

§23.2 The state space

If you think that’s weird, well, it gets worse.

In classical computation, a bit is either 0 or 1. More generally, we can think of a
classical space of n possible states 0, . . . , n − 1. Thus in the classical situation, the space
of possible states is just a discrete set with n elements.
In quantum computation, a qubit is instead any complex linear combination of 0 and

1. To be precise, consider the normed complex vector space

H = C⊕2

and denote the orthonormal basis elements by |0(cid:105) and |1(cid:105). Then a qubit is a nonzero
element |ψ(cid:105) ∈ H, so that it can be written in the form
|ψ(cid:105) = α|0(cid:105) + β |1(cid:105)

where α and β are not both zero. Typically, we normalize so that |ψ(cid:105) has norm 1:

(cid:104)ψ|ψ(cid:105) = 1 ⇐⇒ |α|2 + |β|2 = 1.

In particular, we can recover the “classical” situation with |0(cid:105) ∈ H and |1(cid:105) ∈ H, but now
we have some “intermediate” states, such as

1
√2

(|0(cid:105) + |1(cid:105)) .

Philosophically, what has happened is that:

Instead of allowing just the states |0(cid:105) and |1(cid:105), we allow any complex linear
combination of them.

More generally, if dim H = n, then the possible states are nonzero elements

c0 |0(cid:105) + c1 |1(cid:105) + ··· + cn−1 |n − 1(cid:105)

which we usually normalize so that |c0|2 + |c1|2 + ··· + |cn−1|2 = 1.

§23.3 Observations

Prototypical example for this section: id corresponds to not making a measurement since
all its eigenvalues are equal, but any operator with distinct eigenvalues will cause collapse.

If you think that’s weird, well, it gets worse. First, some linear algebra:

Deﬁnition 23.3.1. Let V be a ﬁnite-dimensional inner product space. For a map
T : V → V , the following conditions are equivalent:

 (cid:104)T x, y(cid:105) = (cid:104)x, T y(cid:105) for any x, y ∈ V .
 T = T †.

A map T satisfying these conditions is called Hermitian.

23 Quantum states and measurements

257

Question 23.3.2. Show that T is normal.

Thus, we know that T is diagonalizable with respect to the inner form, so for a suitable
basis we can write it in an orthonormal basis as

T =

λ0
0
...
0

0
λ1
...
0

. . .
0
. . .
0
...
. . .
. . . λn−1

.



As we’ve said, this is fantastic: not only do we have a basis of eigenvectors, but the
eignvectors are pairwise orthogonal, and so they form an orthonormal basis of V .

Question 23.3.3. Show that all eigenvalues of T are real. (T = T †.)

Back to quantum computation. Suppose we have a state |ψ(cid:105) ∈ H, where dim H = 2;
we haven’t distinguished a particular basis yet, so we just have a nonzero vector. Then
the way observations work (and this is physics, so you’ll have to take my word for it) is
as follows:

Pick a Hermitian operator T : H → H; then observations of T return
eigenvalues of T .

To be precise:

 Pick a Hermitian operator T : H → H, which is called the observable.
 Consider its eigenvalues λ0, . . . , λn−1 and corresponding eigenvectors |0(cid:105)T , . . . ,
|n − 1(cid:105)T . Tacitly we may assume that |0(cid:105)T , . . . , |n − 1(cid:105)T form an orthonormal
basis of H. (The subscript T is here to distinguish the eigenvectors of T from the
basis elements of H.)

 Write |ψ(cid:105) in the orthonormal basis as

c0 |0(cid:105)T + c1 |1(cid:105)T + ··· + cn−1 |n − 1(cid:105)T .

 Then the probability of observing λi is

|ci|2

|c0|2 + ··· + |cn−1|2 .

This is called making an observation along T .

Note that in particular, for any nonzero constant c, |ψ(cid:105) and c|ψ(cid:105) are indistinguishable,
which is why we like to normalize |ψ(cid:105). But the queerest thing of all is what happens to
|ψ(cid:105): by measuring it, we actually destroy information. This behavior is called quantum
collapse.

 Suppose for simplicity that we observe |ψ(cid:105) with T and obtain an eigenvalue λ, and
that |i(cid:105)T is the only eigenvector with this eigenvalue. Then, the state |ψ(cid:105) collapses
to just the state ci |i(cid:105)T : all the other information is destroyed. (In fact, we may as
well say it collapses to |i(cid:105)T , since again constant factors are not relevant.)

258

Napkin, by Evan Chen (v1.5.20190718)

 More generally, if we observe λ, consider the generalized eigenspace Hλ (i.e. the
span of eigenvectors with the same eigenvalue). Then the physical state |ψ(cid:105) has
been changed as well: it has now been projected onto the eigenspace Hλ. In still
other words, after observation, the state collapses to

(cid:88)0≤i≤n

λi=λ

ci |i(cid:105)T .

In other words,

When we make a measurement, the coeﬃcients from diﬀerent eigenspaces
are destroyed.

Why does this happen? Beats me. . . physics (and hence real life) is weird. But anyways,
an example.

Example 23.3.4 (Quantum measurement of a state |ψ(cid:105))
Let H = C⊕2 with orthonormal basis |0(cid:105) and |1(cid:105) and consider the state

|ψ(cid:105) =

(a) Let

i
√5 |0(cid:105) +

2/√5(cid:21) ∈ H.

2

√5 |1(cid:105) =(cid:20)i/√5
0 −1(cid:21) .
T =(cid:20)1

0

This has eigenvectors |0(cid:105) = |0(cid:105)T and |1(cid:105) = |1(cid:105)T , with eigenvalues +1 and −1. So
if we measure |ψ(cid:105) to T , we get +1 with probability 1/5 and −1 with probability
4/5. After this measurement, the original state collapses to |0(cid:105) if we measured 0,
and |1(cid:105) if we measured 1. So we never learn the original probabilities.

(b) Now consider T = id, and arbitrarily pick two orthonormal eigenvectors |0(cid:105)T , |1(cid:105)T ;
thus ψ = c0 |0(cid:105)T + c1 |1(cid:105)T . Since all eigenvalues of T are +1, our measurement
will always be +1 no matter what we do. But there is also no collapsing, because
none of the coeﬃcients get destroyed.

(c) Now consider

The two normalized eigenvectors are

7 0(cid:21) .
T =(cid:20)0 7
√2(cid:20)1
1(cid:21)

1

|1(cid:105)T =

|0(cid:105)T =

1

√2(cid:20) 1
−1(cid:21)

with eigenvalues +7 and −7 respectively. In this basis, we have

|ψ(cid:105) =

2 + i

√10 |1(cid:105)T .

√10 |0(cid:105)T + −2 + i
2 and −7 with probability 1

So we get +7 with probability 1
measurement, |ψ(cid:105) collapses to one of |0(cid:105)T and |1(cid:105)T .

2 , and after the

23 Quantum states and measurements

259

Question 23.3.5. Suppose we measure |ψ(cid:105) with T and get λ. What happens if we measure
with T again?

For H = C⊕2 we can come up with more classes of examples using the so-called Pauli

matrices. These are the three Hermitian matrices

σz =(cid:20)1

0 −1(cid:21)

0

σx =(cid:20)0 1
1 0(cid:21)

σy =(cid:20)0 −i
0(cid:21) .

i

These matrices are important because:

Question 23.3.6. Show that these three matrices, plus the identity matrix, form a basis
for the set of Hermitian 2 × 2 matrices.

So the Pauli matrices are a natural choice of basis.

Their normalized eigenvectors are

|↑(cid:105) = |0(cid:105) =(cid:20)1
0(cid:21)
√2(cid:20)1
1(cid:21)
i(cid:21)
√2(cid:20)1

|⊗(cid:105) =

|→(cid:105) =

1

1

|↓(cid:105) = |1(cid:105) =(cid:20)0
1(cid:21)
√2(cid:20) 1
−1(cid:21)
−i(cid:21)
√2(cid:20) 1

|(cid:12)(cid:105) =

|←(cid:105) =

1

1

which we call “z-up”, “z-down”, “x-up”, “x-down”, “y-up”, “y-down”. (The eigenvalues
are +1 for “up” and −1 for “down”.) So, given a state |ψ(cid:105) ∈ C⊕2 we can make a
measurement with respect to any of these three bases by using the corresponding Pauli
matrix.

In light of this, the previous examples were (a) measuring along σz, (b) measuring

along id, and (c) measuring along 7σx.

Notice that if we are given a state |ψ(cid:105), and are told in advance that it is either |→(cid:105)
or |←(cid:105) (or any other orthogonal states) then we are in what is more or less a classical
situation. Speciﬁcally, if we make a measurement along σx, then we ﬁnd out which state
that |ψ(cid:105) was in (with 100% certainty), and the state does not undergo any collapse. Thus,
orthogonal states are reliably distinguishable.

§23.4 Entanglement

Prototypical example for this section: Singlet state: spooky action at a distance.

If you think that’s weird, well, it gets worse.
Qubits don’t just act independently: they can talk to each other by means of a tensor

product. Explicitly, consider

H = C⊕2 ⊗ C⊕2

endowed with the norm described in Problem 13D(cid:63). One should think of this as a qubit
A in a space HA along with a second qubit B in a diﬀerent space HB, which have been
allowed to interact in some way, and H = HA ⊗ HB is the set of possible states of both
qubits. Thus

|0(cid:105)A ⊗ |0(cid:105)B ,

|0(cid:105)A ⊗ |1(cid:105)B ,

|1(cid:105)A ⊗ |0(cid:105)B ,

|1(cid:105)A ⊗ |1(cid:105)B

260

Napkin, by Evan Chen (v1.5.20190718)

is an orthonormal basis of H; here |i(cid:105)A is the basis of the ﬁrst C⊕2 while |i(cid:105)B is the basis
of the second C⊕2, so these vectors should be thought of as “unrelated” just as with any
tensor product. The pure tensors mean exactly what you want: for example |0(cid:105)A ⊗ |1(cid:105)B
means “0 for qubit A and 1 for qubit B”.
As before, a measurement of a state in H requires a Hermitian map H → H. In
particular, if we only want to measure the qubit B along MB, we can use the operator

idA ⊗ MB.

The eigenvalues of this operator coincide with the ones for MB, and the eigenspace for λ
will be the HA ⊗ (HB)λ, so when we take the projection the A qubit will be unaﬀected.

This does what you would hope for pure tensors in H:

Example 23.4.1 (Two non-entangled qubits)
Suppose we have qubit A in the state i√5 |0(cid:105)A + 2√5 |1(cid:105)A and qubit B in the state
1√2 |0(cid:105)B + 1√2 |1(cid:105)B. So, the two qubits in tandem are represented by the pure tensor

|ψ(cid:105) =(cid:18) i

√5 |0(cid:105)A +

2

√5 |1(cid:105)A(cid:19) ⊗(cid:18) 1

√2 |0(cid:105)B +

1

√2 |1(cid:105)B(cid:19) .

Suppose we measure |ψ(cid:105) along

The eigenspace decomposition is

M = idA ⊗ σB
z .

 +1 for the span of |0(cid:105)A ⊗ |0(cid:105)B and |1(cid:105)A ⊗ |0(cid:105)B, and
 −1 for the span of |0(cid:105)A ⊗ |1(cid:105)B and |1(cid:105)A ⊗ |1(cid:105)B.

(We could have used other bases, like |→(cid:105)A ⊗ |0(cid:105)B and |←(cid:105)A ⊗ |0(cid:105)B for the ﬁrst
eigenpsace, but it doesn’t matter.) Expanding |ψ(cid:105) in the four-element basis, we ﬁnd
that we’ll get the ﬁrst eigenspace with probability

2

=

1
2

.

+(cid:12)(cid:12)(cid:12)(cid:12)

2

√10(cid:12)(cid:12)(cid:12)(cid:12)

2

i

(cid:12)(cid:12)(cid:12)(cid:12)
√10(cid:12)(cid:12)(cid:12)(cid:12)
√5 |1(cid:105)A(cid:19) ⊗ |0(cid:105)B

2

and the second eigenspace with probability 1
2 as well. (Note how the coeﬃcients
for A don’t do anything!) After the measurement, we destroy the coeﬃcients of the
other eigenspace; thus (after re-normalization) we obtain the collapsed state

(cid:18) i

√5 |0(cid:105)A +

or

(cid:18) i

√5 |0(cid:105)A +

2

√5 |1(cid:105)A(cid:19) ⊗ |1(cid:105)B

again with 50% probability each.

So this model lets us more or less work with the two qubits independently: when we make
the measurement, we just make sure to not touch the other qubit (which corresponds to
the identity operator).

Exercise 23.4.2. Show that if idA ⊗ σB
collapse at all. What’s the result of this measurement?

x is applied to the |ψ(cid:105) in this example, there is no

23 Quantum states and measurements

261

Since the ⊗ is getting cumbersome to write, we say:

Abuse of Notation 23.4.3. From now on |0(cid:105)A ⊗ |0(cid:105)B will be abbreviated to just |00(cid:105),
and similarly for |01(cid:105), |10(cid:105), |11(cid:105).

Example 23.4.4 (Simultaneously measuring a general 2-Qubit state)
Consider a normalized state |ψ(cid:105) in H = C⊕2 ⊗ C⊕2, say

|ψ(cid:105) = α|00(cid:105) + β |01(cid:105) + γ |10(cid:105) + δ |11(cid:105) .

We can make a measurement along the diagonal matrix T : H → H with

T (|00(cid:105)) = 0|00(cid:105) , T (|01(cid:105)) = 1|01(cid:105) , T (|10(cid:105)) = 2|10(cid:105) , T (|11(cid:105)) = 3|11(cid:105) .

Thus we get each of the eigenvalues 0, 1, 2, 3 with probability |α|2, |β|2, |γ|2, |δ|2.
So if we like we can make “simultaneous” measurements on two qubits in the same
way that we make measurements on one qubit.

However, some states behave very weirdly.

Example 23.4.5 (The singlet state)
Consider the state

|Ψ−(cid:105) =

1
√2 |01(cid:105) −

1
√2 |10(cid:105)

which is called the singlet state. One can see that |Ψ−(cid:105) is not a simple tensor,
which means that it doesn’t just consist of two qubits side by side: the qubits in HA
and HB have become entangled.

Now, what happens if we measure just the qubit A? This corresponds to making

the measurement

T = σA

z ⊗ idB.

The eigenspace decomposition of T can be described as:

 The span of |00(cid:105) and |01(cid:105), with eigenvalue +1.
 The span of |10(cid:105) and |11(cid:105), with eigenvalue −1.

So one of two things will happen:

 With probability 1

 With probability 1

2 , we measure +1 and the collapsed state is |01(cid:105).
2 , we measure −1 and the collapsed state is |10(cid:105).

But now we see that measurement along A has told us what the state of the bit B is
completely!

By solely looking at measurements on A, we learn B; this paradox is called spooky action
at a distance, or in Einstein’s tongue, spukhafte Fernwirkung. Thus,

In tensor products of Hilbert spaces, states which are not pure tensors
correspond to “entangled” states.

262

Napkin, by Evan Chen (v1.5.20190718)

What this really means is that the qubits cannot be described independently; the state
of the system must be given as a whole. That’s what entangled states mean: the qubits
somehow depend on each other.

§23.5 A few harder problems to think about

Problem 23A. We measure |Ψ−(cid:105) by σA
Determine the state of qubit B from this measurement.

x ⊗ idB, and hence obtain either +1 or −1.

Problem 23B (Greenberger-Horne-Zeilinger paradox). Consider the state in (C⊕2)⊗3

|Ψ(cid:105)GHZ =

1
√2

(|0(cid:105)A |0(cid:105)B |0(cid:105)C − |1(cid:105)A |1(cid:105)B |1(cid:105)C) .

Find the value of the measurements along each of

σA
y ⊗ σB

y ⊗ σC
x ,

σA
y ⊗ σB

x ⊗ σC
y ,

σA
x ⊗ σB

y ⊗ σC
y ,

σA
x ⊗ σB

x ⊗ σC
x .

As for the paradox: what happens if you multiply all these measurement together?

24 Quantum circuits

Now that we’ve discussed qubits, we can talk about how to use them in circuits. The
key change — and the reason that quantum circuits can do things that classical circuits
cannot — is the fact that we are allowing linear combinations of 0 and 1.

§24.1 Classical logic gates

In classical logic, we build circuits which take in some bits for input, and output some
more bits for input. These circuits are built out of individual logic gates. For example,
the AND gate can be pictured as follows.

0

0

and

0

0

1

and

0

1

0

and

0

1

1

and

1

One can also represent the AND gate using the “truth table”:

A B A and B
0
0
1
1

0
1
0
1

0
0
0
1

Similarly, we have the OR gate and the NOT gate:

A B A or B
0
0
1
1

0
1
0
1

0
1
1
1

A not A
0
1

1
0

We also have a so-called COPY gate, which duplicates a bit.

1
1
Of course, the ﬁrst theorem you learn about these gates is that:

copy

copy

0
0

0

1

Theorem 24.1.1 (AND, OR, NOT, COPY are universal)

The set of four gates AND, OR, NOT, COPY is universal in the sense that any
boolean function f : {0, 1}n → {0, 1} can be implemented as a circuit using only
these gates.

Proof. Somewhat silly: we essentially write down a circuit that OR’s across all input
strings in f pre(1). For example, suppose we have n = 3 and want to simulate the function
f (abc) with f (011) = f (110) = 1 and 0 otherwise. Then the corresponding Boolean
expression for f is simply

f (abc) = [(not a) and b and c] or [a and b and (not c)] .

Clearly, one can do the same for any other f , and implement this logic into a circuit.

263

264

Napkin, by Evan Chen (v1.5.20190718)

Remark 24.1.2 — Since x and y = not ((not x) or (not y)), it follows that in fact,
we can dispense with the AND gate.

§24.2 Reversible classical logic

Prototypical example for this section: CNOT gate, Toﬀoli gate.

For the purposes of quantum mechanics, this is not enough. To carry through the
analogy we in fact need gates that are reversible, meaning the gates are bijections from
the input space to the output space. In particular, such gates must take the same number
of input and output gates.

Example 24.2.1 (Reversible gates)

(a) None of the gates AND, OR, COPY are reversible for dimension reasons.

(b) The NOT gate, however, is reversible: it is a bijection {0, 1} → {0, 1}.

Example 24.2.2 (The CNOT gate)
The controlled-NOT gate, or the CNOT gate, is a reversible 2-bit gate with the
following truth table.

In Out
0 0 0 0
1 0 1 1
0 1 0 1
1 1 1 0

In other words, this gate XOR’s the ﬁrst bit to the second bit, while leaving the ﬁrst
bit unchanged. It is depicted as follows.

x
y

•

x
x + y mod 2

The ﬁrst dot is called the “control”, while the ⊕ is the “negation” operation: the ﬁrst
bit controls whether the second bit gets ﬂipped or not. Thus, a typical application
might be as follows.

1
0

•

1
1

So, NOT and CNOT are the only nontrivial reversible gates on two bits.
We now need a diﬀerent deﬁnition of universal for our reversible gates.

Deﬁnition 24.2.3. A set of reversible gates can simulate a Boolean function f (x1 . . . xn),
if one can implement a circuit which takes

 As input, x1 . . . xn plus some ﬁxed bits set to 0 or 1, called ancilla bits1.

 As output, the input bits x1, . . . , xn, the output bit f (x1, . . . , xn), and possibly

some extra bits (called garbage bits).

1The English word “ancilla” means “maid”.

24 Quantum circuits

265

The gate(s) are universal if they can simulate any Boolean function.

For example, the CNOT gate can simulate the NOT gate, using a single ancilla bit 1

but with no garbage, according to the following circuit.

Unfortunately, it is not universal.

x
1

•

x
not x

Proposition 24.2.4 (CNOT (cid:54)⇒ AND)
The CNOT gate cannot simulate the boolean function “x and y”.

Sketch of Proof. One can see that any function simulated using only CNOT gates must
be of the form

a1x1 + a2x2 + ··· + anxn

(mod 2)

because CNOT is the map (x, y) (cid:55)→ (x, x + y). Thus, even with ancilla bits, we can only
create functions of the form ax + by + c (mod 2) for ﬁxed a, b, c. The AND gate is not
of this form.

So, we need at least a three-qubit gate. The most commonly used one is:

Deﬁnition 24.2.5. The three-bit Toﬀoli gate, also called the CCNOT gate, is given
by

x
y
z

•
•

x
y
z + xy

(mod 2)

So the Toﬀoli has two controls, and toggles the last bit if and only if both of the control
bits are 1.

This replacement is suﬃcient.

Theorem 24.2.6 (Toﬀoli gate is universal)

The Toﬀoli gate is universal.

Proof. We will show it can reversibly simulate AND, NOT, hence OR, which we know is
enough to show universality. (We don’t need COPY because of reversibility.)

For the AND gate, we draw the circuit

x
y
0

•
•

x
y
x and y

with one ancilla bit, and no garbage bits.

For the NOT gate, we use two ancilla 1 bits and one garbage bit:

1
z
1

•
•

1
z
not z

This completes the proof.

Hence, in theory we can create any classical circuit we desire using the Toﬀoli gate
alone. Of course, this could require exponentially many gates for even the simplest of
functions. Fortunately, this is NO BIG DEAL because I’m a math major, and having 2n
gates is a problem best left for the CS majors.

266

Napkin, by Evan Chen (v1.5.20190718)

§24.3 Quantum logic gates

In quantum mechanics, since we can have linear combinations of basis elements, our
logic gates will instead consist of linear maps. Moreover, in quantum computation, gates
are always reversible, which was why we took the time in the previous section to show
that we can still simulate any function when restricted to reversible gates (e.g. using the
Toﬀoli gate).

First, some linear algebra:

Deﬁnition 24.3.1. Let V be a ﬁnite dimensional inner product space. Then for a map
U : V → V , the following are equivalent:
 (cid:104)U (x), U (y)(cid:105) = (cid:104)x, y(cid:105) for x, y ∈ V .
 U† is the inverse of U .

 (cid:107)x(cid:107) = (cid:107)U (x)(cid:107) for x ∈ V .

The map U is called unitary if it satisﬁes these equivalent conditions.

Then

Quantum logic gates are unitary matrices.

In particular, unlike the classical situation, quantum gates are always reversible (and
hence they always take the same number of input and output bits).

For example, consider the CNOT gate. Its quantum analog should be a unitary map

UCNOT : H → H, where H = C⊕2 ⊗ C⊕2, given on basis elements by

UCNOT(|00(cid:105)) = |00(cid:105) , UCNOT(|01(cid:105)) = |01(cid:105)
UCNOT(|10(cid:105)) = |11(cid:105) , UCNOT(|11(cid:105)) = |10(cid:105) .

So pictorially, the quantum CNOT gate is given by

•

|0(cid:105)
|0(cid:105)

|0(cid:105)
|0(cid:105)

•

|0(cid:105)
|1(cid:105)

|0(cid:105)
|1(cid:105)

|1(cid:105)
|0(cid:105)

•

|1(cid:105)
|1(cid:105)

|1(cid:105)
|1(cid:105)

•

|1(cid:105)
|0(cid:105)

OK, so what? The whole point of quantum mechanics is that we allow linear qubits to be
in linear combinations of |0(cid:105) and |1(cid:105), too, and this will produce interesting results. For
example, let’s take |←(cid:105) = 1√2
(|0(cid:105) − |1(cid:105)) and plug it into the top, with |1(cid:105) on the bottom,
and see what happens:

UCNOT (|←(cid:105) ⊗ |1(cid:105)) = UCNOT(cid:18) 1

√2

(|01(cid:105) − |11(cid:105))(cid:19) =

1
√2

(|01(cid:105) − |10(cid:105)) = |Ψ−(cid:105)

which is the fully entangled singlet state! Picture:

•

|Ψ−(cid:105)

|←(cid:105)
|1(cid:105)

Thus, when we input mixed states into our quantum gates, the outputs are often

entangled states, even when the original inputs are not entangled.

24 Quantum circuits

267

Example 24.3.2 (More examples of quantum gates)

(a) Every reversible classical gate that we encountered before has a quantum analog
obtained in the same way as CNOT: by specifying the values on basis elements.
For example, there is a quantum Tofolli gate which for example sends

|1(cid:105)
|1(cid:105)
|0(cid:105)

•
•

|1(cid:105)
|1(cid:105)
|1(cid:105)

(b) The Hadamard gate on one qubit is a rotation given by

(cid:34) 1√2
1√2 − 1√2(cid:35) .

1√2

Thus, it sends |0(cid:105) to |→(cid:105) and |1(cid:105) to |←(cid:105). Note that the Hadamard gate is its
own inverse. It is depicted by an “H” box.

|0(cid:105) H

|→(cid:105)

(c) More generally, if U is a 2× 2 unitary matrix (i.e. a map C⊕2 → C⊕2) then there
is U -rotation gate similar to the previous one, which applies U to the input.

|ψ(cid:105)

U

U |ψ(cid:105)

For example, the classical NOT gate is represented by U = σx.

(d) A controlled U -rotation gate generalizes the CNOT gate. Let U : C⊕2 → C⊕2
be a rotation gate, and let H = C⊕2⊗C⊕2 be a 2-qubit space. Then the controlled
U gate has the following circuit diagrams.

|0(cid:105)
|ψ(cid:105)

•
U

|0(cid:105)
|ψ(cid:105)

|1(cid:105)
|ψ(cid:105)

•
U

|1(cid:105)
U |ψ(cid:105)

Thus, U is applied when the controlling bit is 1, and CNOT is the special case
U = σx. As before, we get interesting behavior if the control is mixed.

And now, some more counterintuitive quantum behavior. Suppose we try to use CNOT

as a copy, with truth table.

In Out
0 0 0 0
1 0 1 1
0 1 0 1
1 1 1 0

The point of this gate is to be used with a garbage 0 at the bottom to try and simulate a
“copy” operation. So indeed, one can check that

|0(cid:105)
|0(cid:105)

U

|0(cid:105)
|0(cid:105)

|1(cid:105)
|0(cid:105)

U

|1(cid:105)
|1(cid:105)

Thus we can copy |0(cid:105) and |1(cid:105). But as we’ve already seen if we input |←(cid:105) ⊗ |0(cid:105) into U , we
end up with the entangled state |Ψ−(cid:105) which is decisively not the |←(cid:105) ⊗ |←(cid:105) we wanted.

268

Napkin, by Evan Chen (v1.5.20190718)

And in fact, the so-called no-cloning theorem implies that it’s impossible to duplicate
an arbitrary |ψ(cid:105); the best we can do is copy speciﬁc orthogonal states as in the classical
case. See also Problem 24B.

§24.4 Deutsch-Jozsa algorithm

The Deutsch-Jozsa algorithm is the ﬁrst example of a nontrivial quantum algorithm
which cannot be performed classically: it is a “proof of concept” that would later inspire
Grover’s search algorithm and Shor’s factoring algorithm.

The problem is as follows: we’re given a function f : {0, 1}n → {0, 1}, and promised

that the function f is either

 A constant function, or

 A balanced function, meaning that exactly half the inputs map to 0 and half the

inputs map to 1.

The function f is given in the form of a reversible black box Uf which is the control of a
NOT gate, so it can be represented as the circuit diagram

|x1x2 . . . xn(cid:105) /n

|y(cid:105)

Uf

|x1x2 . . . xn(cid:105)
|y + f (x) mod 2(cid:105)

i.e. if f (x1, . . . , xn) = 0 then the gate does nothing, otherwise the gate ﬂips the y bit at
the bottom. The slash with the n indicates that the top of the input really consists of n
qubits, not just the one qubit drawn, and so the black box Uf is a map on n + 1 qubits.
The problem is to determine, with as few calls to the black box Uf as possible, whether

f is balanced or constant.

Question 24.4.1. Classically, show that in the worst case we may need up to 2n−1 + 1 calls
to the function f to answer the question.

So with only classical tools, it would take O(2n) queries to determine whether f is

balanced or constant. However,

Theorem 24.4.2 (Deutsch-Jozsa)

The Deutsch-Jozsa problem can be determined in a quantum circuit with only a
single call to the black box.

Proof. For concreteness, we do the case n = 1 explicitly; the general case is contained in
Problem 24C. We claim that the necessary circuit is

|0(cid:105)
|1(cid:105)

H

H

H

Uf

Here the H’s are Hadamard gates, and meter at the end of the rightmost wire indicates
that we make a measurement along the usual |0(cid:105), |1(cid:105) basis. This is not a typo! Even
though classically the top wire is just a repeat of the input information, we are about to
see that it’s the top we want to measure.

24 Quantum circuits

269

Note that after the two Hadamard operations, the state we get is

|01(cid:105) H⊗2

√2

(|0(cid:105) + |1(cid:105))(cid:19) ⊗(cid:18) 1

(cid:55)−−−→(cid:18) 1
(|0(cid:105) − |1(cid:105))(cid:19)
2(cid:16)|0(cid:105) ⊗(cid:0)|0(cid:105) − |1(cid:105)(cid:1) + |1(cid:105) ⊗(cid:0)|0(cid:105) − |1(cid:105)(cid:1)(cid:17).

√2

=

1

So after applying Uf , we obtain

1

2(cid:16)|0(cid:105) ⊗(cid:0)|0 + f (0)(cid:105) − |1 + f (0)(cid:105)(cid:1) + |1(cid:105) ⊗(cid:0)|0 + f (1)(cid:105) − |1 + f (1)(cid:105)(cid:1)(cid:17)

where the modulo 2 has been left implicit. Now, observe that the eﬀect of going from
|0(cid:105)−|1(cid:105) to |0 + f (x)(cid:105)−|1 + f (x)(cid:105) is merely to either keep the state the same (if f (x) = 0)
or to negate it (if f (x) = 1). So we can simplify and factor to get

1

2(cid:16)(−1)f (0) |0(cid:105) + (−1)f (1) |1(cid:105)(cid:17) ⊗ (|0(cid:105) − |1(cid:105)) .

Thus, the picture so far is:

|0(cid:105)
|1(cid:105)

H

H

Uf

1√2(cid:16)(−1)f (0) |0(cid:105) + (−1)f (1) |1(cid:105)(cid:17)

1√2

(|0(cid:105) − |1(cid:105))

In particular, the resulting state is not entangled, and we can simply discard the last
qubit (!). Now observe:

 If f is constant, then the upper-most state is ±|→(cid:105).
 If f is balanced, then the upper-most state is ±|←(cid:105).

So simply doing a measurement along σx will give us the answer. Equivalently, perform
another H gate (so that H |→(cid:105) = |0(cid:105), H |←(cid:105) = |1(cid:105)) and measuring along σz in the usual
|0(cid:105), |1(cid:105) basis. Thus for n = 1 we only need a single call to the oracle.

§24.5 A few harder problems to think about

Problem 24A (Fredkin gate). The Fredkin gate (also called the controlled swap, or
CSWAP gate) is the three-bit gate with the following truth table:

In

Out

0 0 0 0 0 0
0 0 1 0 0 1
0 1 0 0 1 0
0 1 1 0 1 1
1 0 0 1 0 0
1 0 1 1 1 0
1 1 0 1 0 1
1 1 1 1 1 1

Thus the gate swaps the last two input bits whenever the ﬁrst bit is 1. Show that this
gate is also reversible and universal.

Problem 24B (Baby no-cloning theorem). Show that there is no unitary map U on two
qubits which sends U (|ψ(cid:105) ⊗ |0(cid:105)) = |ψ(cid:105) ⊗ |ψ(cid:105) for any qubit |ψ(cid:105), i.e. the following circuit
diagram is impossible.

|ψ(cid:105)
|0(cid:105)

U

|ψ(cid:105)
|ψ(cid:105)

270

Napkin, by Evan Chen (v1.5.20190718)

Problem 24C (Deutsch-Jozsa). Given the black box Uf described in the Deutsch-Jozsa
algorithm, consider the following circuit.

|0 . . . 0(cid:105) /n H⊗n
H

|1(cid:105)

H⊗n

Uf

That is, take n copies of |0(cid:105), apply the Hadamard rotation to all of them, apply Uf ,
reverse the Hadamard to all n input bits (again discarding the last bit), then measure all
n bits in the |0(cid:105)/|1(cid:105) basis (as in Example 23.4.4).
balanced.

Show that the probability of measuring |0 . . . 0(cid:105) is 1 if f is constant and 0 if f is

Problem 24D† (Barenco et al, 1995; arXiv:quant-ph/9503016v1). Let

P =(cid:20)1 0
0 i(cid:21)

Q =

1

√2(cid:20) 1 −i
1(cid:21)

−i

Verify that the quantum Toﬀoli gate can be implemented using just controlled rotations
via the circuit

•

|x1(cid:105)
|x2(cid:105)
|x3(cid:105) Q Q

•

•

•
P

•

•
Q†

This was a big surprise to researchers when discovered, because classical reversible logic
requires three-bit gates (e.g. Toﬀoli, Fredkind).

25 Shor’s algorithm
OK, now for Shor’s Algorithm: how to factor M = pq in O(cid:0)(log M )2(cid:1) time.

§25.1 The classical (inverse) Fourier transform

The “crux move” in Shor’s algorithm is the so-called quantum Fourier transform. The
Fourier transform is used to extract periodicity in data, and it turns out the quantum
analogue is a lot faster than the classical one.

Let me throw the deﬁnition at you ﬁrst. Let N be a positive integer, and let ωN =

exp(cid:0) 2πi
N (cid:1).

Deﬁnition 25.1.1. Given a tuple of complex numbers

its discrete inverse Fourier transform is the sequence (y0, y1, . . . , yN−1) deﬁned by

(x0, x1, . . . , xN−1)

yk =

1
N

N−1(cid:88)j=0

ωjk
N xj.

Equivalently, one is applying the matrix

1
ωN
ω2
N
...

1
1
1
...
1 ωN−1

N

1
ω2
N
ω4
N
...

ω2(N−1)
N

1

. . .
ωN−1
. . .
N
. . . ω2(N−1)
. . .
. . . ω(N−1)2

...

N

N

y0
y1
...



=

.



xN−1

yN−1

x0
x1
...





1
N



The reason this operation is important is because it lets us detect if the xi are

periodic:

Example 25.1.2 (Example of discrete inverse Fourier transform)
Let N = 6, ω = ω6 = exp( 2πi
(hence xi is periodic modulo 2). Thus,

6 ) and suppose (x0, x1, x2, x3, x4, x5) = (0, 1, 0, 1, 0, 1)

y0 = 1
y1 = 1
y2 = 1
y3 = 1
y4 = 1
y5 = 1

6(cid:0)ω0 + ω0 + ω0(cid:1) = 1/2
6(cid:0)ω1 + ω3 + ω5(cid:1) = 0
6(cid:0)ω2 + ω6 + ω10(cid:1) = 0
6(cid:0)ω3 + ω9 + ω15(cid:1) = −1/2
6(cid:0)ω4 + ω12 + ω20(cid:1) = 0
6(cid:0)ω5 + ω15 + ω25(cid:1) = 0.

Thus, in the inverse transformation the “amplitudes” are all concentrated at multiples
of 3; thus this reveals the periodicity of the original sequence by N

3 = 2.

271

272

Napkin, by Evan Chen (v1.5.20190718)

More generally, given a sequence of 1’s appearing with period r, the amplitudes will peak
at inputs which are divisible by

N

gcd(N,r) .

Remark 25.1.3 — The fact that this operation is called the “inverse” Fourier
transform is mostly a historical accident (as my understanding goes). Confusingly,
the corresponding quantum operation is the (not-inverted) Fourier transform.

If we apply the deﬁnition as written, computing the transform takes O(N 2) time. It
turns out that by an algorithm called the fast Fourier transform (whose details we
won’t discuss), one can reduce this to O(N log N ) time. However, for Shor’s algorithm

this is also insuﬃcient; we need something like O(cid:0)(log N )2(cid:1) instead. This is where the

quantum Fourier transform comes in.

§25.2 The quantum Fourier transform

Note that to compute a Fourier transform, we need to multiply an N × N matrix with an
N -vector, so this takes O(N 2) multiplications. However, we are about to show that with
a quantum computer, one can do this using O((log N )2) quantum gates when N = 2n,
on a system with n qubits.

First, some more notation:

Abuse of Notation 25.2.1. In what follows, |x(cid:105) will refer to |xn(cid:105) ⊗ |xn−1(cid:105) ⊗ ··· ⊗ |x1(cid:105)
where x = xnxn−1 . . . x1 in binary. For example, if n = 3 then |6(cid:105) really means |1(cid:105) ⊗
|1(cid:105) ⊗ |0(cid:105).

Observe that the n-qubit space now has an orthonormal basis |0(cid:105), |1(cid:105), . . . , |N − 1(cid:105)

Deﬁnition 25.2.2. Consider an n-qubit state

The quantum Fourier transform is deﬁned by

|ψ(cid:105) =

xk |k(cid:105) .

N−1(cid:88)k=0
N−1(cid:88)j=0(cid:32)N−1(cid:88)k=0

UQFT(|ψ(cid:105)) =

1
√N

N(cid:33)|j(cid:105) .

ωjk

In other words, using the basis |0(cid:105), . . . , |N − 1(cid:105), UQFT is given by the matrix

UQFT =

1
√N

1
ωN
ω2
N
...

1
1
1
...
1 ωN−1

N

1
ω2
N
ω4
N
...

ω2(N−1)
N

1

. . .
ωN−1
. . .
N
. . . ω2(N−1)
. . .
. . . ω(N−1)2

...

N

N





This is the exactly the same deﬁnition as before, except we have a √N factor added
so that UQFT is unitary. But the trick is that in the quantum setup, the matrix can be
rewritten:

25 Shor’s algorithm

273

Proposition 25.2.3 (Tensor representation)
Let |x(cid:105) = |xnxn−1 . . . x1(cid:105). Then

UQFT(|xnxn−1 . . . x1(cid:105)) =

1
√N

(|0(cid:105) + exp(2πi · 0.x1)|1(cid:105))
⊗ (|0(cid:105) + exp(2πi · 0.x2x1)|1(cid:105))
⊗ . . .
⊗ (|0(cid:105) + exp(2πi · 0.xn . . . x1)|1(cid:105))

Proof. Direct (and quite annoying) computation. In short, expand everything.

So by using mixed states, we can deal with the quantum Fourier transform using this

“multiplication by tensor product” trick that isn’t possible classically.

Now, without further ado, here’s the circuit. Deﬁne the rotation matrices

Rk =(cid:20)1

0 exp(2πi/2k)(cid:21) .

0

Then, for n = 3 the circuit is given by by using controlled Rk’s as follows:

|x3(cid:105)
|x2(cid:105)
|x1(cid:105)

H

R2

R3

•

H

R2

•

•

H

|y1(cid:105)
|y2(cid:105)
|y3(cid:105)

Exercise 25.2.4. Show that in this circuit, the image of of |x3x2x1(cid:105) (for binary xi) is

(cid:16)|0(cid:105) + exp(2πi · 0.x1)|1(cid:105)(cid:17) ⊗(cid:16)|0(cid:105) + exp(2πi · 0.x2x1)|1(cid:105)(cid:17) ⊗(cid:16)|0(cid:105) + exp(2πi · 0.x3x2x1)|1(cid:105)(cid:17)

as claimed.

For general n, we can write this as inductively as

|xn(cid:105)
|xn−1(cid:105)
...
|xi(cid:105)
...
|x2(cid:105)
|x1(cid:105)

QFTn−1

Rn

Rn−1

•

•

···
···

···

···
···

Ri

•

···
···

···

···
···

R2

•

H

|y1(cid:105)
|y2(cid:105)
...
|yn−i+1(cid:105)
...
|yn−1(cid:105)
|yn(cid:105)

Question 25.2.5. Convince yourself that when n = 3 the two circuits displayed are
equivalent.

Thus, the quantum Fourier transform is achievable with O(n2) gates, which is enor-
mously better than the O(N log N ) operations achieved by the classical fast Fourier
transform (where N = 2n).

274

Napkin, by Evan Chen (v1.5.20190718)

§25.3 Shor’s algorithm

The quantum Fourier transform is the key piece of Shor’s algorithm. Now that we have
it, we can solve the factoring problem.

Let p, q > 3 be odd primes, and assume p (cid:54)= q. The main idea is to turn factoring an
integer M = pq into a problem about ﬁnding the order of x (mod M ); the latter is a
“periodicity” problem that the quantum Fourier transform will let us solve. Speciﬁcally,
say that an x (mod M ) is good if

(i) gcd(x, M ) = 1,

(ii) The order r of x (mod M ) is even, and
(iii) Factoring 0 ≡ (xr/2−1)(xr/2 +1) (mod M ), neither of the two factors is 0 (mod M ).

Thus one of them is divisible by p, and the other is divisible by q.

Exercise 25.3.1 (For contest number theory practice). Show that for M = pq at least half
of the residues in (Z/MZ)× are good.

So if we can ﬁnd the order of an arbitrary x ∈ (Z/MZ)×, then we just keep picking x
until we pick a good one (this happens more than half the time); once we do, we compute
gcd(xr/2 − 1, M ) using the Euclidean algorithm to extract one of the prime factors of M ,
and we’re home free.
Now how do we do this? The idea is not so diﬃcult: ﬁrst we generate a sequence which

is periodic modulo r.

Example 25.3.2 (Factoring 77: generating the periodic state)
Let’s say we’re trying to factor M = 77, and we randomly select x = 2, and want to
ﬁnd its order r. Let n = 13 and N = 213, and start by initializing the state

|ψ(cid:105) =

1
√N

N−1(cid:88)k=0

|k(cid:105) .

Now, build a circuit Ux (depending on x = 2!) which takes |k(cid:105)|0(cid:105) to |k(cid:105)|2k mod M(cid:105).
Applying this to |ψ(cid:105) ⊗ |0(cid:105) gives

U (|ψ(cid:105)|0(cid:105)) =

1
√N

N−1(cid:88)k=0

|k(cid:105) ⊗ |2k mod M(cid:105) .

Now suppose we measure the second qubit, and get a state of |128(cid:105). That tells us
that the collapsed state now, up to scaling, is

(|7(cid:105) + |7 + r(cid:105) + |7 + 2r(cid:105) + . . . ) ⊗ |128(cid:105) .

The bottleneck is actually the circuit Ux; one can compute xk (mod M ) by using repeated
squaring, but it’s still the clumsy part of the whole operation.

In general, the operation is:
 Pick a suﬃciently large N = 2n (say, N ≥ M 2).
 Generate |ψ(cid:105) =(cid:80)2n−1

k=0 |k(cid:105).

25 Shor’s algorithm

275

 Build a circuit Ux which computes |xk mod M(cid:105).
 Apply it to get a state

k=0 |k(cid:105) ⊗ |xk mod M(cid:105).

1√N(cid:80)2n−1

 Measure the second qubit to cause the ﬁrst qubit to collapse to something which is

periodic modulo r. Let |φ(cid:105) denote the left qubit.

Suppose we apply the quantum Fourier transform to the left qubit |φ(cid:105) now: since the
left bit is periodic modulo r, we expect the transform will tell us what r is. Unfortunately,
this doesn’t quite work out, since N is a power of two, but we don’t expect r to be.

Nevertheless, consider a state

|φ(cid:105) = |k0(cid:105) + |k0 + r(cid:105) + . . .

so for example previously we had k0 = 7 if we measured 128 on x = 2. Applying the
quantum Fourier transform, we see that the coeﬃcient of |j(cid:105) in the transformed image is
equal to

As this is a sum of roots of unity, we realize we have destructive interference unless
ωjr
N = 1 (since N is large). In other words, we approximately have

ωk0j

N ·(cid:16)ω0

N + ωjr

N + ω2jr

N + ω3jr

N + . . .(cid:17)

UQFT(|φ(cid:105)) ≈ (cid:88)0≤j<N

jr/N∈Z

|j(cid:105)

up to scaling as usual. The bottom line is that
If we measure UQFT |φ(cid:105) we obtain a |j(cid:105) such that jr

N is close to an s ∈ Z.

And thus given suﬃcient luck we can use continued fractions to extract the value of r.

Example 25.3.3 (Finishing the factoring of M = 77)
As before, we made an observation to the second qubit, and thus the ﬁrst qubit
collapses to the state |φ(cid:105) = |7(cid:105) + |7 + r(cid:105) + . . . . Now we make a measurement and
obtain j = 4642, which means that for some integer s we have

4642r
213 ≈ s.
Now, we analyze the continued fraction of 4642

213 ; we ﬁnd the ﬁrst few convergents are

0, 1,

1
2

,

4
7

,

13
23

,

17
30

,

1152
2033

, . . .

30 is a very good approximation, hence we deduce s = 17 and r = 30 as candidates.

So 17
And indeed, one can check that r = 30 is the desired order.

This won’t work all the time (for example, we could get unlucky and measure j = 0,

i.e. s = 0, which would tell us no information at all).

But one can show that we succeed any time that

gcd(s, r) = 1.

This happens at least
many trials, we will eventually extract the correct order r. This is Shor’s algorithm.

1
log r of the time, and since r < M this means that given suﬃciently

VIII

Calculus 101

Part VIII: Contents

26 Limits and series

279
26.1 Completeness and inf/sup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279
26.2 Proofs of the two key completeness properties of R . . . . . . . . . . . . . . . . . . 280
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282
26.3 Monotonic sequences
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283
26.4 Inﬁnite series
. . . . . . . . . . . . . . . . . . 286
26.5 Series addition is not commutative: a horror story
26.6 Limits of functions at points . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 289
26.7 Limits of functions at inﬁnity
. . . . . . . . . . . . . . . . . . . . . . . . 289
26.8 A few harder problems to think about

27 Bonus: A hint of p-adic numbers

291
27.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 291
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292
27.2 Algebraic perspective
27.3 Analytic perspective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295
27.4 Mahler coeﬃcients . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299
. . . . . . . . . . . . . . . . . . . . . . . . 301
27.5 A few harder problems to think about

28 Diﬀerentiation

303
28.1 Deﬁnition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303
28.2 How to compute them . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 307
28.3 Local (and global) maximums
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309
28.4 Rolle and friends
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312
28.5 Smooth functions
. . . . . . . . . . . . . . . . . . . . . . . . 312
28.6 A few harder problems to think about

29 Power series and Taylor series

315
29.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 315
29.2 Power series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 316
29.3 Diﬀerentiating them . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317
29.4 Analytic functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 318
29.5 A deﬁnition of Euler’s constant and exponentiation . . . . . . . . . . . . . . . . . . 320
29.6 This all works over complex numbers as well, except also complex analysis is heaven . . 321
. . . . . . . . . . . . . . . . . . . . . . . . 321
29.7 A few harder problems to think about

30 Riemann integrals

323
30.1 Uniform continuity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323
30.2 Dense sets and extension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 324
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 325
30.3 Deﬁning the Riemann integral
30.4 Meshes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327
. . . . . . . . . . . . . . . . . . . . . . . . 328
30.5 A few harder problems to think about

26 Limits and series

Now that we have developed the theory of metric (and topological) spaces well, we
give a three-chapter sequence which briskly covers the theory of single-variable calculus.
Much of the work has secretly already been done, For example, if xn and yn are real
sequences with limn xn = x and limn yn = y, then in fact limn(xn + yn) = x + y or
limn(xnyn) = xy, because we showed in Proposition 2.5.5 that arithmetic was continuous.
We will also see that completeness plays a crucial role.

§26.1 Completeness and inf/sup

Prototypical example for this section: sup[0, 1] = sup(0, 1) = 1.

As R is a metric space, we may discuss continuity and convergence. There are two

important facts about R which will make most of the following sections tick.

The ﬁrst fact you have already seen before:

Theorem 26.1.1 (R is complete)
As a metric space, R is complete: sequences converge if and only if they are Cauchy.

The second one we have not seen before — it is the existence of inf and sup. Your

intuition should be:

sup is max adjusted slightly for inﬁnite sets. (And inf is adjusted min.)

Why the “adjustment”?

Example 26.1.2 (Why is max not good enough?)
Let’s say we have the open interval S = (0, 1). The elements can get arbitrarily close
to 1, so we would like to think “1 is the max of S”; except the issue is that 1 /∈ S. In
general, inﬁnite sets don’t necessarily have a maximum, and we have to talk about
bounds instead.

So we will deﬁne sup S in such a way that sup S = 1. The deﬁnition is that “1 is

the smallest number which is at least every element of S”.

To write it out:

Deﬁnition 26.1.3. If S is a set of real numbers:

 An upper bound for S is a real number M such that x ≤ M for all x ∈ S. If one

exists, we say S is bounded above;

 A lower bound for S is a real number m such that m ≤ x for all x ∈ S. If one

exists, we say S is bounded below.

 If both upper and lower bounds exist, we say S is bounded.

279

280

Napkin, by Evan Chen (v1.5.20190718)

Theorem 26.1.4 (R has inf’s and sup’s)
Let S be a nonempty set of real numbers.

 If S is bounded above then it has a least upper bound, which we denote by

sup S and refer to as the supremum of S.

 If S is bounded below then it has a greatest lower bound, which we denote by

inf S and refer to as the inﬁmum of S.

Deﬁnition 26.1.5. For convenience, if S has not bounded above, we write sup S = +∞.
Similarly, if S has not bounded below, we write inf S = −∞.

Example 26.1.6 (Supremums)
Since the examples for inﬁmums are basically the same, we stick with supremums
for now.

(a) If S = {1, 2, 3, . . .} then S is not bounded above, so we have sup S = +∞.
(b) If S = {. . . ,−2,−1} denotes the set of negative integers, then sup S = −1.
(c) Let S = [0, 1] be a closed interval. Then sup S = 1.

(d) Let S = (0, 1) be an open interval. Then sup S = 1 as well, even though 1 itself

is not an element of S.

(e) Let S = Q ∩ (0, 1) denote the set of rational numbers between 0 and 1. Then

sup S = 1 still.

(f) If S is a ﬁnite nonempty set, then sup S = max S.

Deﬁnition 26.1.7 (Porting deﬁnitions to sequences). If a1, . . . is a sequence we will
often write

sup

n
inf
n

an := sup{an | n ∈ N}
an := inf {an | n ∈ N}

for the supremum and inﬁmum of the set of elements of the sequence. We also use the
words “bounded above/below” for sequences in the same way.

Example 26.1.8 (Inﬁmum of a sequence)
The sequence an = 1

n has inﬁmum inf an = 0.

§26.2 Proofs of the two key completeness properties of R

Careful readers will note that we have not actually proven either Theorem 26.1.4 or
Theorem 26.1.1. We will do so here.

First, we show that the ability to take inﬁmums and supremums lets you prove

completeness of R.

26 Limits and series

281

Proof that Theorem 26.1.4 implies Theorem 26.1.1. Let a1, a2, . . . be a Cauchy sequence.
By discarding ﬁnitely many leading terms, we may as well assume that |ai − aj| ≤ 100 for
all i and j. In particular, the sequence is now bounded; it lies between [a1 − 100, a1 + 100]
for example.
We want to show this sequence converges, so we have to ﬁrst describe what the limit
is. We know that to do this we are really going to have to use the fact that we live in R.
(For example we know in Q the limit of 1, 1.4, 1.41, 1.414, . . . is nonexistent.)

We propose the following: let

S = {x ∈ R | an ≥ x for inﬁnitely many n } .

We claim that the sequence converges to M = sup S.

Exercise 26.2.1. Show that this supremum makes sense by proving that a1 − 100 ∈ S (so
S is nonempty) while all elements of S are at most a1 + 100 (so S is bounded above). Thus
we are allowed to actually take the supremum.

You can think of this set S with the following picture. We have a Cauchy sequence
drawn in the real line which we think converges, which we can visualize as a bunch of dots
on the real line, with some order on them. We wish to cut the line with a knife such that
only ﬁnitely many dots are do to the left of the knife. (For example, placing the knife all
the way to the left always works.) The set S represents the places where we could put
the knife, and M is “as far right” as we could go. Because of the way supremums work,
M might not itself be a valid knife location, but certainly anything to its left is.

Let ε > 0 be given; we want to show eventually all terms are within ε of M . Because the
sequence is Cauchy, there is an N such that eventually |am − an| < 1
2 ε for m ≥ n ≥ N .
Now suppose we ﬁx n and vary m. By the deﬁnition of M , it should be possible to
pick the index m such that am ≥ M − 1
2 ε (there are inﬁnitely many to choose from since
M − 1

2 ε is a valid knife location, and we only need m ≥ n). In that case we have

|an − M| ≤ |an − am| + |am − M| <

1
2

ε +

1
2

ε = ε

by the triangle inequality. This completes the proof.

Therefore it is enough to prove the latter Theorem 26.1.4. To do this though, we would
need to actually give a rigorous deﬁnition of the real numbers R, since we have not done
so yet!

One approach that makes this easy to use the so-called Dedekind cut construction.
Suppose we take the rational numbers Q. Then one deﬁnes a real number to be a “cut”
A | B of the set of rational numbers: a pair of subsets of Q such that

 Q = A (cid:116) B is a disjoint union;
 A and B are nonempty;

 we have a < b for every a ∈ A and b ∈ B, and

RMM−12εa1a2a3a4a5a6a8a7282

Napkin, by Evan Chen (v1.5.20190718)

 A has no largest element (i.e. sup A /∈ A).

This can again be visualized by taking what you think of as the real line, and slicing at
some real number. The subset Q ⊂ R gets cut into two halves A and B. If the knife
happens to land exactly at a rational number, by convention we consider that number to
be in the right half (which explains the last fourth condition that sup A /∈ A).
With this deﬁnition Theorem 26.1.4 is easy: to take the supremum of a set of real
numbers, we take the union of all the left halves. The hard part is then ﬁguring out how
to deﬁne +, −, ×, ÷ and so on with this rather awkward construction. If you want to
read more about this construction in detail, my favorite reference is [Pu02], in which all
of this is done carefully in Chapter 1.

§26.3 Monotonic sequences

Here is a great exercise.

Exercise 26.3.1 (Mandatory). Prove that if a1 ≥ a2 ≥ ··· ≥ 0 then the limit

lim
n→∞ an

exists. Hint: the idea in the proof of the previous section helps; you can also try to use
completeness of R. Second hint: if you are really stuck, wait until after Proposition 26.4.5,
at which point you can use essentially copy its proof.

The proof here readily adapts by shifting.

Deﬁnition 26.3.2. A sequence an is monotonic if either a1 ≥ a2 ≥ . . . or a1 ≤ a2 ≤
. . . .

Theorem 26.3.3 (Monotonic bounded sequences converge)
Let a1, a2, . . . be a monotonic bounded sequence. Then limn→∞ an exists.

Example 26.3.4 (Silly example of monotonocity)
Consider the sequence deﬁned by

a1 = 1.2
a2 = 1.24
a3 = 1.1248
a4 = 1.124816
a5 = 1.12481632

...

and so on, where in general we stuck on the decimal representation of the next power
of 2. This will converge to some real number, although of course this number is
quite unnatural and there is probably no good description for it.

In general, “inﬁnite decimals” can now be deﬁned as the limit of the truncated ﬁnite
ones.

26 Limits and series

283

Example 26.3.5 (0.9999··· = 1)
In particular, I can ﬁnally make precise the notion you argued about in elementary
school that

We simply deﬁne a repeating decimal to be the limit of the sequence 0.9, 0.99,
0.999 . . . . And it is obvious that the limit of this sequence is 1.

0.9999··· = 1.

Some of you might be a little surprised since it seems like we really should have
0.9999 = 9 · 10−1 + 9 · 10−2 + . . . — the limit of “partial sums”. Don’t worry, we’re about
to deﬁne those in just a moment.

Here is one other great use of monotonic sequences.

Deﬁnition 26.3.6. Let a1, a2, . . . be a sequence (not necessarily monotonic) which is
bounded above. We deﬁne

lim sup
n→∞

an := lim
N→∞

sup
n≥N

an = lim
N→∞

sup{aN , aN +1, . . .} .

This is called the limit supremum of (an). We set lim supn→∞ an to be +∞ if an is
not bounded above.
The limit inﬁmum is deﬁned similarly, with lim inf n→∞ an = −∞ if an is not bounded

below.

Exercise 26.3.7. Show that this deﬁnition makes sense, by checking that the supremums
are non-decreasing, and bounded above.

We can think of lim supn an as “supremum, but allowing ﬁnitely many terms to be
discarded”.

§26.4 Inﬁnite series

Prototypical example for this section: (cid:80)∞k≥1

1

k(k+1) = limn→∞(cid:16)1 − 1

n+1(cid:17) = 1.

We will actually begin by working with inﬁnite series, since in the previous chapters we
deﬁned limits of sequences, and so this is actually the next closest thing to work with.1

This will give you a rigorous way to think about statements like

1
n2 =

π2
6

∞(cid:88)n=1

and help answer questions like “how can you add rational numbers and get an irrational
one?”.

1Conceptually: discrete things are easier to be rigorous about than continuous things, so series are
actually “easier” than derivatives! I suspect the reason that most schools teach series last in calculus
is that most calculus courses do not have proofs.

284

Napkin, by Evan Chen (v1.5.20190718)

Deﬁnition 26.4.1. Consider a sequence a1, . . . of real numbers. The series (cid:80)k ak

converges to a limit L if the sequence of “partial sums”

s1 = a1
s2 = a1 + a2
s3 = a1 + a2 + a3

...

sn = a1 + ··· + an

converges to the limit L. Otherwise it diverges.

Abuse of Notation 26.4.2 (Writing divergence as +∞). It is customary, if all the ak

are nonnegative, to write(cid:80)k ak = ∞ to denote that the series diverges.

You will notice that by using the deﬁnition of sequences, we have masterfully sidestepped
the issue of “adding inﬁnitely many numbers” which would otherwise cause all sorts of
problems.

An “inﬁnite sum” is actually the limit of its partial sums. There is no
inﬁnite addition involved.

That’s why it’s for example okay to have(cid:80)n≥1
6 be irrational; we have already
seen many times that sequences of rational numbers can converge to irrational numbers.
It also means we can gladly ignore all the irritating posts by middle schoolers about
1 + 2 + 3 + ··· = − 1
12 ; the partial sums explode to +∞, end of story, and if you want to
assign a value to that sum it had better be a deﬁnition.

n2 = π2

1

Example 26.4.3 (The classical telescoping series)
We can now prove the classic telescoping series

1

k(k + 1)

∞(cid:88)k=1

in a way that doesn’t just hand-wave the ending. Note that the kth partial sum is

1

k(k + 1)

=

n(cid:88)k=1

1
1 · 2
1 −

=(cid:18) 1

= 1 −

+

1
2 · 3
1

1

.

+ ··· +

1

n(n + 1)

2(cid:19) + ··· +(cid:18) 1

n −

1

n + 1(cid:19)

n + 1
The limit of this partial sum as n → ∞ is 1.

26 Limits and series

285

Example 26.4.4 (Harmonic series diverges)

We can also make sense of the statement that(cid:80)∞k=1

may bound the 2nth partial sums from below:

1

k = ∞ (i.e. it diverges). We

2n(cid:88)k=1

1
k

=

1
1

+

1
2

+ ··· +

1
2n

+

4

+

1
2

1
1

≥

+(cid:18) 1
+ ··· +(cid:18) 1
(cid:124)
1
2
A sequence satisfying s2n ≥ 1 + 1

= 1 +

+

2n + ··· +
2n−1 terms
1
+ ··· +
2

(cid:123)(cid:122)

8

1

1

4(cid:19) +(cid:18) 1
2n(cid:19)
(cid:125)

1
2

+

1
8

+

1
8

+

1

8(cid:19)

= 1 +

n − 1

2

.

2 (n − 1) will never converge to a ﬁnite number!

I had better also mention that for nonnegative sums, convergence is just the same as

having “ﬁnite sum” in the following sense.

Proposition 26.4.5 (Partial sums of nonnegatives bounded implies convergent)

Let(cid:80)k ak be a series of nonnegative real numbers. Then(cid:80)k ak converges to some

limit if and only if there is a constant M such that

for every positive integer n.

a1 + ··· + an < M

Proof. This is actually just Theorem 26.3.3 in disguise, but since we left the proof as an
exercise back then, we’ll write it out this time.

Obviously if no such M exists then convergence will not happen, since this means the

sequence sn of partial sums is unbounded.

Conversely, if such M exists then we have s1 ≤ s2 ≤ ··· < M . Then we contend the
sequence sn converges to M = supn sn. (If you read the proof that completeness implies
Cauchy, the picture is nearly the same here, but simpler.)

Indeed, this means for any ε there are inﬁnitely many terms of the sequence exceeding
M − ε; but since the sequence is monotic, once sn ≥ M − ε for some N then sn(cid:48) ≥ M − ε
for all n ≥ n(cid:48). This implies convergence.
Abuse of Notation 26.4.6 (Writing(cid:80) < ∞). For this reason, if ak are nonnegative

real numbers, it is customary to write

(cid:88)k

ak < ∞

RMM−εs1s2s3s4s5s6s7s8286

Napkin, by Evan Chen (v1.5.20190718)

as a shorthand for “(cid:80)k ak converges to a ﬁnite limit”, (or perhaps shorthand for “(cid:80)k ak

is bounded” — as we have just proved these are equivalent). We will use this notation
too.

§26.5 Series addition is not commutative: a horror story

One unfortunate property of the above deﬁnition is that it actually depends on the order
of the elements. In fact, it turns out that there is an explicit way to describe when
rearrangement is okay.

Deﬁnition 26.5.1. A series(cid:80)k ak of real numbers is said to converge absolutely if

(cid:88)k

|ak| < ∞

i.e. the series of absolute values converges to some limit. If the series converges, but not
absolutely, we say it converges conditionally.

Proposition 26.5.2 (Absolute convergence =⇒ convergence)

If a series(cid:80)k ak of real numbers converges absolutely, then it converges in the usual

sense.

Exercise 26.5.3 (Great exercise). Prove this by using the Cauchy criteria: show that if

the partial sums of(cid:80)k |ak| are Cauchy, then so are the partial sums of(cid:80)k ak.

Then, rearrangement works great.

Theorem 26.5.4 (Permutation of terms okay for absolute convergence)

permutation of the terms will also converge to L.

Consider a series(cid:80)k ak which is absolutely convergent and has limit L. Then any
Proof. Suppose(cid:80)k ak converges to L, and bn is a rearrangement. Let ε > 0. We will

show that sum partial sums of bn are eventually within ε of L.

The hypothesis means that there is a large N in terms of ε such that

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
N(cid:88)k=1

ak − L(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

<

1
2

ε and

n(cid:88)k=N +1

|ak| <

1
2

ε

for every n ≥ N (the former from vanilla convergence of ak and the latter from the fact
that ak converges absolutely, hence its partial sums are Cauchy).
Now suppose M is large enough that a1, . . . , aN are contained within the terms

{b1, . . . , bM}. Then

b1 + ··· + bM = (a1 + ··· + aN )

The terms in the ﬁrst line sum up to within 1
have sum at most 1
of L.

2 ε in absolute value, so the total b1 + ··· + bM is within 1

2 ε of L, and the terms in the second line
2 ε = ε

2 ε + 1

+ ai1 + ai2 + ··· + aiM−N
M − N terms with indices ≥ N

(cid:123)(cid:122)

(cid:124)

(cid:125)

26 Limits and series

287

In particular, when you have nonnegative terms, the world is great:

Nonnegative series can be rearranged at will.

And the good news is that actually, in practice, most of your sums will be nonnegative.
The converse is not true, and in fact, it is almost the worst possible converse you can

imagine.

Theorem 26.5.5 (Permutation of terms meaningless for conditional convergence)

Consider a series(cid:80)k ak which converges conditionally to some real number. Then,

there exists a permutation of the series which converges conditionally to 1337.

(Or any constant. You can also get it to diverge, too.)

So, permutation is as bad as possible for conditionally convergent series, and hence don’t
even bother to try.

§26.6 Limits of functions at points

Prototypical example for this section: limx→∞ 1/x = 0.

We had also better deﬁne the notion of a limit of a real function, which (surprisingly)
we haven’t actually deﬁned yet. The deﬁnition will look like what we have seen before
with continuity.
Deﬁnition 26.6.1. Let f : R → R be a function2 and let p ∈ R be a point in the domain.
Suppose there exists a real number L such that:

For every ε > 0, there exists δ > 0 such that if |x − p| < δ and x (cid:54)= p then
|f (x) − L| < ε.

Then we say L is the limit of f as x → p, and write
f (x) = L.

lim
x→p

There is an important point here: in this deﬁnition we deliberately require that x (cid:54)= p.
The value limx→p f (x) does not depend on f (p), and accordingly we often
do not even bother to deﬁne f (p).

Example 26.6.2 (Function with a hole)
Deﬁne the function f : R → R by

f (x) =(cid:40)3x

2019

if x (cid:54)= 0
otherwise.

Then limx→0 f (x) = 0. The value f (0) = 2019 does not aﬀect the limit. Obviously,
because f (0) was made up to be some artiﬁcial value that did not agree with the
limit, this function is discontinuous at x = 0.

2Or f : (a, b) → R, or variants. We just need f to be deﬁned on an open neighborhood of p.

288

Napkin, by Evan Chen (v1.5.20190718)

Question 26.6.3 (Mandatory). Show that a function f is continuous at p if and only if
limx→p f (x) exists and equals f (p).

Example 26.6.4 (Less trivial example: a rational piecewise function)
Deﬁne the function f : R → R as follows:

f (x) =

1
1
q
0

if x = 0
if x = p
if x /∈ Q.

q where q > 0 and gcd(p, q) = 1

For example, f (π) = 0, f (2/3) = 1

3 , f (0.17) = 1

100 . Then

f (x) = 0.

lim
x→0

For example, if |x| < 1/100 and x (cid:54)= 0 then f (x) is either zero (for x irrational) or
else is at most
As f (0) = 1, this function is also discontinuous at x = 0. However, if we change

1
101 (if x is rational).

the deﬁnition so that f (0) = 0 instead, then f becomes continuous.

Example 26.6.5 (Famous example)
Let f (x) = sin x

x , f : R → R, where f (0) is assigned any value. Then

f (x) = 1.

lim
x→0

We will not prove this here, since I don’t want to get into trig yet. In general, I will
basically only use trig functions for examples and not for any theory, so most properties
of the trig functions will just be quoted.

Abuse of Notation 26.6.6 (The usual notation). From now on, the above example
will usually be abbreviated to just

sin x

x

lim
x→0

= 1.

The reason there is a slight abuse here is that I’m supposed to feed a function f into the
limit, and instead I’ve written down an expression which is deﬁned everywhere — except
at x = 0. But that f (0) value doesn’t change anything. So the above means: “the limit
of the function described by f (x) = sin x
x , except f (0) can be whatever it wants because
it doesn’t matter”.

Remark 26.6.7 (For metric spaces) — You might be surprised that I didn’t deﬁne
the notion of limx→p f (x) earlier for f : M → N a function on metric spaces. We
can actually do so as above, but there is one nuance: what if our metric space M
is discrete, so p has no points nearby it? (Or even more simply, what if M is a
one-point space?) We then cannot deﬁne limx→p f (x) at all.

Thus if f : M → N and we want to deﬁne limx→p f (x), we have the requirement

26 Limits and series

289

that p should have a point within ε of it, for any ε > 0. In other words, p should
not be an isolated point.

As usual, there are no surprises with arithmetic, we have limx→p(f (x) ± g(x)) =
limx→p f (x) ± limx→p g(x), and so on and so forth. We have eﬀectively done this proof
before so we won’t repeat it again.

§26.7 Limits of functions at inﬁnity

Annoyingly, we actually have to make this deﬁnition separately, even though it will not
feel any diﬀerent from earlier examples.

Deﬁnition 26.7.1. Let f : R → R. Suppose there exists a real number L such that:

For every ε > 0, there exists a constant M such that if x > M , then
|f (x) − L| < ε.

Then we say L is the limit of f as x approaches ∞ and write

f (x) = L.

lim
x→∞

The limit limx→−∞ f (x) is deﬁned similarly, with x > M replaced by x < M .

Fortunately, as ∞ is not an element of R, we don’t have to do the same antics about
f (∞) like we had to do with “f (p) set arbitrarily”. So these examples can be more easily
written down.

Example 26.7.2 (Limit at inﬁnity)
The usual:

1
x

lim
x→∞

= 0.

I’ll even write out the proof: for any ε > 0, if x > 1/ε then(cid:12)(cid:12) 1

x − 0(cid:12)(cid:12) < 1

ε .

There are no surprises with arithmetic: we have limx→∞(f (x)± g(x)) = limx→∞ f (x)±
limx→p g(x), and so on and so forth. This is about the fourth time I’ve mentioned this,
so I will not say more.

§26.8 A few harder problems to think about

Problem 26A. Deﬁne the sequence

an = (−1)n +

n3
2n

for every positive integer n. Compute the limit inﬁmum and the limit supremum.

Problem 26B. For which bounded sequences an does lim inf n an = lim supn an?

Problem 26C† (Comparison test). Let(cid:80) an and(cid:80) bn be two series. Assume(cid:80) bn is
absolutely convergent, and |an| ≤ |bn| for all integers n. Prove that(cid:80)n an is absolutely

convergent.

290

Napkin, by Evan Chen (v1.5.20190718)

Problem 26D (Geometric series). Let −1 < r < 1 be a real number. Show that the
series

1 + r + r2 + r3 + . . .

converges absolutely and determine what it converges to.

Problem 26E (Alternating series test). Let a0 ≥ a1 ≥ a2 ≥ a3 ≥ . . . be a weakly
decreasing sequence of nonnegative real numbers, and assume that limn→∞ an = 0. Show

Problem 26F ([Pu02, Chapter 3, Exercise 55]). Let (an)n≥1 and (bn)n≥1 be sequences

that the series(cid:80)n(−1)nan is convergent (it need not be absolutely convergent).
of real numbers. Assume a1 ≤ a2 ≤ ··· ≤ 1000 and moreover that (cid:80)n bn converges.
Prove that(cid:80)n anbn converges. (Note that in both the hypothesis and statement, we do

not have absolute convergence.)

Problem 26G (Putnam 2016 B1). Let x0, x1, x2, . . . be the sequence such that x0 = 1
and for n ≥ 0,

xn+1 = log(exn − xn)

(as usual, log is the natural logarithm). Prove that the inﬁnite series x0 + x1 + . . .
converges and determine its value.
Problem 26H. Consider again the function f : R → R in Example 26.6.4 deﬁned by

f (x) =

1
1
q
0

if x = 0
if x = p
if x /∈ Q.

q where q > 0 and gcd(p, q) = 1

For every real number p, compute limx→p f (x), if it exists. At which points is f continu-
ous?

27 Bonus: A hint of p-adic numbers

This is a bonus chapter meant for those who have also read about rings and ﬁelds:

it’s a nice tidbit at the intersection of algebra and analysis.

In this chapter, we are going to redo most of the previous chapter with the absolute
value |−| replaced by the p-adic one. This will give us the p-adic integers Zp, and the
p-adic numbers Qp. The one-sentence description is that these are “integers/rationals
carrying full mod pe information” (and only that information).

In everything that follows p is always assumed to denote a prime. The ﬁrst four sections
will cover the founding deﬁnitions culminating in a short solution to a USA TST problem.
We will then state (mostly without proof) some more surprising results about continuous
functions f : Zp → Qp; ﬁnally we close with the famous proof of the Skolem-Mahler-Lech
theorem using p-adic analysis.

§27.1 Motivation

Before really telling you what Zp and Qp are, let me tell you what you might expect
them to do.

In elementary/olympiad number theory, we’re already well-familiar with the following

two ideas:

 Taking modulo a prime p or prime pe, and

 Looking at the exponent νp.

Let me expand on the ﬁrst point. Suppose we have some Diophantine equation. In
olympiad contexts, one can take an equation modulo p to gain something else to work
with. Unfortunately, taking modulo p loses some information: the reduction Z (cid:16) Z/p is
far from injective.

If we want ﬁner control, we could consider instead taking modulo p2, rather than
taking modulo p. This can also give some new information (cubes modulo 9, anyone?),
but it has the disadvantage that Z/p2 isn’t a ﬁeld, so we lose a lot of the nice algebraic
properties that we got if we take modulo p.

One of the goals of p-adic numbers is that we can get around these two issues I

described. The p-adic numbers we introduce is going to have the following properties:

1. You can “take modulo pe for all e at once”. In olympiad contexts, we are
used to picking a particular modulus and then seeing what happens if we take
that modulus. But with p-adic numbers, we won’t have to make that choice. An
equation of p-adic numbers carries enough information to take modulo pe.

2. The numbers Qp form a ﬁeld, the nicest possible algebraic structure: 1/p makes

sense. Contrast this with Z/p2, which is not even an integral domain.

3. It doesn’t lose as much information as taking modulo p does: rather than the

surjective Z (cid:16) Z/p we have an injective map Z (cid:44)→ Zp.

291

292

Napkin, by Evan Chen (v1.5.20190718)

4. Despite this, you “ignore” some “irrelevant” data. Just like taking modulo
p, you want to zoom-in on a particular type of algebraic information, and this
means necessarily losing sight of other things.1

So, you can think of p-adic numbers as the right tool to use if you only really care about
modulo pe information, but normal Z/pe isn’t quite powerful enough.

To be more concrete, I’ll give a poster example now:

Example 27.1.1 (USA TST 2002/2)
For a prime p, show the value of

fp(x) =

1

(px + k)2

p−1(cid:88)k=1

(mod p3)

does not depend on x.

Here is a problem where we clearly only care about pe-type information. Yet it’s a
nontrivial challenge to do the necessary manipulations mod p3 (try it!). The basic issue
is that there is no good way to deal with the denominators modulo p3 (in part Z/p3 is
not even an integral domain).

However, with p-adic analysis we’re going to be able to overcome these limitations and

give a “straightforward” proof by using the identity

(cid:16)1 +

px

k (cid:17)−2

=(cid:88)n≥0(cid:18)−2

n(cid:19)(cid:16) px
k (cid:17)n

.

Such an identity makes no sense over Q or R for convergence reasons, but it will work
ﬁne over the Qp, which is all we need.

§27.2 Algebraic perspective

Prototypical example for this section: 1/2 = 1 + 3 + 32 + 33 + ··· ∈ Z3.

We now construct Zp and Qp. I promised earlier that a p-adic integer will let you look

at “all residues modulo pe” at once. This deﬁnition will formalize this.

§27.2.i Deﬁnition of Zp
Deﬁnition 27.2.1 (Introducing Zp). A p-adic integer is a sequence

x = (x1 mod p, x2 mod p2, x3 mod p3, . . . )

of residues xe modulo pe for each integer e, satisfying the compatibility relations xi ≡ xj
(mod pi) for i < j.
The set Zp of p-adic integers forms a ring under component-wise addition and multipli-

cation.

1To draw an analogy: the equation a2 + b2 + c2 + d2 = −1 has no integer solutions, because, well, squares
are nonnegative. But you will ﬁnd that this equation has solutions modulo any prime p, because once
you take modulo p you stop being able to talk about numbers being nonnegative. The same thing
will happen if we work in p-adics: the above equation has a solution in Zp for every prime p.

27 Bonus: A hint of p-adic numbers

293

Example 27.2.2 (Some 3-adic integers)
Let p = 3. Every usual integer n generates a (compatible) sequence of residues
modulo pe for each e, so we can view each ordinary integer as p-adic one:

50 = (2 mod 3, 5 mod 9, 23 mod 27, 50 mod 81, 50 mod 243, . . . ) .

On the other hand, there are sequences of residues which do not correspond to any
usual integer despite satisfying compatibility relations, such as

(1 mod 3, 4 mod 9, 13 mod 27, 40 mod 81, . . . )

which can be thought of as x = 1 + p + p2 + . . . .

In this way we get an injective map

Z (cid:44)→ Zp

n (cid:55)→(cid:0)n mod p, n mod p2, n mod p3, . . .(cid:1)

which is not surjective. So there are more p-adic integers than usual integers.

(Remark for experts: those of you familiar with category theory might recognize that

this deﬁnition can be written concisely as

where the inverse limit is taken across e ≥ 1.)

Zp := lim←− Z/peZ

Exercise 27.2.3. Check that Zp is an integral domain.

§27.2.ii Base p expansion

Here is another way to think about p-adic integers using “base p”. As in the example
earlier, every usual integer can be written in base p, for example
50 = 12123 = 2 · 30 + 1 · 31 + 2 · 32 + 1 · 33.

More generally, given any x = (x1, . . . ) ∈ Zp, we can write down a “base p” expansion
in the sense that there are exactly p choices of xk given xk−1. Continuing the example
earlier, we would write

(1 mod 3, 4 mod 9, 13 mod 27, 40 mod 81, . . . ) = 1 + 3 + 32 + . . .

= . . . 11113

and in general we can write

x =(cid:88)k≥0

akpk = . . . a2a1a0p

where ak ∈ {0, . . . , p − 1}, such that the equation holds modulo pe for each e. Note the
expansion is inﬁnite to the left, which is diﬀerent from what you’re used to.
(Amusingly, negative integers also have inﬁnite base p expansions: −4 = . . . 2222123,
Thus you may often hear the advertisement that a p-adic integer is an “possibly inﬁnite
base p expansion”. This is correct, but later on we’ll be thinking of Zp in a more and
more “analytic” way, and so I prefer to think of this as

corresponding to (2 mod 3, 5 mod 9, 23 mod 27, 77 mod 81 . . . ).)

294

Napkin, by Evan Chen (v1.5.20190718)

p-adic integers are Taylor series with base p.

Indeed, much of your intuition from generating functions K[[X]] (where K is a ﬁeld) will
carry over to Zp.

§27.2.iii Constructing Qp
Here is one way in which your intuition from generating functions carries over:

Proposition 27.2.4 (Non-multiples of p are all invertible)
The number x ∈ Zp is invertible if and only if x1 (cid:54)= 0. In symbols,

x ∈ Z×p ⇐⇒ x (cid:54)≡ 0

(mod p).

Contrast this with the corresponding statement for K[[X]]: a generating function F ∈
K[[X]] is invertible iﬀ F (0) (cid:54)= 0.
Proof. If x ≡ 0 (mod p) then x1 = 0, so clearly not invertible. Otherwise, xe (cid:54)≡ 0
(mod p) for all e, so we can take an inverse ye modulo pe, with xeye ≡ 1 (mod pe). As
the ye are themselves compatible, the element (y1, y2, . . . ) is an inverse.

Example 27.2.5 (We have − 1
We claim the earlier example is actually

2 = . . . 11113 ∈ Z3)

1
2

−

= (1 mod 3, 4 mod 9, 13 mod 27, 40 mod 81, . . . ) = 1 + 3 + 32 + . . .

= . . . 11113.

Indeed, multiplying it by −2 gives

(−2 mod 3, −8 mod 9, −26 mod 27, −80 mod 81, . . . ) = 1.

(Compare this with the “geometric series” 1 + 3 + 32 + ··· = 1
able to formalize this later, but not yet.)

1−3 . We’ll actually be

Remark 27.2.6 ( 1
2 is an integer for p > 2) — The earlier proposition implies that
1
2 ∈ Z3 (among other things); your intuition about what is an “integer” is diﬀerent
here! In olympiad terms, we already knew 1
2 (mod 3) made sense, which is why
calling 1
2 an “integer” in the 3-adics is correct, even though it doesn’t correspond to
any element of Z.

Exercise 27.2.7 (Unimportant but tricky). Rational numbers correspond exactly to even-
tually periodic base p expansions.

With this observation, here is now the deﬁnition of Qp.

Deﬁnition 27.2.8 (Introducing Qp). Since Zp is an integral domain, we let Qp denote
its ﬁeld of fractions. These are the p-adic numbers.

27 Bonus: A hint of p-adic numbers

295

Continuing our generating functions analogy:

Zp is to Qp

as K[[X]] is to K((X)).

This means

Qp can be thought of as Laurent series with base p.

and in particular according to the earlier proposition we deduce:

Proposition 27.2.9 (Qp looks like formal Laurent series)
Every nonzero element of Qp is uniquely of the form

pku

where k ∈ Z, u ∈ Z×p .

Thus, continuing our base p analogy, elements of Qp are in bijection with “Laurent series”

(cid:88)k≥−n

akpk = . . . a2a1a0.a−1a−2 . . . a−np

for ak ∈ {0, . . . , p − 1}. So the base p representations of elements of Qp can be thought
of as the same as usual, but extending inﬁnitely far to the left (rather than to the right).

Remark 27.2.10 (Warning) — The ﬁeld Qp has characteristic zero, not p.

Remark 27.2.11 (Warning on fraction ﬁeld) — This result implies that you shouldn’t
think about elements of Qp as x/y (for x, y ∈ Zp) in practice, even though this is the
oﬃcial deﬁnition (and what you’d expect from the name Qp). The only denominators
you need are powers of p.

To keep pushing the formal Laurent series analogy, K((X)) is usually not thought
of as quotient of generating functions but rather as “formal series with some negative
exponents”. You should apply the same intuition on Qp.

Remark 27.2.12 — At this point I want to make a remark about the fact 1/p ∈ Qp,
connecting it to the wish-list of properties I had before. In elementary number theory
you can take equations modulo p, but if you do the quantity n/p mod p doesn’t
make sense unless you know n mod p2. You can’t ﬁx this by just taking modulo
p2 since then you need n mod p3 to get n/p mod p2, ad inﬁnitum. You can work
around issues like this, but the nice feature of Zp and Qp is that you have modulo pe
information for “all e at once”: the information of x ∈ Qp packages all the modulo
pe information simultaneously. So you can divide by p with no repercussions.

§27.3 Analytic perspective

§27.3.i Deﬁnition

Up until now we’ve been thinking about things mostly algebraically, but moving forward
it will be helpful to start using the language of analysis. Usually, two real numbers are

296

Napkin, by Evan Chen (v1.5.20190718)

considered “close” if they are close on the number of line, but for p-adic purposes we
only care about modulo pe information. So, we’ll instead think of two elements of Zp or
Qp as “close” if they diﬀer by a large multiple of pe.

For this we’ll borrow the familiar νp from elementary number theory.

Deﬁnition 27.3.1 (p-adic valuation and absolute value). We deﬁne the p-adic valua-
tion νp : Q×p → Z in the following two equivalent ways:

 For x = (x1, x2, . . . ) ∈ Zp we let νp(x) be the largest e such that xe ≡ 0 (mod pe)
(or e = 0 if x ∈ Z×p ). Then extend to all of Q×p by νp(xy) = νp(x) + νp(y).
 Each x ∈ Q×p can be written uniquely as pku for u ∈ Z×p , k ∈ Z. We let νp(x) = k.
By convention we set νp(0) = +∞. Finally, deﬁne the p-adic absolute value |•|p by

|x|p = p−νp(x).

In particular |0|p = 0.

This fulﬁlls the promise that x and y are close if they look the same modulo pe for

large e; in that case νp(x − y) is large and accordingly |x − y|p is small.
§27.3.ii Ultrametric space
In this way, Qp and Zp becomes a metric space with metric given by |x − y|p.

Exercise 27.3.2. Suppose f : Zp → Qp is continuous and f (n) = (−1)n for every n ∈ Z≥0.
Prove that p = 2.

In fact, these spaces satisfy a stronger form of the triangle inequality than you are

used to from R.

Proposition 27.3.3 (|•|p is an ultrametric)
For any x, y ∈ Zp, we have the strong triangle inequality

|x + y|p ≤ max(cid:110)|x|p ,|y|p(cid:111) .

Equality holds if (but not only if) |x|p (cid:54)= |y|p.

However, Qp is more than just a metric space: it is a ﬁeld, with its own addition and
multiplication. This means we can do analysis just like in R or C: basically, any notion
such as “continuous function”, “convergent series”, et cetera has a p-adic analog. In
particular, we can deﬁne what it means for an inﬁnite sum to converge:

Deﬁnition 27.3.4 (Convergence notions). Here are some examples of p-adic analogs of
“real-world” notions.

 A sequence s1, . . . converges to a limit L if limn→∞ |sn − L|p = 0.

 The inﬁnite series(cid:80)k xk converges if the sequence of partial sums s1 = x1, s2 =

x1 + x2, . . . , converges to some limit.

 . . . et cetera . . .

27 Bonus: A hint of p-adic numbers

297

With this deﬁnition in place, the “base p” discussion we had earlier is now true in the

analytic sense: if x = . . . a2a1a0p ∈ Zp then

akpk

converges to x.

∞(cid:88)k=0

Indeed, the nth partial sum is divisible by pn, hence the partial sums approach x as
n → ∞.
be true. For example, in Qp convergence of partial sums is simpler:

While the deﬁnitions are all the same, there are some changes in properties that should

Proposition 27.3.5 (|xk|p → 0 iﬀ convergence of series)

n = ∞ in R. You can think of this as a consequence of strong

A series(cid:80)∞k=1 xk in Qp converges to some limit if and only if limk→∞ |xk|p = 0.
Contrast this with (cid:80) 1
triangle inequality.
Proof. By multiplying by a large enough power of p, we may assume xk ∈ Zp. (This isn’t
actually necessary, but makes the notation nicer.)
Observe that xk (mod p) must eventually stabilize, since for large enough n we have
|xn|p < 1 ⇐⇒ νp(xn) ≥ 1. So let a1 be the eventual residue modulo p of (cid:80)N
k=0 xk
(mod p) for large N . In the same way let a2 be the eventual residue modulo p2, and so
on. Then one can check we approach the limit a = (a1, a2, . . . ).

§27.3.iii More fun with geometric series

Let’s ﬁnally state the p-adic analog of the geometric series formula.

Proposition 27.3.6 (Geometric series)
Let x ∈ Zp with |x|p < 1. Then
1

= 1 + x + x2 + x3 + . . . .

1 − x

Proof. Note that the partial sums satisfy 1 + x + x2 + ··· + xn = 1−xn
n → ∞ since |x|p < 1.

1−x , and xn → 0 as

So, 1 + 3 + 32 + ··· = − 1
If you buy the analogy that Zp is generating functions with base p, then all the olympiad
generating functions you might be used to have p-adic analogs. For example, you can
prove more generally that:

2 is really a correct convergence in Z3. And so on.

Theorem 27.3.7 (Generalized binomial theorem)
If x ∈ Zp and |x|p < 1, then for any r ∈ Q we have the series convergence

(cid:88)n≥0(cid:18)r

n(cid:19)xn = (1 + x)r.

(I haven’t deﬁned (1 + x)r, but it has the properties you expect.)

298

Napkin, by Evan Chen (v1.5.20190718)

§27.3.iv Completeness

Note that the deﬁnition of |•|p could have been given for Q as well; we didn’t need Qp to
introduce it (after all, we have νp in olympiads already). The big important theorem I
must state now is:

Theorem 27.3.8 (Qp is complete)
The space Qp is the completion of Q with respect to |•|p.

This is the deﬁnition of Qp you’ll see more frequently; one then deﬁnes Zp in terms of Qp
(rather than vice-versa) according to

Zp =(cid:110)x ∈ Qp : |x|p ≤ 1(cid:111) .

§27.3.v Philosophical notes

Let me justify why this deﬁnition is philosophically nice. Suppose you are an ancient
Greek mathematician who is given:

Problem for Ancient Greeks. Estimate the value of the sum

S =

1
12 +

1
22 + ··· +

1

100002

to within 0.001.

The sum S consists entirely of rational numbers, so the problem statement would be fair
game for ancient Greece. But it turns out that in order to get a good estimate, it really
helps if you know about the real numbers: because then you can construct the inﬁnite
6 , up to some small error term from the

6 π2, and deduce that S ≈ π2

series(cid:80)n≥1 n−2 = 1

terms past

1

100012 , which can be bounded.

Of course, in order to have access to enough theory to prove that S = π2/6, you need
to have the real numbers; it’s impossible to do calculus in Q (the sequence 1, 1.4, 1.41,
1.414, is considered “not convergent”!)

Now fast-forward to 2002, and suppose you are given

Problem from USA TST 2002. Estimate the sum

fp(x) =

1

(px + k)2

p−1(cid:88)k=1

to within mod p3.

Even though fp(x) is a rational number, it still helps to be able to do analysis with
inﬁnite sums, and then bound the error term (i.e. take mod p3). But the space Q is not
complete with respect to |•|p either, and thus it makes sense to work in the completion
of Q with respect to |•|p. This is exactly Qp.

In any case, let’s ﬁnally solve Example 27.1.1.

27 Bonus: A hint of p-adic numbers

299

Example 27.3.9 (USA TST 2002)
We will now compute

fp(x) =

1

(px + k)2

p−1(cid:88)k=1

(mod p3).

Armed with the generalized binomial theorem, this becomes straightforward.

fp(x) =

1

1

px

(px + k)2 =

k (cid:17)−2

p−1(cid:88)k=1
p−1(cid:88)k=1
=(cid:88)n≥0(cid:18)−2
p−1(cid:88)k=1

k2(cid:16)1 +
p−1(cid:88)k=1
n(cid:19)(cid:16) px
k2(cid:88)n≥0(cid:18)−2
k (cid:17)n
n(cid:19) p−1(cid:88)k=1
k2(cid:16) x
k(cid:17)n
k3(cid:33) p + 3x2(cid:32)p−1(cid:88)k=1
k2 − 2x(cid:32)p−1(cid:88)k=1

1

1

=

≡

1

pn

1

1

k4(cid:33) p2

(mod p3).

Using the elementary facts that p2 |(cid:80)k k−3 and p |(cid:80)k k−4, this solves the problem.

§27.4 Mahler coeﬃcients

One of the big surprises of p-adic analysis is that:

We can basically describe all continuous functions Zp → Qp.

They are given by a basis of functions

(cid:18)x
n(cid:19) :=

x(x − 1) . . . (x − (n − 1))

n!

in the following way.

Theorem 27.4.1 (Mahler; see [Sc07, Theorem 51.1, Exercise 51.b])
Let f : Zp → Qp be continuous, and deﬁne

(27.1)

Then limn an = 0 and

an =

n(cid:88)k=0(cid:18)n
f (x) =(cid:88)n≥0

k(cid:19)(−1)n−kf (n).
an(cid:18)x
n(cid:19).

Conversely, if an is any sequence converging to zero, then f (x) = (cid:80)n≥0 an(cid:0)x
n(cid:1)

deﬁnes a continuous function satisfying (27.1).

The ai are called the Mahler coeﬃcients of f .

300

Napkin, by Evan Chen (v1.5.20190718)

Exercise 27.4.2. Last post we proved that if f : Zp → Qp is continuous and f (n) = (−1)n
for every n ∈ Z≥0 then p = 2. Re-prove this using Mahler’s theorem, and this time show
conversely that a unique such f exists when p = 2.

You’ll note that these are the same ﬁnite diﬀerences that one uses on polynomials in

high school math contests, which is why they are also called “Mahler diﬀerences”.

a0 = f (0)
a1 = f (1) − f (0)
a2 = f (2) − 2f (1) − f (0)
a3 = f (3) − 3f (2) + 3f (1) − f (0).

Thus one can think of an → 0 as saying that the values of f (0), f (1), . . . behave like a
polynomial modulo pe for every e ≥ 0.

The notion “analytic” also has a Mahler interpretation. First, the deﬁnition.

Deﬁnition 27.4.3. We say that a function f : Zp → Qp is analytic if it has a power

series expansion (cid:88)n≥0

cnxn

cn ∈ Qp

converging for x ∈ Zp.

Theorem 27.4.4 ([Sc07, Theorem 54.4])

The function f (x) =(cid:80)n≥0 an(cid:0)x

n(cid:1) is analytic if and only if

an
n!

lim
n→∞

= 0.

Analytic functions also satisfy the following niceness result:

Theorem 27.4.5 (Strassmann’s theorem)
Let f : Zp → Qp be analytic. Then f has ﬁnitely many zeros.

Te give an application of these results, we will prove the following result, which was

interesting even before p-adics came along!

Theorem 27.4.6 (Skolem-Mahler-Lech)
Let (xi)i≥0 be an integral linear recurrence, meaning (xi)i≥0 is a sequence of integers

xn = c1xn−1 + c2xn−2 + ··· + ckxn−k

n = 1, 2, . . .

holds for some choice of integers c1, . . . , ck. Then the set of indices {i | xi = 0} is
eventually periodic.

Proof. According to the theory of linear recurrences, there exists a matrix A such that
we can write xi as a dot product

xi =(cid:10)Aiu, v(cid:11) .

27 Bonus: A hint of p-adic numbers

301

Let p be a prime not dividing det A. Let T be an integer such that AT ≡ id (mod p)
(with id denoting the identity matrix).

Fix any 0 ≤ r < N . We will prove that either all the terms

f (n) = xnT +r

n = 0, 1, . . .

are zero, or at most ﬁnitely many of them are. This will conclude the proof.

Let AT = id + pB for some integer matrix B. We have

f (n) =(cid:10)AnT +ru, v(cid:11) = (cid:104)(id + pB)nAru, v(cid:105)

=(cid:88)k≥0(cid:18)n
k(cid:19) · pn (cid:104)BnAru, v(cid:105)
an(cid:18)n
k(cid:19)
=(cid:88)k≥0

where an = pn (cid:104)BnAru, v(cid:105) ∈ pnZ.

Thus we have written f in Mahler form. Initially, we deﬁne f : Z≥0 → Z, but by Mahler’s
theorem (since limn an = 0) it follows that f extends to a function f : Zp → Qp. Also,
we can check that limn
Thus by Strassman’s theorem, f is either identically zero, or else it has ﬁnitely many

an
n! = 0 hence f is even analytic.

zeros, as desired.

§27.5 A few harder problems to think about

Problem 27A† (Zp is compact). Show that Qp is not compact, but Zp is. (For the
latter, I recommend using sequential continuity.)

Problem 27B† (Totally disconnected). Show that both Zp and Qp are totally discon-
nected : there are no connected sets other than the empty set and singleton sets.

Problem 27C (USA TST 2011). Let p be a prime. We say that a sequence of integers
{zn}∞n=0 is a p-pod if for each e ≥ 0, there is an N ≥ 0 such that whenever m ≥ N , pe
divides the sum

(−1)k(cid:18)m
m(cid:88)k=0

k(cid:19)zk.

Prove that if both sequences {xn}∞n=0 and {yn}∞n=0 are p-pods, then the sequence
{xnyn}∞n=0 is a p-pod.

28 Diﬀerentiation

§28.1 Deﬁnition

Prototypical example for this section: x3 has derivative 3x2.

I suspect most of you have seen this before, but:

Deﬁnition 28.1.1. Let U be an open subset1 of R and let f : U → R be a function. Let
p ∈ U . We say f is diﬀerentiable at p if the limit2
f (p + h) − f (p)

lim
h→0

h

exists. If so, we denote its value by f(cid:48)(p) and refer to this as the derivative of f at p.
The function f is diﬀerentiable if it is diﬀerentiable at every point. In that case, we

regard the derivative f(cid:48) : (a, b) → R as a function it its own right.

Exercise 28.1.2. Show that if f is diﬀerentiable at p then it is continuous at p too.

Here is the picture. Suppose f : R → R is diﬀerentiable (hence continuous). We draw
a graph of f in the usual way and consider values of h. For any nonzero h, what we get
is the slope of the secant line joining (p, f (p)) to (p + h, f (p + h)). However, as h gets
close to zero, that secant line begins to approach a line which is tangent to the graph of
the curve. A picture with f a parabola is shown below, with the tangent in red, and the
secant in dashed green.

So the picture in your head should be that

f(cid:48)(p) looks like the slope of the tangent line at (p, f (p)).

1We will almost always use U = (a, b) or U = R, and you will not lose much by restricting the deﬁnition

2Remember we are following the convention in Abuse of Notation 26.6.6. So we mean “the limit of the
except the value at h = 0 can be anything”. And this is important because

to those.
function h (cid:55)→ f (p+h)−f (p)
that fraction does not have a deﬁnition at h = 0. As promised, we pay this no attention.

h

303

yx(p,f(p))Slopef′(p)(p+h,f(p+h))Slopef(p+h)−f(p)h304

Napkin, by Evan Chen (v1.5.20190718)

Remark 28.1.3 — Note that the derivatives are deﬁned for functions on open
intervals. This is important. If f : [a, b] → R for example, we could still deﬁne the
derivative at each interior point, but f (a) no longer makes sense since f is not given
a value on any open neighborhood of a.

Let’s do one computation and get on with this.

Example 28.1.4 (Derivative of x3 is 3x2)
Let f : R → R by f (x) = x3. For any point p, and nonzero h we can compute

f (p + h) − f (p)

h

=

=

(p + h)3 − p3
3p2h + 3ph2 + h3

h

h

= 3p2 + 3ph + h2.

Thus,

f (p + h) − f (p)

h

lim
h→0

(3p2 + 3ph + h2) = 3p2.

= lim
h→0

Thus the slope at each point of f is given by the formula 3p2. It is customary to
then write f(cid:48)(x) = 3x2 as the derivative of the entire function f .

Abuse of Notation 28.1.5. We will now be sloppy and write this as (x3)(cid:48) = 3x2. This
is shorthand for the signiﬁcantly more verbose “the real-valued function x3 on domain
so-and-so has derivative 3p2 at every point p in its domain”.

In general, a real-valued diﬀerentiable function f : U → R naturally gives rise to
derivative f(cid:48)(p) at every point p ∈ U , so it is customary to just give up on p altogether
and treat f(cid:48) as function itself U → R, even though this real number is of a “diﬀerent
interpretation”: f(cid:48)(p) is meant to interpret a slope (e.g. your hourly pay rate) as opposed
to a value (e.g. your total dollar worth at time t). If f is a function from real life, the
units do not even match!

This convention is so deeply entrenched I cannot uproot it without more confusion
than it is worth. But if you read the chapters on multivariable calculus you will see how
it comes back to bite us, when I need to re-deﬁne the derivative to be a linear mas, rather
than single real numbers.

§28.2 How to compute them

Same old, right? Sum rule, all that jazz.

Theorem 28.2.1 (Your friendly high school calculus rules)
In what follows f and g are diﬀerentiable functions, and U , V are open subsets of R.

 (Sum rule) If f, g : U → R then then (f + g)(cid:48)(x) = f(cid:48)(x) + g(cid:48)(x).
 (Product rule) If f, g : U → R then then (f · g)(cid:48)(x) = f(cid:48)(x)g(x) + f (x)g(cid:48)(x).
 (Chain rule) If f : U → V and g : V → R then the derivative of the composed

function g ◦ f : U → R is g(cid:48)(f (x)) · f(cid:48)(x).

28 Diﬀerentiation

305

Proof.

 Sum rule: trivial, do it yourself if you care.

 Product rule: for every nonzero h and point p ∈ U we may write

f (p + h)g(p + h) − f (p)g(p)

h

=

f (p + h) − f (p)

h

· g(p + h) +

g(p + h) − g(p)

h

· f (p)

which as h → 0 gives the desired expression.

 Chain rule: this is where Abuse of Notation 26.6.6 will actually bite us. Let p ∈ U ,

q = f (p) ∈ V , so that

(g ◦ f )(cid:48)(p) = lim
h→0

g(f (p + h)) − g(q)

h

.

We would like to write the expression in the limit as

g(f (p + h)) − g(q)

h

=

g(f (p + h)) − g(q)

f (p + h) − q

·

f (p + h) − f (p)

h

.

The problem is that the denominator f (p + h) − f (p) might be zero. So instead,
we deﬁne the expression

Q(y) =(cid:40) g(y)−g(q)

y−q
g(cid:48)(q)

if y (cid:54)= q
if y = q

which is continuous since g was diﬀerentiable at q. Then, we do have the equality

g(f (p + h)) − g(q)

h

= Q (f (p + h)) ·

f (p + h) − f (p)

h

.

because if f (p + h) = q with h (cid:54)= 0, then both sides are equal to zero anyways.
Then, in the limit as h → 0, we have limh→0
h)) = Q(q) = g(cid:48)(q) by continuity. This was the desired result.

= f(cid:48)(p), while limh→0 Q(f (p+

f (p+h)−f (p)

h

Exercise 28.2.2. Compute the derivative of the polynomial f (x) = x3 +10x2 +2019, viewed
as a function f : R → R.

Remark 28.2.3 — Quick linguistic point: the theorems above all hold at each
individual point. For example the sum rule really should say that if f, g : U → R are
diﬀerentiable at the point p then so is f + g and the derivative equals f(cid:48)(p) + g(cid:48)(p).
Thus f and g are diﬀerentiable on all of U , then it of course follows that (f + g)(cid:48) =
f(cid:48) + g(cid:48). So each of the above rules has a “point-by-point” form which then implies
the “whole U ” form.

We only state the latter since that is what is used in practice. However, in the
rare situations where you have a function diﬀerentiable only at certain points of U
rather than the whole interval U , you can still use the below.

We next list some derivatives of well-known functions, but as we do not give rigorous

deﬁnitions of these functions, we do not prove these here.

306

Napkin, by Evan Chen (v1.5.20190718)

Proposition 28.2.4 (Derivatives of some well-known functions)

 The exponential function exp : R → R deﬁned by exp(x) = ex is its own

derivative.

 The trig functions sin and cos have sin(cid:48) = cos, cos(cid:48) = − sin.

Example 28.2.5 (A typical high-school calculus question)
This means that you can mechanically compute the derivatives of any artiﬁcial
function obtained by using the above, which makes it a great source of busy work in
American high schools and universities. For example, if

f (x) = ex + x sin(x2)

f : R → R

then one can compute f(cid:48) by:

f(cid:48)(x) = (ex)(cid:48) + (x sin(x2))(cid:48)

= ex + (x sin(x2))(cid:48)
= ex + (x)(cid:48) sin(x2) + x(sin(x2))(cid:48)
= ex + sin(x2) + x(sin(x2))(cid:48)
= ex + sin(x2) + x · 2x · cos(x2)

sum rule

above table

product rule
(x)(cid:48) = 1
chain rule.

Of course, this function f is totally artiﬁcial and has no meaning, which is why calculus
is the topic of widespread scorn in the USA. That said, it is worth appreciating that
calculations like this are possible: it would be better to write the pseudo-theorem
“derivatives can actually be computed”.

If we take for granted that (ex)(cid:48) = ex, then we can derive two more useful functions to

add to our library of functions we can diﬀerentiate.

Corollary 28.2.6 (Power rule)
Let r be a real number. The function R>0 → R by x (cid:55)→ xr has derivative (xr)(cid:48) =
rxr−1.

Proof. We knew this for integers r already, but now we can prove it for any positive real
number r. Write

f (x) = xr = er log x

considered as a function f : R>0 → R. The chain rule (together with the fact that
(ex)(cid:48) = ex) now gives

f(cid:48)(x) = er log x · (r log x)(cid:48)
= xr ·

= er log x ·

r
x

r
x

= rxr−1.

The reason we don’t prove the formulas for ex and log x is that we don’t at the moment
even have a rigorous deﬁnition for either, or even for 2x if x is not rational. However it’s
nice to know that some things imply the other.

28 Diﬀerentiation

307

Corollary 28.2.7 (Derivative of log is 1/x)
The function log : R>0 → R has derivative (log x)(cid:48) = 1/x.

Proof. We have that x = elog x. Diﬀerentiate both sides, and again use the chain rule3

1 = elog x · (log x)(cid:48).

Thus (log x)(cid:48) = 1

elog x = 1/x.

§28.3 Local (and global) maximums

Prototypical example for this section: Horizontal tangent lines to the parabola are typically
good pictures.

You may remember from high school that one classical use of calculus was to extract
the minimum or maximum values of functions. We will give a rigorous description of
how to do this here.
Deﬁnition 28.3.1. Let f : U → R be a function. A local maximum is a point p ∈ U
such that there exists an open neighborhood V of p (contained inside U ) such that
f (p) ≥ f (x) for every x ∈ V .

A local minimum is deﬁned similarly.4

Deﬁnition 28.3.2. A point p is a local extrema if it satisﬁes either of these.

The nice thing about derivatives is that they pick up all extrema.

Theorem 28.3.3 (Fermat’s theorem on stationary points)
Suppose f : U → R is diﬀerentiable and p ∈ U is a local extrema. Then f(cid:48)(p) = 0.

If you draw a picture, this result is not surprising.

(Note also: the converse is not true. Say, f (x) = x2019 has f(cid:48)(0) = 0 but x = 0 is not a
local extrema for f .)

Proof. Assume for contradiction f(cid:48)(p) > 0. Choose any ε > 0 with ε < f(cid:48)(p). Then for
suﬃciently small |h| we should have

f (p + h) − f (p)

h

> ε.

3There is actually a small subtlety here: we are taking for granted that log is diﬀerentiable.
4Equivalently, it is a local maximum of −f .

yx(p,f(p))308

Napkin, by Evan Chen (v1.5.20190718)

In particular f (p + h) > f (p) for h > 0 while f (p − h) < f (p) for h < 0. So p is not a
local extremum.

The proof for f(cid:48)(p) < 0 is similar.

However, this is not actually adequate if we want a complete method for optimization.
The issue is that we seek global extrema, which may not even exist: for example f (x) = x
(which has f(cid:48)(x) = 1) obviously has no local extrema at all. The key to resolving this is
to use compactness: we change the domain to be a compact set Z, for which we know
that f will achieve some global maximum. The set Z will naturally have some interior
S, and calculus will give us all the extrema within S. Then we manually check all cases
outside Z.

Let’s see two extended examples. The one is simple, and you probably already know
about it, but I want to show you how to use compactness to argue thoroughly, and how
the “boundary” points naturally show up.

Example 28.3.4 (Rectangle area optimization)
Suppose we consider rectangles with perimeter 20 and want the rectangle with the
smallest or largest area.

If we choose the legs of the rectangle to be x and 10 − x, then we are trying to
optimize the function

f (x) = x(10 − x) = 10x − x2

f : [0, 10] → R.

By compactness, there exists some global maximum and some global minimum.

As f is diﬀerentiable on (0, 10), we ﬁnd that for any p ∈ (0, 10), a global maximum

will be a local maximum too, and hence should satisfy

0 = f(cid:48)(p) = 10 − 2p =⇒ p = 5.

Also, the points x = 0 and x = 10 lie in the domain but not the interior (0, 10).
Therefore the global extrema (in addition to existing) must be among the three
suspects {0, 5, 10}.
largest area and the degenerate rectangles have the smallest (zero) area.

We ﬁnally check f (0) = 0, f (5) = 25, f (10) = 0. So the 5 × 5 square has the

Here is a non-elementary example.

Proposition 28.3.5 (ex ≥ 1 + x)
For all real numbers x we have ex ≥ 1 + x.

Proof. Deﬁne the diﬀerentiable function

f (x) = ex − (x + 1)

f : R → R.

10−xx28 Diﬀerentiation

309

Consider the compact interval Z = [−1, 100].
If x ≤ −1 then obviously f (x) > 0.
Similarly if x ≥ 100 then obviously f (x) > 0 too. So we just want to prove that if x ∈ Z,
we have f (x) ≥ 0.

Indeed, there exists some global minimum p. It could be the endpoints −1 or 100.

Otherwise, if it lies in U = (−1, 100) then it would have to satisfy

0 = f(cid:48)(p) = ep − 1 =⇒ p = 0.

As f (−1) > 0, f (100) > 0, f (0) = 0, we conclude p = 0 is the global minimum of Z; and
hence f (x) ≥ 0 for all x ∈ Z, hence for all x.

Remark 28.3.6 — If you are willing to use limits at ±∞, you can rewrite proofs
like the above in such a way that you don’t have to explicitly come up with endpoints
like −1 or 100. We won’t do so here, but it’s nice food for thought.

§28.4 Rolle and friends

Prototypical example for this section: The racetrack principle, perhaps?

One corollary of the work in the previous section is Rolle’s theorem.

Theorem 28.4.1 (Rolle’s theorem)
Suppose f : [a, b] → R is a continuous function, which is diﬀerentiable on the open
interval (a, b), such that f (a) = f (b). Then there is a point c ∈ (a, b) such that
f(cid:48)(c) = 0.

Proof. Assume f is nonconstant (otherwise any c works). By compactness, there exists
both a global maximum and minimum. As f (a) = f (b), either the global maximum or
the global minimum must lie inside the open interval (a, b), and then Fermat’s theorem
on stationary points ﬁnishes.

I was going to draw a picture until I realized xkcd #2042 has one already.

310

Napkin, by Evan Chen (v1.5.20190718)

One can adapt the theorem as follows.

Image from [Mu]

Theorem 28.4.2 (Mean value theorem)
Suppose f : [a, b] → R is a continuous function, which is diﬀerentiable on the open
interval (a, b). Then there is a point c ∈ (a, b) such that

f(cid:48)(c) =

f (b) − f (a)

b − a

.

Pictorially, there is a c such that the tangent at c has the same slope as the secant
joining (a, f (a)), to (b, f (b)); and Rolle’s theorem is the special case where that secant is
horizontal.

Proof of mean value theorem. Let s = f (b)−f (a)

b−a

be the slope of the secant line, and deﬁne

g(x) = f (x) − sx

yx(a,f(a))(b,f(b))Slopef(b)−f(a)b−aab(c,f(c))28 Diﬀerentiation

which intuitively shears f downwards so that the secant becomes vertical.
g(a) = g(b) now, so we apply Rolle’s theorem to g.

311

In fact

Remark 28.4.3 (For people with driver’s licenses) — There is a nice real-life inter-
pretation of this I should mention. A car is travelling along a one-dimensional road
(with f (t) denoting the position at time t). Suppose you cover 900 kilometers in
your car over the course of 5 hours (say f (0) = 0, f (5) = 900). Then there is some
point at time in which your speed at that moment was exactly 180 kilometers per
hour, and so you cannot really complain when the cops pull you over for speeding.

The mean value theorem is important because it lets you relate use derivative
information to get information about the function in a way that is really not
possible without it. Here is one quick application to illustrate my point:

Proposition 28.4.4 (Racetrack principle)
Let f, g : R → R be two diﬀerentiable functions with f (0) = g(0).
(a) If f(cid:48)(x) ≥ g(cid:48)(x) for every x > 0, then f (x) ≥ g(x) for every x > 0.
(b) If f(cid:48)(x) > g(cid:48)(x) for every x > 0, then f (x) > g(x) for every x > 0.

This proposition might seem obvious. You can think of it as a race track for a reason:
if f and g denote the positions of two cars (or horses etc) and the ﬁrst car is always faster
than the second car, then the ﬁrst car should end up ahead of the second car. At a special
case g = 0, this says that if f(cid:48)(x) ≥ 0, i.e. “f is increasing”, then, well, f (x) ≥ f (0) for
x > 0, which had better be true. However, if you try to prove this by deﬁnition from
derivatives, you will ﬁnd that it is not easy! However, it’s almost a prototype for the
mean value theorem.

Proof of racetrack principle. We prove (a). Let h = f − g, so h(0) = 0. Assume for
contradiction h(p) < 0 for some p > 0. Then the secant joining (0, h(0)) to (p, h(p)) has
negative slope; in other words by mean value theorem there is a 0 < c < p such that

f(cid:48)(c) − g(cid:48)(c) = h(cid:48)(c) =

h(p) − h(0)

p

=

h(p)

p

< 0

so f(cid:48)(c) < g(cid:48)(c), contradiction. Part (b) is the same.

Sometimes you will be faced with two functions which you cannot easily decouple; the

following form may be more useful in that case.

Theorem 28.4.5 (Ratio mean value theorem)
Let f, g : [a, b] → R be two continuous functions which are diﬀerentiable on (a, b),
and such that g(a) (cid:54)= g(b). Then there is a c ∈ (a, b) such that g(cid:48)(c) (cid:54)= 0 and

f(cid:48)(c)
g(cid:48)(c)

=

f (b) − f (a)
g(b) − g(a)

.

Proof. Use Rolle’s theorem on the function

h(x) = [f (x) − f (a)] [g(b) − g(a)] − [g(x) − g(a)] [f (b) − f (a)] .

312

Napkin, by Evan Chen (v1.5.20190718)

Remark 28.4.6 — You can capture the case g(a) = g(b) as well if you are willing to
write the conclusion in the less intuitive form g(cid:48)(c) [f (b) − f (a)] = f(cid:48)(c) [g(b) − g(a)].
In the event g(a) = g(b) then this is just the mean value theorem for g, and the data
of f is irrelevant.

§28.5 Smooth functions

Prototypical example for this section: All the functions you’re used to.

Let f : U → R be diﬀerentiable, thus giving us a function f(cid:48) : U → R. If our initial
function was nice enough, then we can take the derivative again, giving a function
f(cid:48)(cid:48) : U → R, and so on. In general, after taking the derivative n times, we denote the
resulting function by f (n). By convention, f (0) = f .

Deﬁnition 28.5.1. A function f : U → R is smooth if it is inﬁnitely diﬀerentiable; that
is the function f (n) exists for all n.

Question 28.5.2. Show that the absolute value function is not smooth.

Most of the functions we encounter, such as polynomials, ex, log, sin, cos are smooth,
and so are their compositions. Here is a weird example which we’ll grow more next
time.

Example 28.5.3 (A smooth function with all derivatives zero)
Consider the function

f (x) =(cid:40)e−1/x x > 0

0

x ≤ 0.

This function can be shown to be smooth, with f (n)(0) = 0. So this function has
every derivative at the origin equal to zero, despite being nonconstant!

§28.6 A few harder problems to think about

Problem 28A (Quotient rule). Let f : (a, b) → R and g : (a, b) → R>0 be diﬀerentiable
functions. Let h = f /g be their quotient (also a function (a, b) → R). Show that the
derivative of h is given by

h(cid:48)(x) =

f(cid:48)(x)g(x) − f (x)g(cid:48)(x)

g(x)2

.

Problem 28B. For real numbers x > 0, how small can xx be?

Problem 28C (RMM 2017). Determine whether or not there exist nonconstant polyno-
mials P (x) and Q(x) with real coeﬃcients satisfying

P (x)10 + P (x)9 = Q(x)21 + Q(x)20.

Problem 28D. Let P (x) be a degree n polynomial with real coeﬃcients. Prove that
the equation ex = P (x) has at most n + 1 real solutions in x.

28 Diﬀerentiation

313

Problem 28E (Jensen’s inequality). Let f : (a, b) → R be a twice diﬀerentiable function
such that f(cid:48)(cid:48)(x) ≥ 0 for all x (i.e. f is convex ). Prove that
f (x) + f (y)

f(cid:18) x + y

2 (cid:19) ≤

2

for all real numbers x and y in the interval (a, b).
Problem 28F (L’Hˆopital rule, or at least one case). Let f, g : R → R be diﬀerentiable
functions and let p be a real number. Suppose that

lim
x→p

f (x) = lim
x→p

g(x) = 0.

Prove that

lim
x→p
provided the right-hand limit exists.

f (x)
g(x)

f(cid:48)(x)
g(cid:48)(x)

= lim
x→p

29 Power series and Taylor series

Polynomials are very well-behaved functions, and are studied extensively for that
reason. From an analytic perspective, for example, they are smooth, and their derivatives
are easy to compute.

In this chapter we will study power series, which are literally “inﬁnite polynomials”

(cid:80)n anxn. Armed with our understanding of series and diﬀerentiation, we will see three

great things:

 Many of the functions we see in nature actually are given by power series. Among

them are ex, log x, sin x.

 Their convergence properties are actually quite well behaved: from the string of

coeﬃcients, we can ﬁgure out which x they converge for.

 The derivative of(cid:80)n anxn is actually just(cid:80)n nanxn−1.

§29.1 Motivation

To get the ball rolling, let’s start with one inﬁnite polynomial you’ll recognize: for any
ﬁxed number −1 < x < 1 we have the series convergence
= 1 + x + x2 + . . .

1

by the geometric series formula.

1 − x

Let’s pretend we didn’t see this already in Problem 26D. So, we instead have a smooth

function f : (−1, 1) → R by

f (x) =

1

.

1 − x

Suppose we wanted to pretend that it was equal to an “inﬁnite polynomial” near the
origin, that is

(1 − x)−1 = a0 + a1x + a2x2 + a3x3 + a4x4 + . . . .

How could we ﬁnd that polynomial, if we didn’t already know?

Well, for starters we can ﬁrst note that by plugging in x = 0 we obviously want a0 = 1.
We have derivatives, so actually, we can then diﬀerentiate both sides to obtain that

(1 − x)−2 = a1 + 2a2x + 3a3x2 + 4a4x3.

If we now set x = 0, we get a1 = 1. In fact, let’s keep taking derivatives and see what we
get.

(1 − x)−1 = a0 + a1x + a2x2 + a3x3 + a4x4 + a5x5 + . . .
(1 − x)−2 =
a1 + 2a2x + 3a3x2 + 4a4x3 + 5a5x4 + . . .
2(1 − x)−3 =
2a2 + 6a3x + 12a4x2 + 20a5x3 + . . .
6(1 − x)−4 =
6a3 + 24a4x + 60a5x2 + . . .
24(1 − x)−5 =
24a4 + 120a5x + . . .
... .

315

316

Napkin, by Evan Chen (v1.5.20190718)

If we set x = 0 we ﬁnd 1 = a0 = a1 = a2 = . . . which is what we expect; the geometric
1−x = 1 + x + x2 + . . . . And so actually taking derivatives was enough to get the
series
right claim!

1

§29.2 Power series

Prototypical example for this section:

1−z = 1 + z + z2 + . . . , which converges on (−1, 1).
Of course this is not rigorous, since we haven’t described what the right-hand side
is, much less show that it can be diﬀerentiated term by term. So we deﬁne the main
character now.

1

Deﬁnition 29.2.1. A power series is a sum of the form

anzn = a0 + a1z + a2z2 + . . .

∞(cid:88)n=0

where a0, a1, . . . are real numbers, and z is a variable.

Abuse of Notation 29.2.2 (00 = 1). If you are very careful, you might notice that
when z = 0 and n = 0 we ﬁnd 00 terms appearing. For this chapter the convention is
that they are all equal to one.

answer for this.

Now, if I plug in a particular real number h, then I get a series of real numbers

(cid:80)∞n=0 anhn. So I can ask, when does this series converge? It terms out there is a precise
Deﬁnition 29.2.3. Given a power series(cid:80)∞n=0 anzn, the radius of convergence R is

deﬁned by the formula

1
R

= lim sup

n→∞ |an|1/n .

with the convention that R = 0 if the right-hand side is ∞, and R = ∞ if the right-hand
side is zero.

Theorem 29.2.4 (Cauchy-Hadamard theorem)

Let (cid:80)∞n=0 anzn be a power series with radius of convergence R. Let h be a real

number, and consider the inﬁnite series

anhn

∞(cid:88)n=0

of real numbers. Then:

 The series converges absolutely if |h| < R.
 The series diverges if |h| > R.

Proof. This is not actually hard, but it won’t be essential, so not included.

29 Power series and Taylor series

317

Remark 29.2.5 — In the case |h| = R, it could go either way.

we get R = 1, which is what we expected.

Example 29.2.6 ((cid:80) zn has radius 1)
Consider the geometric series(cid:80)n zn = 1 + z + z2 + . . . . Since an = 1 for every n,
Therefore, if(cid:80)n anzn is a power series with a nonzero radius R > 0 of convergence,

then it can also be thought of as a function
(−R, R) → R by

h (cid:55)→(cid:88)n≥0

anhn.

This is great. Note also that if R = ∞, this means we get a function R → R.
Abuse of Notation 29.2.7 (Power series vs. functions). There is some subtly going on
with “types” of objects again. Analogies with polynomials can help.

Consider P (x) = x3 + 7x + 9, a polynomial. You can, for any real number h, plug in
P (h) to get a real number. However, in the polynomial itself, the symbol x is supposed
to be a variable — which sometimes we will plug in a real number for, but that happens
only after the polynomial is deﬁned.

Despite this, “the polynomial p(x) = x3 + 7x + 9” (which can be thought of as the
coeﬃcients) and “the real-valued function x (cid:55)→ x3 +7x+9” are often used interchangeably.
The same is about to happen with power series: while they were initially thought of
as a sequence of coeﬃcients, the Cauchy-Hadamard theorem lets us think of them as
functions too, and thus we blur the distinction between them.

§29.3 Diﬀerentiating them

Prototypical example for this section: We saw earlier 1 + x + x2 + x3 + . . . has derivative
1 + 2x + 3x2 + . . . .

As promised, diﬀerentiation works exactly as you want.

Theorem 29.3.1 (Diﬀerentiation works term by term)

Let(cid:80)n≥0 anzn be a power series with radius of convergence R > 0, and consider

the corresponding function

f : (−R, R) → R by f (x) =(cid:88)n≥0

anxn.

Then all the derivatives of f exist and are given by power series

f(cid:48)(x) =(cid:88)n≥1
f(cid:48)(cid:48)(x) =(cid:88)n≥2

...

nanxn−1

n(n − 1)anxn−2

which also converge for any x ∈ (−R, R). In particular, f is smooth.

318

Napkin, by Evan Chen (v1.5.20190718)

Proof. Also omitted. The right way to prove it is to deﬁne the notion “converges
uniformly”, and strengthen Cauchy-Hadamard to have this is as a conclusion as well.
However, we won’t use this later.

Corollary 29.3.2 (A description of power series coeﬃcients)

Let(cid:80)n≥0 anzn be a power series with radius of convergence R > 0, and consider

the corresponding function f (x) as above Then

an =

f (n)(x)

n!

.

Proof. Take the nth derivative and plug in x = 0.

§29.4 Analytic functions

Prototypical example for this section: The piecewise e−1/x or 0 function is not analytic,
but is smooth.

With all these nice results about power series, we now have a way to do this process
the other way: suppose that f : U → R is a function. Can we express it as a power series?

Functions for which this is true are called analytic.

Deﬁnition 29.4.1. A function f : U → R is analytic at the point p ∈ U if there exists
an open neighborhood V of p (inside U ) and a power series(cid:80)n anzn such that

f (x) =(cid:88)n≥0

an(x − p)n

for any x ∈ V . As usual, the whole function is analytic if it is analytic at each point.

Question 29.4.2. Show that if f is analytic, then it’s smooth.

Moreover, if f is analytic, then by the corollary above its coeﬃcients are actually described
exactly by

f (x) =(cid:88)n≥0

f (n)(p)

n!

(x − p)n.

Even if f is smooth but not analytic, we can at least write down the power series; we
give this a name.

Deﬁnition 29.4.3. For smooth f , the power series(cid:80)n≥0

series of f at p.

f (n)(p)

n!

zn is called the Taylor

29 Power series and Taylor series

319

Example 29.4.4 (Examples of analytic functions)
(a) Polynomials, sin, cos, ex, log all turn out to be analytic.

(b) The smooth function from before deﬁned by

f (x) =(cid:40)exp(−1/x) x > 0

0

x ≤ 0

is not analytic. Indeed, suppose for contradiction it was. As all the derivatives
are zero, its Taylor series would be 0 + 0x + 0x2 + . . . . This Taylor series does
converge, but not to the right value — as f (ε) > 0 for any ε > 0, contradiction.

Theorem 29.4.5 (Analytic iﬀ Taylor series has positive radius)
Let f : U → R be a smooth function. Then f is analytic if and only if for any point
p ∈ U , its Taylor series at p has positive radius of convergence.

Example 29.4.6

It now follows that f (x) = sin(x) is analytic. To see that, we can compute

f (0) = sin 0 = 0
f(cid:48)(0) = cos 0 = 1
f(cid:48)(cid:48)(0) = − sin 0 = 0
f (3)(0) = − cos 0 = −1
f (4)(0) = sin 0 = 0
f (5)(0) = cos 0 = 1
f (7)(0) = − sin 0 = 0

...

and so by continuing the pattern (which repeats every four) we ﬁnd the Taylor series
is

z3
3!

z −

+ . . .

+

z5
5! −

z7
7!

which is seen to have radius of convergence R = ∞.

Like with diﬀerentiable functions:

Proposition 29.4.7 (All your usual closure properties for analytic functions)

The sums, products, compositions, nonzero quotients of analytic functions are
analytic.

The upshot of this is is that most of your usual functions that occur in nature, or even
artiﬁcial ones like f (x) = ex + x sin(x2), will be analytic, hence describable locally by
Taylor series.

320

Napkin, by Evan Chen (v1.5.20190718)

§29.5 A deﬁnition of Euler’s constant and exponentiation

We can actually give a deﬁnition of ex using the tools we have now.
Deﬁnition 29.5.1. We deﬁne the map exp : R → R by using the following power series,
which has inﬁnite radius of convergence:

exp(x) =(cid:88)n≥0

xn
n!

.

We then deﬁne Euler’s constant as e = exp(1).

Question 29.5.2. Show that under this deﬁnition, exp(cid:48) = exp.

We are then settled with:

Proposition 29.5.3 (exp is multiplicative)
Under this deﬁnition,

exp(x + y) = exp(x) exp(y).

Idea of proof. There is some subtlety here with switching the order of summation that
we won’t address. Modulo that:

exp(x) exp(y) =(cid:88)n≥0

ym
m!

xnym
n!m!

xn

n! (cid:88)m≥0
=(cid:88)k≥0 (cid:88)m+n=k
=(cid:88)k≥0

m,n≥0
(x + y)k

k!

ym
m!

xn
n!

=(cid:88)n≥0(cid:88)m≥0
=(cid:88)k≥0 (cid:88)m+n=k

m,n≥0

(cid:18)k
n(cid:19) xnym

k!

= exp(x + y).

Corollary 29.5.4 (exp is positive)

(a) We have exp(x) > 0 for any real number x.

(b) The function exp is strictly increasing.

Proof. First

which shows exp is positive. Also, 1 = exp(0) = exp(x) exp(−x) implies exp(x) (cid:54)= 0 for
any x, proving (a).

exp(x) = exp(x/2)2 ≥ 0

(b) is just since exp(cid:48) is strictly positive (racetrack principle).

The log function then comes after.

Deﬁnition 29.5.5. We may deﬁne log : R>0 → R to be the inverse function of exp.

Since its derivative is 1/x it is smooth; and then one may compute its coeﬃcients to

show it is analytic.

Note that this actually gives us a rigorous way to deﬁne ar for any a > 0 and r > 0,

namely

ar := exp (r log a) .

29 Power series and Taylor series

321

§29.6 This all works over complex numbers as well, except also

complex analysis is heaven

We now mention that every theorem we referred to above holds equally well if we work
over C, with essentially no modiﬁcations.

 Power series are deﬁned by(cid:80)n anzn with an ∈ C, rather than an ∈ R.

 The deﬁnition of radius of convergence R is unchanged! The series will converge if

|z| < R.

 Diﬀerentiation still works great. (The deﬁnition of the derivative is unchanged.)
 Analytic still works great for functions f : U → C, with U ⊆ C open.

In particular, we can now even deﬁne complex exponentials, giving us a function

since the power series still has R = ∞. More generally if a > 0 and z ∈ C we may still
deﬁne

exp : C → C

az := exp(z log a).

(We still require the base a to be a positive real so that log a is deﬁned, though. So this
ii issue is still there.)

However, if one tries to study calculus for complex functions as we did for the real

case, in addition to most results carrying over, we run into a huge surprise:

If f : C → C is diﬀerentiable, it is analytic.

And this is just the beginning of the nearly unbelievable results that hold for complex
analytic functions. But this is the part on real analysis, so you will have to read about
this later!

§29.7 A few harder problems to think about

Problem 29A. Find the Taylor series of log(1 − x).
Problem 29B† (Euler formula). Show that

exp(iθ) = cos θ + i sin θ

for any real number θ.
Problem 29C† (Taylor’s theorem, Lagrange form). Let f : [a, b] → R be continuous and
n + 1 times diﬀerentiable on (a, b). Deﬁne

Pn =

n(cid:88)k=0

f (k)(b)

k!

· (b − a)k.

Prove that there exists ξ ∈ (a, b) such that

f (n)(ξ) = (n + 1)! ·

f (b) − Pn
(b − a)n+1 .

This generalizes the mean value theorem (which is the special case n = 0, where
P0 = f (a)).
Problem 29D (Putnam 2018 A5). Let f : R → R be smooth, and assume that f (0) = 0,
f (1) = 1, and f (x) ≥ 0 for every real number x. Prove that f (n)(x) < 0 for some positive
integer n and real number x.

30 Riemann integrals

“Trying to Riemann integrate discontinuous functions is kind of outdated.”
— Dennis Gaitsgory, [Ga15]

We will go ahead and deﬁne the Riemann integral, but we won’t do very much with it.
The reason is that the Lebesgue integral is basically better, so we will deﬁne it, check
the fundamental theorem of calculus (or rather, leave it as a problem at the end of the
chapter), and then always use Lebesgue integrals forever after.

§30.1 Uniform continuity

Prototypical example for this section: f (x) = x2 is not uniformly continuous on R, but
functions on compact sets are always uniformly continuous.

Deﬁnition 30.1.1. Let f : M → N be a continuous map between two metric spaces.
We say that f is uniformly continuous if for all ε > 0 there exists a δ > 0 such that

dM (p, q) < δ =⇒ dN (f (p), f (q)) < ε.

This diﬀerence is that given an ε > 0 we must specify a δ > 0 which works for every
choice p and q of inputs; whereas usually δ is allowed to depend on p and q. (Also, this
deﬁnition can’t be ported to a general topological space.)

Example 30.1.2 (Uniform continuity failure)
(a) The function f : R → R by x (cid:55)→ x2 is not uniformly continuous. Suppose we take
ε = 0.1 for example. There is no δ such that if |x − y| < δ then |x2 − y2| < 0.1,
since as x and y get large, the function f becomes increasingly sensitive to small
changes.

(b) The function (0, 1) → R by x (cid:55)→ x−1 is not uniformly continuous.
(c) The function R>0 → R by x (cid:55)→ √x does turn out to be uniformly continuous

(despite having unbounded derivatives!). Indeed, you can check that the assertion

holds for any x, y, ε > 0.

|x − y| < ε2 =⇒ (cid:12)(cid:12)√x − √y(cid:12)(cid:12) < ε

The good news is that in the compact case all is well.

Theorem 30.1.3 (Uniform continuity free for compact spaces)
Let M be a compact metric space. Then any continuous map f : M → N is also
uniformly continuous.

Proof. Assume for contradiction there is some bad ε > 0. Then taking δ = 1/n, we ﬁnd
that for each integer n there exists points pn and qn which are within 1/n of each other,

323

324

Napkin, by Evan Chen (v1.5.20190718)

but are mapped more than ε away from each other by f . In symbols, dM (pn, qn) < 1/n
but dN (f (pn), f (qn)) > 1/n.

By compactness of M , we can ﬁnd a convergent subsequences pi1, pi2, . . . converging
to some x ∈ M .. Since the qin is within 1/in of pin, it ought to converge as well, to
the same point x ∈ M . Then the sequences f (pin) and f (qin) should both converge to
f (x) ∈ N , but this is impossible as they are always ε away from each other.
This means for example that x2 viewed as a continuous function [0, 1] → R is automatically
uniformly continuous. Man, isn’t compactness great?

§30.2 Dense sets and extension

Prototypical example for this section: Functions from Q → N extend to R → N if they’re
uniformly continuous and N is complete. See also counterexamples below.

Deﬁnition 30.2.1. Let S be a subset (or subspace) of a topological space X. Then we
say that S is dense if every open subset of X contains a point of S.

Example 30.2.2 (Dense sets)
(a) Q is dense in R.

(b) In general, any metric space M is dense in its completion M .

Dense sets lend themselves to having functions completed. The idea is that if I have a
continuous function f : Q → N , for some metric space N , then there should be at most
one way to extend it to a function (cid:101)f : R → N . For we can approximate each rational
number by real numbers: if I know f (1), f (1.4), f (1.41), . . . (cid:101)f (√2) had better be the

limit of this sequence. So it is certainly unique.

However, there are two ways this could go wrong:

Example 30.2.3 (Non-existence of extension)

(a) It could be than N is not complete, so the limit may not even exist in N . For
example if N = Q, then certainly there is no way to extend even the identify

function f : Q → N to a function (cid:101)f : Q → N .

let N = R and deﬁne

(b) Even if N was complete, we might run into issues where f explodes. For example,

f (x) =

1

x − √2
There is also no way to extend this due to the explosion of f near √2 /∈ Q, which
would cause (cid:101)f (√2) to be undeﬁned.

f : Q → R.

However, the way to ﬁx this is to require f to be uniformly continuous, and in that case
we do get a unique extension.

30 Riemann integrals

325

Theorem 30.2.4 (Extending uniformly continuous functions)
Let M be a metric space, N a complete metric space, and S a dense subspace of M .
Suppose ψ : S → N is a uniformly continuous function. Then there exist as unique

continuous function (cid:101)ψ : M → N such that the digaram

(cid:101)ψ

M

N

ψ

S

commutes.

Outline of proof. As mentioned in the discussion, each x ∈ M can be approximated by
a sequence x1, x2, . . . in S with xi → x. The two main hypotheses, completeness and
uniform continuity, are now used:

Exercise 30.2.5. Prove that ψ(x1), ψ(x2), . . . converges in N by using uniform continuity
to show that it is Cauchy, and then appealing to completeness of N .

Hence we deﬁne (cid:101)ψ(x) to be the limit of that sequence; this doesn’t depend on the choice
of sequence, and one can use sequential continuity to show (cid:101)ψ is continuous.

§30.3 Deﬁning the Riemann integral

Extensions will allow us to deﬁne the Riemann integral. I need to introduce a bit of
notation so bear with me.

Deﬁnition 30.3.1. Let [a, b] be a closed interval.

 We let C0([a, b]) denote the set of continuous functions on [a, b] → R.
 We let R([a, b]) denote the set of rectangle functions on [a, b] → R. These
functions which are constant on the intervals [t0, t1), [t1, t2), [t2, t3), . . . , [tn−2, tn−1),
and also [tn−1, tn], for some a = t0 < t1 < t2 < ··· < tn = b.

 We let M ([a, b]) = C0([a, b]) ∪ R([a, b]).
Warning: only C0([a, b]) is common notation, and the other two are made up.
See picture below for a typical a rectangle function. (It is irritating that we have to
oﬃcially assign a single value to each ti, even though there are naturally two values we
want to use, and so we use the convention of letting the left endpoint be closed).

a=t0b=t4t1t2t3326

Napkin, by Evan Chen (v1.5.20190718)

Deﬁnition 30.3.2. We can impose a metric on M ([a, b]) by deﬁning

d(f, g) = sup

x∈[a,b]|f (x) − g(x)| .

Now, there is a natural notion of integral for rectangle functions: just sum up the

obvious rectangles! Oﬃcially, this is the expression

f (a)(t1 − a) + f (t1)(t2 − t1) + +f (t2) (t3 − t2) + ··· + f (tn) (b − tn) .

We denote this function by

Σ : R([a, b]) → R.

Theorem 30.3.3 (The Riemann integral)

There exists a unique continuous map

such that the diagram

(cid:82) b
a : M ([a, b]) → R

(cid:82) b

a

M ([a, b])

R

Σ

R([a, b])

commutes.

Proof. We want to apply the extension theorem, so we just have to check a few things:

 We claim R([a, b]) is a dense subset of M ([a, b]). In other words, for any contin-
uous f : [a, b] → R and ε > 0, we want there to exist a rectangle function that
approximates f within ε.

This follows by uniform continuity. We know there exists a δ > 0 such that whenever
|x − y| < δ we have |f (x) − f (y)| < ε. So as long as we select a rectangle function
whose rectangles have width less than δ, and such that the upper-left corner of
each rectangle lies on the graph of f , then we are all set.

 The “add-the-rectangles” map Σ : R([a, b]) → R is uniformly continuous. Actually
this is pretty obvious: if two rectangle functions f and g have d(f, g) < ε, then
d(Σf, Σg) < ε(b − a).

 R is complete.

ab30 Riemann integrals

§30.4 Meshes

327

The above deﬁnition might seem fantastical, overcomplicated, hilarious, or terrible,
depending on your taste. But if you unravel it, it’s really the picture you are used to.
What we have done is taking every continuous function f : [a, b] → R and showed that it
can be approximated by a rectangle function (which we phrased as a dense inclusion).
Then we added the area of the rectangles. Nonetheless, we will give a deﬁnition that’s
more like what you’re used to seeing in other places.

Deﬁnition 30.4.1. A tagged partition P of [a, b] consists of a partition of [a, b] into n
intervals, with a point ξi in the nth interval, denoted

a = t0 < t1 < t2 < ··· < tn = b

and

ξi ∈ [ti−1, ti] ∀ 1 ≤ i ≤ n.

The mesh of P is the width of the longest interval, i.e. maxi(ti − ti−1).

Of course the point of this deﬁnition is that we add the rectangles, but the ξi are the

sample points.

Theorem 30.4.2 (Riemann integral)
Let f : [a, b] → R be continuous. Then

(cid:90) b

a

f (x) dx =

P tagged partition

mesh P→0 (cid:32) n(cid:88)i=1

lim

f (ξi)(ti − ti−1)(cid:33) .

Here the limit means that we can take any sequence of partitions whose mesh
approaches zero.

Proof. The right-hand side corresponds to the areas of some rectangle functions g1, g2,
. . . with increasingly narrow rectangles. As in the proof Theorem 30.3.3, as the meshes
of those rectangles approaches zero, by uniform continuity, we have d(f, gn) → 0 as
needed.

well. Thus by continuity in the diagram of Theorem 30.3.3, we get limn Σ(gn) =(cid:82) (f ) as

Combined with the mean value theorem, this can be used to give a short proof of the
fundamental theorem of calculus for functions f with a continuous derivative. The idea
is that for any choice of partition a ≤ t0 < t1 < t2 < ··· < tn ≤ b, using the Mean Value
Theorem it should be possible to pick ξi in each interval to match with the slope of the
secant: at which point the areas sum to the total change in f . We illustrate this situation
with three points, and invite the reader to ﬁll in the details as Problem 30B(cid:63).

ξab328

Napkin, by Evan Chen (v1.5.20190718)

One quick note is that although I’ve only deﬁned the Riemann integral for continuous
functions, there ought to be other functions for which it exists (including “piecewise
continuous functions” for example, or functions “continuous almost everywhere”). The
relevant deﬁnition is:

Deﬁnition 30.4.3. If f : [a, b] → R is a function which is not necessarily continuous,
but for which the limit

mesh P→0 (cid:32) n(cid:88)i=1

lim

P tagged partition

f (ξi)(ti − ti−1)(cid:33) .

exists anyways, then we say f is Riemann integrable on [a, b] and deﬁne its value to

be that limit(cid:82) b

a f (x) dx.

We won’t really use this deﬁnition much, because we will see that every Riemann

integrable function is Lebesgue integrable, and the Lebesgue integral is better.

Example 30.4.4 (Your AP calculus returns)
We had better mention that Problem 30B(cid:63) implies that we can compute Riemann
integrals in practice, although most of you may already know this from high-school
calculus For example, on the interval (1, 4), the derivative of the function F (x) = 1
3 x3
is F (cid:48)(x) = x2. As f (x) = x2 is a continuous function f : [1, 4] → R, we get

(cid:90) 4

1

x2 dx = F (4) − F (1) =

64
3 −

1
3

= 21.

Note that we could also have picked F (x) = 1
3 x3 + 2019; the function F is unique up
to shifting, and this constant cancels out when we subtract. This is why it’s common

in high school to (really) abuse notation and write(cid:82) x2 dx = 1

3 x3 + C.

§30.5 A few harder problems to think about

Problem 30A. Let f : (a, b) → R be diﬀerentiable and assume f(cid:48) is bounded. Show
that f is uniformly continuous.

Problem 30B(cid:63) (Fundamental theorem of calculus). Let f : [a, b] → R be continuous,
diﬀerentiable on (a, b), and assume the derivative f(cid:48) extends to a continuous function

yxft0t1t2t3ξ1ξ2ξ3Netchange30 Riemann integrals

f(cid:48) : [a, b] → R. Prove that

(cid:90) b

a

f(cid:48)(x) dx = f (b) − f (a).

329

Problem 30C(cid:63) (Improper integrals). For each real number r > 0, evaluate the limit1

ε→0+(cid:90) 1

lim

ε

1
xr dx

or show it does not exist.

This can intuitively be thought of as the “improper” integral (cid:82) 1

0 x−r dx; it doesn’t
make sense in our original deﬁnition since we did not (and cannot) deﬁne the integral
over the non-compact (0, 1] but we can still consider the integral over [ε, 1] for any ε > 0.

Problem 30D. Show that

n→∞(cid:18) 1

lim

n + 1

+

1

n + 2

+ ··· +

1

2n(cid:19) = log 2.

1If you are not familiar with the notation ε → 0+, you can replace ε with 1/M for M > 0, and let

M → ∞ instead.

IX

Complex Analysis

Part IX: Contents

31 Holomorphic functions

333
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 333
31.1 The nicest functions on earth
31.2 Complex diﬀerentiation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336
31.3 Contour integrals
31.4 Cauchy-Goursat theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 338
31.5 Cauchy’s integral theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339
31.6 Holomorphic functions are analytic . . . . . . . . . . . . . . . . . . . . . . . . . . 341
. . . . . . . . . . . . . . . . . . . . . . . . 343
31.7 A few harder problems to think about

32 Meromorphic functions

345
32.1 The second nicest functions on earth . . . . . . . . . . . . . . . . . . . . . . . . . 345
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345
32.2 Meromorphic functions
32.3 Winding numbers and the residue theorem . . . . . . . . . . . . . . . . . . . . . . 348
32.4 Argument principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 350
32.5 Philosophy: why are holomorphic functions so nice? . . . . . . . . . . . . . . . . . . 351
. . . . . . . . . . . . . . . . . . . . . . . . 352
32.6 A few harder problems to think about

33 Holomorphic square roots and logarithms

353
33.1 Motivation: square root of a complex number . . . . . . . . . . . . . . . . . . . . . 353
. . . . . . . . . . . . . . . . . . . . . . . . 355
33.2 Square roots of holomorphic functions
33.3 Covering projections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356
33.4 Complex logarithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 356
33.5 Some special cases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357
. . . . . . . . . . . . . . . . . . . . . . . . 358
33.6 A few harder problems to think about

31 Holomorphic functions

Throughout this chapter, we denote by U an open subset of the complex plane, and by
Ω an open subset which is also simply connected. The main references for this chapter
were [Ya12; Ba10].

§31.1 The nicest functions on earth

In high school you were told how to diﬀerentiate and integrate real-valued functions. In
this chapter on complex analysis, we’ll extend it to diﬀerentiation and integration of
complex-valued functions.

Big deal, you say. Calculus was boring enough. Why do I care about complex calculus?
Perhaps it’s easiest to motivate things if I compare real analysis to complex analysis.
In real analysis, your input lives inside the real line R. This line is not terribly discerning –
you can construct a lot of unfortunate functions. Here are some examples.

Example 31.1.1 (Optional: evil real functions)
You can skim over these very quickly: they’re just here to make a point.

(a) The Devil’s Staircase (or Cantor function) is a continuous function H : [0, 1] →
[0, 1] which has derivative zero “almost everywhere”, yet H(0) = 0 and H(1) = 1.

(b) The Weierstraß function

x (cid:55)→

∞(cid:88)n=0(cid:18) 1
2(cid:19)n

cos (2015nπx)

is continuous everywhere but diﬀerentiable nowhere.

(c) The function

x ≥ 0
−x100 x < 0
has the ﬁrst 99 derivatives but not the 100th one.

x (cid:55)→(cid:40)x100

(d) If a function has all derivatives (we call these smooth functions), then it has a
Taylor series. But for real functions that Taylor series might still be wrong. The
function

x (cid:55)→(cid:40)e−1/x x > 0

0

x ≤ 0

has derivatives at every point. But if you expand the Taylor series at x = 0, you
get 0 + 0x + 0x2 + . . . , which is wrong for any x > 0 (even x = 0.0001).

Let’s even put aside the pathology. If I tell you the value of a real smooth function on
the interval [−1, 1], that still doesn’t tell you anything about the function as a whole.
It could be literally anything, because it’s somehow possible to “fuse together” smooth
functions.

333

334

Napkin, by Evan Chen (v1.5.20190718)

Figure 31.1: The Weierstraß Function (image from [Ee]).

So what about complex functions? If you just consider them as functions R2 → R2, you
now have the interesting property that you can integrate along things that are not just
line segments: you can write integrals across curves in the plane. But C has something
more: it is a ﬁeld, so you can multiply and divide two complex numbers.

So we restrict our attention to diﬀerentiable functions called holomorphic functions. It
turns out that the multiplication on C makes all the diﬀerence. The primary theme in
what follows is that holomorphic functions are really, really nice, and that knowing tiny
amounts of data about the function can determine all its values.

The two main highlights of this chapter, from which all other results are more or less

corollaries:

 Contour integrals of loops are always zero.

 A holomorphic function is essentially given by its Taylor series; in particular, single-
diﬀerentiable implies inﬁnitely diﬀerentiable. Thus, holomorphic functions behave
quite like polynomials.

Some of the resulting corollaries:

 It’ll turn out that knowing just the values of a holomorphic function on the boundary

of the unit circle will tell you the values in its interior.

 Knowing just the values of the function at 1, 1

whole function!

2 , 1

3 , . . . are enough to determine the

 Bounded holomorphic functions C → C must be constant
 And more. . .

As [Pu02] writes: “Complex analysis is the good twin and real analysis is the evil one:
beautiful formulas and elegant theorems seem to blossom spontaneously in the complex
domain, while toil and pathology rule the reals”.

31 Holomorphic functions

335

§31.2 Complex diﬀerentiation

Prototypical example for this section: Polynomials are holomorphic; z is not.

Let f : U → C be a complex function. Then for some z0 ∈ U , we deﬁne the derivative

at z0 to be

f (z0 + h) − f (z0)

h

.

lim
h→0

Note that this limit may not exist; when it does we say f is diﬀerentiable at z0.

What do I mean by a “complex” limit h → 0? It’s what you might expect: for every

ε > 0 there should be a δ > 0 such that

0 < |h| < δ =⇒ (cid:12)(cid:12)(cid:12)(cid:12)

f (z0 + h) − f (z0)

h

− L(cid:12)(cid:12)(cid:12)(cid:12) < ε.

If you like topology, you are encouraged to think of this in terms of open neighborhoods
in the complex plane. (This is why we require U to be open: it makes it possible to take
δ-neighborhoods in it.)

But note that having a complex derivative is actually much stronger than a real
function having a derivative. In the real line, h can only approach zero from below and
above, and for the limit to exist we need the “left limit” to equal the “right limit”. But
the complex numbers form a plane: h can approach zero from many directions, and we
need all the limits to be equal.

Example 31.2.1 (Important: conjugation is not holomorphic)
Let f (z) = z be complex conjugation, f : C → C. This function, despite its simple
nature, is not holomorphic! Indeed, at z = 0 we have,

f (h) − f (0)

h

=

h
h

.

This does not have a limit as h → 0, because depending on “which direction” we
approach zero from we have diﬀerent values.

If a function f : U → C is complex diﬀerentiable at all the points in its domain it is
called holomorphic. In the special case of a holomorphic function with domain U = C,

ImRe011i−1f(z)=zf(0+h)−f(0)h336

Napkin, by Evan Chen (v1.5.20190718)

we call the function entire.1

Example 31.2.2 (Examples of holomorphic functions)
In all the examples below, the derivative of the function is the same as in their real
analogues (e.g. the derivative of ez is ez).
(a) Any polynomial z (cid:55)→ zn + cn−1zn−1 + ··· + c0 is holomorphic.
(b) The complex exponential exp : x + yi (cid:55)→ ex(cos y + i sin y) can be shown to be

holomorphic.

(c) sin and cos are holomorphic when extended to the complex plane by cos z =

eiz+e−iz

2

and sin z = eiz−e−iz

.

2i

(d) As usual, the sum, product, chain rules and so on apply, and hence sums, prod-
ucts, nonzero quotients, and compositions of holomorphic functions
are also holomorphic.

You are welcome to try and prove these results, but I won’t bother to do so.

§31.3 Contour integrals

Prototypical example for this section: (cid:72)γ zm dz around the unit circle.

In the real line we knew how to integrate a function across a line segment [a, b]:
essentially, we’d “follow along” the line segment adding up the values of f we see to get
some area. Unlike in the real line, in the complex plane we have the power to integrate
over arbitrary paths: for example, we might compute an integral around a unit circle. A
contour integral lets us formalize this.

First of all, if f : R → C and f (t) = u(t) + iv(t) for u, v ∈ R, we can deﬁne an integral
a by just adding the real and imaginary parts:

(cid:82) b

(cid:90) b

a

f (t) dt =(cid:18)(cid:90) b

a

u(t) dt(cid:19) + i(cid:18)(cid:90) b

a

v(t) dt(cid:19) .

Now let α : [a, b] → C be a path, thought of as a complex diﬀerentiable2 function. Such
a path is called a contour, and we deﬁne its contour integral by

(cid:73)α

f (z) dz =(cid:90) b

a

f (α(t)) · α(cid:48)(t) dt.

You can almost think of this as a u-substitution (which is where the α(cid:48) comes from).
In particular, it turns out this integral does not depend on how α is “parametrized”: a
circle given by

and another circle given by

[0, 2π] → C : t (cid:55)→ eit

[0, 1] → C : t (cid:55)→ e2πit

1Sorry, I know the word “holomorphic” sounds so much cooler. I’ll try to do things more generally for

that sole reason.

2This isn’t entirely correct here: you want the path α to be continuous and mostly diﬀerentiable, but
you allow a ﬁnite number of points to have “sharp bends”; in other words, you can consider paths
which are combinations of n smooth pieces. But for this we also require that α has “bounded length”.

31 Holomorphic functions

and yet another circle given by

337

[0, 1] → C : t (cid:55)→ e2πit5

will all give the same contour integral, because the paths they represent have the same
geometric description: “run around the unit circle once”.

In what follows I try to use α for general contours and γ in the special case of loops.
Let’s see an example of a contour integral.

Theorem 31.3.1
Take γ : [0, 2π] → C to be the unit circle speciﬁed by

t (cid:55)→ eit.

Then for any integer m, we have

zm dz =(cid:40)2πi m = −1

otherwise

0

(cid:73)γ

Proof. The derivative of eit is ieit. So, by deﬁnition the answer is the value of

i(eit)1+m dt

(cid:90) 2π

0

0

(eit)m · (ieit) dt =(cid:90) 2π
= i(cid:90) 2π
= −(cid:90) 2π

cos[(1 + m)t] + i sin[(1 + m)t] dt

0

0

sin[(1 + m)t] dt + i(cid:90) 2π

0

cos[(1 + m)t] dt.

This is now an elementary calculus question. One can see that this equals 2πi if m = −1
and otherwise the integrals vanish.

Let me try to explain why this intuitively ought to be true for m = 0. In that case we

just have(cid:72)γ 1 dz. So as the integral walks around the unit circle, it “sums up” all the
tangent vectors at every point (that’s the direction it’s walking in), multiplied by 1. And
given the nice symmetry of the circle, it should come as no surprise that everything
cancels out. The theorem says that even if we multiply by zm for m (cid:54)= −1, we get the
same cancellation.

Deﬁnition 31.3.2. Given α : [0, 1] → C, we denote by α the “backwards” contour
α(t) = α(1 − t).

338

Napkin, by Evan Chen (v1.5.20190718)

Question 31.3.3. What’s the relation between(cid:72)α f dz and(cid:72)α f dz? Prove it.

This might seem a little boring. Things will get really cool really soon, I promise.

§31.4 Cauchy-Goursat theorem

theorem does not apply.

Prototypical example for this section: (cid:72)γ zm dz = 0 for m ≥ 0. But if m < 0, Cauchy’s
Let Ω ⊆ C be simply connected (for example, Ω = C), and consider two paths α, β

with the same start and end points.

some relation between them, considering that the space Ω is simply connected. But you
probably wouldn’t expect there to be much of a relation.

What’s the relation between(cid:72)α f (z) dz and(cid:72)β f (z) dz? You might expect there to be
As a concrete example, let Ψ : C → C be the function z (cid:55)→ z − Re[z] (for example,
Ψ(2015 + 3i) = 3i). Let’s consider two paths from −1 to 1. Thus β just walking along
the real axis, and α which follows an upper semicircle.

Obviously(cid:72)β Ψ(z) dz = 0. But heaven knows what(cid:72)α Ψ(z) dz is supposed to equal.

We can compute it now just out of non-laziness. If you like, you are welcome to compute
it yourself (it’s a little annoying but not hard). If I myself didn’t mess up, it is

(cid:73)α

Ψ(z) dz = −(cid:73)α

Ψ(z) dz = −(cid:90) π

0

(i sin(t)) · ieit dt =

1
2

πi

which in particular is not zero.

But somehow Ψ is not a really natural function. It’s not respecting any of the nice,
multiplicative structure of C since it just rudely lops oﬀ the real part of its inputs. More
precisely,

Ωαβ−11αβ31 Holomorphic functions

339

Question 31.4.1. Show that Ψ(z) = z − Re[z] is not holomorphic.
holomorphic.)

(Hint: z is not

Now here’s a miracle: for holomorphic functions, the two integrals are always equal.
Equivalently, (by considering α followed by β) contour integrals of loops are always zero.
This is the celebrated Cauchy-Goursat theorem (also called the Cauchy integral theorem,
but later we’ll have a “Cauchy Integral Formula” so blah).

Theorem 31.4.2 (Cauchy-Goursat theorem)
Let γ be a loop, and f : Ω → C a holomorphic function where Ω is open in C and
simply connected. Then

(cid:73)γ

f (z) dz = 0.

Remark 31.4.3 (Sanity check) — This might look surprising considering that we

saw(cid:72)γ z−1 dz = 2πi earlier. The subtlety is that z−1 is not even deﬁned at z = 0.
On the other hand, the function C \ {0} → C by z (cid:55)→ 1
z is holomorphic! The defect
now is that Ω = C \ {0} is not simply connected. So the theorem passes our sanity
checks, albeit just barely.

The typical proof of Cauchy’s Theorem assumes additionally that the partial derivatives
of f are continuous and then applies the so-called Green’s theorem. But it was Goursat
who successfully proved the fully general theorem we’ve stated above, which assumed only
that f was holomorphic. I’ll only outline the proof, and very brieﬂy. You can show that
if f : Ω → C has an antiderivative F : Ω → C which is also holomorphic, and moreover Ω
is simply connected, then you get a “fundamental theorem of calculus”, a la

(cid:73)α

f (z) dz = F (α(b)) − F (α(a))

where α : [a, b] → C is some path. So to prove Cauchy-Goursat, you just have to show
this antiderivative F exists. Goursat works very hard to prove the result in the special
case that γ is a triangle, and hence by induction for any polygon. Once he has the result
for a rectangle, he uses this special case to construct the function F explicitly. Goursat
then shows that F is holomorphic, completing the proof.

Anyways, the theorem implies that(cid:72)γ zm dz = 0 when m ≥ 0. So much for all our

hard work earlier. But so far we’ve only played with circles. This theorem holds for any
contour which is a loop. So what else can we do?

§31.5 Cauchy’s integral theorem

We now present a stunning application of Cauchy-Goursat, a “representation theorem”:
essentially, it says that values of f inside a disk are determined by just the values on the
boundary! In fact, we even write down the exact formula. As [Ya12] says, “any time a
certain type of function satisﬁes some sort of representation theorem, it is likely that
many more deep theorems will follow.” Let’s pull back the curtain:

340

Napkin, by Evan Chen (v1.5.20190718)

Theorem 31.5.1 (Cauchy’s integral formula)
Let γ : [0, 2π] → C be a circle in the plane given by t (cid:55)→ Reit, which bounds a disk D.
Suppose f : U → C is holomorphic such that U contains the circle and its interior.
Then for any point a in the interior of D, we have

f (a) =

1

2πi(cid:73)γ

f (z)
z − a

dz.

Note that we don’t require U to be simply connected, but the reason is pretty silly: we’re
only going to ever integrate f over D, which is an open disk, and hence the disk is simply
connected anyways.

The presence of 2πi, which you saw earlier in the form(cid:72)circle z−1 dz, is no accident. In

fact, that’s the central result we’re going to use to prove the result.

Proof. There are several proofs out there, but I want to give the one that really draws
out the power of Cauchy’s theorem. Here’s the picture we have: there’s a point a sitting
inside a circle γ, and we want to get our hands on the value f (a).

We’re going to do a trick: construct a keyhole contour Γδ,ε which has an outer circle
γ, plus an inner circle γε, which is a circle centered at a with radius ε, running clockwise
(so that γε runs counterclockwise). The “width” of the corridor is δ. See picture:

Hence Γδ,ε consists of four smooth curves.

Question 31.5.2. Draw a simply connected open set Ω which contains the entire Γδ,ε but
does not contain the point a.

Hence, the function f (z)
applies and tells us that

z−a manages to be holomorphic on all of Ω. Thus Cauchy’s theorem

0 =(cid:73)Γδ,ε

f (z)
z − a

dz.

As we let δ → 0, the two walls of the keyhole will cancel each other (because f is
continuous, and the walls run in opposite directions). So taking the limit as δ → 0, we

γaγaγε31 Holomorphic functions

341

are left with just γ and γε, which (taking again orientation into account) gives

(cid:73)γ

f (z)
z − a

dz = −(cid:73)γε

f (z)
z − a

dz =(cid:73)γε

f (z)
z − a

dz.

Thus we’ve managed to replace γ with a much smaller circle γε centered
around a, and the rest is just algebra.

To compute the last quantity, write

(cid:73)γε

f (z)
z − a

dz =(cid:73)γε
=(cid:73)γε
(cid:73)γε

dz + f (a) ·(cid:73)γε

dz + 2πif (a).

1

z − a

dz

f (z) − f (a)

f (z) − f (a)

z − a

z − a

f (z) − f (a)

z − a

dz = 0.

where we’ve used Theorem 31.3.1 Thus, all we have to do is show that

For this we can basically use the weakest bound possible, the so-called M L lemma
which I’ll just cite without proof: it just says “bound the function everywhere by its
maximum”.

Lemma 31.5.3 (M L estimation lemma)
Let f be a holomorphic function and α a path. Suppose M = maxz on α |f (z)|, and
let L be the length of α. Then

(cid:12)(cid:12)(cid:12)(cid:12)(cid:73)α

f (z) dz(cid:12)(cid:12)(cid:12)(cid:12) ≤ M L.

(This is straightforward to prove if you know the deﬁnition of length: L =(cid:82) b
a |α(cid:48)(t)| dt,
where α : [a, b] → C.)
Anyways, as ε → 0, the quantity f (z)−f (a)
just approaches f(cid:48)(a), and so for small
enough ε (i.e. z close to a) there’s some upper bound M . Yet the length of γε is just the
circumference 2πε. So the M L lemma says that

z−a

as desired.

(cid:12)(cid:12)(cid:12)(cid:12)(cid:73)γε

f (z) − f (a)

z − a

(cid:12)(cid:12)(cid:12)(cid:12) ≤ 2πε · M → 0

§31.6 Holomorphic functions are analytic

Prototypical example for this section: Imagine a formal series(cid:80)k ckxk!
In the setup of the previous problem, we have a circle γ : [0, 2π] → C and a holomorphic
function f : U → C which contains the disk D. We can write

f (a) =

=

=

1

1

2πi(cid:73)γ
2πi(cid:73)γ
2πi(cid:73)γ

1

dz

f (z)
z − a
f (z)/z
1 − a

z

dz

f (z)/z ·(cid:88)k≥0(cid:16) a
z(cid:17)k

dz

342

Napkin, by Evan Chen (v1.5.20190718)

You can prove (using the so-called Weierstrass M-test) that the summation order can be
switched:

z

dz

f (a) =

1

1

=

f (z)

·(cid:16) a
z(cid:17)k

2πi(cid:88)k≥0(cid:73)γ
2πi(cid:88)k≥0(cid:73)γ
ak ·
zk+1 dz(cid:19) ak.
=(cid:88)k≥0(cid:18) 1
2πi(cid:73)γ
f (z)

f (z)
zk+1 dz

Letting ck = 1

2πi(cid:72)γ

f (z)
zk+1 dz, and noting this is independent of a, this is

f (a) =(cid:88)k≥0

ckak

and that’s the miracle: holomorphic functions are given by a Taylor series! This is one
of the biggest results in complex analysis. Moreover, if one is willing to believe that we
can take the derivative k times, we obtain

ck =

f (k)(0)

k!

and this gives us f (k)(0) = k! · ck.
the full result below, with arbitrary center p.

Naturally, we can do this with any circle (not just one centered at zero). So let’s state

Theorem 31.6.1 (Cauchy’s diﬀerentiation formula)
Let f : U → C be a holomorphic function and let D be a disk centered at point p
bounded by a circle γ. Suppose D is contained inside U . Then f is given everywhere
in D by a Taylor series

f (z) = c0 + c1(z − p) + c2(z − p)2 + . . .

where

In particular,

ck =

f k(p)

k!

=

f (k)(p) = k!ck =

1

2πi(cid:73)γ
2πi(cid:73)γ

k!

f (w − p)
(w − p)k+1 dw
f (w − p)
(w − p)k+1 dw.

Most importantly,

Over any disk, a holomorphic function is given exactly by a Taylor series.

This establishes a result we stated at the beginning of the chapter: that a function being
complex diﬀerentiable once means it is not only inﬁnitely diﬀerentiable, but in fact equal
to its Taylor series.

I should maybe emphasize a small subtlety of the result: the Taylor series centered at
p is only valid in a disk centered at p which lies entirely in the domain U . If U = C this

31 Holomorphic functions

343

is no issue, since you can make the disk big enough to accommodate any point you want.
It’s more subtle in the case that U is, for example, a square; you can’t cover the entire
square with a disk centered at some point without going outside the square. However,
since U is open we can at any rate at least ﬁnd some open neighborhood for which the
Taylor series is correct – in stark contrast to the real case. Indeed, as you’ll see in the
problems, the existence of a Taylor series is incredibly powerful.

§31.7 A few harder problems to think about

These aren’t olympiad problems, but I think they’re especially nice! In the next complex
analysis chapter we’ll see some more nice applications.

The ﬁrst few results are the most important.

Problem 31A(cid:63) (Liouville’s theorem). Let f : C → C be an entire function. Suppose
that |f (z)| < 1000 for all complex numbers z. Prove that f is a constant function.
Problem 31B(cid:63) (Zeros are isolated). An isolated set in the complex plane is a set of
points S such that around each point in S, one can draw an open neighborhood not
intersecting any other point of S.

Show that the zero set of any nonzero holomorphic function f : U → C is an isolated

set, unless there exists a nonempty open subset of U on which f is identically zero.
Problem 31C(cid:63) (Identity theorem). Let f, g : U → C be holomorphic, and assume that
U is connected. Prove that if f and g agree on some open neighborhood, then f = g.
Problem 31D† (Maximums Occur On Boundaries). Let f : U → C be holomorphic, let
Y ⊆ U be compact, and let ∂Y be boundary3 of Y . Show that

max

z∈Y |f (z)| = max

z∈∂Y |f (z)| .

In other words, the maximum values of |f| occur on the boundary. (Such maximums
exist by compactness.)
Problem 31E (Harvard quals). Let f : C → C be a nonconstant entire function. Prove
that f img(C) is dense in C. (In fact, a much stronger result is true: Little Picard’s
theorem says that the image of a nonconstant entire function omits at most one point.)

3 The boundary ∂Y is the set of points p such that no open neighborhood of p is contained in Y . It is

also a compact set if Y is compact.

32 Meromorphic functions

§32.1 The second nicest functions on earth

If holomorphic functions are like polynomials, then meromorphic functions are like
rational functions. Basically, a meromorphic function is a function of the form A(z)
B(z) where
A, B : U → C are holomorphic and B is not zero. The most important example of a
meromorphic function is 1
z .
We are going to see that meromorphic functions behave like “almost-holomorphic”
functions. Speciﬁcally, a meromorphic function A/B will be holomorphic at all points
except the zeros of B (called poles). By the identity theorem, there cannot be too many
zeros of B! So meromorphic functions can be thought of as “almost holomorphic” (like
1
z , which is holomorphic everywhere but the origin). We saw that

1

2πi(cid:73)γ

1
z

dz = 1

for γ(t) = eit the unit circle. We will extend our results on contours to such situations.

It turns out that, instead of just getting(cid:72)γ f (z) dz = 0 like we did in the holomorphic

case, the contour integrals will actually be used to count the number of poles inside the
loop γ. It’s ridiculous, I know.

§32.2 Meromorphic functions

Prototypical example for this section: 1

z , with a pole of order 1 and residue 1 at z = 0.

Let U be an open subset of C again.

Deﬁnition 32.2.1. A function f : U → C is meromorphic if there exists holomorphic
functions A, B : U → C with B not identically zero in any open neighborhood, and
f (z) = A(z)/B(z) whenever B(z) (cid:54)= 0.

Let’s see how this function f behaves. If z ∈ U has B(z) (cid:54)= 0, then in some small open
neighborhood the function B isn’t zero at all, and thus A/B is in fact holomorphic; thus
f is holomorphic at z. (Concrete example: 1
z is holomorphic in any disk not containing
0.)

On the other hand, suppose p ∈ U has B(p) = 0: without loss of generality, p = 0 to

ease notation. By using the Taylor series at p = 0 we can put

B(z) = ckzk + ck+1zk+1 + . . .

with ck (cid:54)= 0 (certainly some coeﬃcient is nonzero since B is not identically zero!). Then
we can write

1

B(z)

=

1
zk ·

1

ck + ck+1z + . . .

.

But the fraction on the right is a holomorphic function in this open neighborhood! So all
that’s happened is that we have an extra z−k kicking around.

This gives us an equivalent way of viewing meromorphic functions:

345

346

Napkin, by Evan Chen (v1.5.20190718)

Deﬁnition 32.2.2. Let f : U → C as usual. A meromorphic function is a function
which is holomorphic on U except at an isolated set S of points (meaning it is holomorphic
as a function U \ S → C). For each p ∈ S, called a pole of f , the function f must admit
a Laurent series, meaning that

f (z) =

c−m
(z − p)m +

c−m+1
(z − p)m−1 + ··· +

c−1
z − p

+ c0 + c1(z − p) + . . .

for all z in some open neighborhood of p, other than z = p. Here m is a positive integer
(and c−m (cid:54)= 0).

Note that the trailing end must terminate. By “isolated set”, I mean that we can draw
open neighborhoods around each pole in S, in such a way that no two open neighborhoods
intersect.

Example 32.2.3 (Example of a meromorphic function)
Consider the function

z + 1
sin z

.

It is meromorphic, because it is holomorphic everywhere except at the zeros of sin z.
At each of these points we can put a Laurent series: for example at z = 0 we have

z + 1
sin z

= (z + 1) ·

1

1
z ·

3! + z5
z + 1
5! + z6

z − z3
1 −(cid:16) z2
z · (z + 1)(cid:88)k≥0(cid:18) z2

3! − z4

3! −

1

=

=

5! − . . .

7! − . . .(cid:17)

z4
5!

+

z6

7! − . . .(cid:19)k

.

If we expand out the horrible sum (which I won’t do), then you get 1
ﬁne Taylor series, i.e. a Laurent series.

z times a perfectly

Abuse of Notation 32.2.4. We’ll often say something like “consider the function
f : C → C by z (cid:55)→ 1
z ”. Of course this isn’t completely correct, because f doesn’t have a
value at z = 0. If I was going to be completely rigorous I would just set f (0) = 2015 or
something and move on with life, but for all intents let’s just think of it as “undeﬁned at
z = 0”.

Why don’t I just write g : C \ {0} → C? The reason I have to do this is that it’s still
important for f to remember it’s “trying” to be holomorphic on C, even if isn’t assigned
a value at z = 0. As a function C \ {0} → C the function 1

z is actually holomorphic.

Remark 32.2.5 — I have shown that any function A(z)/B(z) has this charac-
terization with poles, but an important result is that the converse is true too: if
f : U \ S → C is holomorphic for some isolated set S, and moreover f admits a
Laurent series at each point in S, then f can be written as a rational quotient of
holomorphic functions. I won’t prove this here, but it is good to be aware of.

Deﬁnition 32.2.6. Let p be a pole of a meromorphic function f , with Laurent series

f (z) =

c−m
(z − p)m +

c−m+1
(z − p)m−1 + ··· +

c−1
z − p

+ c0 + c1(z − p) + . . . .

32 Meromorphic functions

347

The integer m is called the order of the pole. A pole of order 1 is called a simple pole.
We also give the coeﬃcient c−1 a name, the residue of f at p, which we write Res(f ; p).

The order of a pole tells you how “bad” the pole is. The order of a pole is the “opposite”
concept of the multiplicity of a zero. If f has a pole at zero, then its Taylor series near
z = 0 might look something like

f (z) =

1
z5 +

8
z3 −

2
z2 +

4
z

+ 9 − 3z + 8z2 + . . .

and so f has a pole of order ﬁve. By analogy, if g has a zero at z = 0, it might look
something like

g(z) = 3z3 + 2z4 + 9z5 + . . .

and so g has a zero of multiplicity three. These orders are additive: f (z)g(z) still has
a pole of order 5 − 3 = 2, but f (z)g(z)2 is completely patched now, and in fact has a
simple zero now (that is, a zero of degree 1).

Exercise 32.2.7. Convince yourself that orders are additive as described above. (This is
obvious once you understand that you are multiplying Taylor/Laurent series.)

Metaphorically, poles can be thought of as “negative zeros”.

We can now give many more examples.

Example 32.2.8 (Examples of meromorphic functions)

(a) Any holomorphic function is a meromorphic function which happens to have no

poles. Stupid, yes.

(b) The function C → C by z (cid:55)→ 100z−1 for z (cid:54)= 0 but undeﬁned at zero is a
meromorphic function. Its only pole is at zero, which has order 1 and residue
100.

(c) The function C → C by z (cid:55)→ z−3 + z2 + z9 is also a meromorphic function. Its

only pole is at zero, and it has order 3, and residue 0.

(d) The function C → C by z (cid:55)→ ez

given by

z2 is meromorphic, with the Laurent series at z = 0

ez
z2 =

1
z2 +

1
z

+

1
2

+

z
6

+

z2
24

+

z3
120

+ . . . .

Hence the pole z = 0 has order 2 and residue 1.

348

Napkin, by Evan Chen (v1.5.20190718)

Example 32.2.9 (A rational meromorphic function)
Consider the function C → C given by

= z2 + 1 +

2

(z − 1)(z + 1)
1

z4 + 1
z (cid:55)→
z2 − 1
= z2 + 1 +

=

2
z − 1

+

3
2

1

z − 1 ·

+

9
4

1 + z−1
2

(z − 1) +

7
8

(z − 1)2 − . . .

It has a pole of order 1 and residue 2 at z = 1. (It also has a pole of order 1 at
z = −1; you are invited to compute the residue.)

Example 32.2.10 (Function with inﬁnitely many poles)
The function C → C by

1

z (cid:55)→

sin(z)

has inﬁnitely many poles: the numbers z = 2πk, where k is an integer. Let’s compute
the Laurent series at just z = 0:

1

sin(2πz)

=

=

=

1

3! + z5

5! − . . .

z

1! − z3
1
1 −(cid:16) z2
z ·
z(cid:88)k≥0(cid:18) z2

1

1
3! − z4
z4
5!

5! + . . .(cid:17)
+ . . .(cid:19)k

3! −

.

which is a Laurent series, though I have no clue what the coeﬃcients are. You can
at least see the residue; the constant term of that huge sum is 1, so the residue is 1.
Also, the pole has order 1.

The Laurent series, if it exists, is unique (as you might have guessed), and by our
result on holomorphic functions it is actually valid for any disk centered at p (minus the
point p). The part c−1
(z−p)m is called the principal part, and the rest of the
series c0 + c1(z − p) + . . . is called the analytic part.

z−p + ··· + c−m

§32.3 Winding numbers and the residue theorem

Recall that for a counterclockwise circle γ and a point p inside it, we had

(z − p)m dz =(cid:40)0

(cid:73)γ

m (cid:54)= −1
2πi m = −1

where m is an integer. One can extend this result to in fact show that(cid:72)γ(z − p)m dz = 0

for any loop γ, where m (cid:54)= −1. So we associate a special name for the nonzero value at
m = −1.

32 Meromorphic functions

349

Deﬁnition 32.3.1. For a point p ∈ C and a loop γ not passing through it, we deﬁne
the winding number, denoted I(p, γ), by

I(γ, p) =

1

2πi(cid:73)γ

1
z − p

dz

For example, by our previous results we see that if γ is a circle, we have

I(circle, p) =(cid:40)1 p inside the circle

0 p outside the circle.

If you’ve read the chapter on fundamental groups, then this is just the fundamental
group associated to C \ {p}. In particular, the winding number is always an integer (the
proof of this requires the complex logarithm, so we omit it here). In the simplest case
the winding numbers are either 0 or 1.

Deﬁnition 32.3.2. We say a loop γ is regular if I(p, γ) = 1 for all points p in the
interior of γ (for example, if γ is a counterclockwise circle).

With all these ingredients we get a stunning generalization of the Cauchy-Goursat

theorem:

Theorem 32.3.3 (Cauchy’s residue theorem)
Let f : Ω → C be meromorphic, where Ω is simply connected. Then for any loop γ
not passing through any of its poles, we have

In particular, if γ is regular then the contour integral is the sum of all the residues,
in the form

1

2πi(cid:73)γ

f (z) dz = (cid:88)pole p
2πi(cid:73)γ

f (z) dz = (cid:88)pole p

1

inside γ

I(γ, p) Res(f ; p).

Res(f ; p).

Question 32.3.4. Verify that this result coincides with what you expect when you integrate

(cid:72)γ cz−1 dz for γ a counter-clockwise circle.

The proof from here is not really too impressive – the “work” was already done in our

statements about the winding number.

Proof. Let the poles with nonzero winding number be p1, . . . , pk (the others do not aﬀect
the sum).1 Then we can write f in the form

f (z) = g(z) +

Pi(cid:18) 1
z − pi(cid:19)

k(cid:88)i=1

1 To show that there must be ﬁnitely many such poles: recall that all our contours γ : [a, b] → C are in
fact bounded, so there is some big closed disk D which contains all of γ. The poles outside D thus
have winding number zero. Now we cannot have inﬁnitely many poles inside the disk D, for D is
compact and the set of poles is a closed and isolated set!

350

Napkin, by Evan Chen (v1.5.20190718)

z−pi(cid:17) is the principal part of the pole pi. (For example, if f (z) = z3−z+1

z(z+1) we

where Pi(cid:16) 1
would write f (z) = (z − 1) + 1
“bad” parts), so

z − 1

1+z .)

The point of doing so is that the function g is holomorphic (we’ve removed all the

(cid:73)γ

g(z) dz = 0

by Cauchy-Goursat.

On the other hand, if Pi(x) = c1x + c2x2 + ··· + cdxd then
c2 ·(cid:18) 1

z − pi(cid:19) dz =(cid:73)γ

Pi(cid:18) 1

c1 ·(cid:18) 1

z − pi(cid:19) dz +(cid:73)γ

(cid:73)γ

z − pi(cid:19)2

dz + . . .

= c1 · I(γ, pi) + 0 + 0 + . . .
= I(γ, pi) Res(f ; pi).

which gives the conclusion.

§32.4 Argument principle

One tricky application is as follows. Given a polynomial P (x) = (x−a1)e1(x−a2)e2 . . . (x−
an)en, you might know that we have

P (cid:48)(x)
P (x)

=

e1

x − a1

+

e2

x − a2

+ ··· +

en

x − an

.

The quantity P (cid:48)/P is called the logarithmic derivative, as it is the derivative of log P .
This trick allows us to convert zeros of P into poles of P (cid:48)/P with order 1; moreover the
residues of these poles are the multiplicities of the roots.

In an analogous fashion, we can obtain a similar result for any meromorphic function

f .

Proposition 32.4.1 (The logarithmic derivative)
Let f : U → C be a meromorphic function. Then the logarithmic derivative f(cid:48)/f is
meromorphic as a function from U to C; its zeros and poles are:

(i) A pole at each zero of f whose residue is the multiplicity, and

(ii) A pole at each pole of f whose residue is the negative of the pole’s order.

Again, you can almost think of a pole as a zero of negative multiplicity. This spirit is
exempliﬁed below.

Proof. Dead easy with Taylor series. Let a be a zero/pole of f , and WLOG set a = 0 for
convenience. We take the Taylor series at zero to get

f (z) = ckzk + ck+1zk+1 + . . .

where k < 0 if 0 is a pole and k > 0 if 0 is a zero. Taking the derivative gives

f(cid:48)(z) = kckzk−1 + (k + 1)ck+1zk + . . . .

32 Meromorphic functions

351

Now look at f(cid:48)/f ; with some computation, it equals

f(cid:48)(z)
f (z)

=

1
z

kck + (k + 1)ck+1z + . . .

ck + ck+1z + . . .

.

So we get a simple pole at z = 0, with residue k.

Using this trick you can determine the number of zeros and poles inside a regular

closed curve, using the so-called Argument Principle.

Theorem 32.4.2 (Argument principle)
Let γ be a regular curve. Suppose f : U → C is meromorphic inside and on γ, and
none of its zeros or poles lie on γ. Then

1

2πi(cid:73)γ

f(cid:48)
f

dz = Z − P

where Z is the number of zeros inside γ (counted with multiplicity) and P is the
number of poles inside γ (again with multiplicity).

Proof. Immediate by applying Cauchy’s residue theorem alongside the preceding propo-
sition. In fact you can generalize to any curve γ via the winding number: the integral
is

1

2πi(cid:73)γ

f(cid:48)
f

dz = (cid:88)zero z

I(γ, z) − (cid:88)pole p

I(γ, p)

where the sums are with multiplicity.

Thus the Argument Principle allows one to count zeros and poles inside any region of

choice.

Computers can use this to get information on functions whose values can be computed
but whose behavior as a whole is hard to understand. Suppose you have a holomorphic
function f , and you want to understand where its zeros are. Then just start picking
various circles γ. Even with machine rounding error, the integral will be close enough
to the true integer value that we can decide how many zeros are in any given circle.
Numerical evidence for the Riemann Hypothesis (concerning the zeros of the Riemann
zeta function) can be obtained in this way.

§32.5 Philosophy: why are holomorphic functions so nice?

All the fun we’ve had with holomorphic and meromorphic functions comes down to the
fact that complex diﬀerentiability is such a strong requirement. It’s a small miracle that
C, which a priori looks only like R2, is in fact a ﬁeld. Moreover, R2 has the nice property
a f dx = 0, but

that one can draw nontrivial loops (it’s also true for real functions that(cid:82) a

this is not so interesting!), and this makes the theory much more interesting.

As another piece of intuition from Siu2: If you try to get (left) diﬀerentiable functions

over quaternions, you ﬁnd yourself with just linear functions.

2Harvard professor.

352

Napkin, by Evan Chen (v1.5.20190718)

§32.6 A few harder problems to think about

Problem 32A (Fundamental theorem of algebra). Prove that if f is a nonzero polynomial
of degree n then it has n roots.
Problem 32B† (Rouch´e’s theorem). Let f, g : U → C be holomorphic functions, where
U contains the unit disk. Suppose that |f (z)| > |g(z)| for all z on the unit circle. Prove
that f and f + g have the same number of zeros which lie strictly inside the unit circle
(counting multiplicities).

Problem 32C (Wedge contour). For each odd integer n, evaluate the improper integral

Problem 32D (Another contour). Prove that the integral

cos x
x2 + 1

dx

−∞

1

1 + xn dx.

0

(cid:90) ∞
(cid:90) ∞

converges and determine its value.
Problem 32E(cid:63). Let f : U → C be a nonconstant holomorphic function.
(a) (Open mapping theorem) Prove that f img(U ) is open in C.3

(b) (Maximum modulus principle) Show that |f| cannot have a maximum over U . That

is, show that for any z ∈ U , there is some z(cid:48) ∈ U such that |f (z)| < |f (z(cid:48))|.

3Thus the image of any open set V ⊆ U is open in C (by repeating the proof for f(cid:22)V ).

33 Holomorphic square roots and

logarithms

In this chapter we’ll make sense of a holomorphic square root and logarithm. The main
results are Theorem 33.3.2, Theorem 33.4.2, Corollary 33.5.1, and Theorem 33.5.2. If you
like, you can read just these four results, and skip the discussion of how they came to be.
Let f : U → C be a holomorphic function. A holomorphic nth root of f is a
function g : U → C such that f (z) = g(z)n for all z ∈ U . A logarithm of f is a function
g : U → C such that f (z) = eg(z) for all z ∈ U . The main question we’ll try to ﬁgure out
is: when do these exist? In particular, what if f = id?

§33.1 Motivation: square root of a complex number
To start us oﬀ, can we deﬁne √z for any complex number z?

The ﬁrst obvious problem that comes up is that for any z, there are two numbers w
such that w2 = z. How can we pick one to use? For our ordinary square root function,
we had a notion of “positive”, and so we simply took the positive root.

Let’s expand on this: given z = r (cos θ + i sin θ) (here r ≥ 0) we should take the root

to be

w = √r (cos α + i sin α) .

such that 2α ≡ θ (mod 2π); there are two choices for α (mod 2π), diﬀering by π.
For complex numbers, we don’t have an obvious way to pick α. Nonetheless, perhaps
we can also get away with an arbitrary distinction: let’s see what happens if we just
choose the α with − 1
Pictured below are some points (in red) and their images (in blue) under this “upper-
half” square root. The condition on α means we are forcing the blue points to lie on the
right-half plane.

2 π < α ≤ 1

2 π.

Here, w2

i = zi for each i, and we are constraining the wi to lie in the right half of
the complex plane. We see there is an obvious issue: there is a big discontinuity near

353

ImRe0z0z1z2z3z4z5z6z7w0w1w2w3w4w5w6w7354

Napkin, by Evan Chen (v1.5.20190718)

2 π, we are at the very left edge.

the points w5 and w7! The nearby point w6 has been mapped very far away. This
discontinuity occurs since the points on the negative real axis are at the “boundary”.
For example, given −4, we send it to −2i, but we have hit the boundary: in our interval
− 1
2 π ≤ α < 1
The negative real axis that we must not touch is what we will later call a branch cut,
but for now I call it a ray of death. It is a warning to the red points: if you cross this
line, you will die! However, if we move the red circle just a little upwards (so that it
misses the negative real axis) this issue is avoided entirely, and we get what seems to be
a “nice” square root.

In fact, the ray of death is fairly arbitrary: it is the set of “boundary issues” that arose
when we picked − 1
2 π. Suppose we instead insisted on the interval 0 ≤ α < π;
then the ray of death would be the positive real axis instead. The earlier circle we had
now works just ﬁne.

2 π < α ≤ 1

What we see is that picking a particular α-interval leads to a diﬀerent set of edge cases,
and hence a diﬀerent ray of death. The only thing these rays have in common is their
starting point of zero. In other words, given a red circle and a restriction of α, I can
make a nice “square rooted” blue circle as long as the ray of death misses it.

So, what exactly is going on?

ImRe0z0z1z2z3z4z5z6z7w0w1w2w3w4w5w6w7ImRe0z0z1z2z3z4z5z6z7w0w1w2w3w4w5w6w733 Holomorphic square roots and logarithms

355

§33.2 Square roots of holomorphic functions

To get a picture of what’s happening, we would like to consider a more general problem:
let f : U → C be holomorphic. Then we want to decide whether there is a g : U → C
such that

f (z) = g(z)2.

Our previous discussion with f = id tells us we cannot hope to achieve this for U = C;
there is a “half-ray” which causes problems. However, there are certainly functions
f : C → C such that a g exists. As a simplest example, f (z) = z2 should deﬁnitely have
a square root!
Now let’s see if we can fudge together a square root. Earlier, what we did was try to
specify a rule to force one of the two choices at each point. This is unnecessarily strict.
Perhaps we can do something like: start at a point in z0 ∈ U , pick a square root w0 of
f (z0), and then try to “fudge” from there the square roots of the other points. What
do I mean by fudge? Well, suppose z1 is a point very close to z0, and we want to pick
a square root w1 of f (z1). While there are two choices, we also would expect w0 to be
close to w1. Unless we are highly unlucky, this should tell us which choice of w1 to pick.
(Stupid concrete example: if I have taken the square root −4.12i of −17 and then ask
you to continue this square root to −16, which sign should you pick for ±4i?)

There are two possible ways we could get unlucky in the scheme above: ﬁrst, if w0 = 0,
then we’re sunk. But even if we avoid that, we have to worry that if we run a full loop
in the complex plane, we might end up in a diﬀerent place from where we started. For
concreteness, consider the following situation, again with f = id:

We started at the point z0, with one of its square roots as w0. We then wound a full
red circle around the origin, only to ﬁnd that at the end of it, the blue arc is at a diﬀerent
place where it started!

The interval construction from earlier doesn’t work either: no matter how we pick the
interval for α, any ray of death must hit our red circle. The problem somehow lies with
the fact that we have enclosed the very special point 0.

Nevertheless, we know that if we take f (z) = z2, then we don’t run into any problems

with our “make it up as you go” procedure. So, what exactly is going on?

ImRe0z0z1z2z3z4z5z6z7w0w1w2w3w4w5w6w7356

Napkin, by Evan Chen (v1.5.20190718)

§33.3 Covering projections

By now, if you have read the part on algebraic topology, this should all seem quite
familiar. The “fudging” procedure exactly describes the idea of a lifting.

More precisely, recall that there is a covering projection

Let V = {z ∈ U | f (z) (cid:54)= 0}. For z ∈ U \ V , we already have the square root g(z) =

Then essentially, what we are trying to do is construct a lifting g in the diagram

(−)2 : C \ {0} → C \ {0}.
(cid:112)f (z) = √0 = 0. So the burden is completing g : V → C.
E = C \ {0}
-

g

-

p=(−)2
?

B = C \ {0}.

V

f

Our map p can be described as “winding around twice”. Our Theorem 59.2.5 now tells

us that this lifting exists if and only if

f img
∗

(π1(V )) ⊆ pimg
∗

(π1(E))

is a subset of the image of π1(E) by p. Since B and E are both punctured planes, we
can identify them with S1.

Question 33.3.1. Show that the image under p is exactly 2Z once we identify π1(B) = Z.

That means that for any loop γ in V , we need f ◦ γ to have an even winding number

around 0 ∈ B. This amounts to

1

2π(cid:73)γ

f(cid:48)
f

dz ∈ 2Z

since f has no poles.

Replacing 2 with n and carrying over the discussion gives the ﬁrst main result.

Theorem 33.3.2 (Existence of holomorphic nth roots)
Let f : U → C be holomorphic. Then f has a holomorphic nth root if and only if

1

2πi(cid:73)γ

f(cid:48)
f

dz ∈ nZ

for every contour γ in U .

§33.4 Complex logarithms

The multivalued nature of the complex logarithm comes from the fact that

exp(z + 2πi) = exp(z).

33 Holomorphic square roots and logarithms

357

So if ew = z, then any complex number w + 2πik is also a solution.

We can handle this in the same way as before: it amounts to a lifting of the following

diagram.

E = C
-

g

-

p=exp

?

B = C \ {0}

U

f

There is no longer a need to work with a separate V since:

Question 33.4.1. Show that if f has any zeros then g possibly can’t exist.

In fact, the map exp : C → C \ {0} is a universal cover, since C is simply connected.
Thus, pimg(π1(C)) is trivial. So in addition to being zero-free, f cannot have any winding
number around 0 ∈ B at all. In other words:

Theorem 33.4.2 (Existence of logarithms)
Let f : U → C be holomorphic. Then f has a logarithm if and only if

1

2πi(cid:73)γ

f(cid:48)
f

dz = 0

for every contour γ in U .

§33.5 Some special cases

The most common special case is

Corollary 33.5.1 (Nonvanishing functions from simply connected domains)
Let f : Ω → C be continuous, where Ω is simply connected. If f (z) (cid:54)= 0 for every
z ∈ Ω, then f has both a logarithm and holomorphic nth root.

Finally, let’s return to the question of f = id from the very beginning. What’s the best

domain U such that

√− : U → C

is well-deﬁned? Clearly U = C cannot be made to work, but we can do almost as well.
For note that the only zero of f = id is at the origin. Thus if we want to make a logarithm
exist, all we have to do is make an incision in the complex plane that renders it impossible
to make a loop around the origin. The usual choice is to delete negative half of the real
axis, our very ﬁrst ray of death; we call this a branch cut, with branch point at 0 ∈ C
(the point which we cannot circle around). This gives

358

Napkin, by Evan Chen (v1.5.20190718)

Theorem 33.5.2 (Branch cut functions)

There exist holomorphic functions

log : C \ (−∞, 0] → C
n√− : C \ (−∞, 0] → C

satisfying the obvious properties.

There are many possible choices of such functions (n choices for the nth root and inﬁnitely
many for log); a choice of such a function is called a branch. So this is what is meant
by a “branch” of a logarithm.

The principal branch is the “canonical” branch, analogous to the way we arbitrarily

pick the positive branch to deﬁne √− : R≥0 → R≥0. For log, we take the w such that
ew = z and the imaginary part of w lies in (−π, π] (since we can shift by integer multiples
of 2πi). Often, authors will write Log z to emphasize this choice.

§33.6 A few harder problems to think about

Problem 33A. Show that a holomorphic function f : U → C has a holomorphic
logarithm if and only if it has a holomorphic nth root for every integer n.
Problem 33B. Show that the function f : U → C by z (cid:55)→ z(z − 1) has a holomorphic
square root, where U is the entire complex plane minus the closed interval [0, 1].

X

Measure Theory

Part X: Contents

34 Measure spaces

361
. . . . . . . . . . . . . . . . . . . 361
34.1 Motivating measure spaces via random variables
34.2 Motivating measure spaces geometrically . . . . . . . . . . . . . . . . . . . . . . . 362
. . . . . . . . . . . . . . . . . . . . . . . . . . 363
34.3 σ-algebras and measurable spaces
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 364
34.4 Measure spaces
34.5 A hint of Banach-Tarski . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 365
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 366
34.6 Measurable functions
34.7 On the word “almost” (TO DO) . . . . . . . . . . . . . . . . . . . . . . . . . . . 366
. . . . . . . . . . . . . . . . . . . . . . . . 366
34.8 A few harder problems to think about

35 Constructing the Borel and Lebesgue measure

367
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367
35.1 Pre-measures
35.2 Outer measures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 368
35.3 Carath´eodory extension for outer measures . . . . . . . . . . . . . . . . . . . . . . 370
35.4 Deﬁning the Lebesgue measure . . . . . . . . . . . . . . . . . . . . . . . . . . . . 372
35.5 A fourth row: Carath´eodory for pre-measures . . . . . . . . . . . . . . . . . . . . . 374
. . . . . . . . . . . . . . . . . . . . . 375
35.6 From now on, we assume the Borel measure
. . . . . . . . . . . . . . . . . . . . . . . . 375
35.7 A few harder problems to think about

36 Lebesgue integration

377
36.1 The deﬁnition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377
36.2 Relation to Riemann integrals (or: actually computing Lebesgue integrals) . . . . . . . 379
. . . . . . . . . . . . . . . . . . . . . . . . 380
36.3 A few harder problems to think about

37 Swapping order with Lebesgue integrals

381
37.1 Motivating limit interchange . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381
37.2 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382
37.3 Fatou’s lemma
37.4 Everything else . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382
37.5 Fubini and Tonelli . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384
. . . . . . . . . . . . . . . . . . . . . . . . 384
37.6 A few harder problems to think about

38 Bonus: A hint of Pontryagin duality

385
38.1 LCA groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 385
38.2 The Pontryagin dual . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 386
38.3 The orthonormal basis in the compact case . . . . . . . . . . . . . . . . . . . . . . 387
38.4 The Fourier transform of the non-compact case . . . . . . . . . . . . . . . . . . . . 388
38.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 388
. . . . . . . . . . . . . . . . . . . . . . . . 389
38.6 A few harder problems to think about

34 Measure spaces

Here is an outline of where we are going next. Our goal over the next few chapters is
to develop the machinery to state (and in some cases prove) the law of large numbers
and the central limit theorem. For these purposes, the scant amount of work we did in
Calculus 101 is going to be awfully insuﬃcient: integration over R (or even Rn) is just
not going to cut it.

This chapter will develop the theory of “measure spaces”, which you can think of as
“spaces equipped with a notion of size”. We will then be able to integrate over these with
the so-called Lebesgue integral (which in some senses is almost strictly better than the
Riemann one).

Letter connotations

There are a lot of “types” of objects moving forward, so here are the letter connotations
we’ll use throughout the next several chapters. This makes it easier to tell what the
“type” of each object is just by which letter is used.

 Measure spaces denoted by Ω, their elements denoted by ω.

 Algebras and σ-algebras denoted by script A , B, . . . . Sets in them denoted by

early capital Roman A, B, C, D, E, . . . .

 Measures (i.e. functions assigning sets to reals) denoted usually by µ or ρ.

 Random variables (functions sending worlds to reals) denoted usually by late capital

Roman X, Y , Z, . . . .

 Functions from R → R by Roman letters like f and g for pdf’s and F and G for

cdf’s.

 Real numbers denoted by lower Roman letters like x, y, z.

§34.1 Motivating measure spaces via random variables

To motivate why we want to construct measure spaces, I want to talk about a (real)
random variable, which you might think of as

 the result of a coin ﬂip,

 the high temperature in Boston on Saturday,

 the possibility of rain on your 18.725 date next weekend.

Why does this need a long theory to develop well? For a simple coin ﬂip one intuitively
just thinks “50% heads, 50% tails” and is done with it. The situation is a little trickier
with temperature since it is continuous rather than discrete, but if all you care about is
that one temperature, calculus seems like it might be enough to deal with this.

But it gets more slippery once the variables start to “talk to” each other: the high
temperature tells you a little bit about whether it will rain, because e.g. if the temperature

361

362

Napkin, by Evan Chen (v1.5.20190718)

is very high it’s quite likely to be sunny. Suddenly we ﬁnd ourselves wishing we could talk
about conditional probability, but this a whole can of worms — the relations between
these sorts of things can get very complicated very quickly.

The big idea to getting a formalism for this is that:

Our measure spaces Ω will be thought of as a space of entire worlds, with
each ω ∈ Ω representing a world. Random variables are functions from
worlds to R.

This way, the space of “worlds” takes care of all the messy interdependence.

Then, we can assign “measures” to sets of worlds: for example, to be fair coin means
that if you are only interested in that one coin ﬂip, the “fraction” of worlds which that
coin showed heads should be 1
2 . This is in some ways backwards from what you were
told in high-school: oﬃcially, we start with the space of worlds, rather than starting with
the probabilities.

It will soon be clear that there is no way we can assign a well-deﬁned measure to every
single one of the 2Ω subsets. Fortunately, in practice, we won’t need to, and the notion
of a σ-algebra will capture the idea of “enough measur-able sets for us to get by”.

Remark 34.1.1 (Random seeds) — Another analogy if you do some programming:
each ω ∈ Ω is a random seed, and everything is determined from there.

§34.2 Motivating measure spaces geometrically

So, we have a set Ω of possible points (which in the context of the previous discussion
can be thought of as the set of worlds), and we want to assign a measure (think volume)
to subsets of points in Ω. We will now describe some of the obstacles that we will face, in
order to motivate how measure spaces are deﬁned (as the previous section only motivated
why we want such things).

If you try to do this na¨ıvely, you basically immediately run into set-theoretic issues.
A good example to think about why this might happen is if Ω = R2 with the measure
corresponding to area. You can deﬁne the area of a triangle as in high school, and you
can then try and deﬁne the area of a circle, maybe by approximating it with polygons.
But what area would you assign to the subset Q2, for example? (It turns out “zero” is
actually a working answer.) Or, a unit disk is composed of inﬁnitely many points; each
of the points better have measure zero, but why does their union have measure π then?
Blah blah blah.

We’ll say more about this later, but you might have already heard of the Banach-
Tarski paradox which essentially shows there is no good way that you can assign a
measure to every single subset of R3 and still satisfy basic sanity checks. There are just
too many possible subsets of Euclidean space.

However, the good news is that most of these sets are not ones that we will ever
care about, and it’s enough to deﬁne measures for certain “suﬃciently nice sets”. The
adjective we will use is measurable, and it will turn out that this will be way, way more
than good enough for any practical purposes.

We will generally use A, B, . . . for measurable sets and denote the entire family of

measurable sets by curly A .

34 Measure spaces

363

§34.3 σ-algebras and measurable spaces

Here’s the machine code.

Deﬁnition 34.3.1. A measurable space consists of a space Ω of points, and a σ-
algebra A of subsets of Ω (the “measurable sets” of Ω). The set A is required to satisfy
the following axioms:

 A contains ∅ and Ω.

 A should be closed under complements and countable unions/intersections. (Hint

on nomenclature: σ usually indicates some sort of “countably ﬁnite” condition.)

(Complaint: this terminology is phonetically confusing, because it can be confused
with “measure space” later. The way to think about is that “measurable spaces have a
σ-algebra, so we could try to put a measure on it, but we haven’t, yet.”)

Though this deﬁnition is how we actually think about it in a few select cases, for the

most part, and we will usually instantiate A in practice in a diﬀerent way:

Deﬁnition 34.3.2. Let Ω be a set, and consider some family of subsets F of Ω. Then
the σ-algebra generated by F is the smallest σ-algebra A which contains F .

As is commonplace in math, when we see “generated”, this means we sort of let the
deﬁnition “take care of itself”. So, if Ω = R, maybe I want A to contain all open sets.
Well, then the deﬁnition means it should contain every complements too, so it contains
all the closed sets. Then it has to contain all the half-open intervals too, and then. . . .
Rather than try to reason out what exactly the ﬁnal shape A looks like (which basically
turns out to be impossible), we just give up and say “A is all the sets you can get if you
start with the open sets and apply repeatedly union/complement operations”. Or even
more bluntly: “start with closed sets, shake vigorously”.

I’ve gone on too long with no examples.

Example 34.3.3 (Examples of measurable spaces)
The ﬁrst two examples actually say what A is; the third example (most important)
will use generation.

(a) If Ω is any set, then the power set A = 2Ω is obviously a σ-algebra. This will

be used if Ω is countably ﬁnite, but it won’t be very helpful if Ω is huge.

(b) If Ω is an uncountable set, then we can declare A to be all subsets of Ω which
are either countable, or which have countable complement. (You should check
this satisﬁes the deﬁnitions.) This is a very “coarse” algebra.

(c) If Ω is a topological space, the Borel σ-algebra is deﬁned as the σ-algebra
generated by all the open sets of Ω. We denote it by B(Ω), and call the space
a Borel space. As warned earlier, it is basically impossible to describe what
it looks like, and instead you should think of it as saying “we can measure the
open sets”.

Question 34.3.4. Show that the closed sets are in B(Ω) for any topological space Ω. Show
that [0, 1) is also in B(R).

364

Napkin, by Evan Chen (v1.5.20190718)

§34.4 Measure spaces

Deﬁnition 34.4.1. Measurable spaces (Ω, A ) are then equipped with a function µ : A →
[0, +∞] called the measure, which is required to satisfy the following axioms:

 µ(∅) = 0

 Countable additivity: If A1, A2, . . . are disjoint sets in A , then

µ(cid:32)(cid:71)n

An(cid:33) =(cid:88)n

µ(An).

The triple (Ω, A , µ) is called a measure space. It’s called a probability space if
µ(Ω) = 1.

Exercise 34.4.2 (Weaker equivalent deﬁnitions). I chose to give axioms for A and µ that
capture how people think of them in practice, which means there is some redundancy: for
example, being closed under complements and unions is enough to get intersections, by de
Morgan’s law. Here are more minimal deﬁnitions, which are useful if you are trying to prove
something satisﬁes them to reduce the amount of work you have to do:
(a) The axioms on A can be weakened to (i) ∅ ∈ A and (ii) A is closed under complements
(b) The axioms on µ can be weakened to (i) µ(∅) = 0, (ii) µ(A (cid:116) B) = µ(A) + µ(B), and
(iii) for A1 ⊃ A2 ⊃ . . . we have µ ((cid:84)n An) = limn µ(An).

and countable disjoint unions.

Remark 34.4.3 — Here are some immediate remarks on these deﬁnitions.

 If A ⊆ B are measurable, then µ(A) ≤ µ(B) since µ(B) = µ(A) + µ(B − A).
 In particular, in a probability space all measures are in [0, 1]. On the other
hand, for general measure spaces we’ll allow +∞ as a possible measure (hence
the choice of [0, +∞] as codomain for µ).

 We want to allow at least countable unions / additivity because with ﬁnite
unions it’s too hard to make progress: it’s too hard to estimate the area of
a circle without being able to talk about limits of countably inﬁnitely many
triangles.

We don’t want to allow uncountable unions and additivity, because uncountable sums
basically never work out. In particular, there is a nice elementary exercise as follows:

Exercise 34.4.4 (Tricky). Let S be an uncountable set of positive real numbers. Show that
some ﬁnite subset T ⊆ S has sum greater than 102019. Colloquially, “uncountable many
positive reals cannot have ﬁnite sum”.

So countable sums are as far as we’ll let the inﬁnite sums go. This is the reason why we
considered σ-algebras in the ﬁrst place.

34 Measure spaces

365

Example 34.4.5 (Measures)
We now discuss measures on each of the spaces in our previous examples.
(a) If A = 2Ω (or for that matter any A ) we may declare µ(A) = |A| for each A ∈ A
(even if |A| = ∞). This is called the counting measure, simply counting the
number of elements.

This is useful if Ω is countably ﬁnite, and optimal if Ω is ﬁnite (and nonempty).
In the latter case, we will often normalize by µ(A) = |A|
so that Ω becomes a
|Ω|
probability space.

(b) Suppose Ω was uncountable and we took A to be the countable sets and their

complements. Then

µ(A) =(cid:40)0 A is countable

1 Ω \ A is countable

is a measure. (Check this.)

(c) Elephant in the room: deﬁning a measure on B(Ω) is hard even for Ω = R, and
is done in the next chapter. So you will have to hold your breath. Right now, all
you know is that by declaring my intent to deﬁne a measure B(Ω), I am hoping
that at least every open set will have a volume.

§34.5 A hint of Banach-Tarski

I will now try to convince you that B(Ω) is a necessary concession, and for general topo-
logical spaces like Ω = Rn, there is no hope of assigning a measure to 2Ω.

Example 34.5.1 (A geometric example of why A = 2Ω is unsuitable)
Let Ω denote the closed unit disk in R2 and A = 2Ω. We will show that any measure
µ on Ω with µ(Ω) = 1 will have undesirable properties.

Let ∼ denote an equivalence relation on Ω deﬁned as follows: two points are
equivalent if they diﬀer by a rotation around the origin by a multiple of π. We
may pick a representative from each equivalence class, letting X denote the set of
representatives. Then

Ω = (cid:71)q∈Q

0≤q<2

(X rotated by qπ radians) .

Since we’ve only rotated X, each of the rotations should have the same measure m.
But µ(Ω) = 1, and there is no value we can assign that measure: if m = 0 we get
µ(Ω) = 0 and m > 0 we get µ(Ω) = ∞.

Remark 34.5.2 (Choice) — Experts may recognize that picking a representative
(i.e. creating set X) technically requires the Axiom of Choice. That is why, when
people talk about Banach-Tarski issues, the Axiom of Choice almost always gets
honorable mention as well.

366

Napkin, by Evan Chen (v1.5.20190718)

Stay tuned to actually see a construction for B(Rn) in the next chapter.

§34.6 Measurable functions

In the past, when we had topological spaces, we considered continuous functions. The
analog here is:

Deﬁnition 34.6.1. Let (X, A ) and (Y, B) be measurable spaces (or measure spaces).
A function f : X → Y is measurable if for any measurable set S ⊆ Y (i.e. S ∈ B) we
have f pre(S) is measurable (i.e. f pre(S) ∈ A ).

In practice, most functions you encounter will be continuous anyways, and in that case

we are ﬁne.

Proposition 34.6.2 (Continuous implies Borel measurable)

Suppose X and Y are topological spaces and we pick the Borel measures on both.
A function f : X → Y which is continuous as a map of topological spaces is also
measurable.

Proof. Follows from the fact that pre-images of open sets are open, and the Borel measure
is generated by open sets.

§34.7 On the word “almost” (TO DO)

write this
write this

In later chapters we will begin seeing the phrase “almost everywhere” start to come up,
and it seems prudent to take the time to talk about it now.

§34.8 A few harder problems to think about

Problem 34A†. Let (Ω, A , µ) be a probability space. Show that the intersection of
countably many sets of measure 1 also has measure 1.

Problem 34B. Let A be a σ-algebra on a set Ω. Suppose that A has countable
cardinality. Prove that |A | is ﬁnite and equals a power of 2.

35 Constructing the Borel and

Lebesgue measure

It’s very diﬃcult to deﬁne in one breath a measure on the Borel space B(Rn). It is
easier if we deﬁne a weaker notion ﬁrst. There are two such weaker notions that we will
deﬁne:

 A pre-measure: satisﬁes the axioms of a measure, but deﬁned on fewer sets
than a measure: they’ll be deﬁned on an “algebra” rather than the full-ﬂedged
“σ-algebra”.

 An outer measure: deﬁned on 2Ω but satisﬁes weaker axioms.

It will turn out that pre-measures yield outer measures, and outer measures yield
measures.

§35.1 Pre-measures

Prototypical example for this section: Let Ω = R2. Then we take A0 generated by
rectangles, with µ0 the usual area.

The way to deﬁne a pre-measure is to weaken the σ-algebra to an algebra.

Deﬁnition 35.1.1. Let Ω be a set. We deﬁne notions of an algebra, which is the same
as σ-algebra except with “countable” replaced by ﬁnite everywhere.

That is: an algebra A0 on Ω is a nonempty subset of 2Ω, which is closed under
complement and ﬁnite union. The smallest algebra containing a subset F ⊆ 2Ω is the
algebra generated by F .

In practice, we will basically always use generation for algebras.

Example 35.1.2
When Ω = Rn, we can let L0 be the algebra generated by [a1, b1] × ··· × [an, bn]. A
typical element might look like:

Unsurprisingly, since we have ﬁnitely many rectangles and their complements involved,
in this case we actually can unambiguously assign an area, and will do so soon.

Deﬁnition 35.1.3. A pre-measure µ0 on a algebra A0 is a function µ0 : A0 → [0, +∞]
which satisﬁes the axioms

 µ0(∅) = 0, and

367

368

Napkin, by Evan Chen (v1.5.20190718)

 Countable additivity: if A1, A2, . . . are disjoint sets in A0 and moreover the

disjoint union(cid:70) Ai is contained in A0 (not guaranteed by algebra axioms!), then

µ0(An).

µ0(cid:32)(cid:71)n

An(cid:33) =(cid:88)n

Example 35.1.4 (The pre-measure on Rn)
Let Ω = R2. Then, let L0 be the algebra generated by rectangles [a1, b1] × [a2, b2].
We then let

µ0 ([a1, b1] × [a2, b2]) = (a2 − a1)(b2 − b1)

the area of the rectangle. As elements of L0 are simply ﬁnite unions of rectangles
and their complements (picture drawn earlier), it’s not diﬃcult to extend this to a
pre-measure λ0 which behaves as you expect — although we won’t do this.

Since we are sweeping something under the rug that turns out to be conceptually

important, I’ll go ahead and blue-box it.

Proposition 35.1.5 (Geometry sanity check that we won’t prove)
For Ω = Rn and L0 the algebra generated by rectangular prisms, one can deﬁne a
pre-measure λ0 on L0.

From this point forwards, we will basically do almost no geometry1 whatsoever in deﬁning
the measure B(Rn), and only use set theory to extend our measure. So, Proposition 35.1.5
is the only sentry which checks to make sure that our “initial deﬁnition” is sane.

To put the point another way, suppose an insane scientist2 tried to deﬁne a notion
of area in which every rectangle had area 1. Intuitively, this shouldn’t be possible: every
rectangle can be dissected into two halves and we ought to have 1 + 1 (cid:54)= 1. However, the
only thing that would stop them is that they couldn’t extend their pre-measure on the
algebra L0. If they somehow got past that barrier and got a pre-measure, nothing in the
rest of the section would prevent them from getting an entire bona ﬁde measure with
this property. Thus, in our construction of the Lebesgue measure, most of the geometric
work is captured in the (omitted) proof of Proposition 35.1.5.

§35.2 Outer measures

Prototypical example for this section: Keep taking Ω = R2; see the picture to follow.

The other way to weaken a measure is to relax the countable additivity, and this yields

the following:
Deﬁnition 35.2.1. An outer measure µ∗ on a set Ω is a function µ∗ : 2Ω → [0, +∞]
satisfying the following axioms:

 µ∗(∅) = 0;

1White lie. Technically, we will use one more fact: that open sets of Rn can be covered by countably
inﬁnitely many rectangles, as in Exercise 35.5.1. This step doesn’t involve any area assignments,
though.

2Because “mad scientists” are overrated.

35 Constructing the Borel and Lebesgue measure

369

 if E ⊆ F and E, F ∈ 2Ω then µ∗(E) ≤ µ∗(F );
 for any subsets E1, E2, . . . of Ω we have

µ∗(cid:32)(cid:91)n

En(cid:33) ≤(cid:88)n

µ∗(En).

(I don’t really like the word “outer measure”, since I think it is a bit of a misnomer: I
would rather call it “fake measure”, since it’s not a measure either.)

The reason for the name “outer measure” is that you almost always obtain outer
measures by approximating them from “outside” sets. Oﬃcially, the result is often stated
as follows (as Problem 35A†).

For a set Ω, let E be any subset of 2Ω and let ρ : E → [0, +∞] be any function.
Then

µ∗(E) = inf(cid:40) ∞(cid:88)n=1

ρ(En) | En ∈ E, E ⊆

En(cid:41)

∞(cid:91)n=1

is an outer measure.

However, I think the above theorem is basically always wrong to use in practice,
because it is way too general. As I warned with the insane scientist, we really do want
some sort of sanity conditions on ρ: otherwise, if we apply the above result as stated,
there is no guarantee that µ∗ will be compatible with ρ in any way.

So, I think it is really better to apply the theorem to pre-measures µ0 for which one
does have some sort of guarantee that the resulting µ∗ is compatible with µ0. In practice,
this is always how we will want to construct our outer measures.

Theorem 35.2.2 (Constructing outer measures from pre-measures)
Let µ0 be a pre-measure on an algebra A0 on a set Ω.
(a) The map µ∗ : 2Ω → [0, +∞] deﬁned by

µ∗(E) = inf(cid:40) ∞(cid:88)n=1

µ0(An) | An ∈ A0, E ⊆

∞(cid:91)n=1

An(cid:41)

is an outer measure.

(b) Moreover, this measure agrees with µ0 on sets in A0.

Intuitively, what is going on is that µ∗(A) is the inﬁmum of coverings of A by countable
unions of elements in A0. Part (b) is the ﬁrst half of the compatibility condition I
promised; the other half appears later as Proposition 35.3.2.

Proof of Theorem 35.2.2. As alluded to already, part (a) is a special case of Problem 35A†
(and proving it in this generality is actually easier, because you won’t be distracted by
unnecessary properties).

We now check (b), that µ∗(A) = µ0(A) for A ∈ A0. One bound is quick:
Question 35.2.3. Show that µ∗(A) ≤ µ0(A).

370

Napkin, by Evan Chen (v1.5.20190718)

For the reverse, suppose that A ⊆(cid:83)n An. Then, deﬁne the sets

B1 = A ∩ A1
B2 = (A ∩ A2) \ B1
B3 = (A ∩ A3) \ B2

...

and so on. Then the Bn are disjoint elements of A0 with Bn ⊂ An, and we have rigged
the deﬁnition so that(cid:70)n Bn = A. Thus by deﬁnition of pre-measure,

µ0(A) =(cid:88)n

µ0(Bn) ≤(cid:88)n

µ0(An)

as desired.

Example 35.2.4
Let Ω = R2 and λ0 the pre-measure from before. Then λ∗(A) is, intuitively, the
inﬁmum of coverings of the set A by rectangles. Here is a picture you might use to
imagine the situation with A being the unit disk.

Missing

ﬁgure

circles covered by rectangles

§35.3 Carath´eodory extension for outer measures

We will now take any outer measure and turn it into a proper measure. To do this, we
ﬁrst need to specify the σ-algebra on which we will deﬁne the measure.

Deﬁnition 35.3.1. Let µ∗ be an outer measure. We say a set A is Carath´eodory
measurable with respect to µ∗, or just µ∗-measurable, if the following condition
holds: for any set E ∈ 2Ω,

µ∗(E) = µ∗(E ∩ A) + µ∗(E \ A).

This deﬁnition is hard to motivate, but turns out to be the right one. One way to
motivate is this: it turns out that in Rn, it will be equivalent to a reasonable geometric
condition (which I will state in Proposition 35.4.3), but since that geometric deﬁnition
requires information about Rn itself, this is the “right” generalization for general measure
spaces.

Since our goal was to extend our A0, we had better make sure this deﬁnition lets us

measure the initial sets that we started with!

35 Constructing the Borel and Lebesgue measure

371

Proposition 35.3.2 (Carath´eodory measurability is compatible with the initial A0)
Suppose µ∗ was obtained from a pre-measure µ0 on an algebra A0, as in Theo-
rem 35.2.2. Then every set in A0 is µ∗-measurable.

This is the second half of the compatibility condition that we get if we make sure our initial
µ0 at least satisﬁes the pre-measure axioms. (The ﬁrst half was (b) of Theorem 35.2.2.)

Proof. Let A ∈ A0 and E ∈ 2Ω; we wish to prove µ∗(E) = µ∗(E ∩ A) + µ∗(E \ A). The
deﬁnition of outer measure already requires µ∗(E) ≤ µ∗(E ∩ A) + µ∗(E \ A) and so it’s
enough to prove the reverse inequality.
By deﬁnition of inﬁmum, for any ε > 0, there is a covering E ⊂(cid:83)n An with µ∗(E)+ε ≥
(cid:80)n µ0(An). But

(µ0(An ∩ A) + µ0(An \ A)) ≥ µ∗(E ∩ A) + µ∗(E \ A)

(cid:88)n

µ0(An) =(cid:88)n

with the ﬁrst equality being the deﬁnition of pre-measure on A0, the second just being
by deﬁnition of µ∗ (since An ∩ A certainly covers E ∩ A, for example). Thus µ∗(E) + ε ≥
µ∗(E ∩ A) + µ∗(E \ A). Since the inequality holds for any ε > 0, we’re done.

To add extra icing onto the cake, here is one more niceness condition which our

constructed measure will happen to satisfy.

Deﬁnition 35.3.3. A null set of a measure space (Ω, A , µ) is a set A ∈ A with
µ(A) = 0. A measure space (Ω, A , µ) is complete if whenever A is a null set, then all
subsets of A are in A as well (and hence null sets).

This is a nice property to have, for obvious reasons. Visually, if I have a bunch of
dust which I already assigned weight zero, and I blow away some of the dust, then the
remainder should still have an assigned weight — zero. The extension theorem will give
us σ-algebras with this property.

Theorem 35.3.4 (Carath´eodory extension theorem for outer measures)
If µ∗ is an outer measure, and A cm is the set of µ∗-measurable sets with respect
to µ∗, then A cm is a σ-algebra on Ω, and the restriction µcm of µ∗ to A cm gives a
complete measure space.

(Phonetic remark: you can think of the superscript cm as standing for either “Carath´eodory
measurable” or “complete”. Both are helpful for remembering what this represents. This
notation is not standard but the pun was too good to resist.)

Thus, if we compose Theorem 35.2.2 with Theorem 35.3.4, we ﬁnd that every pre-
measure µ0 on an algebra A0 naturally gives a σ-algebra A cm with a complete measure
µcm, and our two compatibility results (namely (b) of Theorem 35.2.2, together with
Proposition 35.3.2) means that A cm ⊃ A0 and µcm agrees with µ.
to restriction process.

Here is a table showing the process, where going down each row of the table corresponds

372

Napkin, by Evan Chen (v1.5.20190718)

Construct order Notes

2Ω

µ∗

A cm µcm

Step 2

Step 3

µ∗ is outer measure obtained from µ0

A cm deﬁned as µ∗-measurable sets,
(A cm, µcm) is complete.

A0

µ0

Step 1

µ0 is a pre-measure

§35.4 Deﬁning the Lebesgue measure

This lets us ﬁnally deﬁne the Lebesgue measure on Rn. We wrap everything together at
once now.

Deﬁnition 35.4.1. We create a measure on Rn by the following procedure.

 Start with the algebra L0 generated by rectangular prisms, and deﬁne a pre-measure

λ0 on this L0 (this was glossed over in the example).

 By Theorem 35.2.2, this gives the Lebesgue outer measure λ∗ on 2Rn

, which is

compatible on all the rectangular prisms.

 By Carath´eodory (Theorem 35.3.4), this restricts to a complete measure λ on the
σ-algebra L(Rn) of λ∗-measurable sets (which as promised contains all rectangular
prisms).3

The resulting complete measure, denoted λ, is called the Lebesgue measure.

The algebra L(Rn) we obtained will be called the Lebesgue σ-algebra; sets in it are

said to be Lebesgue measurable.

Here is the same table from before, with the values ﬁlled in for the special case Ω = Rn,

which gives us the Lebesgue algebra.

2Rn

λ∗

L(Rn) λ
L0
λ0

Construct order Notes

Step 2

λ∗ is Lebesgue outer measure

Step 3

Lebesgue σ-algebra (complete)

Step 1

Deﬁne pre-measure on rectangles

Of course, now that we’ve gotten all the way here, if we actually want to compute any
measures, we can mostly gleefully forget about how we actually constructed the measure
and just use the properties. The hard part was to showing that there is a way to assign
measures consistently; actually ﬁguring out what that measure’s value is given that it
exists is often much easier. Here is an example.

3If I wanted to be consistent with the previous theorems, I might prefer to write Lcm and λcm for

emphasis. It seems no one does this, though, so I won’t.

35 Constructing the Borel and Lebesgue measure

373

Example 35.4.2 (The Cantor set has measure zero)
The standard middle-thirds Cantor set is the subset [0, 1] obtained as follows: we
ﬁrst delete the open interval (1/3, 2/3). This leaves two intervals [0, 1/3] and [2/3, 1]
from which we delete the middle thirds again from both, i.e. deleting (1/9, 2/9) and
(7/9, 8/9). We repeat this procedure indeﬁnitely and let C denote the result. An
illustration is shown below.

Image from [1207]

It is a classic fact that C is uncountable (it consists of ternary expansions omitting
the digit 1). But it is measurable (it is an intersection of closed sets!) and we contend
it has measure zero. Indeed, at the nth step, the result has measure (2/3)n leftover.
So µ(C) ≤ (2/3)n for every n, forcing µ(C) = 0.

This is fantastic, but there is one elephant in the room: how are the Lebesgue σ-algebra
and the Borel σ-algebra related? To answer this question brieﬂy, I will state two results
(but another answer is given in the next section). The ﬁrst is a geometric interpretation
of the strange Carath´eodory measurable hypothesis.

Proposition 35.4.3 (A geometric interpretation of Lebesgue measurability)
A set A ⊆ Rn is Lebesgue measurable if and only if for every ε > 0, there is an open
set U ⊃ A such that

where λ∗ is the Lebesgue outer measure.

λ∗(U \ A) < ε

I want to say that this was Lebesgue’s original formulation of “measurable”, but I’m not
sure about that. In any case, we won’t need to use this, but it’s good to see that our
deﬁnition of Lebesgue measurable has a down-to-earth geometric interpretation.

Question 35.4.4. Deduce that every open set is Lebesgue measurable. Conclude that the
Lebesgue σ-algebra contains the Borel σ-algebra. (A diﬀerent proof is given later on.)

However, the containment is proper: there are more Lebesgue measurable sets than
Borel ones. Indeed, it can actually be proven using transﬁnite induction (though we
won’t) that |B(R)| = |R|. Using this, one obtains:

Exercise 35.4.5. Show the Borel σ-algebra is not complete. (Hint: consider the Cantor
set. You won’t be able to write down an example of a non-measurable set, but you can use
cardinality arguments.) Thus the Lebesgue σ-algebra strictly contains the Borel one.

Nonetheless, there is a great way to describe the Lebesgue σ-algebra, using the idea of

completeness.
Deﬁnition 35.4.6. Let (Ω, A , µ) be a measure space. The completion (Ω, A , µ) is
deﬁned as follows: we let

A = {A ∪ N | A ∈ A , N subset of null set} .

374

Napkin, by Evan Chen (v1.5.20190718)

and µ(A ∪ N ) = µ(A). One can check this is well-deﬁned, and in fact µ is the unique
extension of µ from A to A .
This looks more complicated than it is. Intuitively, all we are doing is “completing”
the measure by telling µ to regard any subset of a null set as having measure zero, too.

Then, the saving grace:

Theorem 35.4.7 (Lebesgue is completion of Borel)
For Rn, the Lebesgue measure is the completion of the Borel measure.

Proof. This actually follows from results in the next section, namely Exercise 35.5.1 and
part (c) of Carath´eodory for pre-measures (Theorem 35.5.5).

§35.5 A fourth row: Carath´eodory for pre-measures

Prototypical example for this section: The fourth row for the Lebesgue measure is B(Rn).
In many cases, A cm is actually bigger than our original goal, and instead we only need
to extend µ0 on A0 to µ on A , where A is the σ-algebra generated by A0. Indeed, our
original goal was to get B(Rn), and in fact:

Exercise 35.5.1. Show that B(Rn) is the σ-algebra generated by the L0 we deﬁned earlier.

Fortunately, this restriction is trivial to do.
Question 35.5.2. Show that A cm ⊃ A , so we can just restrict µcm to A .

We will in a moment add this as the fourth row in our table.

However, if this is the end goal, than a somewhat diﬀerent Carath´eodory theorem can

be stated because often one more niceness condition holds:

Deﬁnition 35.5.3. A pre-measure or measure µ on Ω is σ-ﬁnite if Ω can be written as

a countable union Ω =(cid:83)n An with µ(An) < ∞ for each n.

Question 35.5.4. Show that the pre-measure λ0 we had, as well as the Borel measure
B(Rn), are both σ-ﬁnite.

Actually, for us, σ-ﬁnite is basically always going to be true, so you can more or less just
take it for granted.

Theorem 35.5.5 (Carath´eodory extension theorem for pre-measures)
Let µ0 be a pre-measure on an algebra A0 of Ω, and let A denote the σ-algebra
generated by A0. Let A cm, µcm be as in Theorem 35.3.4. Then:

(a) The restriction of µcm to A gives a measure µ extending µ0.

(b) If µ0 was σ-ﬁnite, then µ is the unique extension of µ0 to A .

(c) If µ0 was σ-ﬁnite, then µcm is the completion of µ, hence the unique extension

of µ0 to A cm.

35 Constructing the Borel and Lebesgue measure

375

Here is the updated table, with comments if µ0 was indeed σ-ﬁnite.

Construct order Notes

2Ω

µ∗

A cm µcm

A

A0

µ

µ0

Step 2

Step 3

Step 4

Step 1

µ∗ is outer measure obtained from µ0

(A cm, µcm) is completion (A , µ),
A cm deﬁned as µ∗-measurable sets

A deﬁned as σ-alg. generated by A0

µ0 is a pre-measure

And here is the table for Ω = Rn, with Borel and Lebesgue in it.

2Rn

λ∗

λ

L(Rn)
B(Rn) µ

L0

λ0

Construct order Notes

Step 2

λ∗ is Lebesgue outer measure

Step 3

Lebesgue σ-algebra, completion of Borel one

Step 4

Step 1

Borel σ-algebra, generated by L0
Deﬁne pre-measure on rectangles

Going down one row of the table corresponds to restriction, while each of µ0 → µ → µcm

is a unique extension when µ0 is σ-ﬁnite.

Proof of Theorem 35.5.5. For (a): this is just Theorem 35.2.2 and Theorem 35.3.4 put
together, combined with the observation that A ∗ ⊃ A0 and hence A ∗ ⊃ A . Parts (b)
and (c) are more technical, and omitted.

§35.6 From now on, we assume the Borel measure

explain why
explain why

§35.7 A few harder problems to think about

Problem 35A† (Constructing outer measures from arbitrary ρ). For a set Ω, let E be
any subset of 2Ω and let ρ : E → [0, +∞] be any function. Prove that
En(cid:41)

ρ(En) | En ∈ E, E ⊆

µ∗(E) = inf(cid:40) ∞(cid:88)n=1

∞(cid:91)n=1

is an outer measure.
Problem 35B (The insane scientist). Let Ω = R2, and let E be the set of (non-
degenerate) rectangles. Let ρ(E) = 1 for every rectangle E ∈ E. Ignoring my advice, the
insane scientist uses ρ to construct an outer measure µ∗, as in Problem 35A†.
(a) Find µ∗(S) for each subset S of R2.
(b) Which sets are µ∗-measurable?
You should ﬁnd that no rectangle is µ∗-measurable, unsurprisingly foiling the scientist.
Problem 35C. A function f : R → R is continuous. Must f be measurable with respect
to the Lebesgue measure on R?

36 Lebesgue integration

On any measure space (Ω, A , µ) we can then, for a function f : Ω → [0,∞] deﬁne an

integral

(cid:90)Ω

f dµ.

This integral may be +∞ (even if f is ﬁnite). As the details of the construction won’t
matter for us later on, we will state the relevant deﬁnitions, skip all the proofs, and also
state all the properties that we actually care about. Consequently, this chapter will be
quite short.

§36.1 The deﬁnition

The construction is done in four steps.

Deﬁnition 36.1.1. If A is a measurable set of Ω, then the indicator function 1A : Ω →
R is deﬁned by

1A(ω) =(cid:40)1 ω ∈ A

0 ω /∈ A.

Step 1 (Indicator functions) — For an indicator function, we require

(cid:90)Ω

1A dµ := µ(A)

(which may be inﬁnite).

We extend this linearly now for nonnegative functions which are sums of indicators: these
functions are called simple functions.

Step 2 (Simple functions) — Let A1, . . . , An be a ﬁnite collection of measurable
sets. Let c1, . . . , cn be either nonnegative real numbers or +∞. Then we deﬁne

(cid:90)Ω(cid:32) n(cid:88)i=1

ci1Ai(cid:33) dµ :=

ciµ(Ai).

n(cid:88)i=1

If ci = ∞ and µ(Ai) = 0, we treat ciµ(Ai) = 0.

One can check the resulting sum does not depend on the representation of the simple

function as(cid:80) ci1Ai. In particular, it is compatible with the previous step.

Conveniently, this is already enough to deﬁne the integral for f : Ω → [0, +∞]. Note
that [0, +∞] can be thought of as a topological space where we add new open sets (a, +∞]
for each real number a to our usual basis of open intervals. Thus we can equip it with
the Borel sigma-algebra.1

1We could also try to deﬁne a measure on it, but we will not: it is a good enough for us that it is a

measurable space.

377

378

Napkin, by Evan Chen (v1.5.20190718)

Step 3 (Nonnegative functions) — For each measurable function f : Ω → [0, +∞],
let

(cid:90)Ω

f dµ := sup

0≤s≤f(cid:18)(cid:90)Ω

s dµ(cid:19)

where the supremum is taken over all simple s such that 0 ≤ s ≤ f . As before, this
integral may be +∞.

One can check this is compatible with the previous deﬁnitions. At this point, we introduce
an important term.
Deﬁnition 36.1.2. A measurable (nonnegative) function f : Ω → [0, +∞] is absolutely

integrable or Lebesgue integrable if(cid:82)Ω f dµ < ∞.

Warning: I ﬁnd “Lebesgue integrable” to be really confusing terminology. Indeed,
every measurable function from Ω to [0, +∞] can be assigned a Lebesgue integral, it’s
just that this integral may be +∞. So the deﬁnition is far more stringent than the name
suggests. Even constant functions can fail to be Lebesgue integrable:

Example 36.1.3 (Lebesgue integrable functions)

The constant function 1 is not Lebesgue integrable on R, since(cid:82)R 1 dµ = µ(R) + ∞.

For this reason, I will usually prefer the term “absolutely integrable”. (If it were up to
me, I would call it “ﬁnitely integrable”, and usually do so privately.)

Finally, this lets us integrate general functions.

Deﬁnition 36.1.4. In general, a measurable function f : Ω → [−∞,∞] is absolutely
integrable or Lebesgue integrable if |f| is.

Since we’ll be using the ﬁrst word, this is easy to remember: “absolutely integrable”

requires taking absolute values.

Step 4 (Absolutely integrable functions) — If f : Ω → [−∞,∞] is absolutely inte-
grable, then we deﬁne

f +(x) = max{f (x), 0}
f−(x) = min{f (x), 0}

and set

(cid:90)Ω

f dµ =(cid:90)Ω |f +| dµ −(cid:90)Ω |f−| dµ

which in particular is ﬁnite.

You may already start to see that we really like nonnegative functions: with the theory of
measures, it is possible to integrate them, and it’s even okay to throw in +∞’s everywhere.
But once we start dealing with functions that can be either positive or negative, we
have to start adding ﬁniteness restrictions — actually essentially what we’re doing is
splitting the function into its positive and negative part, requiring both are ﬁnite, and
then integrating.

To ﬁnish this section, we state for completeness some results that you probably could
have guessed were true. Fix Ω = (Ω, A , µ), and let f and g be measurable real-valued
functions such that f (x) = g(x) almost everywhere.

36 Lebesgue integration

379

 (Almost-everywhere preservation) The function f is absolutely integrable if and

only if g is, and if so, their Lebesgue integrals match.

 (Additivity) If f and g are absolutely integrable then

(cid:90)Ω

f + g dµ =(cid:90)Ω

f dµ +(cid:90)Ω

g dµ.

The “absolutely integrable” hypothesis can be dropped if f and g are nonnegative.
 (Scaling) If f is absolutely integrable and c ∈ R then cf is absolutely integrable

and

(cid:90)Ω

cf dµ = c(cid:90)Ω

f dµ.

The “absolutely integrable” hypothesis can be dropped if f is nonnegative and
c > 0.

 (Monotoncity) If f and g are absolutely integrable and f ≤ g, then

(cid:90)Ω

f dµ ≤(cid:90)Ω

g dµ.

The “absolutely integrable” hypothesis can be dropped if f and g are nonnegative.

There are more famous results like monotone/dominated convergence that are also true,
but we won’t state them here as we won’t really have a use for them in the context of
probability. (They appear later on in a bonus chapter.)

§36.2 Relation to Riemann integrals (or: actually computing

Lebesgue integrals)

For closed intervals, this actually just works out of the box.

Theorem 36.2.1 (Lebesgue integral generalizes Riemann integral)
Let f : [a, b] → R be a Riemann integrable function (where [a, b] is equipped with
the Borel measure). Then f is also Lebesgue integrable and the integrals agree:

(cid:90) b

a

f (x) dx =(cid:90)[a,b]

f dµ.

Thus in practice, we do all theory with Lebesgue integrals (they’re nicer), but when

we actually need to compute(cid:82)[1,4] x2 dµ we just revert back to our usual antics with the

Fundamental Theorem of Calculus.

Example 36.2.2 (Integrating x2 over [1, 4])
Reprising our old example:

(cid:90)[1,4]

x2 dµ =(cid:90) 4

1

x2 dx =

1
3 · 43 −

1
3 · 13 = 21.

This even works for improper integrals, if the functions are nonnegative. The statement

is a bit cumbersome to write down, but here it is.

380

Napkin, by Evan Chen (v1.5.20190718)

Theorem 36.2.3 (Improper integrals are nice Lebesgue ones)
Let f ≥ 0 be a nonnegative continuous function deﬁned on (a, b) ⊆ R, possibly
allowing a = −∞ or b = ∞. Then

(cid:90)(a,b)

f dµ = lim
→a+

a(cid:48)
b(cid:48)

→b−(cid:90) b(cid:48)

a(cid:48)

f (x) dx

where we allow both sides to be +∞ if f is not absolutely integrable.

The right-hand side makes sense since [a(cid:48), b(cid:48)] (cid:40) (a, b) is a compact interval on which f is
continuous. This means that improper Riemann integrals of nonnegative functions can
just be regarded as Lebesgue ones over the corresponding open intervals.

It’s probably better to just look at an example though.

Example 36.2.4 (Integrating 1/√x on (0, 1))
For example, you might be familiar with improper integrals like

(cid:90) 1

0

1
√x

dx := lim

ε→0+(cid:90) 1

ε

1
√x

dx = lim

ε→0+(cid:16)2√1 − 2√ε(cid:17) = 2.

(Note this appeared before as Problem 30C(cid:63).) In the Riemann integration situation,
we needed the limit as ε → 0+ since otherwise 1√x is not deﬁned as a function
[0, 1] → R. However, it is a measurable nonnegative function (0, 1) → [0, +∞], and
hence

(cid:90)(0,1)

1
√x

dµ = 2.

If f is not nonnegative, then all bets are oﬀ. Indeed Problem 36B† is the famous

counterexample.

§36.3 A few harder problems to think about

Problem 36A(cid:63) (The indicator of the rationals). Take the indicator function 1Q : R →
{0, 1} ⊆ R for the rational numbers.
(a) Prove that 1Q is not Riemann integrable.
(b) Show that(cid:82)R 1Q exists and determine its value — the one you expect!
Problem 36B† (An improper Riemann integral with sign changes). Deﬁne f : (1,∞) →
R by f (x) = sin(x)
. Show that f is not absolutely integrable, but that the improper
x
Riemann integral

nonetheless exists.

(cid:90) ∞

1

f (x) dx := lim

f (x) dx

B→∞(cid:90) b

a

37 Swapping order with Lebesgue

integrals

§37.1 Motivating limit interchange

Prototypical example for this section: 1Q is good!

One of the issues with the Riemann integral is that it behaves badly with respect to
convergence of functions, and the Lebesgue integral deals with this. This is therefore
often given as a poster child or why the Lebesgue integral has better behaviors than the
Riemann one.

We technically have already seen this: consider the indicator function 1Q, which is not
Riemann integrable by Problem 36A(cid:63). But we can readily compute its Lebesgue integral
over [0, 1], as

(cid:90)[0,1]

1Q dµ = µ ([0, 1] ∩ Q) = 0

since it is countable.

This could be thought of as a failure of convergence for the Riemann integral.

Example 37.1.1 (1Q is a limit of ﬁnitely supported functions)
We can deﬁne the sequence of functions g1, g2, . . . by

gn(x) =(cid:40)1 nx is an integer

else.

0

Then each gn is piecewise continuous and hence Riemann integrable on [0, 1] (with
integral zero), but limn→∞ gn = 1Q is not.

The limit here is deﬁned in the following sense:

Deﬁnition 37.1.2. Let f and f1, f2, . . . : Ω → R be a sequence of functions. Suppose
that for each ω ∈ Ω, the sequence

f1(ω), f2(ω), f3(ω), , . . .

converges to f (ω). Then we say (fn)n converges pointwise to the limit f , written
limn→∞ fn = f .

We can deﬁne lim inf n→∞ fn and lim supn→∞ fn similarly.
This is actually a fairly weak notion of convergence, for example:

Exercise 37.1.3 (Witch’s hat). Find a sequence of continuous function on [−1, 1] → R

381

382

Napkin, by Evan Chen (v1.5.20190718)

which converges pointwise to the function f given by

f (x) =(cid:40)1 x = 0

0

otherwise.

This is why when thinking about the Riemann integral it is commonplace to work with
stronger conditions like “uniformly convergent” and the like. However, with the Lebesgue
integral, we can mostly not think about these!

§37.2 Overview

The three big-name results for exchanging pointwise limits with Lebesgue integrals is:

 Fatou’s lemma: the most general statement possible, for any nonnegative measurable

functions.

 Monotone convergence: “increasing limits” just work.

 Dominated convergence (actually Fatou-Lebesgue):

limits that are not too big

(bounded by some absolutely integrable function) just work.

§37.3 Fatou’s lemma

Without further ado:

Lemma 37.3.1 (Fatou’s lemma)
Let f1, f2, . . . : Ω → [0, +∞] be a sequence of nonnegative measurable functions.
Then lim inf n : Ω → [0, +∞] is measurable and

(cid:90)Ω(cid:16)lim inf

n→∞

fn(cid:17) dµ ≤ lim inf

n→∞ (cid:18)(cid:90)Ω

fn dµ(cid:19) .

Here we allow either side to be +∞.

Notice that there are no extra hypothesis on fn other than nonnegative: which makes
this quite surprisingly versatile if you ever are trying to prove some general result.

§37.4 Everything else

The big surprise is how quickly all the “big-name” theorem follows from Fatou’s lemma.
Here is the so-called “monotone convergence theorem”.

Corollary 37.4.1 (Monotone convergence theorem)
Let f and f1, f2, . . . : Ω → [0, +∞] be a sequence of nonnegative measurable functions
such that limn fn = f and fn(ω) ≤ f (ω) for each n. Then f is measurable and

n→∞(cid:18)(cid:90)Ω

lim

fn dµ(cid:19) =(cid:90)Ω

f dµ.

Here we allow either side to be +∞.

37 Swapping order with Lebesgue integrals

383

Proof. We have

(cid:90)Ω

fn(cid:17) dµ

fn dµ

n→∞

≤ lim inf

f dµ =(cid:90)Ω(cid:16)lim inf
n→∞ (cid:90)Ω
n→∞ (cid:90)Ω
≤(cid:90)Ω

≤ lim sup

f dµ

fn dµ

where the ﬁrst ≤ is by Fatou lemma, and the second by the fact that(cid:82)Ω fn ≤(cid:82)Ω f for

every n. This implies all the inequalities are equalities and we are done.

Remark 37.4.2 (The monotone convergence theorem does not require monotonicity!)
— In the literature it is much more common to see the hypothesis f1(ω) ≤ f2(ω) ≤
··· ≤ f (ω) rather than just fn(ω) ≤ f (ω) for all n, which is where the theorem gets
its name. However as we have shown this hypothesis is superﬂuous! This is pointed
out in https://mathoverflow.net/a/296540/70654, as a response to a question
entitled “Do you know of any very important theorems that remain unknown?”.

Example 37.4.3 (Monotone convergence gives 1Q)
This already implies Example 37.1.1. Letting gn be the indicator function for 1
nZ as
described in that example, we have gn ≤ 1Q and limn→∞ gn(x) = 1Q(x), for each
knew.

individual x. So since(cid:82)[0,1] gn dµ = 0 for each n, this gives(cid:82)[0,1] 1Q = 0 as we already

The most famous result, though is the following.

Corollary 37.4.4 (Fatou–Lebesgue theorem)
Let f and f1, f2, . . . : Ω → R be a sequence of measurable functions. Assume that
g : Ω → R is an absolutely integrable function for which |fn(ω)| ≤ |g(ω)| for all ω ∈ Ω.
Then the inequality

(cid:90)Ω(cid:16)lim inf

n→∞

fn(cid:17) dµ ≤ lim inf

n→∞ (cid:18)(cid:90)Ω
n→∞ (cid:18)(cid:90)Ω

fn dµ(cid:19)
fn dµ(cid:19) ≤(cid:90)Ω(cid:18)lim sup

n→∞

≤ lim sup

fn(cid:19) dµ.

Proof. There are three inequalities:

 The ﬁrst inequality follows by Fatou on g + fn which is nonnegative.

 The second inequality is just lim inf ≤ lim sup. (This makes the theorem statement

easy to remember!)

 The third inequality follows by Fatou on g − fn which is nonnegative.

384

Napkin, by Evan Chen (v1.5.20190718)

Exercise 37.4.5. Where is the fact that g is absolutely integrable used in this proof?

Corollary 37.4.6 (Dominated convergence theorem)
Let f1, f2, . . . : Ω → R be a sequence of measurable functions such that f =
limn→∞ fn exists. Assume that g : Ω → R is an absolutely integrable function
for which |fn(ω)| ≤ |g(ω)| for all ω ∈ Ω. Then

n→∞(cid:18)(cid:90)Ω

fn dµ(cid:19) .

(cid:90)Ω

f dµ = lim

Proof. If f (ω) = limn→∞ fn(ω), then f (ω) = lim inf n→∞ fn(ω) = lim supn→∞ fn(ω). So
all the inequalities in the Fatou-Lebesgue theorem become equalities, since the leftmost
and rightmost sides are equal.

Note this gives yet another way to verify Example 37.1.1. In general, the dominated
convergence theorem is a favorite clich´e for undergraduate exams, because it is easy to
create questions for it. Here is one example showing how they all look.

Example 37.4.7 (The usual Lebesgue dominated convergence examples)
Suppose one wishes to compute

n→∞(cid:32)(cid:90)(0,1)

lim

n sin(n−1x)

√x

(cid:33) dx

then one starts by observing that the inner term is bounded by the absolutely
integrable function x−1/2. Therefore it equals

(cid:90)(0,1)

n→∞(cid:18) n sin(n−1x)

√x

lim

(cid:19) dx =(cid:90)(0,1)
=(cid:90)(0,1)

dx

x
√x
√x dx =

2
3

.

§37.5 Fubini and Tonelli

§37.6 A few harder problems to think about

TO BE
TO BE
WRITTEN
WRITTEN

problems
problems

38 Bonus: A hint of Pontryagin duality

In this short chapter we will give statements about how to generalize our Fourier

analysis (a bonus chapter Chapter 14) to a much wider class of groups G.

§38.1 LCA groups

Prototypical example for this section: T, R.

Earlier we played with R, which is nice because in addition to being a topological
space, it is also an abelian group under addition. These sorts of objects which are both
groups and spaces have a name.

Deﬁnition 38.1.1. A group G is a topological group is a Hausdorﬀ1 topological space
equipped also with a group operation (G,·), such that both maps

G × G → G by
G → G by

(x, y) (cid:55)→ xy
x (cid:55)→ x−1

are continuous.

For our Fourier analysis, we need some additional conditions.

Deﬁnition 38.1.2. A locally compact abelian (LCA) group G is one for which
the group operation is abelian, and moreover the topology is locally compact: for every
point p of G, there exists a compact subset K of G such that K (cid:51) p, and K contains
some open neighborhood of p.

Our previous examples all fall into this category:

Example 38.1.3 (Examples of locally compact abelian groups)

 Any ﬁnite group Z with the discrete topology is LCA.

 The circle group T is LCA and also in fact compact.

 The real numbers R are an example of an LCA group which is not compact.

These conditions turn out to be enough for us to deﬁne a measure on the space G.

The relevant theorem, which we will just quote:

1Some authors omit the Hausdorﬀ condition.

385

386

Napkin, by Evan Chen (v1.5.20190718)

Theorem 38.1.4 (Haar measure)

Let G be a locally compact abelian group. We regard it as a measurable space using
its Borel σ-algebra B(G). There exists a measure µ : B(G) → [0,∞], called the
Haar measure, satisfying the following properties:

 µ(gS) = µ(S) for every g ∈ G and measurable S. That means that µ is
“translation-invariant” under translation by G.

 µ(K) is ﬁnite for any compact set K.

 if S is measurable, then µ(S) = inf {µ(U ) : U ⊇ S open}.
 if U is open, then µ(U ) = sup{µ(S) : S ⊇ U measurable}.

Moreover, it is unique up to scaling by a positive constant.

Remark 38.1.5 — Note that if G is compact, then µ(G) is ﬁnite (and positive). For
this reason the Haar measure on a LCA group G is usually normalized so µ(G) = 1.

For this chapter, we will only use the ﬁrst two properties at all, and the other two are
just mentioned for completeness. Note that this actually generalizes the chapter where
we constructed a measure on B(Rn), since Rn is an LCA group!

So, in short: if we have an LCA group, we have a measure µ on it.

§38.2 The Pontryagin dual

Now the key deﬁnition is:

Deﬁnition 38.2.1. Let G be an LCA group. Then its Pontryagin dual is the abelian
group

The maps ξ are called characters. It can be itself made into an LCA group.2

(cid:98)G := {continuous group homomorphisms ξ : G → T} .

Example 38.2.2 (Examples of Pontryagin duals)

determined by the ﬁber above 1 ∈ S1.
covering projections here.)

 (cid:98)Z ∼= T, since group homomorphisms Z → T are determined by the image of 1.
 (cid:98)T ∼= Z. The characters are given by θ (cid:55)→ nθ for n ∈ Z.
 (cid:98)R ∼= R. This is because a nonzero continuous homomorphism R → S1 is
 (cid:92)Z/nZ ∼= Z/nZ, characters ξ being determined by the image ξ(1) ∈ T.
 (cid:92)G × H ∼= (cid:98)G × (cid:98)H.

(Algebraic topologists might see

2If you must know the topology, it is the compact-open topology: for any compact set K ⊆ G and
open set U ⊆ T, we declare the set of all ξ with ξimg(K) ⊆ U to be open, and then take the smallest
topology containing all such sets. We won’t use this at all.

38 Bonus: A hint of Pontryagin duality

387

Exercise 38.2.3 ((cid:98)Z ∼= Z, for those who read Section 18.1). If Z is a ﬁnite abelian group,
show that (cid:98)Z ∼= Z, using the results of the previous example. You may now recognize that the
bilinear form · : Z × Z → T is exactly a choice of isomorphism Z → (cid:98)Z. It is not “canonical”.
True to its name as the dual, and in analogy with (V ∨)∨ ∼= V for vector spaces V , we

have:

Theorem 38.2.4 (Pontryagin duality theorem)

For any LCA group G, there is an isomorphism

G ∼= (cid:98)(cid:98)G

by

x (cid:55)→ (ξ (cid:55)→ ξ(x)) .

The compact case is especially nice.

Proposition 38.2.5 (G compact ⇐⇒ (cid:98)G discrete)
Let G be an LCA group. Then G is compact if and only if (cid:98)G is discrete.

Proof. Problem 38B.

§38.3 The orthonormal basis in the compact case

Let G be a compact LCA group, and work with its Haar measure. We may now let L2(G)
be the space of square-integrable functions to C, i.e.

L2(G) =(cid:26)f : G → C such that

(cid:90)G |f|2 < ∞(cid:27) .

Thus we can equip it with the inner form

f · g.
In that case, we get all the results we wanted before:

(cid:104)f, g(cid:105) =(cid:90)G

Theorem 38.3.1 (Characters of (cid:98)G forms an orthonormal basis)
Assume G is LCA and compact (so (cid:98)G is discrete). Then the characters
form an orthonormal basis of L2(G). Thus for each f ∈ L2(G) we have

eξ(x) = e(ξ(x)) = exp(2πiξ(x))

(eξ)ξ∈(cid:98)G

by

f =(cid:88)ξ∈(cid:98)G(cid:98)f (ξ)eξ

where

(cid:98)f (ξ) = (cid:104)f, eξ(cid:105) =(cid:90)G

f (x) exp(−2πiξ(x)) dµ.

The sum(cid:80)ξ∈(cid:98)G makes sense since (cid:98)G is discrete. In particular,

388

Napkin, by Evan Chen (v1.5.20190718)

 Letting G = Z for a ﬁnite group G gives “Fourier transform on ﬁnite groups”.

 The special case G = Z/nZ has its own Wikipedia page: the “discrete-time Fourier

transform”.

 Letting G = T gives the “Fourier series” earlier.

§38.4 The Fourier transform of the non-compact case

let

If G is LCA but not compact, then Theorem 38.3.1 becomes false. On the other hand,

it’s still possible to deﬁne (cid:98)G. We can then try to write the Fourier coeﬃcients anyways:
for ξ ∈ (cid:98)G and f : G → C. The results are less fun in this case, but we still have, for

(cid:98)f (ξ) =(cid:90)G

f · eξ dµ

example:

Theorem 38.4.1 (Fourier inverison formula in the non-compact case)

Let µ be a Haar measure on G. Then there exists a unique Haar measure ν on (cid:98)G
(called the dual measure) such that: whenever f ∈ L1(G) and (cid:98)f ∈ L1((cid:98)G), we have

f (x) =(cid:90)(cid:98)G (cid:98)f (ξ)ξ(x) dν

for almost all x ∈ G (with respect to µ). If f is continuous, this holds for all x.

So while we don’t have the niceness of a full inner product from before, we can still in
some situations at least write f as integral in sort of the same way as before.

In particular, they have special names for a few special G:

 If G = R, then (cid:98)G = R, yielding the “(continuous) Fourier transform”.
 If G = Z, then (cid:98)G = T, yielding the “discrete time Fourier transform.

§38.5 Summary

We summarize our various ﬂavors of Fourier analysis from the previous sections in the
following table. In the ﬁrst part G is compact, in the second half G is not.

Name
Binary Fourier analysis
Fourier transform on ﬁnite groups Z
Discrete Fourier transform
Fourier series
Continuous Fourier transform
Discrete time Fourier transform

Characters

Domain G Dual (cid:98)G
S ⊆ {1, . . . , n} (cid:81)s∈S xs
{±1}n
ξ ∈ (cid:98)Z ∼= Z
Z/nZ
ξ ∈ Z/nZ
T ∼= [−π, π] n ∈ Z
R
ξ ∈ R
ξ ∈ T ∼= [−π, π]
Z

e(iξ · x)
e(ξx/n)
exp(inx)
e(ξx)
exp(iξn)

You might notice that the various names are awful. This is part of the reason I
got confused as a high school student: every type of Fourier series above has its own
Wikipedia article. If it were up to me, we would just use the term “G-Fourier transform”,
and that would make everyone’s lives a lot easier.

38 Bonus: A hint of Pontryagin duality

389

§38.6 A few harder problems to think about

Problem 38A. If G is compact, so (cid:98)G is discrete, describe the dual measure ν.
Problem 38B. Show that an LCA group G is compact if and only if (cid:98)G is discrete. (You

will need the compact-open topology for this.)

XI

Probability (TO DO)

Part XI: Contents

39 Random variables (TO DO)

393
39.1 Random variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 393
39.2 Distribution functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 394
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 394
39.3 Examples of random variables
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 394
39.4 Characteristic functions
39.5 Independent random variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . 394
. . . . . . . . . . . . . . . . . . . . . . . . 394
39.6 A few harder problems to think about

40 Large number laws (TO DO)

40.1 Notions of convergence
40.2 A few harder problems to think about

395
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395
. . . . . . . . . . . . . . . . . . . . . . . . 396

41 Stopped martingales (TO DO)

397

39 Random variables (TO DO)

write chapter
write chapter

Having properly developed the Lebesgue measure and the integral on it, we can now

proceed to develop random variables.

§39.1 Random variables

With all this set-up, random variables are going to be really quick to deﬁne.

Deﬁnition 39.1.1. A (real) random variable X on a probability space Ω = (Ω, A , µ)
is a measurable function X : Ω → R, where R is equipped with the Borel σ-algebra.

In particular, addition of random variables, etc. all makes sense, as we can just add.

Also, we can integrate X over Ω, by previous chapter.

Deﬁnition 39.1.2 (First properties of random variables). Given a random variable X,
the expected value of X is deﬁned by the Lebesgue integral

E[X] =(cid:90)Ω

X(ω) dµ.

Confusingly, the letter µ is often used for expected values.

The kth moment of X is deﬁned as E[X k], for each positive integer k ≥ 1. The

variance of X is then deﬁned as

Var(X) = E(cid:2)(X − E[X])2(cid:3) .

Question 39.1.3. Show that 1A is a random variable (just check that it is Borel measurable),
and its expected value is µ(A).

An important property of expected value you probably already know:

Theorem 39.1.4 (Linearity of expectation)

If X and Y are random variables on Ω then

E[X + Y ] = E[X] + E[Y ].

Proof. E[X + Y ] =(cid:82)Ω X(ω) + Y (ω) dµ =(cid:82)Ω X(ω) dµ +(cid:82)Ω Y (ω) dµ = E[X] + E[Y ].

Note that X and Y do not have to be “independent” here: a notion we will deﬁne shortly.

393

394

Napkin, by Evan Chen (v1.5.20190718)

§39.2 Distribution functions

§39.3 Examples of random variables

§39.4 Characteristic functions

§39.5 Independent random variables

§39.6 A few harder problems to think about

Problem 39A (Equidistribution). Let X1, X2, . . . be i.i.d. uniform random variables
on [0, 1]. Show that almost surely the Xi are equidistributed, meaning that

#{1 ≤ i ≤ N | a ≤ Xi(ω) ≤ b}

N

lim
N→∞

= b − a

∀0 ≤ a < b ≤ 1

holds for almost all choices of ω.

Problem 39B (Side length of triangle independent from median). Let X1, Y1, X2, Y2,
X3, Y3 be six independent standard Gaussians. Deﬁne triangle ABC in the Cartesian
plane by A = (X1, Y1), B = (X2, Y2), C = (X3, Y3). Prove that the length of side BC is
independent from the length of the A-median.

40 Large number laws (TO DO)

write chapter
write chapter

§40.1 Notions of convergence

§40.1.i Almost sure convergence

Deﬁnition 40.1.1. Let X, Xn be random variables on a probability space Ω. We say
Xn converges almost surely to X if

µ(cid:16)ω ∈ Ω : lim

n

Xn(ω) = X(ω)(cid:17) = 1.

This is a very strong notion of convergence: it says in almost every world, the values of

Xn converge to X. In fact, it is almost better for me to give a non-example.

Example 40.1.2 (Non-example of almost sure convergence)
Imagine an immortal skeleton archer is practicing shots, and on the nth shot, he
scores a bulls-eye with probability 1− 1
n (which tends to 1 because the archer improves
over time). Let Xn ∈ {0, 1, . . . , 10} be the score of the nth shot.
worlds in which the archer misses only ﬁnitely many shots: that is

Although the skeleton is gradually approaching perfection, there are almost no

µ(cid:16)ω ∈ Ω : lim

n

Xn(ω) = 10(cid:17) = 0.

§40.1.ii Convergence in probability

Therefore, for many purposes we need a weaker notion of convergence.

Deﬁnition 40.1.3. Let X, Xn be random variables on a probability space Ω. We say
Xn converges in probability to X if if for every ε > 0 and δ > 0, we have

µ (ω ∈ Ω : |Xn(ω) − X(ω)| < ε) ≥ 1 − δ

for n large enough (in terms of ε and δ).

In this sense, our skeleton archer does succeed: for any δ > 0, if n > δ−1 then the
skeleton archer does hit a bulls-eye in a 1 − δ fraction of the worlds. In general, you can
think of this as saying that for any δ > 0, the chance of an ε-anomaly event at the nth
stage eventually drops below δ.

Remark 40.1.4 — To mask δ from the deﬁnition, this is sometimes written instead
as: for all ε

lim
n→∞

µ (ω ∈ Ω : |Xn(ω) − X(ω)| < ε) = 1.

I suppose it doesn’t make much diﬀerence, though I personally don’t like the
asymmetry.

395

396

Napkin, by Evan Chen (v1.5.20190718)

§40.1.iii Convergence in law

§40.2 A few harder problems to think about

Problem 40A (Quantiﬁer hell). In the deﬁnition of convergence in probability suppose
we allowed δ = 0 (rather than δ > 0). Show that the modiﬁed deﬁnition is equivalent to
almost sure convergence.

Problem 40B (Almost sure convorgence is not topologizable). Consider the space of
all random variables on Ω = [0, 1]. Prove that it’s impossible to impose a metric on this
space which makes the following statement true:

A sequence X1, X2, . . . , of converges almost surely to X if and only if Xi
converge to X in the metric.

41 Stopped martingales (TO DO)

write chapter
write chapter

397

XII

Diﬀerential Geometry

Part XII: Contents

42 Multivariable calculus done correctly

401
42.1 The total derivative . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 401
42.2 The projection principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 403
42.3 Total and partial derivatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 404
. . . . . . . . . . . . . . . . . . . . . . . 406
42.4 (Optional) A word on higher derivatives
42.5 Towards diﬀerential forms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 407
. . . . . . . . . . . . . . . . . . . . . . . . 407
42.6 A few harder problems to think about

43 Diﬀerential forms

409
43.1 Pictures of diﬀerential forms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 409
43.2 Pictures of exterior derivatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . 411
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 412
43.3 Diﬀerential forms
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 413
43.4 Exterior derivatives
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415
43.5 Closed and exact forms
. . . . . . . . . . . . . . . . . . . . . . . . 416
43.6 A few harder problems to think about

44 Integrating diﬀerential forms

417
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 417
44.1 Motivation: line integrals
44.2 Pullbacks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 418
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 419
44.3 Cells
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 421
44.4 Boundaries
44.5 Stokes’ theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 423
. . . . . . . . . . . . . . . . . . . . . . . . 423
44.6 A few harder problems to think about

45 A bit of manifolds

425
45.1 Topological manifolds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 425
45.2 Smooth manifolds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 426
45.3 Regular value theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 427
45.4 Diﬀerential forms on manifolds . . . . . . . . . . . . . . . . . . . . . . . . . . . . 428
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 429
45.5 Orientations
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 430
45.6 Stokes’ theorem for manifolds
45.7 (Optional) The tangent and contangent space . . . . . . . . . . . . . . . . . . . . . 430
. . . . . . . . . . . . . . . . . . . . . . . . 433
45.8 A few harder problems to think about

42 Multivariable calculus done correctly

As I have ranted about before, linear algebra is done wrong by the extensive use
of matrices to obscure the structure of a linear map. Similar problems occur with
multivariable calculus, so here I would like to set the record straight.

Since we are doing this chapter using morally correct linear algebra, it’s imperative
you’re comfortable with linear maps, and in particular the dual space V ∨ which we will
repeatedly use.

In this chapter, all vector spaces have norms and are ﬁnite-dimensional over R. So in
particular every vector space is also a metric space (with metric given by the norm), and
we can talk about open sets as usual.

§42.1 The total derivative

Prototypical example for this section: If f (x, y) = x2 + y2, then (Df )(x,y) = 2xe∨1 + 2ye∨2 .
First, let f : [a, b] → R. You might recall from high school calculus that for every
point p ∈ R, we deﬁned f(cid:48)(p) as the derivative at the point p (if it existed), which we
interpreted as the slope of the “tangent line”.

That’s ﬁne, but I claim that the “better” way to interpret the derivative at that point
is as a linear map, that is, as a function. If f(cid:48)(p) = 1.5, then the derivative tells me that
if I move ε away from p then I should expect f to change by about 1.5ε. In other words,

The derivative of f at p approximates f near p by a linear function.

What about more generally? Suppose I have a function like f : R2 → R, say

f (x, y) = x2 + y2

for concreteness or something. For a point p ∈ R2, the “derivative” of f at p ought to
represent a linear map that approximates f at that point p. That means I want a linear
map T : R2 → R such that

f (p + v) ≈ f (p) + T (v)

for small displacements v ∈ R2.

401

xpf′(p)402

Napkin, by Evan Chen (v1.5.20190718)

Even more generally, if f : U → W with U ⊆ V open (in the (cid:107)•(cid:107)V metric as usual),

then the derivative at p ∈ U ought to be so that

f (p + v) ≈ f (p) + T (v) ∈ W.

(We need U open so that for small enough v, p + v ∈ U as well.) In fact this is exactly
what we were doing earlier with f(cid:48)(p) in high school.

Image derived from [gk]

The only diﬀerence is that, by an unfortunate coincidence, a linear map R → R can be
represented by just its slope. And in the unending quest to make everything a number so
that it can be AP tested, we immediately forgot all about what we were trying to do in
the ﬁrst place and just deﬁned the derivative of f to be a number instead of a function.

The fundamental idea of Calculus is the local approximation of functions
by linear functions. The derivative does exactly this.

Jean Dieudonn´e as quoted in [Pu02] continues:

In the classical teaching of Calculus, this idea is immediately obscured by
the accidental fact that, on a one-dimensional vector space, there is a one-
to-one correspondence between linear forms and numbers, and therefore the
derivative at a point is deﬁned as a number instead of a linear form. This
slavish subservience to the shibboleth of numerical interpretation
at any cost becomes much worse . . .

So let’s do this right. The only thing that we have to do is say what “≈” means, and

for this we use the norm of the vector space.
Deﬁnition 42.1.1. Let U ⊆ V be open. Let f : U → W be a continuous function, and
p ∈ U . Suppose there exists a linear map T : V → W such that
= 0.

(cid:107)f (p + v) − f (p) − T (v)(cid:107)W

lim

(cid:107)v(cid:107)V →0

(cid:107)v(cid:107)V

Then T is the total derivative of f at p. We denote this by (Df )p, and say f is
diﬀerentiable at p.

If (Df )p exists at every point, we say f is diﬀerentiable.

p+vp42 Multivariable calculus done correctly

403

Question 42.1.2. Check if that V = W = R, this is equivalent to the single-variable
deﬁnition. (What are the linear maps from V to W ?)

Example 42.1.3 (Total derivative of f (x, y) = x2 + y2)
Let V = R2 with standard basis e1, e2 and let W = R, and let f (xe1 + ye2) = x2+y2.
Let p = ae1 + be2. Then, we claim that

(Df )p : R2 → R by v (cid:55)→ 2a · e∨1 (v) + 2b · e∨2 (v).

Here, the notation e∨1 and e∨2 makes sense, because by deﬁnition (Df )p ∈ V ∨: these are
functions from V to R!
Let’s check this manually with the limit deﬁnition. Set v = xe1 + ye2, and note that
the norm on V is (cid:107)(x, y)(cid:107)V =(cid:112)x2 + y2 while the norm on W is just the absolute value

(cid:107)c(cid:107)W = |c|. Then we compute
(cid:107)f (p + v) − f (p) − T (v)(cid:107)W

(cid:107)v(cid:107)V

= (cid:12)(cid:12)(a + x)2 + (b + y)2 − (a2 + b2) − (2ax + 2by)(cid:12)(cid:12)
(cid:112)x2 + y2

=(cid:112)x2 + y2

(cid:112)x2 + y2

x2 + y2

=

→ 0

as (cid:107)v(cid:107) → 0. Thus, for p = ae1 + be2 we indeed have (Df )p = 2a · e∨1 + 2b · e∨2 .

Remark 42.1.4 — As usual, diﬀerentiability implies continuity.

Remark 42.1.5 — Although U ⊆ V , it might be helpful to think of vectors from U
and V as diﬀerent types of objects (in particular, note that it’s possible for 0V /∈ U ).
The vectors in U are “inputs” on our space while the vectors coming from V are
“small displacements”. For this reason, I deliberately try to use p ∈ U and v ∈ V
when possible.

§42.2 The projection principle

Before proceeding I need to say something really important.

Theorem 42.2.1 (Projection principle)

Let U be an open subset of the vector space V . Let W be an n-dimensional real
vector space with basis w1, . . . , wn. Then there is a bijection between continuous
functions f : U → W and n-tuples of continuous f1, f2, . . . , fn : U → R by projection
onto the ith basis element, i.e.

f (v) = f1(v)w1 + ··· + fn(v)wn.

Proof. Obvious.

The theorem remains true if one replaces “continuous” by “diﬀerentiable”, “smooth”,
“arbitrary”, or most other reasonable words. Translation:

404

Napkin, by Evan Chen (v1.5.20190718)

To think about a function f : U → Rn, it suﬃces to think about each
coordinate separately.

For this reason, we’ll most often be interested in functions f : U → R. That’s why the
dual space V ∨ is so important.

§42.3 Total and partial derivatives

Prototypical example for this section: If f (x, y) = x2 + y2, then (Df ) : (x, y) (cid:55)→ 2x · e∨1 +
2y · e∨2 , and ∂f

∂x = 2x, ∂f

∂y = 2y.

Let U ⊆ V be open and let V have a basis e1, . . . , en. Suppose f : U → R is a function
which is diﬀerentiable everywhere, meaning (Df )p ∈ V ∨ exists for every p. In that case,
one can consider Df as itself a function:

Df : U → V ∨

p (cid:55)→ (Df )p.

This is a little crazy: to every point in U we associate a function in V ∨. We say Df is
the total derivative of f , to reﬂect how much information we’re dealing with. We say
(Df )p is the total derivative at p.

Let’s apply the projection principle now to Df . Since we picked a basis e1, . . . , en of
V , there is a corresponding dual basis e∨1 , e∨2 , . . . , e∨n. The Projection Principle tells us
that Df can thus be thought of as just n functions, so we can write

Df = ψ1e∨1 + ··· + ψne∨n.

In fact, we can even describe what the ψi are.
Deﬁnition 42.3.1. The ith partial derivative of f : U → R, denoted

is deﬁned by

∂f
∂ei

: U → R

∂f
∂ei

(p) := lim
t→0

f (p + tei) − f (p)

t

.

You can think of it as “f(cid:48) along ei”.

Question 42.3.2. Check that if Df exists, then

(Df )p(ei) =

∂f
∂ei

(p).

Remark 42.3.3 — Of course you can write down a deﬁnition of ∂f
than just the ei).

∂v for any v (rather

From the above remarks, we can derive that

Df =

∂f

∂e1 · e∨1 + ··· +

∂f

∂en · e∨n.

and so given a basis of V , we can think of Df as just the n partials.

42 Multivariable calculus done correctly

405

Remark 42.3.4 — Keep in mind that each ∂f
∂ei
That is to say,

is a function from U to the reals.

(Df )p =

·e∨1 + ··· +

·e∨n ∈ V ∨.

(p)

∂f
∂e1
∈R

(cid:124) (cid:123)(cid:122) (cid:125)

(p)

∂f
∂en
∈R

(cid:124) (cid:123)(cid:122) (cid:125)

Example 42.3.5 (Partial derivatives of f (x, y) = x2 + y2)
Let f : R2 → R by (x, y) (cid:55)→ x2 + y2. Then in our new language,

Df : (x, y) (cid:55)→ 2x · e∨1 + 2y · e∨2 .

Thus the partials are

∂f
∂x

: (x, y) (cid:55)→ 2x ∈ R and

∂f
∂y

: (x, y) (cid:55)→ 2y ∈ R

With all that said, I haven’t really said much about how to ﬁnd the total derivative

itself. For example, if I told you

f (x, y) = x sin y + x2y4

you might want to be able to compute Df without going through that horrible limit
deﬁnition I told you about earlier.

Fortunately, it turns out you already know how to compute partial derivatives, because
you had to take AP Calculus at some point in your life. It turns out for most reasonable
functions, this is all you’ll ever need.

Theorem 42.3.6 (Continuous partials implies diﬀerentiable)
Let U ⊆ V be open and pick any basis e1, . . . , en. Let f : U → R and suppose that
∂f
is deﬁned for each i and moreover is continuous. Then f is diﬀerentiable and
∂ei
Df is given by

Df =

∂f

∂ei · e∨i .

n(cid:88)i=1

Proof. Not going to write out the details, but. . . given v = t1e1 +··· + tnen, the idea is to
just walk from p to p + t1e1, p + t1e1 + t2e2, . . . , up to p + t1e1 + t2e2 +··· + tnen = p + v,
picking up the partial derivatives on the way. Do some calculation.

Remark 42.3.7 — The continuous condition cannot be dropped. The function

f (x, y) =(cid:40) xy

x2+y2
0

(x, y) (cid:54)= (0, 0)
(x, y) = (0, 0).

is the classic counterexample – the total derivative Df does not exist at zero, even
though both partials do.

406

Napkin, by Evan Chen (v1.5.20190718)

Example 42.3.8 (Actually computing a total derivative)
Let f (x, y) = x sin y + x2y4. Then

∂f
∂x
∂f
∂y

(x, y) = sin y + y4 · 2x
(x, y) = x cos y + x2 · 4y3.

So Theorem 42.3.6 applies, and Df = ∂f
out.

∂x e∨1 + ∂f

∂y e∨2 , which I won’t bother to write

The example f (x, y) = x2 + y2 is the same thing. That being said, who cares about

x sin y + x2y4 anyways?

§42.4 (Optional) A word on higher derivatives

Let U ⊆ V be open, and take f : U → W , so that Df : U → Hom(V, W ).
turns out that one can deﬁne an operator norm on it by setting

Well, Hom(V, W ) can also be thought of as a normed vector space in its own right: it

(cid:107)T(cid:107) := sup(cid:26)(cid:107)T (v)(cid:107)W

(cid:107)v(cid:107)V

| v (cid:54)= 0V(cid:27) .

So Hom(V, W ) can be thought of as a normed vector space as well. Thus it makes sense
to write

which we abbreviate as D2f . Dropping all doubt and plunging on,

D(Df ) : U → Hom(V, Hom(V, W ))

D3f : U → Hom(V, Hom(V, Hom(V, W ))).

I’m sorry. As consolation, we at least know that Hom(V, W ) ∼= V ∨ ⊗ W in a natural way,

so we can at least condense this to

rather than writing a bunch of Hom’s.

Dkf : V → (V ∨)⊗k ⊗ W

Remark 42.4.1 — If k = 2, W = R, then D2f (v) ∈ (V ∨)⊗2, so it can be represented
as an n × n matrix, which for some reason is called a Hessian.

The most important property of the second derivative is that

Theorem 42.4.2 (Symmetry of D2f )
Let f : U → W with U ⊆ V open. If (D2f )p exists at some p ∈ U , then it is
symmetric, meaning

(D2f )p(v1, v2) = (D2f )p(v2, v1).

I’ll just quote this without proof (see e.g.
derivatives make my head spin. An important corollary of this theorem:

[Pu02, §5, theorem 16]), because double

42 Multivariable calculus done correctly

407

Corollary 42.4.3 (Clairaut’s theorem: mixed partials are symmetric)
Let f : U → R with U ⊆ V open be twice diﬀerentiable. Then for any point p such
that the quantities are deﬁned,

∂
∂ei

∂
∂ej

f (p) =

∂
∂ej

∂
∂ei

f (p).

§42.5 Towards diﬀerential forms

This concludes the exposition of what the derivative really is: the key idea I want to
communicate in this chapter is that Df should be thought of as a map from U → V ∨.
The next natural thing to do is talk about integration. The correct way to do this is
through a so-called diﬀerential form: you’ll ﬁnally know what all those stupid dx’s and
dy’s really mean. (They weren’t just there for decoration!)

§42.6 A few harder problems to think about

Problem 42A(cid:63) (Chain rule). Let U1
−→ U3 be diﬀerentiable maps between open
sets of normed vector spaces Vi, and let h = g ◦ f . Prove the Chain Rule: for any point
p ∈ U1, we have

−→ U2

(Dh)p = (Dg)f (p) ◦ (Df )p.

f

g

Problem 42B. Let U ⊆ V be open, and f : U → R be diﬀerentiable k times. Show
that (Dkf )p is symmetric in its k arguments, meaning for any v1, . . . , vk ∈ V and any
permutation σ on {1, . . . , k} we have

(Dkf )p(v1, . . . , vk) = (Dkf )p(vσ(1), . . . , vσ(k)).

43 Diﬀerential forms

In this chapter, all vector spaces are ﬁnite-dimensional real inner product spaces. We
ﬁrst start by (non-rigorously) drawing pictures of all the things that we will deﬁne in
this chapter. Then we re-do everything again in its proper algebraic context.

§43.1 Pictures of diﬀerential forms

Before deﬁning a diﬀerential form, we ﬁrst draw some pictures. The key thing to keep in
mind is

“The deﬁnition of a diﬀerential form is: something you can integrate.”
— Joe Harris

We’ll assume that all functions are smooth, i.e. inﬁnitely diﬀerentiable.
Let U ⊆ V be an open set of a vector space V . Suppose that we have a function

f : U → R, i.e. we assign a value to every point of U .

Deﬁnition 43.1.1. A 0-form f on U is just a smooth function f : U → R.
Thus, if we specify a ﬁnite set S of points in U we can “integrate” over S by just adding
up the values of the points:

0 + √2 + 3 + (−1) = 2 + √2.

So, a 0-form f lets us integrate over 0-dimensional “cells”.

But this is quite boring, because as we know we like to integrate over things like curves,
not single points. So, by analogy, we want a 1-form to let us integrate over 1-dimensional
cells: i.e. over curves. What information would we need to do that? To answer this, let’s
draw a picture of a curve c, which can be thought of as a function c : [0, 1] → U .

409

U3√2−10Ucpv410

Napkin, by Evan Chen (v1.5.20190718)

We might think that we could get away with just specifying a number on every point
of U (i.e. a 0-form f ), and then somehow “add up” all the values of f along the curve.
We’ll use this idea in a moment, but we can in fact do something more general. Notice
how when we walk along a smooth curve, at every point p we also have some extra
information: a tangent vector v. So, we can deﬁne a 1-form α as follows. A 0-form just
took a point and gave a real number, but a 1-form will take both a point and a
tangent vector at that point, and spit out a real number. So a 1-form α is a
smooth function on pairs (p, v), where v is a tangent vector at p, to R. Hence

α : U × V → R.

Actually, for any point p, we will require that α(p,−) is a linear function in terms of
the vectors: i.e. we want for example that α(p, 2v) = 2α(p, v). So it is more customary
to think of α as:

Deﬁnition 43.1.2. A 1-form α is a smooth function

α : U → V ∨.

Like with Df , we’ll use αp instead of α(p). So, at every point p, αp is some linear
functional that eats tangent vectors at p, and spits out a real number. Thus, we think of
αp as an element of V ∨;

αp ∈ V ∨.

Next, we draw pictures of 2-forms. This should, for example, let us integrate over a

blob (a so-called 2-cell) of the form

c : [0, 1] × [0, 1] → U

i.e. for example, a square in U . In the previous example with 1-forms, we looked at
tangent vectors to the curve c. This time, at points we will look at pairs of tangent
vectors in U : in the same sense that lots of tangent vectors approximate the entire curve,
lots of tiny squares will approximate the big square in U .

So what should a 2-form β be? As before, it should start by taking a point p ∈ U , so βp
is now a linear functional: but this time, it should be a linear map on two vectors v and
w. Here v and w are not tangent so much as their span cuts out a small parallelogram.
So, the right thing to do is in fact consider

βp ∈ V ∨ ∧ V ∨.

That is, to use the wedge product to get a handle on the idea that v and w span a
parallelogram. Another valid choice would have been (V ∧ V )∨; in fact, the two are
isomorphic, but it will be more convenient to write it in the former.

Ucpvw43 Diﬀerential forms

411

§43.2 Pictures of exterior derivatives

Next question:

How can we build a 1-form from a 0-form?

Let f be a 0-form on U ; thus, we have a function f : U → R. Then in fact there is a
very natural 1-form on U arising from f , appropriately called df . Namely, given a point
p and a tangent vector v, the diﬀerential form (df )p returns the change in f along v. In
other words, it’s just the total derivative (Df )p(v).

Thus, df measures “the change in f ”.

Now, even if I haven’t deﬁned integration yet, given a curve c from a point a to b,

what do you think

(cid:90)c

df

(cid:90)c

df = f (b) − f (a).

should be equal to? Remember that df is the 1-form that measures “inﬁnitesimal change
in f ”. So if we add up all the change in f along a path from a to b, then the answer we
get should just be

This is the ﬁrst case of something we call Stokes’ theorem.

Generalizing, how should we get from a 1-form to a 2-form? At each point p, the
2-form β gives a βp which takes in a “parallelogram” and returns a real number. Now
suppose we have a 1-form α. Then along each of the edges of a parallelogram, with an
appropriate sign convention the 1-form α gives us a real number. So, given a 1-form α,
we deﬁne dα to be the 2-form that takes in a parallelogram spanned by v and w, and
returns the measure of α along the boundary.

Now, what happens if you integrate df along the entire square c? The right picture
is that, if we think of each little square as making up the big square, then the adjacent
boundaries cancel out, and all we are left is the main boundary. This is again just a case
of the so-called Stokes’ theorem.

Image from [Na]

U3√2−10√2+εvUcp412

Napkin, by Evan Chen (v1.5.20190718)

§43.3 Diﬀerential forms

Prototypical example for this section: Algebraically, something that looks like f e∨1 ∧e∨2 +. . . ,
and geometrically, see the previous section.

Let’s now get a handle on what dx means. Fix a real vector space V of dimension n,

and let e1, . . . , en be a standard basis. Let U be an open set.

Deﬁnition 43.3.1. We deﬁne a diﬀerential k-form α on U to be a smooth (inﬁnitely
diﬀerentiable) map α : U → Λk(V ∨). (Here Λk(V ∨) is the wedge product.)

Like with Df , we’ll use αp instead of α(p).

Example 43.3.2 (k-forms for k = 0, 1)
(a) A 0-form is just a function U → R.
(b) A 1-form is a function U → V ∨. For example, the total derivative Df of a

function V → R is a 1-form.

(c) Let V = R3 with standard basis e1, e2, e3. Then a typical 2-form is given by

αp = f (p) · e∨1 ∧ e∨2 + g(p) · e∨1 ∧ e∨3 + h(p) · e∨2 ∧ e∨3 ∈ Λ2(V )

where f, g, h : V → R are smooth functions.

Now, by the projection principle (Theorem 42.2.1) we only have to specify a function

k(cid:1) basis elements of Λk(V ∨). So, take any basis {ei} of V , and take the usual

on each of(cid:0)n

basis for Λk(V ∨) of elements

e∨i1 ∧ e∨i2 ∧ ··· ∧ e∨ik .

Thus, a general k-form takes the shape

αp = (cid:88)1≤i1<···<ik≤n

fi1,...,ik (p) · e∨i1 ∧ e∨i2 ∧ ··· ∧ e∨ik .

Since this is a huge nuisance to write, we will abbreviate this to just

α =(cid:88)I

fI · deI

where we understand the sum runs over I = (i1, . . . , ik), and deI represents e∨i1 ∧ ··· ∧ e∨ik .
Now that we have an element Λk(V ∨), what can it do? Well, ﬁrst let me get the

deﬁnition on the table, then tell you what it’s doing.

Deﬁnition 43.3.3. For linear functions ξ1, . . . , ξk ∈ V ∨ and vectors v1, . . . , vk ∈ V , set

(ξ1 ∧ ··· ∧ ξk)(v1, . . . , vk) := det

ξ1(v1)

...

ξk(v1)

. . .
. . .
. . .

ξ1(vk)

...

ξk(vk)

 .

You can check that this is well-deﬁned under e.g. v ∧ w = −w ∧ v and so on.

43 Diﬀerential forms

413

Example 43.3.4 (Evaluation of a diﬀerential form)
Set V = R3. Suppose that at some point p, the 2-form α returns

Let v1 = 3e1 + e2 + 4e3 and v2 = 8e1 + 9e2 + 5e3. Then

4 5(cid:21) = 21.

αp = 2e∨1 ∧ e∨2 + e∨1 ∧ e∨3 .
1 9(cid:21) + det(cid:20)3 8

αp(v1, v2) = 2 det(cid:20)3 8

What does this deﬁnition mean? One way to say it is that

If I walk to a point p ∈ U , a k-form α will take in k vectors v1, . . . , vk and
spit out a number, which is to be interpreted as a (signed) volume.

Picture:

In other words, at every point p, we get a function αp. Then I can feed in k vectors to
αp and get a number, which I interpret as a signed volume of the parallelpiped spanned
by the {vi}’s in some way (e.g. the ﬂux of a force ﬁeld). That’s why αp as a “function”
is contrived to lie in the wedge product: this ensures that the notion of “volume” makes
sense, so that for example, the equality αp(v1, v2) = −αp(v2, v1) holds.

This is what makes diﬀerential forms so ﬁt for integration.

§43.4 Exterior derivatives

Prototypical example for this section: Possibly dx1 = e∨1 .

We now deﬁne the exterior derivative df that we gave pictures of at the beginning of
the section. It turns out that the exterior derivative is easy to compute given explicit
coordinates to work with.

First, given a function f : U → R, we deﬁne
df := Df =(cid:88)i

∂fi
∂ei

e∨i

In particular, suppose V = Rn and f (x1, . . . , xn) = x1 (i.e. f = e∨1 ). Then:

Question 43.4.1. Show that for any p ∈ U ,
(d(e∨

1 ))p = e∨
1 .

Upv1v2αp(v1,v2)∈R414

Napkin, by Evan Chen (v1.5.20190718)

Abuse of Notation 43.4.2. Unfortunately, someone somewhere decided it would be
a good idea to use “x1” to denote e∨1 (because obviously 1 x1 means “the function that
takes (x1, . . . , xn) ∈ Rn to x1”) and then decided that

dx1 := e∨1 .

This notation is so entrenched that I have no choice but to grudgingly accept it. Note
that it’s not even right, since technically it’s (dx1)p = e∨1 ; dx1 is a 1-form.

Remark 43.4.3 — This is the reason why we use the notation df
given, say, f : R → R by f (x) = x2, it is indeed true that

dx in calculus now:

and so by (more) abuse of notation we write df /dx = 2x.

df = 2x · e∨1 = 2x · dx

More generally, we can deﬁne the exterior derivative in terms of our basis e1, . . . ,

en as follows: if α =(cid:80)I fI deI then we set

dα :=(cid:88)I

dfI ∧ deI =(cid:88)I (cid:88)j

∂fI
∂ej

dej ∧ deI .

This doesn’t depend on the choice of basis.

Example 43.4.4 (Computing some exterior derivatives)
Let V = R3 with standard basis e1, e2, e3. Let f (x, y, z) = x4 + y3 + 2xz. Then we
compute

df = Df = (4x3 + 2z) dx + 3y2 dy + 2x dz.

Next, we can evaluate d(df ) as prescribed: it is

d2f = (12x2 dx + 2dz) ∧ dx + (6y dy) ∧ dy + 2(dx ∧ dz)

= 12x2(dx ∧ dx) + 4(dz ∧ dx) + 6y(dy ∧ dy) + 4(dx ∧ dz)
= 4(dz ∧ dx) + 4(dx ∧ dz)
= 0.

So surprisingly, d2f is the zero map. Here, we have exploited Abuse of Notation 43.4.2
for the ﬁrst time, in writing dx, dy, dz.

And in fact, this is always true in general:

Theorem 43.4.5 (Exterior derivative vanishes)
Let α be any k-form. Then d2(α) = 0. Even more succinctly,

d2 = 0.

The proof is left as Problem 43B.

1Sarcasm.

43 Diﬀerential forms

415

Exercise 43.4.6. Compare the statement d2 = 0 to the geometric picture of a 2-form given
at the beginning of this chapter. Why does this intuitively make sense?

Here are some other properties of d:

 As we just saw, d2 = 0.

 For a k-form α and (cid:96)-form β, one can show that

d(α ∧ β) = dα ∧ β + (−1)k(α ∧ dβ).

 If f : U → R is smooth, then df = Df .

In fact, one can show that df as deﬁned above is the unique map sending k-forms to
(k + 1)-forms with these properties. So, one way to deﬁne df is to take as axioms the
bulleted properties above and then declare d to be the unique solution to this functional
equation. In any case, this tells us that our deﬁnition of d does not depend on the basis
chosen.

Recall that df measures the change in boundary. In that sense, d2 = 0 is saying
something like “the boundary of the boundary is empty”. We’ll make this precise when
we see Stokes’ theorem in the next chapter.

§43.5 Closed and exact forms

Let α be a k-form.

Deﬁnition 43.5.1. We say α is closed if dα = 0.

Deﬁnition 43.5.2. We say α is exact if for some (k − 1)-form β, dβ = α. If k = 0, α
is exact only when α = 0.

Question 43.5.3. Show that exact forms are closed.

A natural question arises: are there closed forms which are not exact? Surprisingly,

the answer to this question is tied to topology. Here is one important example.

Example 43.5.4 (The angle form)
Let U = R2 \ {0}, and let θ(p) be the angle formed by the x-axis and the line from
the origin to p.

The 1-form α : U → (R2)∨ deﬁned by

α = −y dx + x dy

x2 + y2

is called the angle form: given p ∈ U it measures the change in angle θ(p) along a
tangent vector. So intuitively, “α = dθ”. Indeed, one can check directly that the
angle form is closed.

However, α is not exact: there is no global smooth function θ : U → R having α
as a derivative. This reﬂects the fact that one can actually perform a full 2π rotation
around the origin, i.e. θ only makes sense mod 2π. Thus existence of the angle form
α reﬂects the possibility of “winding” around the origin.

416

Napkin, by Evan Chen (v1.5.20190718)

So the key idea is that the failure of a closed form to be exact corresponds quite well
with “holes” in the space: the same information that homotopy and homology groups are
trying to capture. To draw another analogy, in complex analysis Cauchy-Goursat only
works when U is simply connected. The “hole” in U is being detected by the existence
of a form α. The so-called de Rham cohomology will make this relation explicit.

§43.6 A few harder problems to think about

Problem 43A. Show directly that the angle form

α = −y dx + x dy

x2 + y2

is closed.

Problem 43B. Establish Theorem 43.4.5, which states that d2 = 0.

44 Integrating diﬀerential forms

We now show how to integrate diﬀerential forms over cells, and state Stokes’ theorem

in this context. In this chapter, all vector spaces are ﬁnite-dimensional and real.

§44.1 Motivation: line integrals

Given a function g : [a, b] → R, we know by the fundamental theorem of calculus that

(cid:90)[a,b]

g(t) dt = f (b) − f (a)

where f is a function such that g = df /dt. Equivalently, for f : [a, b] → R,

(cid:90)[a,b]

g dt =(cid:90)[a,b]

df = f (b) − f (a)

where df is the exterior derivative we deﬁned earlier.

Cool, so we can integrate over [a, b]. Now suppose more generally, we have U an
open subset of our real vector space V and a 1-form α : U → V ∨. We consider a
parametrized curve, which is a smooth function c : [a, b] → U . Picture:

We want to deﬁne an(cid:82)c α such that:
The integral (cid:82)c α should add up all the α along the curve c.
Our diﬀerential form α ﬁrst takes in a point p to get αp ∈ V ∨. Then, it eats a tangent
vector v ∈ V to the curve c to ﬁnally give a real number αp(v) ∈ R. We would like to
“add all these numbers up”, using only the notion of an integral over [a, b].

Exercise 44.1.1. Try to guess what the deﬁnition of the integral should be. (By type-
checking, there’s only one reasonable answer.)

417

[a,b]tcUcp=c(t)ααp(v)∈Rc∗α418

Napkin, by Evan Chen (v1.5.20190718)

So, the deﬁnition we give is

(cid:90)c

α :=(cid:90)[a,b]

αc(t)(cid:0)c(cid:48)(t)(cid:1) dt.

Here, c(cid:48)(t) is shorthand for (Dc)c(t)(1). It represents the tangent vector to the curve c
at the point p = c(t), at time t. (Here we are taking advantage of the fact that [a, b] is
one-dimensional.)

Now that deﬁnition was a pain to write, so we will deﬁne a diﬀerential 1-form c∗α on

[a, b] to swallow that entire thing: speciﬁcally, in this case we deﬁne c∗α to be

(here ε is some displacement in time). Thus, we can more succinctly write

(c∗α)t (ε) = αc(t) · (Dc)t(ε)

(cid:90)c

α :=(cid:90)[a,b]

c∗α.

This is a special case of a pullback : roughly, if φ : U → U(cid:48) (where U ⊆ V , U(cid:48) ⊆ V (cid:48)), we
can change any diﬀerential k-form α on U(cid:48) to a k-form on U . In particular, if U = [a, b],1
we can resort to our old deﬁnition of an integral. Let’s now do this in full generality.

§44.2 Pullbacks

Let V and V (cid:48) be ﬁnite dimensional real vector spaces (possibly diﬀerent dimensions) and
suppose U and U(cid:48) are open subsets of each; next, consider a k-form α on U(cid:48).

Given a map φ : U → U(cid:48) we now want to deﬁne a pullback in much the same way as

before. Picture:

Well, there’s a total of about one thing we can do. Speciﬁcally: α accepts a point in U(cid:48)
and k tangent vectors in V (cid:48), and returns a real number. We want φ∗α to accept a point
in p ∈ U and k tangent vectors v1, . . . , vk in V , and feed the corresponding information
to α.
1OK, so [a, b] isn’t actually open, sorry. I ought to write (a − ε, b + ε), or something.

UpU′q=φ(p)φααq(...)∈Rφ∗α44 Integrating diﬀerential forms

419

Clearly we give the point q = φ(p). As for the tangent vectors, since we are interested
in volume, we take the derivative of φ at p, (Dφ)p, which will scale each of our vectors vi
into some vector in the target V (cid:48). To cut a long story short:
Deﬁnition 44.2.1. Given φ : U → U(cid:48) and α a k-form, we deﬁne the pullback

(φ∗α)p(v1, . . . , vk) := αφ(p) ((Dφ)p(v1), . . . , (Dφ)p(vk)) .

There is a more concrete way to deﬁne the pullback using bases. Suppose w1, . . . , wn
is a basis of V (cid:48) and e1, . . . , em is a basis of V . Thus, by the projection principle
(Theorem 42.2.1) the map φ : V → V (cid:48) can be thought of as
φ(v) = φ1(v)w1 + . . . φn(v)wn

where each φi takes in a v ∈ V and returns a real number. We know also that α can be
written concretely as

α = (cid:88)J⊆{1,...,n}

fJ wJ .

Then, we deﬁne

φ∗α = (cid:88)I⊆{1,...,m}

(fI ◦ φ)(Dφi1 ∧ ··· ∧ Dφik ).

A diligent reader can check these deﬁnitions are equivalent.

Example 44.2.2 (Computation of a pullback)
Let V = R2 with basis e1 and e2, and suppose φ : V → V (cid:48) is given by sending

φ(ae1 + be2) = (a2 + b2)w1 + log(a2 + 1)w2 + b3w3

where w1, w2, w3 is a basis for V (cid:48). Consider the form αq = f (q)w1 ∧ w3, where
f : V (cid:48) → R. Then

(φ∗α)p = f (φ(p)) · (2ae∨1 + 2be∨2 ) ∧ (3b2e∨2 ) = f (φ(p)) · 6ab2 · e∨1 ∧ e∨2 .

It turns out that the pullback basically behaves nicely as possible, e.g.

 φ∗(cα + β) = cφ∗α + φ∗β (linearity)
 φ∗(α ∧ β) = (φ∗α) ∧ (φ∗β)
 φ∗1(φ∗2(α)) = (φ2 ◦ φ1)∗(α) (naturality)

but I won’t take the time to check these here (one can verify them all by expanding with
a basis).

§44.3 Cells

Prototypical example for this section: A disk in R2 can be thought of as the cell [0, R] ×
[0, 2π] → R2 by (r, θ) (cid:55)→ (r cos θ)e1 + (r sin θ)e2.

Now that we have the notion of a pullback, we can deﬁne the notion of an integral for

more general spaces. Speciﬁcally, to generalize the notion of integrals we had before:
Deﬁnition 44.3.1. A k-cell is a smooth function c : [a1, b1] × [a2, b2] × . . . [ak, bk] → V .

420

Napkin, by Evan Chen (v1.5.20190718)

Example 44.3.2 (Examples of cells)
Let V = R2 for convenience.

(a) A 0-cell consists of a single point.

(b) As we saw, a 1-cell is an arbitrary curve.

(c) A 2-cell corresponds to a 2-dimensional surface. For example, the map c :

[0, R] × [0, 2π] → V by

can be thought of as a disk of radius R.

c : (r, θ) (cid:55)→ (r cos θ, r sin θ)

Then, to deﬁne an integral

α

(cid:90)c
(cid:90)[0,1]k

c∗α

for a diﬀerential k-form α and a k-cell c : [0, 1]k → V , we simply take the pullback

Since c∗α is a k-form on the k-dimensional unit box, it can be written as f (x1, . . . , xn) dx1∧
··· ∧ dxn, so the above integral can be written as

(cid:90) 1
0 ···(cid:90) 1

0

f (x1, . . . , xn) dx1 ∧ ··· ∧ dxn

Example 44.3.3 (Area of a circle)
Consider V = R2 and let c : (r, θ) (cid:55)→ (r cos θ)e1 + (r sin θ)e2 on [0, R] × [0, 2π] as
before. Take the 2-form α which gives αp = e∨1 ∧ e∨2 at every point p. Then

c∗α = (cos θdr − r sin θdθ) ∧ (sin θdr + r cos θdθ)

= r(cos2 θ + sin2 θ)(dr ∧ dθ)
= r dr ∧ dθ

Thus,

which is the area of a circle.

(cid:90)c

α =(cid:90) R

0 (cid:90) 2π

0

r dr ∧ dθ = πR2

Here’s some geometric intuition for what’s happening. Given a k-cell in V , a diﬀerential
k-form α accepts a point p and some tangent vectors v1, . . . , vk and spits out a number
αp(v1, . . . , vk), which as before we view as a signed hypervolume. Then the integral adds
up all these inﬁnitesimals across the entire cell. In particular, if V = Rk and we take the
form α : p (cid:55)→ e∨1 ∧ ··· ∧ e∨k , then what these α’s give is the kth hypervolume of the cell.
For this reason, this α is called the volume form on Rk.
You’ll notice I’m starting to play loose with the term “cell”: while the cell c : [0, R] ×
[0, 2π] → R2 is supposed to be a function I have been telling you to think of it as a unit

44 Integrating diﬀerential forms

421

disk (i.e. in terms of its image). In the same vein, a curve [0, 1] → V should be thought
of as a curve in space, rather than a function on time.
This error turns out to be benign. Let α be a k-form on U and c : [a1, b1] × ··· ×
[ak, bk] → U a k-cell. Suppose φ : [a(cid:48)1, b(cid:48)1] × . . . [a(cid:48)k, b(cid:48)k] → [a1, b1] × ··· × [ak, bk]; it is a
reparametrization if φ is bijective and (Dφ)p is always invertible (think “change of
variables”); thus

c ◦ φ : [a(cid:48)1, b(cid:48)1] × ··· × [a(cid:48)k, b(cid:48)k] → U

is a k-cell as well. Then it is said to preserve orientation if det(Dφ)p > 0 for all p and
reverse orientation if det(Dφ)p < 0 for all p.

Exercise 44.3.4. Why is it that exactly one of these cases must occur?

Theorem 44.3.5 (Changing variables doesn’t aﬀect integrals)

Let c be a k-cell, α a k-form, and φ a reparametrization. Then

(cid:90)c◦φ

α =(cid:40)(cid:82)c α

φ preserves orientation

−(cid:82)c α φ reverses orientation.

Proof. Use naturality of the pullback to reduce it to the corresponding theorem in normal
calculus.

So for example, if we had parametrized the unit circle as [0, 1] × [0, 1] → R2 by
(r, t) (cid:55)→ R cos(2πt)e1 + R sin(2πt)e2, we would have arrived at the same result. So we
really can think of a k-cell just in terms of the points it speciﬁes.

§44.4 Boundaries

Prototypical example for this section: The boundary of [a, b] is {b} − {a}. The boundary
of a square goes around its edge counterclockwise.

First, I introduce a technical term that lets us consider multiple cells at once.

Deﬁnition 44.4.1. A k-chain U is a formal linear combination of k-cells over U , i.e. a
sum of the form

c = a1c1 + ··· + amcm

where each ai ∈ R and ci is a k-cell. We deﬁne(cid:82)c α =(cid:80)i ai(cid:82) ci.

In particular, a 0-chain consists of several points, each with a given weight.
Now, how do we deﬁne the boundary? For a 1-cell [a, b] → U , as I hinted earlier we

want the answer to be the 0-chain {c(b)} − {c(a)}. Here’s how we do it in general.
Deﬁnition 44.4.2. Suppose c : [0, 1]k → U is a k-cell. Then the boundary of c, denoted
∂c : [0, 1]k−1 → U , is the (k − 1)-chain deﬁned as follows. For each i = 1, . . . , k deﬁne

cstart
i
cstop
i

(t1, . . . , tk−1) = (t1, . . . , ti−1, 0, ti, . . . , tk)
(t1, . . . , tk−1) = (t1, . . . , ti−1, 1, ti, . . . , tk).

Then

∂c :=

k(cid:88)i=1

(−1)i+1(cid:16)cstop

i − cstart

i

(cid:17) .

422

Napkin, by Evan Chen (v1.5.20190718)

Finally, the boundary of a chain is the sum of the boundaries of each cell (with the

appropriate weights). That is, ∂((cid:80) aici) =(cid:80) ai∂ci.

Question 44.4.3. Satisfy yourself that one can extend this deﬁnition to a k-cell c deﬁned
on c : [a1, b1] × ··· × [ak, bk] → V (rather than from [0, 1]k → V ).

Example 44.4.4 (Examples of boundaries)
Consider the 2-cell c : [0, 1]2 → R2 shown below.

Here p1, p2, p3, p4 are the images of (0, 0), (0, 1), (1, 0), (1, 1), respectively. Then we
can think of ∂c as

∂c = [p1, p2] + [p2, p3] + [p3, p4] + [p4, p1]

where each “interval” represents the 1-cell shown by the reddish arrows on the right.
We can take the boundary of this as well, and obtain an empty chain as

∂(∂c) =

4(cid:88)i=1

∂([pi, pi+1]) =

4(cid:88)i=1

{pi+1} − {pi} = 0.

Example 44.4.5 (Boundary of a unit disk)
Consider the unit disk given by

c : [0, 1] × [0, 2π] → R2 by (r, θ) (cid:55)→ s cos(2πt)e1 + s sin(2πt)e2.

The four parts of the boundary are shown in the picture below:

Note that two of the arrows more or less cancel each other out when they are
integrated. Moreover, we interestingly have a degenerate 1-cell at the center of the
circle; it is a constant function [0, 1] → R2 which always gives the origin.

Obligatory theorem, analogous to d2 = 0 and left as a problem.

[0,1]2cp1p2p3p4crθ[0,1]2c44 Integrating diﬀerential forms

423

Theorem 44.4.6 (The boundary of the boundary is empty)
∂2 = 0, in the sense that for any k-chain c we have ∂2(c) = 0.

§44.5 Stokes’ theorem

Prototypical example for this section: (cid:82)[a,b] dg = g(b) − g(a).

We now have all the ingredients to state Stokes’ theorem for cells.

Theorem 44.5.1 (Stokes’ theorem for cells)
Take U ⊆ V as usual, let c : [0, 1]k → U be a k-cell and let α : U → Λk(V ∨) be a
k-form. Then

In particular, if dα = 0 then the left-hand side vanishes.

(cid:90)c

dα =(cid:90)∂c

α.

For example, if c is the interval [a, b] then ∂c = {b} − {a}, and thus we obtain the
fundamental theorem of calculus.

§44.6 A few harder problems to think about

Problem 44A† (Green’s theorem). Let f, g : R2 → R be smooth functions. Prove that

(cid:90)c(cid:18) ∂g

∂x −

∂f

∂y(cid:19) dx ∧ dy =(cid:90)∂c

(f dx + g dy).

Problem 44B. Show that ∂2 = 0.

Problem 44C (Pullback and d commute). Let U and U(cid:48) be open sets of vector spaces
V and V (cid:48) and let φ : U → U(cid:48) be a smooth map between them. Prove that for any
diﬀerential form α on U(cid:48) we have

φ∗(dα) = d(φ∗α).

Problem 44D (Arc length isn’t a form). Show that there does not exist a 1-form α on

R2 such that for a curve c : [0, 1] → R2, the integral(cid:82)c α gives the arc length of c.

Problem 44E. An exact k-form α is one satisfying α = dβ for some β. Prove that

(cid:90)C1

α =(cid:90)C2

α

where C1 and C2 are any concentric circles in the plane and α is some exact 1-form.

45 A bit of manifolds

Last chapter, we stated Stokes’ theorem for cells. It turns out there is a much larger

class of spaces, the so-called smooth manifolds, for which this makes sense.

Unfortunately, the deﬁnition of a smooth manifold is complete garbage, and so by the
time I am done deﬁning diﬀerential forms and orientations, I will be too lazy to actually
deﬁne what the integral on it is, and just wave my hands and state Stokes’ theorem.

§45.1 Topological manifolds

Prototypical example for this section: S2: “the Earth looks ﬂat”.

Long ago, people thought the Earth was ﬂat, i.e. homeomorphic to a plane, and in
particular they thought that π2(Earth) = 0. But in fact, as most of us know, the Earth
is actually a sphere, which is not contractible and in particular π2(Earth) ∼= Z. This
observation underlies the deﬁnition of a manifold:

An n-manifold is a space which locally looks like Rn.

Actually there are two ways to think about a topological manifold M :

 “Locally”: at every point p ∈ M , some open neighborhood of p looks like an open
set of Rn. For example, to someone standing on the surface of the Earth, the Earth
looks much like R2.

 “Globally”: there exists an open cover of M by open sets {Ui}i (possibly inﬁnite)
such that each Ui is homeomorphic to some open subset of Rn. For example, from
outer space, the Earth can be covered by two hemispherical pancakes.

Question 45.1.1. Check that these are equivalent.

While the ﬁrst one is the best motivation for examples, the second one is easier to use
formally.

Deﬁnition 45.1.2. A topological n-manifold M is a Hausdorﬀ space with an open
cover {Ui} of sets homeomorphic to subsets of Rn, say by homeomorphisms

φi : Ui

∼=−→ Ei ⊆ Rn

where each Ei is an open subset of Rn. Each φi : Ui → Ei is called a chart, and together
they form a so-called atlas.

Remark 45.1.3 — Here “E” stands for “Euclidean”. I think this notation is not
standard; usually people just write φi(Ui) instead.

425

426

Napkin, by Evan Chen (v1.5.20190718)

Remark 45.1.4 — This deﬁnition is nice because it doesn’t depend on embeddings:
a manifold is an intrinsic space M , rather than a subset of RN for some N . Analogy:
an abstract group G is an intrinsic object rather than a subgroup of Sn.

Example 45.1.5 (An atlas on S1)
Here is a picture of an atlas for S1, with two open sets.

Question 45.1.6. Where do you think the words “chart” and “atlas” come from?

Example 45.1.7 (Some examples of topological manifolds)
(a) As discussed at length, the sphere S2 is a 2-manifold: every point in the sphere
has a small open neighborhood that looks like D2. One can cover the Earth
with just two hemispheres, and each hemisphere is homeomorphic to a disk.

(b) The circle S1 is a 1-manifold; every point has an open neighborhood that looks

like an open interval.

(c) The torus, Klein bottle, RP2, CP2 are all 2-manifolds.

(d) Rn is trivially a manifold, as are its open sets.

All these spaces are compact except Rn.

A non-example of a manifold is Dn, because it has a boundary; points on the

boundary do not have open neighborhoods that look Euclidean.

§45.2 Smooth manifolds

Prototypical example for this section: All the topological manifolds.

Let M be a topological n-manifold with atlas {Ui

φi−→ Ei}.

Deﬁnition 45.2.1. For any i, j such that Ui ∩ Uj (cid:54)= ∅, the transition map φij is the
composed map

φij : Ei ∩ φimg

i

(Ui ∩ Uj)

φ

−1
i−−→ Ui ∩ Uj

φj−→ Ej ∩ φimg

j

(Ui ∩ Uj).

Sorry for the dense notation, let me explain. The intersection with the image φimg

(Ui ∩
(Ui ∩ Uj) is a notational annoyance to make the map well-deﬁned

Uj) and the image φimg

j

i

S1U2U1E1φ1E2φ245 A bit of manifolds

427

and a homeomorphism. The transition map is just the natural way to go from Ei → Ej,
restricted to overlaps. Picture below, where the intersections are just the green portions
of each E1 and E2:

We want to add enough structure so that we can use diﬀerential forms.

Deﬁnition 45.2.2. We say M is a smooth manifold if all its transition maps are
smooth.

This deﬁnition makes sense, because we know what it means for a map between two

open sets of Rn to be diﬀerentiable.

With smooth manifolds we can try to port over deﬁnitions that we built for Rn onto
our manifolds. So in general, all deﬁnitions involving smooth manifolds will reduce to
something on each of the coordinate charts, with a compatibility condition.

AS an example, here is the deﬁnition of a “smooth map”:

Deﬁnition 45.2.3. (a) Let M be a smooth manifold. A continuous function f : M → R

is called smooth if the composition

φ

−1
i−−→ Ui (cid:44)→ M

f

−→ R

Ei

is smooth as a function Ei → R.

(b) Let M and N be smooth with atlases {U M

i }i and {U N
f : M → N is smooth if for every i and j, the composed map

i

φi−→ EM

φj−→ EN

i }j, A map

j

φ

−1
i−−→ Ui (cid:44)→ M

Ei

f

−→ N (cid:16) Uj

φj−→ Ej

is smooth, as a function Ei → Ej.

§45.3 Regular value theorem

Prototypical example for this section: x2 + y2 = 1 is a circle!

Despite all that I’ve written about general manifolds, it would be sort of mean if I left
you here because I have not really told you how to actually construct manifolds in practice,
even though we know the circle x2 + y2 = 1 is a great example of a one-dimensional
manifold embedded in R2.

S1U2U1E1φ1E2φ2φ12428

Napkin, by Evan Chen (v1.5.20190718)

Theorem 45.3.1 (Regular value theorem)
Let V be an n-dimensional real normed vector space, let U ⊆ V be open and let
f1, . . . , fm : U → R be smooth functions. Let M be the set of points p ∈ U such that
f1(p) = ··· = fm(p) = 0.

Assume M is nonempty and that the map

V → Rm by v (cid:55)→ ((Df1)p(v), . . . , (Dfm)p(v))

has rank m, for every point p ∈ M . Then M is a manifold of dimension n − m.

For a proof, see [Sj05, Theorem 6.3].

One very common special case is to take m = 1 above.

Corollary 45.3.2 (Level hypersurfaces)
Let V be a ﬁnite-dimensional real normed vector space, let U ⊆ V be open and
let f : U → R be smooth. Let M be the set of points p ∈ U such that f (p) = 0.
If M (cid:54)= ∅ and (Df )p is not the zero map for any p ∈ M , then M is a manifold of
dimension n − 1.

Example 45.3.3 (The circle x2 + y2 − c = 0)
Let f (x, y) = x2 + y2 − c, f : R2 → R, where c is a positive real number. Note that

Df = 2x · dx + 2y · dy

which in particular is nonzero as long as (x, y) (cid:54)= (0, 0), i.e. as long as c (cid:54)= 0. Thus:
 When c > 0, the resulting curve — a circle with radius √c — is a one-

dimensional manifold, as we knew.

 When c = 0, the result fails. Indeed, M is a single point, which is actually a

zero-dimensional manifold!

We won’t give further examples since I’m only mentioning this in passing in order to
increase your capacity to write real concrete examples. (But [Sj05, Chapter 6.2] has some
more examples, beautifully illustrated.)

§45.4 Diﬀerential forms on manifolds

We already know what a diﬀerential form is on an open set U ⊆ Rn. So, we naturally
try to port over the deﬁnition of diﬀerentiable form on each subset, plus a compatibility
condition.

Let M be a smooth manifold with atlas {Ui

φi−→ Ei}i.

Deﬁnition 45.4.1. A diﬀerential k-form α on a smooth manifold M is a collection
{αi}i of diﬀerential k-forms on each Ei, such that for any j and i we have that

αj = φ∗ij(αi).

In English: we specify a diﬀerential form on each chart, which is compatible under

pullbacks of the transition maps.

45 A bit of manifolds

§45.5 Orientations

429

Prototypical example for this section: Left versus right, clockwise vs. counterclockwise.

This still isn’t enough to integrate on manifolds. We need one more deﬁnition: that of

an orientation.

The main issue is the observation from standard calculus that

(cid:90) b

a

f (x) dx = −(cid:90) a

b

f (x) dx.

Consider then a space M which is homeomorphic to an interval. If we have a 1-form
α, how do we integrate it over M ? Since M is just a topological space (rather than a
subset of R), there is no default “left” or “right” that we can pick. As another example,
if M = S1 is a circle, there is no default “clockwise” or “counterclockwise” unless we
decide to embed M into R2.

To work around this we have to actually have to make additional assumptions about

our manifold.

Deﬁnition 45.5.1. A smooth n-manifold is orientable if there exists a diﬀerential
n-form ω on M such that for every p ∈ M ,

ωp (cid:54)= 0.

Recall here that ωp is an element of Λn(V ∨). In that case we say ω is a volume form

of M .

How do we picture this deﬁnition? If we recall that an diﬀerential form is supposed to
take tangent vectors of M and return real numbers. To this end, we can think of each
point p ∈ M as having a tangent plane Tp(M ) which is n-dimensional. Now since the
volume form ω is n-dimensional, it takes an entire basis of the Tp(M ) and gives a real
number. So a manifold is orientable if there exists a consistent choice of sign for the basis
of tangent vectors at every point of the manifold.

For “embedded manifolds”, this just amounts to being able to pick a nonzero ﬁeld of

normal vectors to each point p ∈ M . For example, S1 is orientable in this way.

Similarly, one can orient a sphere S2 by having a ﬁeld of vectors pointing away (or
towards) the center. This is all non-rigorous, because I haven’t deﬁned the tangent plane
Tp(M ); since M is in general an intrinsic object one has to be quite roundabout to deﬁne
Tp(M ) (although I do so in an optional section later). In any event, the point is that
guesses about the orientability of spaces are likely to be correct.

S1430

Napkin, by Evan Chen (v1.5.20190718)

Example 45.5.2 (Orientable surfaces)
(a) Spheres Sn, planes, and the torus S1 × S1 are orientable.
(b) The M¨obius strip and Klein bottle are not orientable: they are “one-sided”.

(c) CPn is orientable for any n.

(d) RPn is orientable only for odd n.

§45.6 Stokes’ theorem for manifolds

Stokes’ theorem in the general case is based on the idea of a manifold with boundary
M , which I won’t deﬁne, other than to say its boundary ∂M is an n − 1 dimensional
manifold, and that it is oriented if M is oriented. An example is M = D2, which has
boundary ∂M = S1.

Next,

Deﬁnition 45.6.1. The support of a diﬀerential form α on M is the closure of the set

If this support is compact as a topological space, we say α is compactly supported.

{p ∈ M | αp (cid:54)= 0} .

Remark 45.6.2 — For example, volume forms are supported on all of M .

Now, one can deﬁne integration on oriented manifolds, but I won’t deﬁne this because

the deﬁnition is truly awful. Then Stokes’ theorem says

Theorem 45.6.3 (Stokes’ theorem for manifolds)

Let M be an smooth oriented n-manifold with boundary and let α be a compactly
supported n-form. Then

(cid:90)M

dα =(cid:90)∂M

α.

All the omitted details are developed in full in [Sj05].

§45.7 (Optional) The tangent and contangent space

Prototypical example for this section: Draw a line tangent to a circle, or a plane tangent
to a sphere.

earlier, but want to actually deﬁne it now.

Let M be a smooth manifold and p ∈ M a point. I omitted the deﬁnition of Tp(M )
As I said, geometrically we know what this should look like for our usual examples.
For example, if M = S1 is a circle embedded in R2, then the tangent vector at a point p
should just look like a vector running oﬀ tangent to the circle. Similarly, given a sphere
M = S2, the tangent space at a point p along the sphere would look like plane tangent
to M at p.

45 A bit of manifolds

431

However, one of the points of all this manifold stuﬀ is that we really want to see the
manifold as an intrinsic object, in its own right, rather than as embedded in Rn.1 So, we
would like our notion of a tangent vector to not refer to an ambient space, but only to
intrinsic properties of the manifold M in question.

§45.7.i Tangent space

To motivate this construction, let us start with an embedded case for which we know the
answer already: a sphere.

Suppose f : S2 → R is a function on a sphere, and take a point p. Near the point p, f
looks like a function on some open neighborhood of the origin. Thus we can think of
taking a directional derivative along a vector (cid:126)v in the imagined tangent plane (i.e. some
partial derivative). For a ﬁxed (cid:126)v this partial derivative is a linear map

D(cid:126)v : C∞(M ) → R.

It turns out this goes the other way:

if you know what D(cid:126)v does to every smooth
function, then you can recover v. This is the trick we use in order to create the tangent
space. Rather than trying to specify a vector (cid:126)v directly (which we can’t do because we
don’t have an ambient space),

The vectors are partial-derivative-like maps.

More formally, we have the following.

Deﬁnition 45.7.1. A derivation D at p is a linear map D : C∞(M ) → R (i.e. assigning
a real number to every smooth f ) satisfying the following Leibniz rule: for any f , g we
have the equality

D(f g) = f (p) · D(g) + g(p) · D(f ) ∈ R.

This is just a “product rule”. Then the tangent space is easy to deﬁne:

Deﬁnition 45.7.2. A tangent vector is just a derivation at p, and the tangent space
Tp(M ) is simply the set of all these tangent vectors.

In this way we have constructed the tangent space.

1This can be thought of as analogous to the way that we think of a group as an abstract object in its
own right, even though Cayley’s Theorem tells us that any group is a subgroup of the permutation
group.

Note this wasn’t always the case! During the 19th century, a group was literally deﬁned as a subset
of GL(n) or of Sn. In fact Sylow developed his theorems without the word “group” Only much later
did the abstract deﬁnition of a group was given, an abstract set G which was independent of any
embedding into Sn, and an object in its own right.

S1Tp(M)~v∈Tp(M)p432

Napkin, by Evan Chen (v1.5.20190718)

§45.7.ii The cotangent space

In fact, one can show that the product rule for D is equivalent to the following three
conditions:

1. D is linear, meaning D(af + bg) = aD(f ) + bD(g).

2. D(1M ) = 0, where 1M is the constant function on M .

3. D(f g) = 0 whenever f (p) = g(p) = 0. Intuitively, this means that if a function
h = f g vanishes to second order at p, then its derivative along D should be zero.

This suggests a third equivalent deﬁnition: suppose we deﬁne

mp := {f ∈ C∞M | f (p) = 0}

to be the set of functions which vanish at p (this is called the maximal ideal at p). In
that case,

m2

p =(cid:40)(cid:88)i

fi · gi | fi(p) = gi(p) = 0(cid:41)

is the set of functions vanishing to second order at p. Thus, a tangent vector is really
just a linear map

mp/m2

p → R.

In other words, the tangent space is actually the dual space of mp/m2
p; for this reason, the
space mp/m2
p is deﬁned as the cotangent space (the dual of the tangent space). This
deﬁnition is even more abstract than the one with derivations above, but has some nice
properties:

 it is coordinate-free, and

 it’s deﬁned only in terms of the smooth functions M → R, which will be really
helpful later on in algebraic geometry when we have varieties or schemes and can
repeat this deﬁnition.

§45.7.iii Sanity check

With all these equivalent deﬁnitions, the last thing I should do is check that this deﬁnition
of tangent space actually gives a vector space of dimension n. To do this it suﬃces to
show verify this for open subsets of Rn, which will imply the result for general manifolds
M (which are locally open subsets of Rn). Using some real analysis, one can prove the
following result:

Theorem 45.7.3
Suppose M ⊂ Rn is open and 0 ∈ M . Then

m0 = {smooth functions f : f (0) = 0}
m2
0 = {smooth functions f : f (0) = 0, (∇f )0 = 0}.
0 is the set of functions which vanish at 0 and such that all ﬁrst

In other words m2
derivatives of f vanish at zero.

45 A bit of manifolds

433

Thus, it follows that there is an isomorphism

m0/m2

0 ∼= Rn by f (cid:55)→(cid:20) ∂f

∂x1

(0), . . . ,

∂f
∂xn

(0)(cid:21)

and so the cotangent space, hence tangent space, indeed has dimension n.

§45.8 A few harder problems to think about

Problem 45A. Show that a diﬀerential 0-form on a smooth manifold M is the same
thing as a smooth function M → R.

some appli-
some appli-
cations of
cations of
regular value
regular value
theorem here
theorem here

XIII

Algebraic NT I: Rings of Integers

Part XIII: Contents

46 Algebraic integers

437
46.1 Motivation from high school algebra . . . . . . . . . . . . . . . . . . . . . . . . . 437
46.2 Algebraic numbers and algebraic integers . . . . . . . . . . . . . . . . . . . . . . . 438
46.3 Number ﬁelds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 439
46.4 Primitive element theorem, and monogenic extensions . . . . . . . . . . . . . . . . . 440
. . . . . . . . . . . . . . . . . . . . . . . . 441
46.5 A few harder problems to think about

47 The ring of integers

443
47.1 Norms and traces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 443
47.2 The ring of integers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 446
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 450
47.3 On monogenic extensions
. . . . . . . . . . . . . . . . . . . . . . . . 450
47.4 A few harder problems to think about

48 Unique factorization (ﬁnally!)

451
48.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 451
48.2 Ideal arithmetic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 452
48.3 Dedekind domains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 453
48.4 Unique factorization works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 454
48.5 The factoring algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455
48.6 Fractional ideals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 458
48.7 The ideal norm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 459
. . . . . . . . . . . . . . . . . . . . . . . . 460
48.8 A few harder problems to think about

49 Minkowski bound and class groups

461
49.1 The class group . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 461
49.2 The discriminant of a number ﬁeld . . . . . . . . . . . . . . . . . . . . . . . . . . 461
49.3 The signature of a number ﬁeld . . . . . . . . . . . . . . . . . . . . . . . . . . . 464
49.4 Minkowski’s theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 466
49.5 The trap box . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 467
49.6 The Minkowski bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 468
49.7 The class group is ﬁnite . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 469
49.8 Computation of class numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . 470
. . . . . . . . . . . . . . . . . . . . . . . . 473
49.9 A few harder problems to think about

50 More properties of the discriminant

50.1 A few harder problems to think about

475
. . . . . . . . . . . . . . . . . . . . . . . . 475

51 Bonus: Let’s solve Pell’s equation!

477
51.1 Units . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 477
51.2 Dirichlet’s unit theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 478
51.3 Finding fundamental units . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 479
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 480
51.4 Pell’s equation
. . . . . . . . . . . . . . . . . . . . . . . . 481
51.5 A few harder problems to think about

46 Algebraic integers

Here’s a ﬁrst taste of algebraic number theory.
This is really close to the border between olympiads and higher math. You’ve always

known that a+√2b had a “norm” a2−2b2, and that somehow this norm was multiplicative.

You’ve also always known that roots come in conjugate pairs. You might have heard of
minimal polynomials but not know much about them.

This chapter and the next one will make all these vague notions precise. It’s drawn

largely from the ﬁrst chapter of [Og10].

§46.1 Motivation from high school algebra

This is adapted from my blog, Power Overwhelming 1.

In high school precalculus, you’ll often be asked to ﬁnd the roots of some polynomial

with integer coeﬃcients. For instance,

x3 − x2 − x − 15 = (x − 3)(x2 + 2x + 5)

has roots 3, −1 + 2i, −1 − 2i. Or as another example,

x3 − 3x2 − 2x + 2 = (x + 1)(x2 − 4x + 2)

has roots −1, 2 + √2, 2 − √2. You’ll notice that the irrational roots, like −1 ± 2i and
2 ± √2, are coming up in pairs. In fact, I think precalculus explicitly tells you that the
imaginary roots come in conjugate pairs. More generally, it seems like all the roots of
the form a + b√c come in “conjugate pairs”. And you can see why.

But a polynomial like

x3 − 8x + 4

has no rational roots. (The roots of this are approximately −3.0514, 0.51730, 2.5341.)
Or even simpler,

has only one real root,
“conjugate” pairs. Or do they?

x3 − 2

3√2. These roots, even though they are irrational, have no

Let’s try and ﬁgure out exactly what’s happening. Let α be any complex number. We

deﬁne the minimal polynomial of α over Q to be the polynomial such that

 P (x) has rational coeﬃcients, and leading coeﬃcient 1,

 P (α) = 0.

 The degree of P is as small as possible. We call deg P the degree of α.

1URL: https://usamo.wordpress.com/2014/10/19/why-do-roots-come-in-conjugate-pairs/

437

438

Napkin, by Evan Chen (v1.5.20190718)

Example 46.1.1 (Examples of minimal polynomials)

(a) √2 has minimal polynomial x2 − 2.
(b) The imaginary unit i = √−1 has minimal polynomial x2 + 1.

(c) A primitive pth root of unity, ζp = e

2πi

p , has minimal polynomial xp−1 + xp−2 +

··· + 1, where p is a prime.

Note that 100x2 − 200 is also a polynomial of the same degree which has √2 as a root;

that’s why we want to require the polynomial to be monic. That’s also why we choose to
work in the rational numbers; that way, we can divide by leading coeﬃcients without
worrying if we get non-integers.

Why do we care? The point is as follows: suppose we have another polynomial A(x)
such that A(α) = 0. Then we claim that P (x) actually divides A(x)! That means that
all the other roots of P will also be roots of A.

The proof is by contradiction: if not, by polynomial long division we can ﬁnd a quotient

and remainder Q(x), R(x) such that

A(x) = Q(x)P (x) + R(x)

and R(x) (cid:54)≡ 0. Notice that by plugging in x = α, we ﬁnd that R(α) = 0. But
deg R < deg P , and P (x) was supposed to be the minimal polynomial. That’s impossible!
Let’s look at a more concrete example. Consider A(x) = x3 − 3x2 − 2x + 2 from the
beginning. The minimal polynomial of 2 + √2 is P (x) = x2 − 4x + 2 (why?). Now we
know that if 2 + √2 is a root, then A(x) is divisible by P (x). And that’s how we know
that if 2 + √2 is a root of A, then 2 − √2 must be a root too.
As another example, the minimal polynomial of 3√2 is x3 − 2. So 3√2 actually has two
conjugates, namely, α = 3√2 (cos 120◦ + i sin 120◦) and β = 3√2 (cos 240◦ + i sin 240◦).
Thus any polynomial which vanishes at 3√2 also has α and β as roots!

Question 46.1.2 (Important but tautological: irreducible ⇐⇒ minimal). Let α be a root
of the polynomial P (x). Show that P (x) is the minimal polynomial if and only if it is
irreducible.

§46.2 Algebraic numbers and algebraic integers
Prototypical example for this section: √2 is an algebraic integer (root of x2 − 2), 1
algebraic number but not an algebraic integer (root of x − 1
2 ).

2 is an

Let’s now work in much vaster generality. First, let’s give names to the new numbers

we’ve discussed above.

Deﬁnition 46.2.1. An algebraic number is any α ∈ C which is the root of some
polynomial with coeﬃcients in Q. The set of algebraic numbers is denoted Q.

Remark 46.2.2 — One can equally well say algebraic numbers are those of which
are roots of some polynomial with coeﬃcients in Z (rather than Q), since any
polynomial in Q[x] can be scaled to one in Z[x].

46 Algebraic integers

439

Deﬁnition 46.2.3. Consider an algebraic number α and its minimal polynomial P
(which is monic and has rational coeﬃcients). If it turns out the coeﬃcients of P are
integers, then we say α is an algebraic integer.

The set of algebraic integers is denoted Z.

Remark 46.2.4 — One can show, using Gauss’s Lemma, that if α is the root of
any monic polynomial with integer coeﬃcients, then α is an algebraic integer. So in
practice, if I want to prove that √2 + √3 is an algebraic integer, then I only have to
say “the polynomial (x2 − 5)2 − 24 works” without checking that it’s minimal.

Sometimes for clarity, we refer to elements of Z as rational integers.

Example 46.2.5 (Examples of algebraic integers)
The numbers

4, i = √−1,

3√2, √2 + √3

are all algebraic integers, since they are the roots of the monic polynomials x − 4,
x2 + 1, x3 − 2 and (x2 − 5)2 − 24.
2 has minimal polynomial x − 1
2 , so it’s an algebraic number but not
an algebraic integer. (In fact, the rational root theorem also directly implies that
any monic integer polynomial does not have 1

The number 1

2 as a root!)

There are two properties I want to give for these oﬀ the bat, because they’ll be used
extensively in the tricky (but nice) problems at the end of the section. The ﬁrst we prove
now, since it’s very easy:

Proposition 46.2.6 (Rational algebraic integers are rational integers)

An algebraic integer is rational if and only if it is a rational integer. In symbols,

Z ∩ Q = Z.

Proof. Let α be a rational number. If α is an integer, it is the root of x − α, hence an
algebraic integer too.
Conversely, if P is a monic polynomial with integer coeﬃcients such that P (α) = 0

then (by the rational root theorem, say) it follows α must be an integer.

The other is that:

Proposition 46.2.7 (Z is a ring and Q is a ﬁeld)
The algebraic integers Z form a ring; The algebraic numbers Q form a ring.

We could prove this now if we wanted to, but the results in the next chapter will more or
less do it for us, and so we take this on faith temporarily.

§46.3 Number ﬁelds
Prototypical example for this section: Q(√2) is a typical number ﬁeld.

440

Napkin, by Evan Chen (v1.5.20190718)

Given any algebraic number α, we’re able to consider ﬁelds of the form Q(α). Let us

write down the more full version.

Deﬁnition 46.3.1. A number ﬁeld K is a ﬁeld containing Q as a subﬁeld which is a
ﬁnite-dimensional Q-vector space. The degree of K is its dimension.

Example 46.3.2 (Prototypical example)
Consider the ﬁeld

K = Q(√2) =(cid:110)a + b√2 | a, b ∈ Q(cid:111) .

This is a ﬁeld extension of Q, and has degree 2 (the basis being 1 and √2).
You might be confused that I wrote Q(√2) (which should permit denominators)
instead of Q[√2], say. But if you read through Example 5.5.4, you should see that the
7 (3 + √2) anyways, for example. You can

denominators don’t really matter:
either check this now in general, or just ignore the distinction and pretend I wrote square
brackets everywhere.

= 1

1

3−√2

Exercise 46.3.3 (Unimportant). Show that if α is an algebraic number, then Q(α) ∼= Q[α].

Example 46.3.4 (Adjoining an algebraic number)
Let α be the root of some irreducible polynomial P (x) in Q. The ﬁeld Q(α) is a
ﬁeld extension as well, and the basis is 1, α, α2, . . . , αm−1, where m is the degree of
α. In particular, the degree of Q(α) is just the degree of α.

Example 46.3.5 (Non-examples of number ﬁelds)
R and C are not number ﬁelds since there is no ﬁnite Q-basis of them.

§46.4 Primitive element theorem, and monogenic extensions
Prototypical example for this section: Q(√3,√5) ∼= Q(√3 + √5). Can you see why?

I’m only putting this theorem here because I was upset that no one told me it was true
(it’s a very natural conjecture), and I hope to not do the same to the reader. However,
I’m not going to use it in anything that follows.

Theorem 46.4.1 (Artin’s primitive element theorem)
Every number ﬁeld K is isomorphic to Q(α) for some algebraic number α.

The proof is left as Problem 52E, since to prove it I need to talk about ﬁeld extensions
ﬁrst.

The prototypical example

Q(√3,√5) ∼= Q(√3 + √5)

makes it clear why this theorem should not be too surprising.

46 Algebraic integers

441

§46.5 A few harder problems to think about
Problem 46A. Find a polynomial with integer coeﬃcients which has √2 + 3√3 as a
root.

Problem 46B (Brazil 2006). Let p be an irreducible polynomial in Q[x] and degree
larger than 1. Prove that if p has two roots r and s whose product is 1 then the degree
of p is even.

Problem 46C(cid:63). Consider n roots of unity ε1, . . . , εn. Assume the average 1
n (ε1 +···+εn)
is an algebraic integer. Prove that either the average is zero or ε1 = ··· = εn. (Used in
Lemma 22.2.2.)
Problem 46D†. Which rational numbers q satisfy cos(qπ) ∈ Q?
Problem 46E (MOP 2010). There are n > 2 lamps arranged in a circle; initially one is
on and the others are oﬀ. We may select any regular polygon whose vertices are among
the lamps and toggle the states of all the lamps simultaneously. Show it is impossible to
turn all lamps oﬀ.

Problem 46F (Kronecker’s theorem). Let α be an algebraic integer. Suppose all its
Galois conjugates have absolute value one. Prove that αN = 1 for some positive integer
N .

Problem 46G. Is there an algebraic integer with absolute value one which is not a root
of unity?

47 The ring of integers

§47.1 Norms and traces
Prototypical example for this section: a + b√2 as an element of Q(√2) has norm a2 − 2b2

and trace 2a.

Remember when you did olympiads and we had like a2 + b2 was the “norm” of a + bi?

Cool, let me tell you what’s actually happening.

First, let me make precise the notion of a conjugate.

Deﬁnition 47.1.1. Let α be an algebraic number, and let P (x) be its minimal polynomial,
of degree m. Then the m roots of P are the (Galois) conjugates of α.

It’s worth showing at the moment that there are no repeated conjugates.

Lemma 47.1.2 (Irreducible polynomials have distinct roots)
An irreducible polynomial in Q[x] cannot have a complex double root.

Proof. Let f (x) ∈ Q[x] be the irreducible polynomial and assume it has a double root α.
Take the derivative f(cid:48)(x). This derivative has three interesting properties.

 The degree of f(cid:48) is one less than the degree of f .

 The polynomials f and f(cid:48) are not relatively prime because they share a factor

x − α.

 The coeﬃcients of f(cid:48) are also in Q.

Consider g = gcd(f, f(cid:48)). We must have g ∈ Q[x] by Euclidean algorithm. But the ﬁrst
two facts about f(cid:48) ensure that g is nonconstant and deg g < deg f . Yet g divides f ,
contradiction to the fact that f should be a minimal polynomial.

Hence α has exactly as many conjugates as the degree of α.

Now, we would like to deﬁne the norm of an element N(α) as the product of its
conjugates. For example, we want 2 + i to have norm (2 + i)(2 − i) = 5, and in general
for a + bi to have norm a2 + b2. It would be really cool if the norm was multiplicative;
we already know this is true for complex numbers!
Unfortunately, this doesn’t quite work: consider

N(2 + i) = 5 and N(2 − i) = 5.

But (2 + i)(2 − i) = 5, which doesn’t have norm 25 like we want, since 5 is degree 1 and
has no conjugates at all. The reason this “bad” thing is happening is that we’re trying
to deﬁne the norm of an element, when we really ought to be deﬁning the norm of an
element with respect to a particular K.

What I’m driving at is that the norm should have diﬀerent meanings depending on
which ﬁeld you’re in. If we think of 5 as an element of Q, then its norm is 5. But thought
of as an element of Q(i), its norm really ought to be 25. Let’s make this happen: for K
a number ﬁeld, we will now deﬁne NK/Q(α) to be the norm of α with respect to K as
follows.

443

444

Napkin, by Evan Chen (v1.5.20190718)

Deﬁnition 47.1.3. Let α ∈ K have degree n, so Q(α) ⊆ K, and set k = (deg K)/n.
The norm of α is deﬁned as

The trace is deﬁned as

NK/Q(α) :=(cid:16)(cid:89) Galois conj of α(cid:17)k
TrK/Q(α) := k ·(cid:16)(cid:88) Galois conj of α(cid:17) .

.

The exponent of k is a “correction factor” that makes the norm of 5 into 52 = 25 when
we view 5 as an element of Q(i) rather than an element of Q. For a “generic” element of
K, we expect k = 1.

Exercise 47.1.4. Use what you know about nested vector spaces to convince yourself that
k is actually an integer.

Example 47.1.5 (Norm of a + b√2)
Let α = a + b√2 ∈ Q(√2). If b (cid:54)= 0, then α and K have the degree 2. Thus the only
conjugates of α are a ± b√2, which gives the norm

(a + b√2)(a − b√2) = a2 − 2b2,

The trace is (a − b√2) + (a + b√2) = 2a.
Nicely, the formula a2 − 2b2 and 2a also works when b = 0.

Of importance is:

Proposition 47.1.6 (Norms and traces are rational integers)

If α is an algebraic integer, its norm and trace are rational integers.

Question 47.1.7. Prove it. (Vieta formula.)

That’s great, but it leaves a question unanswered: why is the norm multiplicative? To

do this, I have to give a new deﬁnition of norm and trace.

Theorem 47.1.8 (Morally correct deﬁnition of norm and trace)
Let K be a number ﬁeld of degree n, and let α ∈ K. Let µα : K → K denote the
map

viewed as a linear map of Q-vector spaces. Then,

x (cid:55)→ αx

 the norm of α equals the determinant det µα, and

 the trace of α equals the trace Tr µα.

Since the trace and determinant don’t depend on the choice of basis, you can pick
whatever basis you want and use whatever deﬁnition you got in high school. Fantastic,
right?

47 The ring of integers

445

Example 47.1.9 (Explicit computation of matrices for a + b√2)
Let K = Q(√2), and let 1, √2 be the basis of K. Let

α = a + b√2

(possibly even b = 0), and notice that

(cid:16)a + b√2(cid:17)(cid:16)x + y√2(cid:17) = (ax + 2yb) + (bx + ay)√2.

We can rewrite this in matrix form as

Consequently, we can interpret µα as the matrix

b

y(cid:21) =(cid:20)ax + 2yb
(cid:20)a 2b
a(cid:21)(cid:20)x
bx + ay(cid:21) .
µα =(cid:20)a 2b
a(cid:21) .

b

Of course, the matrix will change if we pick a diﬀerent basis, but the determinant
and trace do not: they are always given by

det µα = a2 − 2b2 and Tr µα = 2a.

This interpretation explains why the same formula should work for a + b√2 even in the
case b = 0.

Proof. I’ll prove the result for just the norm; the trace falls out similarly. Set

n = deg α,

kn = deg K.

The proof is split into two parts, depending on whether or not k = 1.

Proof if k = 1. Set n = deg α = deg K. Thus the norm actually is the product of the
Galois conjugates. Also,

{1, α, . . . , αn−1}

is linearly independent in K, and hence a basis (as dim K = n). Let’s use this as the
basis for µα.

Let

xn + cn−1xn−1 + ··· + c0

be the minimal polynomial of α. Thus µα(1) = α, µα(α) = α2, and so on, but µα(αn−1) =
−cn−1αn−1 − ··· − c0. Therefore, µα is given by the matrix

M =

Thus

0 0 0 . . .
1 0 0 . . .
0 1 0 . . .
...
. . .
0 0 0 . . .

...

...



0 −c0
0 −c1
0 −c2
0 −cn−2
1 −cn−1

.



and we’re done by Vieta’s formulas.

det M = (−1)nc0

(cid:4)

446

Napkin, by Evan Chen (v1.5.20190718)

Proof if k > 1. We have nested vector spaces

Q ⊆ Q(α) ⊆ K.

Let e1, . . . , ek be a Q(α)-basis for K (meaning: interpret K as a vector space over Q(α),
and pick that basis). Since {1, α, . . . , αn−1} is a Q basis for Q(α), the elements

e1,
e1α,
e2,
e2α,
...
...
ek, ekα,

. . . , e1αn−1
. . . , e2αn−1
. . .
. . . , ekαn−1

...

constitute a Q-basis of K. Using this basis, the map µα looks like

M


(cid:124)

M

. . .

k times

(cid:123)(cid:122)


(cid:125)

M

where M is the same matrix as above: we just end up with one copy of our old matrix
(cid:4)
for each ei. Thus det µα = (det M )k, as needed.

Question 47.1.10. Verify the result for traces as well.

From this it follows immediately that

NK/Q(αβ) = NK/Q(α) NK/Q(β)

because by deﬁnition we have

µαβ = µα ◦ µβ,

and that the determinant is multiplicative. In the same way, the trace is additive.

§47.2 The ring of integers
Prototypical example for this section: If K = Q(√2), then OK = Z[√2]. But if K =
Q(√5), then OK = Z[ 1+√5
Z makes for better number theory than Q. In the same way, focusing on the algebraic

].

2

integers of K gives us some really nice structure, and we’ll do that here.

Deﬁnition 47.2.1. Given a number ﬁeld K, we deﬁne

OK := K ∩ Z

to be the ring of integers of K; in other words OK consists of the algebraic integers of
K.

We do the classical example of a quadratic ﬁeld now. Before proceeding, I need to

write a silly number theory fact.

47 The ring of integers

447

Exercise 47.2.2 (Annoying but straightforward). Let a and b be rational numbers, and d
a squarefree positive integer.

 If d ≡ 2, 3 (mod 4), prove that 2a, a2 − db2 ∈ Z if and only if a, b ∈ Z.
 For d ≡ 1 (mod 4), prove that 2a, a2 − db2 ∈ Z if and only if a, b ∈ Z OR if

a − 1

2 , b − 1

2 ∈ Z.

You’ll need to take mod 4.

Example 47.2.3 (Ring of integers of K = Q(√3))

Let K be as above. We claim that

OK = Z[√3] =(cid:110)m + n√3 | m, n ∈ Z(cid:111) .

We set α = a + b√3. Then α ∈ OK when the minimal polynomial has integer
If b = 0, then the minimal polynomial is x − α = x − a, and thus α works if and

coeﬃcients.

only if it’s an integer. If b (cid:54)= 0, then the minimal polynomial is
(x − a)2 − 3b2 = x2 − 2a · x + (a2 − 3b2).

From the exercise, this occurs exactly for a, b ∈ Z.

Example 47.2.4 (Ring of integers of K = Q(√5))

We claim that in this case

OK = Z(cid:34) 1 + √5

2

(cid:35) =(cid:40)m + n ·

1 + √5

2

| m, n ∈ Z(cid:41) .

The proof is exactly the same, except the exercise tells us instead that for b (cid:54)= 0, we
have both the possibility that a, b ∈ Z or that a, b ∈ Z − 1
2 . This reﬂects the fact
that 1+√5

is the root of x2 − x − 1 = 0; no such thing is possible with √3.

2

In general, the ring of integers of K = Q(√d) is

OK =
Z[√d]
Z(cid:104) 1+√d

d ≡ 2, 3

(mod 4)

(mod 4).

2 (cid:105) d ≡ 1

What we’re going to show is that OK behaves in K a lot like the integers do in Q. First we
show K consists of quotients of numbers in OK. In fact, we can do better:

448

Napkin, by Evan Chen (v1.5.20190718)

Example 47.2.5 (Rationalizing the denominator)
For example, consider K = Q(√3). The number x = 1
4+√3

by “rationalizing the denominator” we can write

is an element of K, but

1

4 + √3

=

4 − √3

13

.

So we see that in fact, x is 1

13 of an integer in OK.

The theorem holds true more generally.

Theorem 47.2.6 (K = Q · OK)
Let K be a number ﬁeld, and let x ∈ K be any element. Then there exists an integer
n such that nx ∈ OK; in other words,

x =

1
n

α

for some α ∈ OK.

Exercise 47.2.7. Prove this yourself.
polynomial with integer coeﬃcients. Alternatively, take the norm.)

(Start by using the fact that x has a minimal

Now we are going to show OK is a ring; we’ll check it is closed under addition and

multiplication. To do so, the easiest route is:

Lemma 47.2.8 (α ∈ Z ⇐⇒ Z[α] ﬁnitely generated)
Let α ∈ Q. Then α is an algebraic integer if and only if the abelian group Z[α] is
ﬁnitely generated.

Proof. Note that α is an algebraic integer if and only if it’s the root of some nonzero,
monic polynomial with integer coeﬃcients. Suppose ﬁrst that

αN = cN−1αN−1 + cN−2αN−2 + ··· + c0.

Then the set 1, α, . . . , αN−1 generates Z[α], since we can repeatedly replace αN until all
powers of α are less than N .

Conversely, suppose that Z[α] is ﬁnitely generated by some b1, . . . , bm. Viewing the bi
as polynomials in α, we can select a large integer N (say N = deg b1 +··· + deg bm + 2015)
and express αN in the bi’s to get

αN = c1b1(α) + ··· + cmbm(α).

The above gives us a monic polynomial in α, and the choice of N guarantees it is not
zero. So α is an algebraic integer.

47 The ring of integers

449

Example 47.2.9 ( 1
We already know 1

2 isn’t an algebraic integer)

2 isn’t an algebraic integer. So we expect

Z(cid:20) 1
2(cid:21) =(cid:110) a

2m | a, m ∈ Z and m ≥ 0(cid:111)

to not be ﬁnitely generated, and this is the case.

Question 47.2.10. To make the last example concrete: name all the elements of Z[ 1
cannot be written as an integer combination of

2 ] that

(cid:26) 1

2

,

7
8

,

13
64

,

2015
4096

,

1

1048576(cid:27)

Now we can state the theorem.

Theorem 47.2.11 (Algebraic integers are closed under + and ×)
The set Z is closed under addition and multiplication; i.e. it is a ring. In particular,
OK is also a ring for any number ﬁeld K.

Proof. Let α, β ∈ Z. Then Z[α] and Z[β] are ﬁnitely generated. Hence so is Z[α, β].
(Details: if Z[α] has Z-basis a1, . . . , am and Z[β] has Z-basis b1, . . . , bn, then take the mn
elements aibj.)

Now Z[α ± β] and Z[αβ] are subsets of Z[α, β] and so they are also ﬁnitely generated.
Hence α ± β and αβ are algebraic integers.
In fact, something even better is true. As you saw, for Q(√3) we had OK = Z[√3];
in other words, OK was generated by 1 and √3. Something similar was true for Q(√5).

We claim that in fact, the general picture looks exactly like this.

Theorem 47.2.12 (OK is a free Z-module of rank n)
Let K be a number ﬁeld of degree n. Then OK is a free Z-module of rank n, i.e.
OK ∼= Z⊕n as an abelian group. In other words, OK has a Z-basis of n elements as

OK = {c1α1 + ··· + cn−1αn−1 + cnαn | ci ∈ Z}

where αi are algebraic integers in OK.

Proof. This is a kind of fun proof, so it may be worth trying to work out yourself before
reading it.

Pick a Q-basis of α1, . . . , αn of K and WLOG the αi are in OK by scaling.
Consider α ∈ OK, and write α = c1α1 + ··· + cnαn. We will try to bound the
denominators of ci. Look at N(α) = N(c1α1 + ··· + cnαn).
If we do a giant norm computation, we ﬁnd that N(α) is a polynomial in the ci with
ﬁxed coeﬃcients. (For example, N(c1 + c2√2) = c2
1 − 2c2
2, say.) But N(α) is an integer,
so the denominators of the ci have to be bounded by some very large integer N . Thus
N (cid:77)i

Z · αi ⊆ OK ⊆

(cid:77)i

1

Z · αi.

450

Napkin, by Evan Chen (v1.5.20190718)

The latter inclusion shows that OK is a subgroup of a free group, and hence it is itself
free. On the other hand, the ﬁrst inclusion shows it’s rank n.

This last theorem shows that in many ways OK is a “lattice” in K. That is, for a

number ﬁeld K we can ﬁnd α1, . . . , αn in OK such that
OK ∼= α1Z ⊕ α2Z ⊕ ··· ⊕ αnZ
K ∼= α1Q ⊕ α2Q ⊕ ··· ⊕ αnQ

as abelian groups.

§47.3 On monogenic extensions

Recall that it turned out number ﬁelds K could all be expressed as Q(α) for some α. We
might hope that something similar is true of the ring of integers: that we can write

OK = Z[θ]

in which case {1, θ, . . . , θn−1} serves both as a basis of K and as the Z-basis for OK (here
n = [K : Q]). In other words, we hope that the basis of OK is actually a “power basis”.

This is true for the most common examples we use:

 the quadratic ﬁeld, and

 the cyclotomic ﬁeld in Problem 47E†.

Unfortunately, it is not true in general: the ﬁrst counterexample is Q(α) for α a root of
X 3 − X 2 − 2X − 8.
extensions have a really nice factoring algorithm, Theorem 48.5.4.

We call an extension with this nice property monogenic. As we’ll later see, monogenic

§47.4 A few harder problems to think about

Problem 47A(cid:63). Show that α is a unit of OK (meaning α−1 ∈ OK) if and only if
NK/Q(α) = ±1.
Problem 47B(cid:63). Let K be a number ﬁeld. What is the ﬁeld of fractions of OK?
Problem 47C (Russian olympiad 1984). Find all integers m and n such that

(cid:16)5 + 3√2(cid:17)m

=(cid:16)3 + 5√2(cid:17)n

.

Problem 47D (USA TST 2012). Decide whether there exist a, b, c > 2010 satisfying

a3 + 2b3 + 4c3 = 6abc + 1.

Problem 47E† (Cyclotomic Field). Let p be an odd rational prime and ζp a primitive
pth root of unity. Let K = Q(ζp). Prove that OK = Z[ζp]. (In fact, the result is true
even if p is not a prime.)

48 Unique factorization (ﬁnally!)

Took long enough.

§48.1 Motivation

Suppose we’re interested in solutions to the Diophantine equation n = x2 + 5y2 for a

given n. The idea is to try and “factor” n in Z[√−5], for example

6 = (1 + √−5)(1 − √−5).

Unfortunately, this is not so simple, because as I’ve said before we don’t have unique
factorization of elements:

6 = 2 · 3 =(cid:0)1 + √−5(cid:1)(cid:0)1 − √−5(cid:1) .

One reason this doesn’t work is that we don’t have a notion of a greatest common divisor.

We can write (35, 77) = 7, but what do we make of (3, 1 + √−5)?
ideal {ax + by | x, y ∈ Z[√−5]}. You can see that (35, 77) = (7), but (3, 1 + √−5) will

The trick is to use ideals as a “generalized GCD”. Recall that by (a, b) I mean the

be left “unsimpliﬁed” because it doesn’t represent an actual value in the ring. Using
these sets (ideals) as elements, it turns out that we can develop a full theory of prime
factorization, and we do so in this chapter.

In other words, we use the ideal (a1, . . . , am) to interpret a “generalized GCD” of a1,
. . . , am. In particular, if we have a number x we want to represent, we encode it as just
(x).

Going back to our example of 6,

(6) = (2) · (3) =(cid:0)1 + √−5(cid:1) ·(cid:0)1 − √−5(cid:1) .

Please take my word for it that in fact, the complete prime factorization of (6) into prime
ideals is

(6) = (2, 1 − √−5)2(3, 1 + √−5)(3, 1 − √−5) = p2q1q2.

In fact, (2) = p2, (3) = q1q2, (1 + √−5) = pq1, (1 − √−5) = pq2. So 6 indeed factorizes

uniquely into ideals, even though it doesn’t factor into elements.

As one can see above, ideal factorization is more reﬁned than element factorization.
Once you have the factorization into ideals, you can from there recover all the factorizations
into elements. The upshot of this is that if we want to write n as x2 + 5y2, we just have
to factor n into ideals, and from there we can recover all factorizations into elements,
and ﬁnally all ways to write n as x2 + 5y2. Since we can already break n into rational
prime factors (for example 6 = 2 · 3 above) we just have to ﬁgure out how each rational
prime p | n breaks down. There’s a recipe for this, Theorem 48.5.4! In fact, I’ll even tell
you what is says in this special case:
 If t2 + 5 factors as (t + c)(t − c) (mod p), then (p) = (p, c + √−5)(p, c − √−5).

 Otherwise, (p) is a prime ideal.

In this chapter we’ll develop this theory of unique factorization in full generality.

451

452

Napkin, by Evan Chen (v1.5.20190718)

Remark 48.1.1 — In this chapter, I’ll be using the letters a, b, p, q for ideals of
OK. When fractional ideals arise, I’ll use I and J for them.

§48.2 Ideal arithmetic

Prototypical example for this section: (x)(y) = (xy). In any case, think in terms of
generators.

First, I have to tell you how to add and multiply two ideals a and b.

Deﬁnition 48.2.1. Given two ideals a and b of a ring R, we deﬁne

a + b := {a + b | a ∈ a, b ∈ b}
a · b := {a1b1 + ··· + anbn | ai ∈ a, bi ∈ b} .

(Note that inﬁnite sums don’t make sense in general rings, which is why in a · b we cut
oﬀ the sum after some ﬁnite number of terms.) You can readily check these are actually
ideals. This deﬁnition is more natural if you think about it in terms of the generators of
a and b.

Proposition 48.2.2 (Ideal arithmetic via generators)

Suppose a = (a1, a2, . . . , an) and b = (b1, . . . , bm) are ideals in a ring R. Then

(a) a + b is the ideal generated by a1, . . . , an, b1, . . . , bm.

(b) a · b is the ideal generated by aibj, for 1 ≤ i ≤ n and 1 ≤ j ≤ m.

Proof. Pretty straightforward; just convince yourself that this result is correct.

In other words, for sums you append the two sets of generators together, and for products
you take products of the generators. Note that for principal ideals, this coincides with
“normal” multiplication, for example

in Z.

(3) · (5) = (15)

Remark 48.2.3 — Note that for an ideal a and an element c, the set

ca = {ca | a ∈ a}

is equal to (c) · a. So “scaling” and “multiplying by principal ideals” are the same
thing. This is important, since we’ll be using the two notions interchangably.

Finally, since we want to do factorization we better have some notion of divisibility.

So we deﬁne:
Deﬁnition 48.2.4. We say a divides b and write a | b if a ⊇ b.

Note the reversal of inclusions! So (3) divides (15), because (15) is contained in (3);

every multiple of 15 is a multiple of 3.

Finally, the prime ideals are deﬁned as in Deﬁnition 5.3.1: p is prime if xy ∈ p implies

x ∈ p or y ∈ p. This is compatible with the deﬁnition of divisibility:

48 Unique factorization (ﬁnally!)

453

Exercise 48.2.5. A nonzero proper ideal p is prime if and only if whenever p divides ab, p
divides one of a or b.

As mentioned in Remark 5.3.3, this also lets us ignore multiplication by units: (−3) = (3).

§48.3 Dedekind domains

Prototypical example for this section: Any OK is a Dedekind domain.

We now deﬁne a Dedekind domain as follows.

Deﬁnition 48.3.1. An integral domain A is a Dedekind domain if it is Noetherian,
integrally closed, and every nonzero prime ideal of A is in fact maximal. (The last
condition is the important one.)

Here there’s one new word I have to deﬁne for you, but we won’t make much use of it.

Deﬁnition 48.3.2. Let R be an integral domain and let K be its ﬁeld of fractions.
We say R is integrally closed if the only elements a ∈ K which are roots of monic
polynomials in R are the elements of R (which are roots of the trivial x − r polynomial).
The interesting condition in the deﬁnition of a Dedekind domain is the last one: prime
ideals and maximal ideals are the same thing. The other conditions are just technicalities,
but “primes are maximal” has real substance.

Example 48.3.3 (Z is a Dedekind domain)
The ring Z is a Dedekind domain. Note that

 Z is Noetherian (for obvious reasons).
 Z has ﬁeld of fractions Q. If f (x) ∈ Z[x] is monic, then by the rational root
theorem any rational roots are integers (this is the same as the proof that
Z ∩ Q = Z). Hence Z is integrally closed.

 The nonzero prime ideals of Z are (p), which also happen to be maximal.

The case of interest is a ring OK in which we wish to do factorizing. We’re now going
to show that for any number ﬁeld K, the ring OK is a Dedekind domain. First, the
boring part.

Proposition 48.3.4 (OK integrally closed and Noetherian)
For any number ﬁeld K, the ring OK is integrally closed and Noetherian.

Proof. Boring, but here it is anyways for completeness.
Since OK ∼= Z⊕n, we get that it’s Noetherian.
Now we show that OK is integrally closed. Suppose that η ∈ K is the root of some
polynomial with coeﬃcients in OK. Thus

ηn = αn−1 · ηn−1 + αn−2 · ηn−2 + ··· + α0

where αi ∈ OK. We want to show that η ∈ OK as well.
generated. So η ∈ Z, and hence η ∈ K ∩ Z = OK.

Well, from the above, OK[η] is ﬁnitely generated. . . thus Z[η] ⊆ OK[η] is ﬁnitely

454

Napkin, by Evan Chen (v1.5.20190718)

Now let’s do the fun part. We’ll prove a stronger result, which will re-appear repeat-
edly.

Theorem 48.3.5 (Important: prime ideals divide rational primes)
Let OK be a ring of integers and p a nonzero prime ideal inside it. Then p contains
a rational prime p. Moreover, p is maximal.

Proof. Take any α (cid:54)= 0 in p. Its Galois conjugates are algebraic integers so their product
N(α)/α is in OK (even though each individual conjugate need not be in K). Consequently,
N(α) ∈ p, and we conclude p contains some integer.
Then take the smallest positive integer in p, say p. We must have that p is a rational
prime, since otherwise p (cid:51) p = xy implies one of x, y ∈ p. This shows the ﬁrst part.
We now do something pretty tricky to show p is maximal. Look at OK/p; since p is
prime it’s supposed to be an integral domain. . . but we claim that it’s actually ﬁnite! To
do this, we forget that we can multiply on OK. Recalling that OK ∼= Z⊕n as an abelian

group, we obtain a map

Fp⊕n ∼= OK/(p) (cid:16) OK/p.

Hence |OK/p| ≤ pn is ﬁnite. Since ﬁnite integral domains are ﬁelds (Problem 5D(cid:63)) we
are done.

Since every nonzero prime p is maximal, we now know that OK is a Dedekind domain.
Note that this tricky proof is essentially inspired by the solution to Problem 5G.

§48.4 Unique factorization works

Okay, I’ll just say it now!

Unique factorization works perfectly in Dedekind domains!

Theorem 48.4.1 (Prime factorization works)
Let a be a nonzero proper ideal of a Dedekind domain A. Then a can be written as
a ﬁnite product of nonzero prime ideals pi, say

a = pe1

g

1 pe2

2 . . . peg

and this factorization is unique up to the order of the pi.

Moreover, a divides b if and only if for every prime ideal p, the exponent of p in a

is less than the corresponding exponent in b.

I won’t write out the proof, but I’ll describe the basic method of attack. Section 3
of [Ul08] does a nice job of explaining it. When we proved the fundamental theorem of
arithmetic, the basic plot was:
(1) Show that if p is a rational prime1 then p | bc means p | b or p | c. (This is called

Euclid’s Lemma.)

1 Note that the kindergarten deﬁnition of a prime is that “p isn’t the product of two smaller integers”.
This isn’t the correct deﬁnition of a prime: the deﬁnition of a prime is that p | bc means p | b or
p | c. The kindergarten deﬁnition is something called “irreducible”. Fortunately, in Z, primes and
irreducibles are the same thing, so no one ever told you that your deﬁnition of “prime” was wrong.

48 Unique factorization (ﬁnally!)

455

(2) Use strong induction to show that every N > 1 can be written as the product of

primes (easy).

(3) Show that if p1 . . . pm = q1 . . . qn for some primes (not necessarily unique), then

p1 = qi for some i, say q1.

(4) Divide both sides by p1 and use induction.

What happens if we try to repeat the proof here? We get step 1 for free, because we’re
using a better deﬁnition of “prime”. We can also do step 3, since it follows from step 1.
But step 2 doesn’t work, because for abstract Dedekind domains we don’t really have a
notion of size. And step 4 doesn’t work because we don’t yet have a notion of what the
inverse of a prime ideal is.

Well, it turns out that we can deﬁne the inverse a−1 of an ideal, and I’ll do so by the
end of this chapter. You then need to check that a · a−1 = (1) = A. In fact, even this
isn’t easy. You have to check it’s true for prime ideals p, then prove prime factorization,
and then prove that this is true. Moreover, a−1 is not actually an ideal, so you need to
work in the ﬁeld of fractions K instead of A.

So the main steps in the new situation are as follows:

(1) First, show that every ideal a divides p1 . . . pg for some ﬁnite collection of primes.

(This is an application of Zorn’s Lemma.)

(2) Deﬁne p−1 and show that pp−1 = (1).

(3) Show that a factorization exists (again using Zorn’s Lemma).

(4) Show that it’s unique, using the new inverse we’ve deﬁned.

Finally, let me comment on how nice this is if A is a PID (like Z). Thus every element
a ∈ A is in direct correspondence with an ideal (a). Now suppose (a) factors as a product
of ideals pi = (pi), say,

(a) = (p1)e1(p2)e2 . . . (pn)en.

This verbatim reads

a = upe1

1 pe2

2 . . . pen
n

where u is some unit (recall Deﬁnition 5.1.1). Hence, Dedekind domains which are PID’s
satisfy unique factorization for elements, just like in Z. (In fact, the converse of this is
true.)

§48.5 The factoring algorithm
Let’s look at some examples from quadratic ﬁelds. Recall that if K = Q(√d), then

OK =(cid:40)Z[√d]
Z(cid:104) 1+√d

2 (cid:105) d ≡ 1

Also, recall that the norm of a + b√−d is given by a2 + db2.

d ≡ 2, 3

(mod 4)

(mod 4).

456

Napkin, by Evan Chen (v1.5.20190718)

Example 48.5.1 (Factoring 6 in the integers of Q(√−5))
Let OK = Z[√−5] arise from K = Q(√−5). We’ve already seen that
(6) = (2) · (3) =(cid:0)1 + √−5(cid:1)(cid:0)1 − √−5(cid:1)

and you can’t get any further with these principal ideals. But let

p =(cid:0)1 + √−5, 2(cid:1) =(cid:0)1 − √−5, 2(cid:1)
(1 + √−5) = pq1 and (1 − √−5) = pq2. (Proof in just a moment.)

and q1 = (1 + √−5, 3), q2 = (1 − √−5, 3).

Then it turns out (6) = p2q1q2. More speciﬁcally, (2) = p2, (3) = q1q2, and

I want to stress that all our ideals are computed relative to OK. So for example,

(2) = {2x | x ∈ OK} .

How do we know in this example that p is prime/maximal? (Again, these are the same
since we’re in a Dedekind domain.) Answer: look at OK/p and see if it’s a ﬁeld. There
is a trick to this: we can express

OK = Z[√−5] ∼= Z[x]/(x2 + 5).

So when we take that mod p, we get that

OK/p = Z[x]/(x2 + 5, 2, 1 + x) ∼= F2[x]/(x2 + 5, x + 1)

as rings.

Question 48.5.2. Conclude that OK/p ∼= F2, and satisfy yourself that q1 and q2 are also

maximal.

I should give an explicit example of an ideal multiplication: let’s compute

q1q2 =(cid:0)(1 + √−5)(1 − √−5), 3(1 + √−5), 3(1 − √−5), 9(cid:1)

=(cid:0)6, 3 + 3√−5, 3 − 3√−5, 9(cid:1)
=(cid:0)6, 3 + 3√−5, 3 − 3√−5, 3(cid:1)

= (3)

where we ﬁrst did 9 − 6 = 3 (think Euclidean algorithm!), then noted that all the other
generators don’t contribute anything we don’t already have with the 3 (again these are
ideals computed in OK). You can do the computation for p2, pq1, pq2 in the same way.
Finally, it’s worth pointing out that we should quickly verify that p (cid:54)= (x) for some
x; in other words, that p is not principal. Assume for contradiction that it is. Then x
divides both 1 + √−5 and 2, in the sense that 1 + √−5 = α1x and 2 = α2x for some
α1, α2 ∈ OK. (Principal ideals are exactly the “multiples” of x, so (x) = xOK.) Taking
the norms, we ﬁnd that NK/Q(x) divides both

NK/Q(1 + √−5) = 6

and NK/Q(2) = 4.

Since p (cid:54)= (1), x cannot be a unit, so its norm must be 2. But there are no elements of
norm 2 = a2 + 5b2 in OK.

48 Unique factorization (ﬁnally!)

457

Example 48.5.3 (Factoring 3 in the integers of Q(√−17))
Let OK = Z[√−17] arise from K = Q(√−17). We know OK ∼= Z[x]/(x2 + 17). Now

OK/3OK ∼= Z[x]/(3, x2 + 17) ∼= F3[x]/(x2 − 1).

This already shows that (3) cannot be a prime (i.e. maximal) ideal, since otherwise
our result should be a ﬁeld. Anyways, we have a projection

OK (cid:16) F3[x]/ ((x − 1)(x + 1)) .

Let q1 be the pre-image (x − 1) in the image, that is,
q1 = (3,√−17 − 1).
q2 = (3,√−17 + 1).

Similarly,

We have OK/q1 ∼= F3, so q1 is maximal (prime). Similarly q2 is prime. Magically,

you can check explicitly that

Hence this is the factorization of (3) into prime ideals.

q1q2 = (3).

The fact that q1q2 = (3) looks magical, but it’s really true:

q1q2 = (3,√−17 − 1)(3,√−17 + 1)
= (9, 3√−17 + 3, 3√−17 − 3, 18)
= (9, 3√−17 + 3, 6)
= (3, 3√−17 + 3, 6)

= (3).

In fact, it turns out this always works in general: given a rational prime p, there is an
algorithm to factor p in any OK of the form Z[θ].

Theorem 48.5.4 (Factoring algorithm / Dedekind-Kummer theorem)
Let K be a number ﬁeld. Let θ ∈ OK with [OK : Z[θ]] = j < ∞, and let p be a
prime not dividing j. Then (p) = pOK is factored as follows:

Let f be the minimal polynomial of θ and factor f mod p as

f ≡

g(cid:89)i=1

(f i)ei

(mod p).

Then pi = (fi(θ), p) is prime for each i and the factorization of (p) is

OK ⊇ (p) =

pei
i .

g(cid:89)i=1

In particular, if K is monogenic with OK = Z[θ] then j = 1 and the theorem applies
for all primes p.

458

Napkin, by Evan Chen (v1.5.20190718)

In almost all our applications in this book, K will be monogenic; i.e. j = 1. Here ψ
denotes the image in Fp[x] of a polynomial ψ ∈ Z[x].

Question 48.5.5. There are many possible pre-images fi we could have chosen (for example
if fi = x2 + 1 (mod 3), we could pick fi = x2 + 3x + 7.) Why does this not aﬀect the value
of pi?

Note that earlier, we could check the factorization worked for any particular case. The
proof that this works is much the same, but we need one extra tool, the ideal norm. After
that we leave the proposition as Problem 48E.

This algorithm gives us a concrete way to compute prime factorizations of (p) in any

monogenic number ﬁeld with OK = Z[θ]. To summarize the recipe:

1. Find the minimal polynomial of θ, say f ∈ Z[x].
2. Factor f mod p into irreducible polynomials f 1

e1f 2

e2 . . . f g

eg .

3. Compute pi = (fi(θ), p) for each i.

Then your (p) = pe1

1 . . . peg
g .

Exercise 48.5.6. Factor (29) in Q(i) using the above algorithm.

§48.6 Fractional ideals

Prototypical example for this section: Analog to Q for Z, allowing us to take inverses of
ideals. Prime factorization works in the nicest way possible.

We now have a neat theory of factoring ideals of A, just like factoring the integers.
Now note that our factorization of Z naturally gives a way to factor elements of Q; just
factor the numerator and denominator separately.

Let’s make the analogy clearer. The analogue of a rational number is as follows.

Deﬁnition 48.6.1. Let A be a Dedekind domain with ﬁeld of fractions K. A fractional
ideal J of K is a set of the form

J =

1
x · a where x ∈ A, and a is an integral ideal.

For emphasis, ideals of A will be sometimes referred to as integral ideals.

You might be a little surprised by this deﬁnition: one would expect that a fractional
ideal should be of the form a
b for some integral ideals a, b. But in fact, it suﬃces to just
take x ∈ A in the denominator. The analogy is that when we looked at OK, we found
13 (4 + √3). Similarly here, it will
that we only needed integer denominators:
turn out that we only need to look at 1
b , and so we deﬁne it this way
from the beginning. See Problem 48D† for a diﬀerent equivalent deﬁnition.

x · a rather than a

4−√3

1

= 1

Example 48.6.2 ( 5
The set

2Z is a fractional ideal)

5
2

Z =(cid:26) 5

2

n | n ∈ Z(cid:27) =

1
2

(5)

is a fractional ideal of Z.

Now, as we prescribed, the fractional ideals form a multiplicative group:

48 Unique factorization (ﬁnally!)

459

Theorem 48.6.3 (Fractional ideals form a group)
Let A be a Dedekind domain and K its ﬁeld of fractions. For any integral ideal a,
the set

is a fractional ideal with aa−1 = (1).

a−1 = {x ∈ K | xa ⊆ (1) = A}

Deﬁnition 48.6.4. Thus nonzero fractional ideals of K form a group under multiplication
with identity (1) = A. This ideal group is denoted JK.

Example 48.6.5 ((3)−1 in Z)
Please check that in Z we have

(3)−1 =(cid:26) 1

3

n | n ∈ Z(cid:27) =

1
3

Z.

It follows that every fractional ideal J can be uniquely written as

J =(cid:89)i

pni
i

·(cid:89) q−mi

i

where ni and mi are positive integers. In fact, a is an integral ideal if and only if all its
exponents are nonnegative, just like the case with integers. So, a perhaps better way
to think about fractional ideals is as products of prime ideals, possibly with negative
exponents.

§48.7 The ideal norm

One last tool is the ideal norm, which gives us a notion of the “size” of an ideal.

Deﬁnition 48.7.1. The ideal norm (or absolute norm) of a nonzero ideal a ⊆ OK is
deﬁned as |OK/a| and denoted N(a).

Example 48.7.2 (Ideal norm of (5) in the Gaussian integers)
Let K = Q(i), OK = Z[i]. Consider the ideal (5) in OK. We have that

OK/(5) ∼= {a + bi | a, b ∈ Z/5Z}

so (5) has ideal norm 25, corresponding to the fact that OK/(5) has 52 = 25 elements.

Example 48.7.3 (Ideal norm of (2 + i) in the Gaussian integers)
You’ll notice that

OK/(2 + i) ∼= F5

since mod 2 + i we have both 5 ≡ 0 and i ≡ −2. (Indeed, since (2 + i) is prime we
had better get a ﬁeld!) Thus N ((2 + i)) = 5; similarly N ((2 − i)) = 5.

460

Napkin, by Evan Chen (v1.5.20190718)

Thus the ideal norm measures how “roomy” the ideal is: that is, (5) is a lot more
spaced out in Z[i] than it is in Z. (This intuition will be important when we will actually
view OK as a lattice.)

Question 48.7.4. What are the ideals with ideal norm one?

Our example with (5) suggests several properties of the ideal norm which turn out to

be true:

Lemma 48.7.5 (Properties of the absolute norm)
Let a be a nonzero ideal of OK.
(a) N(a) is ﬁnite.

(b) For any other nonzero ideal b, N(ab) = N(a) N(b).

(c) If a = (a) is principal, then N(a) = NK/Q(a).

I unfortunately won’t prove these properties, though we already did (a) in our proof that
OK was a Dedekind domain.
ideal J by the natural extension

The fact that N is completely multiplicative lets us also consider the norm of a fractional

J =(cid:89)i

pni
i

q−mi
i

·(cid:89)i

=⇒ N(J) := (cid:81)i N(pi)ni
(cid:81)i N(qi)mi

.

Thus N is a natural group homomorphism JK → Q \ {0}.

§48.8 A few harder problems to think about

Problem 48A. Show that there are three diﬀerent factorizations of 77 in OK, where
K = Q(√−13).
Problem 48B. Let K = Q( 3√2); take for granted that OK = Z[ 3√2]. Find the factoriza-
tion of (5) in OK.
Problem 48C (Fermat’s little theorem). Let p be a prime ideal in some ring of integers
OK. Show that for α ∈ OK,

αN(p) ≡ α (mod p).

Problem 48D†. Let A be a Dedekind domain with ﬁeld of fractions K, and pick J ⊆ K.
Show that J is a fractional ideal if and only if
(i) J is closed under addition and multiplication by elements of A, and
(ii) J is ﬁnitely generated as an abelian group.
More succinctly: J is a fractional ideal ⇐⇒ J is a ﬁnitely generated A-module.

Problem 48E. In the notation of Theorem 48.5.4, let I = (cid:81)g
simplicity that K is monogenic, hence OK = Z[θ].
(a) Prove that each pi is prime.

i=1 pei

i . Assume for

(b) Show that (p) divides I.

(c) Use the norm to show that (p) = I.

49 Minkowski bound and class groups

We now have a neat theory of unique factorization of ideals. In the case of a PID, this

in fact gives us a UFD. Sweet.

We’ll deﬁne, in a moment, something called the class group which measures how far
OK is from being a PID; the bigger the class group, the farther OK is from being a PID.
In particular, the OK is a PID if it has trivial class group.
Then we will provide some inequalities which let us put restrictions on the class group;
for instance, this will let us show in some cases that the class group must be trivial.
Astonishingly, the proof will use Minkowski’s theorem, a result from geometry.

§49.1 The class group

Prototypical example for this section: PID’s have trivial class group

Let K be a number ﬁeld, and let JK denote the multiplicative group of fractional
ideals of OK. Let PK denote the multiplicative group of principal fractional ideals:
those of the form (x) = xOK for some x ∈ K.

Question 49.1.1. Check that PK is also a multiplicative group. (This is really easy: name

xOK · yOK and (xOK)−1.)

As JK is abelian, we can now deﬁne the class group to be the quotient

ClK := JK/PK.

The elements of ClK are called classes.

Equivalently,

The class group ClK is the set of nonzero fractional ideals modulo scaling
by a constant in K.

In particular, ClK is trivial if all ideals are principal, since the nonzero principal ideals
are the same up to scaling.

The size of the class group is called the class number. It’s a beautiful theorem that
the class number is always ﬁnite, and the bulk of this chapter will build up to this result.
It requires several ingredients.

§49.2 The discriminant of a number ﬁeld

Prototypical example for this section: Quadratic ﬁelds.

Let’s say I have K = Q(√2). As we’ve seen before, this means OK = Z[√2], meaning

OK =(cid:110)a + b√2 | a, b ∈ Z(cid:111) .

The key insight now is that you might think of this as a lattice: geometrically, we want
to think about this the same way we think about Z2.

461

462

Napkin, by Evan Chen (v1.5.20190718)

Perversely, we might try to embed this into Q2 by sending a + b√2 to (a, b). But this
is a little stupid, since we’re rudely making K, which somehow lives inside Q and is
“one-dimensional” in that sense, into a two-dimensional space. It also depends on a choice
of basis, which we don’t like. A better way is to think about the fact that there are two
embeddings σ1 : K → C and σ2 : K → C, namely the identity, and conjugation:

σ1(a + b√2) = a + b√2
σ2(a + b√2) = a − b√2.

Fortunately for us, these embeddings both have real image. This leads us to consider the
set of points

(σ1(α), σ2(α)) ∈ R2

as α ∈ K.

This lets us visualize what OK looks like in R2. The points of K are dense in R2, but
the points of OK cut out a lattice.

To see how big the lattice is, we look at how {1,√2}, the generators of OK, behave.
The point corresponding to a + b√2 in the lattice is
a · (1, 1) + b · (√2,−

√2).

The mesh of the lattice1 is deﬁned as the hypervolume of the “fundamental parallelepiped”
I’ve colored blue above. For this particular case, it ought to be equal to the area of that
parallelogram, which is

det(cid:20)1 −√2

1 √2 (cid:21) = 2√2.

The deﬁnition of the discriminant is precisely this, except with an extra square factor (since
permutation of rows could lead to changes in sign in the matrix above). Problem 50B(cid:63)
shows that the squaring makes ∆K an integer.

To make the next deﬁnition, we invoke:

1Most authors call this the volume, but I think this is not the right word to use – lattices have “volume”
zero since they are just a bunch of points! In contrast, the English word “mesh” really does refer to
the width of a “gap”.

−2−1012−√2√21+√249 Minkowski bound and class groups

463

Theorem 49.2.1 (The n embeddings of a number ﬁeld)
Let K be a number ﬁeld of degree n. Then there are exactly n ﬁeld homomorphisms
K (cid:44)→ C, say σ1, . . . , σn, which ﬁx Q.

Proof. Deferred to Theorem 52.3.1, once we have the tools of Galois theory.

In fact, in Theorem 52.3.4 we see that for α ∈ K, we have that σi(α) runs over the
conjugates of α as i = 1, . . . , n. It follows that

TrK/Q(α) =

This allows us to deﬁne:

n(cid:88)i=1

σi(α)

and NK/Q(α) =

σi(α).

n(cid:89)i=1

Deﬁnition 49.2.2. Suppose α1, . . . , αn is a Z-basis of OK. The discriminant of the
number ﬁeld K is deﬁned by

∆K := det

σ1(α1)

...

σ1(αn)

. . . σn(α1)
. . .
. . . σn(αn)

...

2

.



This does not depend on the choice of the {αi}; we will not prove this here.
Example 49.2.3 (Discriminant of K = Q(√2))
We have OK = Z[√2] and as discussed above the discriminant is

∆K = (−2√2)2 = 8.

Example 49.2.4 (Discriminant of Q(i))
Let K = Q(i). We have OK = Z[i] = Z ⊕ iZ. The embeddings are the identity and
complex conjugation which take 1 to (1, 1) and i to (i,−i). So

∆K = det(cid:20)1

i −i(cid:21)2

1

= (−2i)2 = −4.

This example illustrates that the discriminant need not be positive for number ﬁelds
which wander into the complex plane (the lattice picture is a less perfect analogy).
But again, as we’ll prove in the problems the discriminant is always an integer.

Example 49.2.5 (Discriminant of Q(√5))
Let K = Q(√5). This time, OK = Z ⊕ 1+√5
2 Z, and so the discriminant is going to
look a little bit diﬀerent. The embeddings are still a + b√5 (cid:55)→ a + b√5, a − b√5.
Applying this to the Z-basis(cid:110)1, 1+√5

2 (cid:111), we get
2 (cid:35)2
1
1−√5

√5)2 = 5.

= (−

K = det(cid:34) 1

2

1+√5

∆2

464

Napkin, by Evan Chen (v1.5.20190718)

Exercise 49.2.6. Extend all this to show that if K = Q(√d) for d (cid:54)= 1 squarefree, we have

∆K =(cid:40)d

if d ≡ 1

4d if d ≡ 2, 3

(mod 4)

(mod 4).

Actually, let me point out something curious: recall that the polynomial discriminant

of Ax2 + Bx + C is B2 − 4AC. Then:

 In the d ≡ 1 (mod 4) case, ∆K is the discriminant of x2 − x − d−1
2 (1 + √d)].

2 (1 + √d). Of course, OK = Z[ 1

minimal polynomial of 1

4 , which is the

 In the d ≡ 2, 3 (mod 4) case, ∆K is the discriminant of x2 − d which is the minimal
polynomial of √d. Once again, OK = Z[√d].

This is not a coincidence! Problem 50C(cid:63) asserts that this is true in general; hence the
name “discriminant”.

§49.3 The signature of a number ﬁeld
Prototypical example for this section: Q( 100√2) has signature (2, 49).

In the example of K = Q(i), we more or less embedded K into the space C. However,
K is a degree two extension, so what we’d really like to do is embed it into R2. To do so,
we’re going to take advantage of complex conjugation.

Let K be a number ﬁeld and σ1, . . . , σn be its embeddings. We distinguish between the
real embeddings (which map all of K into R) and the complex embeddings (which
map some part of K outside R) Notice that if σ is a complex embedding, then so is the
conjugate σ (cid:54)= σ; hence complex embeddings come in pairs.
Deﬁnition 49.3.1. Let K be a number ﬁeld of degree n, and set

r1 = number of real embeddings
r2 = number of pairs of complex embeddings.

The signature of K is the pair (r1, r2). Observe that r1 + 2r2 = n.

Example 49.3.2 (Basic examples of signatures)
(a) Q has signature (1, 0).
(b) Q(√2) has signature (2, 0).
(c) Q(i) has signature (0, 1).
(d) Let K = Q( 3√2), and let ω be a cube root of unity. The elements of K are

K =(cid:110)a + b 3√2 + c 3√4 | a, b, c ∈ Q(cid:111) .

Then the signature is (1, 1), because the three embeddings are

σ1 : 3√2 (cid:55)→ 3√2,

σ2 : 3√2 (cid:55)→ 3√2ω,

σ3 : 3√2 (cid:55)→ 3√2ω2.

The ﬁrst of these is real and the latter two are conjugate pairs.

49 Minkowski bound and class groups

465

Example 49.3.3 (Even more signatures)
In the same vein Q( 99√2) and Q( 100√2) have signatures (1, 49) and (2, 49).

Question 49.3.4. Verify the signatures of the above two number ﬁelds.

From now on, we will number the embeddings of K in such a way that

are the real embeddings, while

σ1, σ2, . . . , σr1

σr1+1 = σr1+r2+1,

σr1+2 = σr1+r2+2,

. . . ,

σr1+r2 = σr1+2r2.

are the r2 pairs of complex embeddings. We deﬁne the canonical embedding of K as

K

ι

(cid:44)→ Rr1 × Cr2

by α (cid:55)→ (σ1(α), . . . , σr1(α), σr1+1(α), . . . , σr1+r2(α)) .

All we’ve done is omit, for the complex case, the second of the embeddings in each
conjugate pair. This is no big deal, since they are just conjugates; the above tuple is all
the information we need.

For reasons that will become obvious in a moment, I’ll let τ denote the isomorphism

by breaking each complex number into its real and imaginary part, as

τ : Rr1 × Cr2 ∼−→ Rr1+2r2 = Rn
α (cid:55)→(cid:0)σ1(α), . . . , σr1(α),

Re σr1+1(α), Im σr1+1(α),
Re σr1+2(α), Im σr1+2(α),
. . . ,

Re σr1+r2(α), Im σr1+r2(α)(cid:1).

Example 49.3.5 (Example of canonical embedding)
As before let K = Q( 3√2) and set

σ1 : 3√2 (cid:55)→ 3√2,
√3
2 i, noting that we’ve already arranged indices so σ1 = id is real
(cid:44)→ R × C ∼−→ R3 are

where ω = − 1
while σ2 and σ3 are a conjugate pair. So the embeddings K
given by

σ3 : 3√2 (cid:55)→ 3√2ω2

σ2 : 3√2 (cid:55)→ 3√2ω,

2 +

ι

α ι

(cid:55)−→ (σ1(α), σ2(α))

τ(cid:55)−→ (σ1(α), Re σ2(α), Im σ2(α)) .

For concreteness, taking α = 9 + 3√2 gives

9 + 3√2 ι(cid:55)→(cid:16)9 + 3√2, 9 + 3√2ω(cid:17)
3√2 +

1
2

=(cid:32)9 + 3√2, 9 −
τ(cid:55)→(cid:32)9 + 3√2, 9 −

3√2,

1
2

6√108
2
6√108

i(cid:33) ∈ R × C
2 (cid:33) ∈ R3.

466

Napkin, by Evan Chen (v1.5.20190718)

Now, the whole point of this is that we want to consider the resulting lattice when we

take OK. In fact, we have:

Lemma 49.3.6

Consider the composition of the embeddings K (cid:44)→ Rr1 × Cr2 ∼−→ Rn. Then as before,
OK becomes a lattice L in Rn, with mesh equal to
2r2(cid:112)|∆K|.

1

Proof. Fun linear algebra problem (you just need to manipulate determinants). Left as
Problem 49D.

From this we can deduce:

Lemma 49.3.7

Consider the composition of the embeddings K (cid:44)→ Rr1 × Cr2 ∼−→ Rn. Let a be an
ideal in OK. Then the image of a is a lattice La in Rn with mesh equal to

N(a)

2r2 (cid:112)|∆K|.

Sketch of Proof. Let

d = N(a) := [OK : a].

Then in the lattice La, we somehow only take 1
d th of the points which appear in the
lattice L, which is why the area increases by a factor of N(a). To make this all precise
I would need to do a lot more with lattices and geometry than I have space for in this
chapter, so I will omit the details. But I hope you can see why this is intuitively true.

§49.4 Minkowski’s theorem

Now I can tell you why I insisted we move from Rr1 × Cr2 to Rn. In geometry, there’s a
really cool theorem of Minkowski’s that goes as follows.

Theorem 49.4.1 (Minkowski)
Let S ⊆ Rn be a convex set containing 0 which is centrally symmetric (meaning
that x ∈ S ⇐⇒ −x ∈ S). Let L be a lattice with mesh d. If either
(a) The volume of S exceeds 2nd, or

(b) The volume of S equals 2nd and S is compact,

then S contains a nonzero lattice point of L.

Question 49.4.2. Show that the condition 0 ∈ S is actually extraneous in the sense that
any nonempty, convex, centrally symmetric set contains the origin.

Sketch of Proof. Part (a) is surprisingly simple and has a very olympiad-esque solution:
it’s basically Pigeonhole on areas. We’ll prove part (a) in the special case n = 2, L = Z2

49 Minkowski bound and class groups

467

for simplicity as the proof can easily be generalized to any lattice and any n. Thus we
want to show that any such convex set S with area more than 4 contains a lattice point.

Dissect the plane into 2 × 2 squares

[2a − 1, 2a + 1] × [2b − 1, 2b + 1]

2 (p − q) ∈ S (convexity) and is a nonzero lattice point.

and overlay all these squares on top of each other. By the Pigeonhole Principle, we ﬁnd
there exist two points p (cid:54)= q ∈ S which map to the same point. Since S is symmetric,
−q ∈ S. Then 1
I’ll brieﬂy sketch part (b): the idea is to consider (1+ε)S for ε > 0 (this is “S magniﬁed
by a small factor 1 + ε”). This satisﬁes condition (a). So for each ε > 0 the set of nonzero
lattice points in (1 + ε)S, say Sε, is a ﬁnite nonempty set of (discrete) points (the “ﬁnite”
part follows from the fact that (1 + ε)S is bounded). So there has to be some point that’s
in Sε for every ε > 0 (why?), which implies it’s in S.

§49.5 The trap box

The last ingredient we need is a set to apply Minkowski’s theorem to. I propose:

Deﬁnition 49.5.1. Let M be a positive real. In Rr1 × Cr2, deﬁne the box S to be the
set of points (x1, . . . , xr1, z1, . . . , zr2) such that

r1(cid:88)i=1

|xi| + 2

r2(cid:88)j=1

|zj| ≤ M.

Note that this depends on the value of M .

Think of this box as a mousetrap: anything that falls in it is going to have a small

norm, and our goal is to use Minkowski to lure some nonzero element into it.

That is, suppose α ∈ a falls into the box I’ve deﬁned above, which means

M ≥

r1(cid:88)i=1

|σi(α)| + 2

r1+r2(cid:88)i=r1+1

|σi(α)| =

n(cid:88)i=1

|σi(α)| ,

where we are remembering that the last few σ’s come in conjugate pairs. This looks
like the trace, but the absolute values are in the way. So instead, we apply AM-GM to
obtain:

S−2−1012−√2√21+√2468

Napkin, by Evan Chen (v1.5.20190718)

Lemma 49.5.2 (Eﬀect of the mousetrap)
Let α ∈ OK, and suppose ι(α) is in S (where ι : K (cid:44)→ Rr1 × Cr2 as usual). Then

NK/Q(α) =

n(cid:89)i=1

n(cid:19)n
|σi(α)| ≤(cid:18) M

.

The last step we need to do is compute the volume of the box. This is again some
geometry I won’t do, but take my word for it:

Lemma 49.5.3 (Size of the mousetrap)

Let τ : Rr1 ×Cr2 ∼−→ Rn as before. Then the image of S under τ is a convex, compact,

centrally symmetric set with volume

2r1 ·(cid:16) π
2(cid:17)r2

M n
n!

.

·

Question 49.5.4. (Sanity check) Verify that the above is correct for the signatures (r1, r2) =
(2, 0) and (r1, r2) = (0, 1), which are the possible signatures when n = 2.

§49.6 The Minkowski bound

We can now put everything we have together to obtain the great Minkowski bound.

Theorem 49.6.1 (Minkowski bound)
Let a ⊆ OK be any nonzero ideal. Then there exists 0 (cid:54)= α ∈ a such that

NK/Q(α) ≤(cid:18) 4

π(cid:19)r2 n!

nn(cid:112)|∆K| · N(a).

Proof. This is a matter of putting all our ingredients together. Let’s see what things
we’ve deﬁned already:

K ⊂

ι- Rr1 × Cr2

τ - Rn

box S

OK

a

n!

- τ img(S)

- Lattice L

2(cid:1)r2 M n
with volume 2r1(cid:0) π
with mesh 2−r2(cid:112)|∆K|
- Lattice La with mesh 2−r2(cid:112)|∆K| N(a)

Pick a value of M such that the mesh of La equals 2−n of the volume of the box. Then
Minkowski’s theorem gives that some 0 (cid:54)= α ∈ a lands inside the box — the mousetrap is
conﬁgured to force NK/Q(α) ≤ 1

M n = M n · 2n ·

nn M n. The correct choice of M is
mesh
vol box

= 2n ·

n!

2r1 ·(cid:0) π

2(cid:1)r2 · 2−r2(cid:112)|∆K| N(a)

49 Minkowski bound and class groups

469

which gives the bound after some arithmetic.

§49.7 The class group is ﬁnite

Deﬁnition 49.7.1. Let MK =(cid:0) 4

depending on K.

π(cid:1)r2 n!

nn(cid:112)|∆K| for brevity. Note that it is a constant

So that’s cool and all, but what we really wanted was to show that the class group is
ﬁnite. How can the Minkowski bound help? Well, you might notice that we can rewrite
it to say

N(cid:0)(α) · a−1(cid:1) ≤ MK

where MK is some constant depending on K, and α ∈ a.

Question 49.7.2. Show that (α) · a−1 is an integral ideal. (Unwind deﬁnitions.)

But in the class group we mod out by principal ideals like (α). If we shut our eyes for
a moment and mod out, the above statement becomes “N(a−1) ≤ MK”. The precise
statement of this is

Corollary 49.7.3

Let K be a number ﬁeld, and pick a fractional ideal J. Then we can ﬁnd α such
that b = (α) · J is integral and N(b) ≤ MK.

Proof. For fractional ideals I and J write I ∼ J to mean that I = (α)J for some α; then
ClK is just modding out by ∼. Let J be a fractional ideal. Then J−1 is some other
fractional ideal. By deﬁnition, for some α ∈ OK we have that αJ−1 is an integral ideal
a. The Minkowski bound tells us that for some x ∈ a, we have N(xa−1) ≤ MK. But
xa−1 ∼ a−1 = (αJ−1)−1 ∼ J.

Corollary 49.7.4 (Finiteness of class group)

Class groups are always ﬁnite.

Proof. For every class in ClK, we can identify an integral ideal a with norm less than
MK. We just have to show there are ﬁnitely many such integral ideals; this will mean
there are ﬁnitely many classes.

Suppose we want to build such an ideal a = pe1

m . Recall that a prime ideal pi
must have some rational prime p inside it, meaning pi divides (p) and p divides N(pi).
So let’s group all the pi we want to build a with based on which (p) they came from.

1 . . . pem

To be more dramatic: imagine you have a cherry tree; each branch corresponds to a
prime (p) and contains as cherries (prime ideals) the factors of (p) (ﬁnitely many). Your
bucket (the ideal a you’re building) can only hold a total weight (norm) of MK. So you

(2)(3)(5)...470

Napkin, by Evan Chen (v1.5.20190718)

can’t even touch the branches higher than MK. You can repeat cherries (oops), but the
weight of a cherry on branch (p) is deﬁnitely ≥ p; all this means that the number of ways
to build a is ﬁnite.

§49.8 Computation of class numbers

Deﬁnition 49.8.1. The order of ClK is called the class number of K.

Remark 49.8.2 — If ClK = 1, then OK is a PID, hence a UFD.

By computing the actual value of MK, we can quite literally build the entire “cherry

tree” mentioned in the previous proof. Let’s give an example how!

Proposition 49.8.3

The ﬁeld Q(√−67) has class number 1.
Proof. Since K = Q(√−67) has signature (0, 1) and discriminant ∆K = −67 (since
−67 ≡ 1 (mod 4)) we can compute

MK =(cid:18) 4
π(cid:19)1

2!
22

√67 ≈ 5.2.

·

That means we can cut oﬀ the cherry tree after (2), (3), (5), since any cherries on these
branches will necessarily have norm ≥ MK. We now want to factor each of these in
OK = Z[θ], where θ = 1+√−67
has minimal polynomial x2 − x + 17. But something

2

miraculous happens:

 When we try to reduce x2 − x + 17 (mod 2), we get an irreducible polynomial
x2 − x + 1. By the factoring algorithm (Theorem 48.5.4) this means (2) is prime.
 Similarly, reducing mod 3 gives x2 − x + 2, which is irreducible. This means (3) is

prime.

 Finally, for the same reason, (5) is prime.

It’s our lucky day; all of the ideals (2), (3), (5) are prime (already principal). To put it
another way, each of the three branches has only one (large) cherry on it. That means
any time we put together an integral ideal with norm ≤ MK, it is actually principal. In
fact, these guys have norm 4, 9, 25 respectively. . . so we can’t even touch (3) and (5),
and the only ideals we can get are (1) and (2) (with norms 1 and 4).

Now we claim that’s all. Pick a fractional ideal J. By Corollary 49.7.3, we can ﬁnd an
integral ideal b ∼ J with N(b) ≤ MK. But by the above, either b = (1) or b = (4), both
of which are principal, and hence trivial in ClK. So J is trivial in ClK too, as needed.

Let’s do a couple more.

Theorem 49.8.4 (Gaussian integers Z[i] form a UFD)
The ﬁeld Q(i) has class number 1.

49 Minkowski bound and class groups

471

Proof. This is OK where K = Q(i), so we just want ClK to be trivial. We have
MK = 2
π

√4 < 2. So every class has an integral ideal of norm b satisfying

N(b)(cid:18) 4
π(cid:19)1

√4 =

2!
22 ·

·

4
π

< 2.

Well, that’s silly: we don’t have any branches to pick from at all. In other words, we can
only have b = (1).

Here’s another example of something that still turns out to be unique factorization,

but this time our cherry tree will actually have cherries that can be picked.

Proposition 49.8.5 (Z[√7] is a UFD)
The ﬁeld Q(√7) has class number 1.

Proof. First we compute the Minkowski bound.

Question 49.8.6. Check that MK ≈ 2.646.

So this time, the only branch is (2). Let’s factor (2) as usual: the polynomial x2 + 7
reduces as (x − 1)(x + 1) (mod 2), and hence

Oops! We now have two cherries, and they both seem reasonable. But actually, I claim
that

(2) =(cid:16)2,√7 − 1(cid:17)(cid:16)2,√7 + 1(cid:17) .
(cid:16)2,√7 − 1(cid:17) =(cid:16)3 −
√7(cid:17) .

Question 49.8.7. Prove this.

So both the cherries are principal ideals, and as before we conclude that ClK is trivial.
But note that this time, the prime ideal (2) actually splits; we got lucky that the two
cherries were principal but this won’t always work.

How about some nontrivial class groups? First, we use a lemma that will help us with

narrowing down the work in our cherry tree.

Lemma 49.8.8 (Ideals divide their norms)

Let b be an integral ideal with N(b) = n. Then b divides the ideal (n).

Proof. By deﬁnition, n = |OK/b|. Treating OK/b as an (additive) abelian group and
using Lagrange’s theorem, we ﬁnd

0 ≡ α + ··· + α

= nα (mod b)

for all α ∈ OK.

(cid:124)

(cid:125)

n times

(cid:123)(cid:122)

Thus (n) ⊆ b, done.

Now we can give such an example.

472

Napkin, by Evan Chen (v1.5.20190718)

Proposition 49.8.9 (Class group of Q(√−17))
The number ﬁeld K = Q(√−17) has class group Z/4Z.

You are not obliged to read the entire proof in detail, as it is somewhat gory. The idea is
just that there are some cherries which are not trivial in the class group.

Proof. Since ∆K = −68, we compute the Minkowski bound

MK =

√17 < 6.

4
π

Now, it suﬃces to factor with (2), (3), (5). The minimal polynomial of √−17 is x2 + 17,

so as usual

(2) = (2,√−17 + 1)2
(3) = (3,√−17 − 1)(3,√−17 + 1)

(5) = (5)

corresponding to the factorizations of x2 +17 modulo each of 2, 3, 5. Set p = (2,√−17+1)
and q1 = (3,√−17 − 1), q2 = (3,√−17 + 1). We can compute

N(p) = 2 and N(q1) = N(q2) = 3.

In particular, they are not principal. The ideal (5) is out the window; it has norm 25.
Hence, the three cherries are p, q1, q2.

The possible ways to arrange these cherries into ideals with norm ≤ 5 are

However, you can compute

(cid:8)(1), p, q1, q2, p2(cid:9) .

p2 = (2)

so p2 and (1) are in the same class group; that is, they are trivial. In particular, the class
group has order at most 4.

From now on, let [a] denote the class (member of the class group) that a is in. Since p
isn’t principal (so [p] (cid:54)= [(1)]), it follows that p has order two. So Lagrange’s theorem
says that ClK has order either 2 or 4.
Now we claim [q1]2 (cid:54)= [(1)], which implies that q1 has order greater than 2. If not, q2
1
1 = (3); this would force

is principal. We know N(q1) = 3, so this can only occur if q2
q1 = q2. This is impossible since q1 + q2 = (1).

Thus, q1 has even order greater than 2. So it has to have order 4. From this we deduce

ClK ∼= Z/4Z.

Remark 49.8.10 — When we did this at Harvard during Math 129, there was a
ﬁve-minute interruption in which students (jokingly) complained about the diﬃculty
of evaluating 4
π

√17. Excerpt:

“Will we be allowed to bring a small calculator on the exam?” – Student
1
“What does the size have to do with anything? You could have an Apple
Watch” – Professor
“Just use the fact that π ≥ 3” – me
“Even [other professor] doesn’t know that, how are we supposed to?” –

49 Minkowski bound and class groups

473

Student 2
“You have to do this yourself!” – Professor
“This is an outrage.” – Student 1

§49.9 A few harder problems to think about
Problem 49A. Show that K = Q(√−163) has trivial class group, and hence OK =
Z[√−163] is a UFD.2
Problem 49B. Determine the class group of Q(√−31).

Problem 49C (China TST 1998). Let n be a positive integer. A polygon in the plane
(not necessarily convex) has area greater than n. Prove that one can translate it so that
it contains at least n + 1 lattice points.

Problem 49D (Lemma 49.3.6). Consider the composition of the embeddings K (cid:44)→
Rr1 × Cr2 ∼−→ Rn. Show that the image of OK ⊆ K has mesh equal to

1

2r2(cid:112)|∆K|.

Problem 49E. Let p ≡ 1 (mod 4) be a prime. Show that there are unique integers
a > b > 0 such that a2 + b2 = p.

Problem 49F (Korea 2014). Let p be an odd prime and k a positive integer such that
p | k2 + 5. Prove that there exist positive integers m, n such that p2 = m2 + 5n2.

2In fact, n = 163 is the largest number for which Q(

is 1, 2, 3, 7, 11, 19, 43, 67, 163, the Heegner numbers. You might notice Euler’s prime-generating
polynomial t2 + t + 41 when doing the above problem. Not a coincidence!

√−n) has trivial class group. The complete list

50 More properties of the discriminant

I’ll remind you that the discriminant of a number ﬁeld K is given by

∆K := det

σ1(α1)

...

σ1(αn)

. . . σn(α1)
. . .
. . . σn(αn)

...

2



where α1, . . . , αn is a Z-basis for K, and the σi are the n embeddings of K into C.

Several examples, properties, and equivalent deﬁnitions follow.

§50.1 A few harder problems to think about

Problem 50A(cid:63) (Discriminant of cyclotomic ﬁeld). Let p be an odd rational prime and
ζp a primitive pth root of unity. Let K = Q(ζp). Show that

∆K = (−1)

p−1
2 pp−2.

Problem 50B(cid:63) (Trace representation of ∆K). Let α1, . . . , αn be a basis for OK. Prove
that

∆K = det

TrK/Q(α2
1)
TrK/Q(α2α1)

...

TrK/Q(α1α2)
TrK/Q(α2
2)
...

TrK/Q(αnα1) TrK/Q(αnα2)

. . . TrK/Q(α1αn)
. . . TrK/Q(α2αn)
. . .
. . . TrK/Q(αnαn)

...

.



In particular, ∆K is an integer.

Problem 50C(cid:63) (Root representation of ∆K). The discriminant of a quadratic polyno-
mial Ax2 + Bx + C is deﬁned as B2 − 4AC. More generally, the polynomial discriminant
of a polynomial f ∈ Z[x] of degree n is

∆(f ) := c2n−2 (cid:89)1≤i<j≤n

(zi − zj)2

where z1, . . . , zn are the roots of f , and c is the leading coeﬃcient of f .

Suppose K is monogenic with OK = Z[θ]. Let f denote the minimal polynomial of θ

(hence monic). Show that

∆K = ∆(f ).

Problem 50D. Show that if K (cid:54)= Q is a number ﬁeld then |∆K| > 1.
Problem 50E (Brill’s theorem). For a number ﬁeld K with signature (r1, r2), show that
∆K > 0 if and only if r2 is even.

Problem 50F (Stickelberger theorem). Let K be a number ﬁeld. Prove that

∆K ≡ 0 or 1

(mod 4).

475

51 Bonus: Let’s solve Pell’s equation!

This is an optional aside, and can be safely ignored. (On the other hand, it’s pretty

short.)

§51.1 Units
Prototypical example for this section: ±1, roots of unity, 3 − 2√2 and its powers.
Recall according to Problem 47A(cid:63) that α ∈ OK is invertible if and only if

NK/Q(α) = ±1.

We let O×K denote the set of units of OK.

Question 51.1.1. Show that O
unit group of OK.

×
K is a group under multiplication. Hence we name it the

What are some examples of units?

Example 51.1.2 (Examples of units in a number ﬁeld)
1. ±1 are certainly units, present in any number ﬁeld.
2. If OK contains a root of unity ω (i.e. ωn = 1), then ω is a unit. (In fact, ±1
3. Of course, not all units of OK are roots of unity. For example, if OK = Z[√3]
(from K = Q(√3)) then the number 2 + √3 is a unit, as its norm is

are special cases of this.)

NK/Q(2 + √3) = 22 − 3 · 12 = 1.

Alternatively, just note that the inverse 2 − √3 ∈ OK as well:

√3(cid:17)(cid:16)2 + √3(cid:17) = 1.

(cid:16)2 −
Either way, 2 − √3 is a unit.
4. Given any unit u ∈ O×K, all its powers are also units. So for example, (3−2√2)n
is always a unit of Z[√2], for any n. If u is not a root of unity, then this
generates inﬁnitely many new units in O×K.

Question 51.1.3. Verify the claims above that

(a) Roots of unity are units, and

(b) Powers of units are units.
One can either proceed from the deﬁnition or use the characterization NK/Q(α) = ±1. If
one deﬁnition seems more natural to you, use the other.

477

478

Napkin, by Evan Chen (v1.5.20190718)

§51.2 Dirichlet’s unit theorem
Prototypical example for this section: The units of Z[√3] are ±(2 + √3)n.
Deﬁnition 51.2.1. Let µ(OK) denote the set of roots of unity contained in a number
ﬁeld K (equivalently, in OK).

Example 51.2.2 (Examples of µ(OK))
(a) If K = Q(i), then OK = Z[i]. So

µ(OK) = {±1,±i} where K = Q(i).

(b) If K = Q(√3), then OK = Z[√3]. So

µ(OK) = {±1} where K = Q(√3).

(c) If K = Q(√−3), then OK = Z[ 1

2 (1 + √−3)]. So

µ(OK) =(cid:26)±1, ±1 ± √−3

2

(cid:27) where K = Q(√−3)

where the ±’s in the second term need not depend on each other; in other words

µ(OK) =(cid:8)z | z6 = 1(cid:9).

Exercise 51.2.3. Show that we always have that µ(OK) comprises the roots to xn − 1 for
some integer n. (First, show it is a ﬁnite group under multiplication.)

We now quote, without proof, the so-called Dirichlet’s unit theorem, which gives us a
much more complete picture of what the units in OK are. Legend says that Dirichlet
found the proof of this theorem during an Easter concert in the Sistine Chapel.

Theorem 51.2.4 (Dirichlet’s unit theorem)

Let K be a number ﬁeld with signature (r1, r2) and set

s = r1 + r2 − 1.

Then there exist units u1, . . . , us such that every unit α ∈ O×K can be written
uniquely in the form

for ω ∈ µ(OK) is a root of unity, and n1, . . . , ns ∈ Z.

α = ω · un1

1 . . . uns
s

More succinctly:

We have O×K ∼= Zr1+r2−1 × µ(OK).

A choice of u1, . . . , us is called a choice of fundamental units.

Here are some example applications.

51 Bonus: Let’s solve Pell’s equation!

479

Example 51.2.5 (Some unit groups)
(a) Let K = Q(i) with signature (0, 1). Then we obtain s = 0, so Dirichlet’s Unit

theorem says that there are no units other than the roots of unity. Thus

O×K = {±1,±i} where K = Q(i).

This is not surprising, since a + bi ∈ Z[i] is a unit if and only if a2 + b2 = 1.
(b) Let K = Q(√3), which has signature (2, 0). Then s = 1, so we expect exactly
one fundamental unit. A fundamental unit is 2 + √3 (or 2 − √3, its inverse)

with norm 1, and so we ﬁnd

O×K =(cid:110)±(2 + √3)n | n ∈ Z(cid:111) .

(c) Let K = Q( 3√2) with signature (1, 1). Then s = 1, so we expect exactly one

fundamental unit. The choice 1 + 3√2 + 3√4. So

O×K =(cid:110)±(cid:16)1 + 3√2 + 3√4(cid:17)n

| n ∈ Z(cid:111) .

I haven’t actually shown you that these are fundamental units, and indeed computing

fundamental units is in general hard.

§51.3 Finding fundamental units

Here is a table with some fundamental units.

d Unit

1

d = 2 1 + √2
d = 3 2 + √3
2 (1 + √5)
d = 5
d = 6 5 + 2√6
d = 7 8 + 3√7
d = 10 3 + √10
d = 11 10 + 3√11

In general, determining fundamental units is computationally hard.
However, once I tell you what the fundamental unit is, it’s not too bad (at least in
the case s = 1) to verify it. For example, suppose we want to show that 10 + 3√11 is
a fundamental unit of K = Q(√11), which has ring of integers Z[√11]. If not, then for

some n > 1, we would have to have

10 + 3√11 = ±(cid:16)x + y√11(cid:17)n

.

For this to happen, at the very least we would need |y| < 3. We would also have
x2 − 11y2 = ±1. So one can just verify (using y = 1, 2) that this fails.
The point is that: Since (10, 3) is the smallest (in the sense of |y|) integer solution to
x2 − 11y2 = ±1, it must be the fundamental unit. This holds more generally, although in
the case that d ≡ 1 (mod 4) a modiﬁcation must be made as x, y might be half-integers
(like 1

2 (1 + √5)).

480

Napkin, by Evan Chen (v1.5.20190718)

Theorem 51.3.1 (Fundamental units of pell equations)

Assume d is a squarefree integer.
(a) If d ≡ 2, 3 (mod 4), and (x, y) is a minimal integer solution to x2 − dy2 = ±1,

then x + y√d is a fundamental unit.

(b) If d ≡ 1 (mod 4), and (x, y) is a minimal half-integer solution to x2 − dy2 = ±1,
then x + y√d is a fundamental unit. (Equivalently, the minimal integer solution
to a2 − db2 = ±4 gives 1

2 (a + b√d).)

(Any reasonable deﬁnition of “minimal” will work, such as sorting by |y|.)

§51.4 Pell’s equation

This class of results completely eradicates Pell’s Equation. After all, solving

a2 − d · b2 = ±1

amounts to ﬁnding elements of Z[√d] with norm ±1. It’s a bit weirder in the d ≡ 1
(mod 4) case, since in that case K = Q(√d) gives OK = Z[ 1
2 (1 + √d)], and so the

fundamental unit may not actually be a solution. (For example, when d = 5, we get the
solution ( 1

2 ).) Nonetheless, all integer solutions are eventually generated.

2 , 1

To make this all concrete, here’s a simple example.

Example 51.4.1 (x2 − 5y2 = ±1)
Set K = Q(√5), so OK = Z[ 1

by a single element u. The choice

2 (1+√5)]. By Dirichlet’s unit theorem, O×K is generated

u =

1
2

+

√5

1
2

serves as a fundamental unit, as there are no smaller integer solutions to a2−5b2 = ±4.

The ﬁrst several powers of u are

n
−2
−1
0
1
2
3
4
5
6

un

1

1

1

1

1

2 (3 − √5)
2 (1 − √5)
2 (1 + √5)
2 (3 + √5)
2 + √5
2 (7 + 3√5)
2 (11 + 5√5)
9 + 4√5

1

1

Norm
1
−1
1
−1
1
−1
1
−1
1

One can see that the ﬁrst integer solution is (2, 1), which gives −1. The ﬁrst solution
with +1 is (9, 4). Continuing the pattern, we ﬁnd that every third power of u gives
an integer solution (see also Problem 51B), with the odd ones giving a solution to
x2 − 5y2 = −1 and the even ones a solution to x2 − 5y2 = +1. All solutions are
generated this way, up to ± signs (by considering ±u±n).

51 Bonus: Let’s solve Pell’s equation!

481

§51.5 A few harder problems to think about

Problem 51A (Fictitious account of the battle of Hastings). Determine the number of
soldiers in the following battle:

The men of Harold stood well together, as their wont was, and formed thirteen
squares, with a like number of men in every square thereof, and woe to the
hardy Norman who ventured to enter their redoubts; for a single blow of
Saxon war-hatched would break his lance and cut through his coat of mail . .
. when Harold threw himself into the fray the Saxons were one might square
of men, shouting the battle-cries, “Ut!”, “Olicrosse!”, “Godemite!”

Problem 51B. Let d > 0 be a squarefree integer, and let u denote the fundamental

unit of Q(√d). Show that either u ∈ Z[√d], or un ∈ Z[√d] ⇐⇒ 3 | n.

Problem 51C. Show that there are no integer solutions to

despite the fact that −1 is a quadratic residue mod 34.

x2 − 34y2 = −1

XIV

Algebraic NT II: Galois and

Ramiﬁcation Theory

Part XIV: Contents

52 Things Galois

485
52.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 485
. . . . . . . . . . . . . . . . 486
52.2 Field extensions, algebraic closures, and splitting ﬁelds
. . . . . . . . . . . . . . . . . 487
52.3 Embeddings into algebraic closures for number ﬁelds
52.4 Everyone hates characteristic 2: separable vs irreducible . . . . . . . . . . . . . . . . 488
. . . . . . . . . . . . . . . . . . . . . 490
52.5 Automorphism groups and Galois extensions
52.6 Fundamental theorem of Galois theory . . . . . . . . . . . . . . . . . . . . . . . . 493
. . . . . . . . . . . . . . . . . . . . . . . . 494
52.7 A few harder problems to think about
. . . . . . . . . . . . . . . . . 495
52.8 (Optional) Proof that Galois extensions are splitting

53 Finite ﬁelds

497
53.1 Example of a ﬁnite ﬁeld . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 497
. . . . . . . . . . . . . . . . . . . . . . . . . 498
53.2 Finite ﬁelds have prime power order
53.3 All ﬁnite ﬁelds are isomorphic . . . . . . . . . . . . . . . . . . . . . . . . . . . . 499
53.4 The Galois theory of ﬁnite ﬁelds . . . . . . . . . . . . . . . . . . . . . . . . . . . 500
. . . . . . . . . . . . . . . . . . . . . . . . 501
53.5 A few harder problems to think about

54 Ramiﬁcation theory

503
54.1 Ramiﬁed / inert / split primes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 503
54.2 Primes ramify if and only if they divide ∆K . . . . . . . . . . . . . . . . . . . . . 504
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 504
54.3 Inertial degrees
54.4 The magic of Galois extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 505
54.5 (Optional) Decomposition and inertia groups . . . . . . . . . . . . . . . . . . . . . 507
. . . . . . . . . . . . . . . . . . 509
54.6 Tangential remark: more general Galois extensions
. . . . . . . . . . . . . . . . . . . . . . . . 509
54.7 A few harder problems to think about

55 The Frobenius element

511
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 511
55.1 Frobenius elements
55.2 Conjugacy classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 512
55.3 Chebotarev density theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 514
55.4 Example: Frobenius elements of cyclotomic ﬁelds . . . . . . . . . . . . . . . . . . . 514
55.5 Frobenius elements behave well with restriction . . . . . . . . . . . . . . . . . . . . 515
55.6 Application: Quadratic reciprocity . . . . . . . . . . . . . . . . . . . . . . . . . . 516
55.7 Frobenius elements control factorization . . . . . . . . . . . . . . . . . . . . . . . 518
55.8 Example application: IMO 2003 problem 6 . . . . . . . . . . . . . . . . . . . . . . 521
. . . . . . . . . . . . . . . . . . . . . . . . 522
55.9 A few harder problems to think about

56 Bonus: A Bit on Artin Reciprocity

523
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 523
56.1 Inﬁnite primes
. . . . . . . . . . . . . . . . . . . . . . . 523
56.2 Modular arithmetic with inﬁnite primes
56.3 Inﬁnite primes in extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 525
. . . . . . . . . . . . . . . . . . . . . . . . . 526
56.4 Frobenius element and Artin symbol
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 528
56.5 Artin reciprocity
. . . . . . . . . . . . . . . . . . . . . . . . 531
56.6 A few harder problems to think about

52 Things Galois

§52.1 Motivation
Prototypical example for this section: Q(√2) and Q( 3√2).

The key idea in Galois theory is that of embeddings, which give us another way to get

at the idea of the “conjugate” we described earlier.

Let K be a number ﬁeld. An embedding σ : K (cid:44)→ C, is an injective ﬁeld homomor-
phism: it needs to preserve addition and multiplication, and in particular it should ﬁx
1.

Question 52.1.1. Show that in this context, σ(q) = q for any rational number q.

Example 52.1.2 (Examples of embeddings)
(a) If K = Q(i), the two embeddings of K into C are z (cid:55)→ z (the identity) and
(b) If K = Q(√2), the two embeddings of K into C are a + b√2 (cid:55)→ a + b√2 (the
(c) If K = Q( 3√2), there are three embeddings:

z (cid:55)→ z (complex conjugation).
identity) and a + b√2 (cid:55)→ a − b√2 (conjugation).
 The identity embedding, which sends 1 (cid:55)→ 1 and 3√2 (cid:55)→ 3√2.
 An embedding which sends 1 (cid:55)→ 1 and 3√2 (cid:55)→ ω 3√2, where ω is a cube root
 An embedding which sends 1 (cid:55)→ 1 and 3√2 (cid:55)→ ω2 3√2.

of unity. Note that this is enough to determine the rest of the embedding.

I want to make several observations about these embeddings, which will form the core

ideas of Galois theory. Pay attention here!

 First, you’ll notice some duality between roots: in the ﬁrst example, i gets sent to

±i, √2 gets sent to ±√2, and 3√2 gets sent to the other roots of x3 − 2. This is no

coincidence, and one can show this occurs in general. Speciﬁcally, suppose α has
minimal polynomial

0 = cnαn + cn−1αn−1 + ··· + c1α + c0

where the ci are rational. Then applying any embedding σ to both sides gives

0 = σ(cnαn + cn−1αn−1 + ··· + c1α + c0)
= σ(cn)σ(α)n + σ(cn−1)σ(α)n−1 + ··· + σ(c1)σ(α) + σ(c0)
= cnσ(α)n + cn−1σ(α)n−1 + ··· + c1σ(α) + c0

where in the last step we have used the fact that ci ∈ Q, so they are ﬁxed by σ. So,
roots of minimal polynomials go to other roots of that polynomial.

485

486

Napkin, by Evan Chen (v1.5.20190718)

 Next, I want to draw out a contrast between the second and third examples.

Speciﬁcally, in example (b) where we consider embeddings K = Q(√2) to C. The

image of these embeddings lands entirely in K: that is, we could just as well have
looked at K → K rather than looking at K → C. However, this is not true in (c):
indeed Q( 3√2) ⊂ R, but the non-identity embeddings have complex outputs!

The key diﬀerence is to again think about conjugates. Key observation:

The ﬁeld K = Q( 3√2) is “deﬁcient” because the minimal polynomial
x3 − 2 has two other roots ω 3√2 and ω2 3√2 not contained in K.
On the other hand K = Q(√2) is just ﬁne because both roots of x2−2 are contained
inside K. Finally, one can actually ﬁx the deﬁciency in K = Q( 3√2) by completing
it to a ﬁeld Q( 3√2, ω). Fields like Q(i) or Q(√2) which are “self-contained” are
called Galois extensions, as we’ll explain shortly.

 Finally, you’ll notice that in the examples above, the number of embeddings from K
to C happens to be the degree of K. This is an important theorem, Theorem 52.3.1.

In this chapter we’ll develop these ideas in full generality, for any ﬁeld other than Q.

§52.2 Field extensions, algebraic closures, and splitting ﬁelds
Prototypical example for this section: Q( 3√2)/Q is an extension, C is an algebraic closure

of any number ﬁeld.

First, we deﬁne a notion of one ﬁeld sitting inside another, in order to generalize the

notion of a number ﬁeld.

Deﬁnition 52.2.1. Let K and F be ﬁelds. If F ⊆ K, we write K/F and say K is a
ﬁeld extension of F .
Thus K is automatically an F -vector space (just like Q(√2) is automatically a Q-vector
space). The degree is the dimension of this space, denoted [K : F ]. If [K : F ] is ﬁnite,
we say K/F is a ﬁnite (ﬁeld) extension.

That’s really all. There’s nothing tricky at all.

Question 52.2.2. What do you call a ﬁnite extension of Q?

Degrees of ﬁnite extensions are multiplicative.

Theorem 52.2.3 (Field extensions have multiplicative degree)
Let F ⊆ K ⊆ L be ﬁelds with L/K, K/F ﬁnite. Then
[L : K][K : F ] = [L : F ].

Proof. Basis bash: you can ﬁnd a basis of L over K, and then expand that into a basis
L over F . (Diligent readers can ﬁll in details.)

Next, given a ﬁeld (like Q( 3√2)) we want something to embed it into (in our case C).

So we just want a ﬁeld that contains all the roots of all the polynomials:

52 Things Galois

487

Theorem 52.2.4 (Algebraic closures)

Let F be a ﬁeld. Then there exists a ﬁeld extension F containing F , called an
algebraic closure, such that all polynomials in F [x] factor completely.

Example 52.2.5 (C)
C is an algebraic closure of Q, R and even itself.

Abuse of Notation 52.2.6. Some authors also require the algebraic closure to be
for example, given Q they would want only Q (the algebraic
minimal by inclusion:
numbers). It’s a theorem that such a minimal algebraic closure is unique, and so these
authors will refer to the algebraic closure of K.

I like C, so I’ll use the looser deﬁnition.

§52.3 Embeddings into algebraic closures for number ﬁelds

Now that I’ve deﬁned all these ingredients, I can prove:

Theorem 52.3.1 (The n embeddings of a number ﬁeld)
Let K be a number ﬁeld of degree n. Then there are exactly n ﬁeld homomorphisms
K (cid:44)→ C, say σ1, . . . , σn which ﬁx Q.

Remark 52.3.2 — Note that a nontrivial homomorphism of ﬁelds is necessarily
injective (the kernel is an ideal). This justiﬁes the use of “(cid:44)→”, and we call each σi
an embedding of K into C.

Proof. This is actually kind of fun! Recall that any irreducible polynomial over Q has
distinct roots (Lemma 47.1.2). We’ll adjoin elements α1, α2, . . . , αm one at a time to Q,
until we eventually get all of K, that is,

Diagrammatically, this is

K = Q(α1, . . . , αn).

Q ⊂
∩

id

?
C

- Q(α1) ⊂ - Q(α1, α2) ⊂ -

∩

∩

τ1

?
- C

τ2

?
- C

. . . ⊂

. . .

- . . .

-

K
∩

τm=σ

?
- C

First, we claim there are exactly

[Q(α1) : Q]

ways to pick τ1. Observe that τ1 is determined by where it sends α1 (since it has to ﬁx
Q). Letting p1 be the minimal polynomial of α1, we see that there are deg p1 choices for
τ1, one for each (distinct) root of p1. That proves the claim.

488

Napkin, by Evan Chen (v1.5.20190718)

Similarly, given a choice of τ1, there are

[Q(α1, α2) : Q(α1)]

ways to pick τ2. (It’s a little diﬀerent: τ1 need not be the identity. But it’s still true
that τ2 is determined by where it sends α2, and as before there are [Q(α1, α2) : Q(α1)]
possible ways.)

Multiplying these all together gives the desired [K : Q].

Remark 52.3.3 — The primitive element theorem actually implies that m = 1 is
suﬃcient; we don’t need to build a whole tower. This simpliﬁes the proof somewhat.

It’s common to see expressions like “let K be a number ﬁeld of degree n, and σ1, . . . , σn
its n embeddings” without further explanation. The relation between these embeddings
and the Galois conjugates is given as follows.

Theorem 52.3.4 (Embeddings are evenly distributed over conjugates)
Let K be a number ﬁeld of degree n with n embeddings σ1, . . . , σn, and let α ∈ K
have m Galois conjugates over Q.
Then σj(α) is “evenly distributed” over each of these m conjugates: for any Galois

conjugate β, exactly n

m of the embeddings send α to β.

Proof. In the previous proof, adjoin α1 = α ﬁrst.

So, now we can deﬁne the trace and norm over Q in a nice way: given a number ﬁeld

K, we set

TrK/Q(α) =

n(cid:88)i=1

where σi are the n embeddings of K into C.

σi(α)

and NK/Q(α) =

σi(α)

n(cid:89)i=1

§52.4 Everyone hates characteristic 2: separable vs irreducible

Prototypical example for this section: Q has characteristic zero, hence irreducible polyno-
mials are separable.

Now, we want a version of the above theorem for any ﬁeld F . If you read the proof,
you’ll see that the only thing that ever uses anything about the ﬁeld Q is Lemma 47.1.2,
where we use the fact that

Irreducible polynomials over F have no double roots.

Let’s call a polynomial with no double roots separable; thus we want irreducible
polynomials to be separable. We did this for Q in the last chapter by taking derivatives.
Should work for any ﬁeld, right?

Nope. Suppose we took the derivative of some polynomial like 2x3 + 24x + 9, namely
6x2 + 24. In C it’s obvious that the derivative of a nonconstant polynomial f(cid:48) isn’t zero.
But suppose we considered the above as a polynomial in F3, i.e. modulo 3. Then the
derivative is zero. Oh, no!

We have to impose a condition that prevents something like this from happening.

52 Things Galois

489

Deﬁnition 52.4.1. For a ﬁeld F , the characteristic of F is the smallest positive integer
p such that,

or zero if no such integer p exists.

1F + ··· + 1F

= 0

(cid:125)

(cid:124)

p times

(cid:123)(cid:122)

Example 52.4.2 (Field characteristics)
Old friends R, Q, C all have characteristic zero. But Fp, the integers modulo p, is a
ﬁeld of characteristic p.

Exercise 52.4.3. Let F be a ﬁeld of characteristic p. Show that if p > 0 then p is a prime
number. (A proof is given next chapter.)

With the assumption of characteristic zero, our earlier proof works.

Lemma 52.4.4 (Separability in characteristic zero)

Any irreducible polynomial in a characteristic zero ﬁeld is separable.

Unfortunately, this lemma is false if the “characteristic zero” condition is dropped.

Remark 52.4.5 — The reason it’s called separable is (I think) this picture: I have
a polynomial and I want to break it into irreducible parts. Normally, if I have a
double root in a polynomial, that means it’s not irreducible. But in characteristic
p > 0 this fails. So inseparable polynomials are strange when you think about them:
somehow you have double roots that can’t be separated from each other.

We can get this to work for any ﬁeld extension in which separability is not an issue.

Deﬁnition 52.4.6. A separable extension K/F is one in which every irreducible
polynomial in F is separable (for example, if F has characteristic zero). A ﬁeld F is
perfect if any ﬁnite ﬁeld extension K/F is separable.

In fact, as we see in the next chapter:

Theorem 52.4.7 (Finite ﬁelds are perfect)

Suppose F is a ﬁeld with ﬁnitely many elements. Then it is perfect.

Thus, we will almost never have to worry about separability since every ﬁeld we see in
the Napkin is either ﬁnite or characteristic 0. So the inclusion of the word “separable” is
mostly a formality.

Proceeding onwards, we obtain

Theorem 52.4.8 (The n embeddings of any separable extension)
Let K/F be a separable extension of degree n and let F be an algebraic closure of F .
Then there are exactly n ﬁeld homomorphisms K (cid:44)→ F , say σ1, . . . , σn, which ﬁx F .

490

Napkin, by Evan Chen (v1.5.20190718)

In any case, this lets us deﬁne the trace for any separable normal extension.

Deﬁnition 52.4.9. Let K/F be a separable extension of degree n, and let σ1, . . . , σn
be the n embeddings into an algebraic closure of F . Then we deﬁne

TrK/F (α) =

n(cid:88)i=1

σi(α)

and NK/F (α) =

σi(α).

n(cid:89)i=1

When F = Q and the algebraic closure is C, this coincides with our earlier deﬁnition!

§52.5 Automorphism groups and Galois extensions
Prototypical example for this section: Q(√2) is Galois but Q( 3√2) is not.

We now want to get back at the idea we stated at the beginning of this section that

Q( 3√2) is deﬁcient in a way that Q(√2) is not.

First, we deﬁne the “internal” automorphisms.

Deﬁnition 52.5.1. Suppose K/F is a ﬁnite extension. Then Aut(K/F ) is the set of
ﬁeld isomorphisms σ : K → K which ﬁx F . In symbols

Aut(K/F ) = {σ : K → K | σ is identity on F} .

This is a group under function composition!

Note that this time, we have a condition that F is ﬁxed by σ. (This was not there

before when we considered F = Q, because we got it for free.)

Example 52.5.2 (Old examples of automorphism groups)
Reprising the example at the beginning of the chapter in the new notation, we have:

(a) Aut(Q(i)/Q) ∼= Z/2Z, with elements z (cid:55)→ z and z (cid:55)→ z.
(b) Aut(Q(√2)/Q) ∼= Z/2Z in the same way.
(c) Aut(Q( 3√2)/Q) is the trivial group, with only the identity embedding!

Example 52.5.3 (Automorphism group of Q(√2,√3))

let K = Q(√2,√3).

It turns out that Aut(K/Q) =

Here’s a new example:
{1, σ, τ, στ}, where

σ :(cid:40)√2

√3

(cid:55)→ −√2
(cid:55)→ √3

and τ :(cid:40)√2 (cid:55)→ √2
√3 (cid:55)→ −√3.

In other words, Aut(K/Q) is the Klein Four Group.

First, let’s repeat the proof of the observation that these embeddings shuﬄe around

roots (akin to the ﬁrst observation in the introduction):

52 Things Galois

491

Lemma 52.5.4 (Root shuﬄing in Aut(K/F ))
Let f ∈ F [x], suppose K/F is a ﬁnite extension, and assume α ∈ K is a root of f .
Then for any σ ∈ Aut(K/F ), σ(α) is also a root of f .

Proof. Let f (x) = cnxn + cn−1xn−1 + ··· + c0, where ci ∈ F . Thus,

0 = σ(f (α)) = σ (cnαn + ··· + c0) = cnσ(α)n + ··· + c0 = f (σ(α)).

In particular, taking f to be the minimal polynomial of α we deduce

An embedding σ ∈ Aut(K/F ) sends an α ∈ K to one of its various Galois
conjugates (over F ).

Next, let’s look again at the “deﬁciency” of certain ﬁelds. Look at K = Q( 3√2). So,
again K/Q is deﬁcient for two reasons. First, while there are three maps Q( 3√2) (cid:44)→ C,
only one of them lives in Aut(K/Q), namely the identity. In other words, |Aut(K/Q)| is
too small. Secondly, K is missing some Galois conjugates (ω 3√2 and ω2 3√2).

The way to capture the fact that there are missing Galois conjugates is the notion of a

splitting ﬁeld.

Deﬁnition 52.5.5. Let F be a ﬁeld and p(x) ∈ F [x] a polynomial of degree n. Then
p(x) has roots α1, . . . , αn in an algebraic closure of F . The splitting ﬁeld of F is deﬁned
as F (α1, . . . , αn).

In other words, the splitting ﬁeld is the smallest ﬁeld in which p(x) splits.

Example 52.5.6 (Examples of splitting ﬁelds)

(a) The splitting ﬁeld of x2 − 5 over Q is Q(√5). This is a degree 2 extension.
(b) The splitting ﬁeld of x2 + x + 1 over Q is Q(ω), where ω is a cube root of unity.

This is a degree 3 extension.

(c) The splitting ﬁeld of x2 + 3x + 2 = (x + 1)(x + 2) is just Q! There’s nothing to

do.

492

Napkin, by Evan Chen (v1.5.20190718)

Example 52.5.7 (Splitting ﬁelds: a cautionary tale)
The splitting ﬁeld of x3 − 2 over Q is in fact
Q( 3√2, ω)

and not just Q( 3√2)! One must really adjoin all the roots, and it’s not necessarily

the case that these roots will generate each other.

To be clear:

 For x2 − 5, we adjoin √5 and this will automatically include −√5.

 For x2 + x + 1, we adjoin ω and get the other root ω2 for free.

 But for x3−2, if we adjoin 3√2, we do NOT get ω 3√2 and ω2 3√2 for free. Indeed,
Q( 3√2) ⊂ R!

Note that in particular, the splitting ﬁeld of x3 − 2 over Q is degree six, not just
degree three.

In general, the splitting ﬁeld of a polynomial can be an extension of degree
up to n!. The reason is that if p(x) has n roots and none of them are “related” to each
other, then any permutation of the roots will work.

Now, we obtain:

Theorem 52.5.8 (Galois extensions are splitting)
For ﬁnite extensions K/F , |Aut(K/F )| divides [K : F ], with equality if and only if
K is the splitting ﬁeld of some separable polynomial with coeﬃcients in F .

The proof of this is deferred to an optional section at the end of the chapter. If K/F is
a ﬁnite extension and |Aut(K/F )| = [K : F ], we say the extension K/F is Galois. In
that case, we denote Aut(K/F ) by Gal(K/F ) instead and call this the Galois group of
K/F .

Example 52.5.9 (Examples and non-examples of Galois extensions)

The Galois group has order two, √2 (cid:55)→ ±√2.

(a) The extension Q(√2)/Q is Galois, since it’s the splitting ﬁeld of x2 − 2 over Q.
(b) The extension Q(√2,√3)/Q is Galois, since it’s the splitting ﬁeld of (x2 − 5)2 − 6
(c) The extension Q( 3√2)/Q is not Galois.

over Q. As discussed before, the Galois group is Z/2Z × Z/2Z.

To explore Q( 3√2) one last time:

52 Things Galois

493

Example 52.5.10 (Galois closures, and the automorphism group of Q( 3√2, ω))
Let’s return to the ﬁeld K = Q( 3√2, ω), which is a ﬁeld with [K : Q] = 6. Consider

the two automorphisms:

σ :(cid:40) 3√2 (cid:55)→ ω 3√2

ω

(cid:55)→ ω

and τ :(cid:40) 3√2 (cid:55)→ 3√2
(cid:55)→ ω2.

ω

Notice that σ3 = τ 2 = id. From this one can see that the automorphism group of
K must have order 6 (it certainly has order ≤ 6; now use Lagrange’s theorem). So,
K/Q is Galois! Actually one can check explicitly that

is the symmetric group on 3 elements, with order 3! = 6.

Gal(K/Q) ∼= S3

This example illustrates the fact that given a non-Galois ﬁeld extension, one can “add in”
missing conjugates to make it Galois. This is called taking a Galois closure.

§52.6 Fundamental theorem of Galois theory

After all this stuﬀ about Galois Theory, I might as well tell you the fundamental theorem,
though I won’t prove it. Basically, it says that if K/F is Galois with Galois group G,
then:

Subgroups of G correspond exactly to ﬁelds E with F ⊆ E ⊆ K.

To tell you how the bijection goes, I have to deﬁne a ﬁxed ﬁeld.

Deﬁnition 52.6.1. Let K be a ﬁeld and H a subgroup of Aut(K/F ). We deﬁne the
ﬁxed ﬁeld of H, denoted KH , as

KH := {x ∈ K : σ(x) = x ∀σ ∈ H} .

Question 52.6.2. Verify quickly that K H is actually a ﬁeld.

Now let’s look at examples again. Consider K = Q(√2,√3), where

G = Gal(K/Q) = {id, σ, τ, στ}

is the Klein four group (where σ(√2) = −√2 but σ(√3) = √3; τ goes the other

way).

Question 52.6.3. Let H = {id, σ}. What is K H ?

In that case, the diagram of ﬁelds between Q and K matches exactly with the subgroups
of G, as follows:

494

Napkin, by Evan Chen (v1.5.20190718)

Q(√2,√3)

{id}

Q(√2) Q(√6) Q(√3)

{id, τ} {id, στ} {id, σ}

We see that subgroups correspond to ﬁxed ﬁelds. That, and much more, holds in general.

Q

G

Theorem 52.6.4 (Fundamental theorem of Galois theory)

Let K/F be a Galois extension with Galois group G = Gal(K/F ).

(a) There is a bijection between ﬁeld towers F ⊆ E ⊆ K and subgroups H ⊆ G:

K
|
E
|
F





⇐⇒

1
|
H
|
G





The bijection sends H to its ﬁxed ﬁeld KH , and hence is inclusion reversing.

(b) Under this bijection, we have [K : E] = |H| and [E : F ] = [G : H].
(c) K/E is always Galois, and its Galois group is Gal(K/E) = H.

(d) E/F is Galois if and only if H is normal in G. If so, Gal(E/F ) = G/H.

Exercise 52.6.5. Suppose we apply this theorem for
K = Q( 3√2, ω).

Verify that the fact E = Q( 3√2) is not Galois corresponds to the fact that S3 does not have

normal subgroups of order 2.

§52.7 A few harder problems to think about

Problem 52A(cid:63) (Galois group of the cyclotomic ﬁeld). Let p be an odd rational prime
and ζp a primitive pth root of unity. Let K = Q(ζp). Show that

Gal(K/Q) ∼= (Z/pZ)∗.

Problem 52B (Greek constructions). Prove that the three Greek constructions

(a) doubling the cube,

52 Things Galois

(b) squaring the circle, and

(c) trisecting an angle

are all impossible. (Assume π is transcendental.)

495

Problem 52C (China Hong Kong Math Olympiad). Prove that there are no rational
numbers p, q, r satisfying

cos(cid:18) 2π

7 (cid:19) = p + √q + 3√r.

Problem 52D. Show that the only automorphism of R is the identity. Hence Aut(R) is
the trivial group.

Problem 52E (Artin’s primitive element theorem). Let K be a number ﬁeld. Show
that K ∼= Q(γ) for some γ.

§52.8 (Optional) Proof that Galois extensions are splitting

We prove Theorem 52.5.8. First, we extract a useful fragment from the fundamental
theorem.

Theorem 52.8.1 (Fixed ﬁeld theorem)
Let K be a ﬁeld and G a subgroup of Aut(K). Then [K : KG] = |G|.

The inequality itself is not diﬃcult:

Exercise 52.8.2. Show that [K : F ] ≥ | Aut(K/F )|, and that equality holds if and only if
the set of elements ﬁxed by all σ ∈ Aut(K/F ) is exactly F . (Use Theorem 52.8.1.)

The equality case is trickier.

The easier direction is when K is a splitting ﬁeld. Assume K = F (α1, . . . , αn) is the
splitting ﬁeld of some separable polynomial p ∈ F [x] with n distinct roots α1, . . . , αn.
Adjoin them one by one:

F ⊂

- F (α1) ⊂ - F (α1, α2) ⊂ - . . . ⊂

- K

id

?
⊂
F

τ1

τ2

. . .

τn=σ

?
- F (α1)

?

⊂ - F (α1, α2)

⊂ - . . . ⊂

?
- K

(Does this diagram look familiar?) Every map K → K which ﬁxes F corresponds to an
above commutative diagram. As before, there are exactly [F (α1) : F ] ways to pick τ1.
(You need the fact that the minimal polynomial p1 of α1 is separable for this: there need
to be exactly deg p1 = [F (α1) : F ] distinct roots to nail p1 into.) Similarly, given a choice
of τ1, there are [F (α1, α2) : F (α1)] ways to pick τ2. Multiplying these all together gives
the desired [K : F ].

Now assume K/F is Galois. First, we state:

496

Napkin, by Evan Chen (v1.5.20190718)

Lemma 52.8.3
Let K/F be Galois, and p ∈ F [x] irreducible. If any root of p (in F ) lies in K, then
all of them do, and in fact p is separable.

Proof. Let α ∈ K be the prescribed root. Consider the set
S = {σ(α) | σ ∈ Gal(K/F )} .

(Note that α ∈ S since Gal(K/F ) (cid:51) id.) By construction, any τ ∈ Gal(K/F ) ﬁxes S. So
if we construct

˜p(x) = (cid:89)β∈S

(x − β),

then by Vieta’s Formulas, we ﬁnd that all the coeﬃcients of ˜p are ﬁxed by elements of σ.
By the equality case we speciﬁed in the exercise, it follows that ˜p has coeﬃcients in F !
(This is where we use the condition.) Also, by Lemma 52.5.4, ˜p divides p.

Yet p was irreducible, so it is the minimal polynomial of α in F [x], and therefore we

must have that p divides ˜p. Hence p = ˜p. Since ˜p was built to be separable, so is p.

Now we’re basically done – pick a basis ω1, . . . , ωn of K/F , and let pi be their minimal
polynomials; by the above, we don’t get any roots outside K. Consider P = p1 . . . pn,
removing any repeated factors. The roots of P are ω1, . . . , ωn and some other guys in
K. So K is the splitting ﬁeld of P .

53 Finite ﬁelds

In this short chapter, we classify all ﬁelds with ﬁnitely many elements and compute the
Galois groups. Nothing in here is very hard, and so most of the proofs are just sketches;
if you like, you should check the details yourself.

The whole point of this chapter is to prove:
 A ﬁnite ﬁeld F must have order pn, with p prime and n an integer.

 In this case, F has characteristic p.

 All such ﬁelds are isomorphic, so it’s customary to use the notation Fpn for “the”

ﬁnite ﬁeld of order pn if we only care up to isomorphism.

 The extension F/Fp is Galois, and Gal(F/Fp) is a cyclic group of order n. The

generator is the automorphism

If you’re in a hurry you can just remember these results and skip to the next chapter.

σ : F → F by x (cid:55)→ xp.

§53.1 Example of a ﬁnite ﬁeld

Before diving in, we give some examples.

Recall that the characteristic of a ﬁeld F is the smallest positive integer p such that

or 0 if no such integer p exists.

1F + ··· + 1F

= 0

p times

(cid:123)(cid:122)

(cid:125)

(cid:124)

Example 53.1.1 (Base ﬁeld)
Let Fp denote the ﬁeld of integers modulo p. This is a ﬁeld with p elements, with
characteristic p.

Example 53.1.2 (The ﬁnite ﬁeld of nine elements)
Let

F ∼= F3[X]/(X 2 + 1) ∼= Z[i]/(3).

We can think of its elements as

{a + bi | 0 ≤ a, b ≤ 2} .

Since (3) is prime in Z[i], the ring of integers of Q(i), we see F is a ﬁeld with 32 = 9
elements inside it. Note that, although this ﬁeld has 9 elements, every element x has
the property that

In particular, F has characteristic 3.

3x = x + ··· + x

3 times

(cid:124)

(cid:123)(cid:122)

(cid:125)

= 0.

497

498

Napkin, by Evan Chen (v1.5.20190718)

§53.2 Finite ﬁelds have prime power order

Lemma 53.2.1

If the characteristic of a ﬁeld F isn’t zero, it must be a prime number.

Proof. Assume not, so n = ab for a, b < n. Then let

A = 1F + ··· + 1F

a times

B = 1F + ··· + 1F

b times

(cid:124)
(cid:124)

(cid:123)(cid:122)
(cid:123)(cid:122)

(cid:54)= 0

(cid:54)= 0.

(cid:125)
(cid:125)

and

Then AB = 0, contradicting the fact that F is a ﬁeld.

We like ﬁelds of characteristic zero, but unfortunately for ﬁnite ﬁelds we are doomed

to have nonzero characteristic.

Lemma 53.2.2 (Finite ﬁelds have prime power orders)

Let F be a ﬁnite ﬁeld. Then

(a) Its characteristic is nonzero, and hence some prime p.

(b) The ﬁeld F is a ﬁnite extension of Fp, and in particular it is an Fp-vector space.
(c) We have |F| = pn for some prime p, integer n.

Proof. Very brieﬂy, since this is easy:

(a) Apply Lagrange’s theorem (or pigeonhole principle!) to (F, +) to get the characteristic

isn’t zero.

(b) The additive subgroup of (F, +) generated by 1F is an isomorphic copy of Fp.
(c) Since it’s a ﬁeld extension, F is a ﬁnite-dimensional vector space over Fp, with some

basis e1, . . . , en. It follows that there are pn elements of F .

Remark 53.2.3 — An amusing alternate proof of (c) by contradiction: if a prime
q (cid:54)= p divides |F|, then by Cauchy’s theorem (Problem 17A(cid:63)) on (F, +) there’s a
(nonzero) element x of order q. Evidently

x · (1F + ··· + 1F

) = 0

(cid:124)

q times

(cid:123)(cid:122)

(cid:125)

then, but x (cid:54)= 0, and hence the characteristic of F also divides q, which is impossible.
An important point in the above proof is that

Lemma 53.2.4 (Finite ﬁelds are ﬁeld extensions of Fp)
If |F| = pn is a ﬁnite ﬁeld, then there is an isomorphic copy of Fp sitting inside F .
Thus F is a ﬁeld extension of Fp.

53 Finite ﬁelds

499

We want to refer a lot to this copy of Fp, so in what follows:

Abuse of Notation 53.2.5. Every integer n can be identiﬁed as an element of F ,
namely

n := 1F + ··· + 1F

.

n times

(cid:123)(cid:122)

(cid:124)

(cid:125)

Note that (as expected) this depends only on n (mod p).

This notation makes it easier to think about statements like the following.

Theorem 53.2.6 (Freshman’s dream)
For any a, b ∈ F we have

(a + b)p = ap + bp.

Proof. Use the Binomial theorem, and the fact that(cid:0)p

i(cid:1) is divisible by p for 0 < i < p.

Exercise 53.2.7. Convince yourself that this proof works.

§53.3 All ﬁnite ﬁelds are isomorphic

We next proceed to prove “Fermat’s little theorem”:

Theorem 53.3.1 (Fermat’s little theorem in ﬁnite ﬁelds)
Let F be a ﬁnite ﬁeld of order pn. Then every element x ∈ F satisﬁes

xpn

− x = 0.

Proof. If x = 0 it’s true; otherwise, use Lagrange’s theorem on the abelian group (F,×)
to get xpn−1 = 1F .

We can now prove the following result, which is the “main surprise” about ﬁnite ﬁelds:

that there is a unique one up to isomorphism for each size.

Theorem 53.3.2 (Complete classiﬁcation of ﬁnite ﬁelds)
A ﬁeld F is a ﬁnite ﬁeld with pn elements if and only if it is a splitting ﬁeld of
xpn

− x over Fp.

Proof. By “Fermat’s little theorem”, all the elements of F satisfy this polynomial. So we
just have to show that the roots of this polynomial are distinct (i.e. that it is separable).

To do this, we use the derivative trick again: the derivative of this polynomial is

pn · xpn−1 − 1 = −1

which has no roots at all, so the polynomial cannot have any double roots.

Deﬁnition 53.3.3. For this reason, it’s customary to denote the ﬁeld with pn elements
by Fpn.

Note that the polynomial xpn
above shows that it’s separable.

− x (mod p) is far from irreducible, but the computation

500

Napkin, by Evan Chen (v1.5.20190718)

Example 53.3.4 (The ﬁnite ﬁeld of order nine again)
The polynomial x9 − x is separable modulo 3 and has factorization

x(x + 1)(x + 2)(x2 + 1)(x2 + x + 2)(x2 + 2x + 2)

(mod 3).

So if F has order 9, then we intuitively expect it to be the ﬁeld generated by
adjoining all the roots: 0, 1, 2, as well as ±i, 1 ± i, 2 ± i. Indeed, that’s the example
we had at the beginning of this chapter.
(Here i denotes an element of F9 satisfying i2 = −1. The notation is deliberately

similar to the usual imaginary unit.)

§53.4 The Galois theory of ﬁnite ﬁelds

Retain the notation Fpn now (instead of F like before). By the above theorem, it’s the
splitting ﬁeld of a separable polynomial, hence we know that Fpn/Fp is a Galois extension.
We would like to ﬁnd the Galois group.

In fact, we are very lucky:

Gal(Fpn/Fp):

it is cyclic. First, we exhibit one such element σp ∈

Theorem 53.4.1 (The pth power automorphism)
The map σp : Fpn → Fpn deﬁned by

σp(x) = xp

is an automorphism, and moreover ﬁxes Fp.

Proof. It’s a homomorphism since it ﬁxes 1, respects multiplication, and respects addi-
tion.

Question 53.4.2. Why does it respect addition?

Next, we claim that it is injective. To see this, note that

xp = yp ⇐⇒ xp − yp = 0 ⇐⇒ (x − y)p = 0 ⇐⇒ x = y.

Here we have again used the Freshman’s Dream. Since Fpn is ﬁnite, this injective map is
automatically bijective. The fact that it ﬁxes Fp is Fermat’s little theorem.

Now we’re done:

Theorem 53.4.3 (Galois group of the extension Fpn/Fp)
We have Gal(Fpn/Fp) ∼= Z/nZ with generator σp.

Proof. Since [Fpn : Fp] = n, the Galois group G has order n. So we just need to show
σp ∈ G has order n.
Note that σp applied k times gives x (cid:55)→ xpk
as all elements of Fpn satisfy xpn
the identity or xpk

. Hence, σp applied n times is the identity,
= x. But if k < n, then σp applied k times cannot be

− x would have too many roots.

We can see an example of this again with the ﬁnite ﬁeld of order 9.

53 Finite ﬁelds

501

Example 53.4.4 (Galois group of ﬁnite ﬁeld of order 9)
Let F9 be the ﬁnite ﬁeld of order 9, and represent it concretely by F9 = Z[i]/(3). Let
σ3 : F9 → F9 be x (cid:55)→ x3. We can witness the fate of all nine elements:

0

1

2

i
6

1 + i 2 + i
6

6

σ

σ

σ

?
−i

?
1 − i

?
2 − i

(As claimed, 0, 1, 2 are the ﬁxed points, so I haven’t drawn arrows for them.) As
predicted, the Galois group has order two:

Gal(F9/F3) = {id, σ3} ∼= Z/2Z.

This concludes the proof of all results stated at the beginning of this chapter.

§53.5 A few harder problems to think about

Problem 53A† (HMMT 2017). What is the period of the Fibonacci sequence modulo
127?

54 Ramiﬁcation theory

We’re very interested in how rational primes p factor in a bigger number ﬁeld K. Some

examples of this behavior: in Z[i] (which is a UFD!), we have factorizations

(2) = (1 + i)2
(3) = (3)
(5) = (2 + i)(2 − i).

In this chapter we’ll learn more about how primes break down when they’re thrown into
bigger number ﬁelds. Using weapons from Galois Theory, this will culminate in a proof
of Quadratic Reciprocity.

§54.1 Ramiﬁed / inert / split primes

Prototypical example for this section: In Z[i], 2 is ramiﬁed, 3 is inert, and 5 splits.

Let p be a rational prime, and toss it into OK. Thus we get a factorization into prime

ideals

We say that each pi is above (p).1 Pictorially, you might draw this as follows:

p · OK = pe1

1 . . . peg
g .

K

Q

⊃

OK

pi

⊃

Z

(p)

Some names for various behavior that can happen:

 We say p is ramiﬁed if ei > 1 for some i. For example 2 is ramiﬁed in Z[i].

 We say p is inert if g = 1 and e1 = 1; i.e. (p) remains prime. For example 3 is

inert in Z[i].

 We say p is split if g > 1. For example 5 is split in Z[i].

Question 54.1.1. More generally, for a prime p in Z[i]:

 p is ramiﬁed exactly when p = 2.
 p is inert exactly when p ≡ 3 (mod 4).
 p is split exactly when p ≡ 1 (mod 4).

Prove this.

1Reminder that p · OK and (p) mean the same thing, and I’ll use both interchangeably.

503

504

Napkin, by Evan Chen (v1.5.20190718)

§54.2 Primes ramify if and only if they divide ∆K

The most unusual case is ramiﬁcation: Just like we don’t expect a randomly selected
polynomial to have a double root, we don’t expect a randomly selected prime to be
ramiﬁed. In fact, the key to understanding ramiﬁcation is the discriminant.

For the sake of discussion, let’s suppose that K is monogenic, OK = Z[θ], where θ
has minimal polynomial f . Let p be a rational prime we’d like to factor. If f factors as
1 . . . f eg
f e1

g , then we know that the prime factorization of (p) is given by

p · OK =(cid:89)i

(p, fi(θ))ei .

In particular, p ramiﬁes exactly when f has a double root mod p! To detect whether this
happens, we look at the polynomial discriminant of f , namely

∆(f ) =(cid:89)i<j

(zi − zj)2

and see whether it is zero mod p – thus p ramiﬁes if and only if this is true.

It turns out that the na¨ıve generalization to any number ﬁeld works if we replace
∆(f ) by just the discriminant ∆K of K; (these are the same for monogenic OK by
Problem 50C(cid:63)). That is,

Theorem 54.2.1 (Discriminant detects ramiﬁcation)

Let p be a rational prime and K a number ﬁeld. Then p is ramiﬁed if and only if p
divides ∆K.

Example 54.2.2 (Ramiﬁcation in the Gaussian integers)
Let K = Q(i) so OK = Z[i] and ∆K = 4. As predicted, the only prime ramifying in
Z[i] is 2, the only prime factor of ∆K.

In particular, only ﬁnitely many primes ramify.

§54.3 Inertial degrees

Prototypical example for this section: (7) has inertial degree 2 in Z[i] and (2 + i) has
inertial degree 1 in Z[i].

Recall that we were able to deﬁne an ideal norm N(a) = |OK/a| measuring how “roomy”

the ideal a is. For example, (5) has ideal norm 52 = 25 in Z[i], since

Z[i]/(5) ∼= {a + bi | a, b ∈ Z/5Z}

has 52 = 25 elements.

Now, let’s look at

p · OK = pe1

1 . . . peg

g

in OK, where K has degree n. Taking the ideal norms of both sides, we have that

pn = N(p1)e1 . . . N(pg)eg .

54 Ramiﬁcation theory

505

We conclude that pi = pfi for some integer fi ≥ 1, and moreover that

n =

eifi.

g(cid:88)i=1

Deﬁnition 54.3.1. We say fi is the inertial degree of pi, and ei is the ramiﬁcation
index.

Example 54.3.2 (Examples of inertial degrees)
Work in Z[i], which is degree 2. The inertial degree detects how “spacy” the given p
is when interpreted in OK.
(a) The prime 7 · Z[i] has inertial degree 2. Indeed, Z[i]/(7) has 72 = 49 elements,

those of the form a + bi for a, b modulo 7. It gives “two degrees” of space.

(b) Let (5) = (2 + i)(2 − i). The inertial degrees of (2 + i) and (2 − i) are both 1.
Indeed, Z[i]/(2 + i) only gives “one degree” of space, since each of its elements
can be viewed as integers modulo 5, and there are only 51 = 5 elements.

If you understand this, it should be intuitively clear why the sum of eifi should
equal n.

§54.4 The magic of Galois extensions

OK, that’s all ﬁne and well. But something really magical happens when we add the
additional hypothesis that K/Q is Galois: all the inertial degrees and ramiﬁcation degrees
are equal. We set about proving this.

Let K/Q be Galois with G = Gal(K/Q). Note that if p ⊆ OK is a prime above p, then
the image σimg(p) is also prime for any σ ∈ G (since σ is an automorphism!). Moreover,
since p ∈ p and σ ﬁxes Q, we know that p ∈ σimg(p) as well.
Thus, by the pointwise mapping, the Galois group acts on the prime ideals
above a rational prime p. Picture:

The notation σimg(p) is hideous in this context, since we’re really thinking of σ as just

doing a group action, and so we give the shorthand:

pp1p2p3p4p5p6σ506

Napkin, by Evan Chen (v1.5.20190718)

Abuse of Notation 54.4.1. Let σp be shorthand for σimg(p).

Since the σ’s are all bijections (they are automorphisms!), it should come as no surprise
that the prime ideals which are in the same orbit are closely related. But miraculously,
it turns out there is only one orbit!

Theorem 54.4.2 (Galois group acts transitively)
Let K/Q be Galois with G = Gal(K/Q). Let {pi} be the set of distinct prime ideals
in the factorization of p · OK (in OK).
Then G acts transitively on the pi: for every i and j, we can ﬁnd σ such that
σpi = pj.

Proof. Fairly slick. Suppose for contradiction that no σ ∈ G sends p1 to p2, say. By the
Chinese remainder theorem, we can ﬁnd an x ∈ OK such that

Then, compute the norm

x ≡ 0
x ≡ 1

(mod p1)
(mod pi) for i ≥ 2

NK/Q(x) = (cid:89)σ∈Gal(K/Q)

σ(x).

Each σ(x) is in K because K/Q is Galois!

Since NK/Q(x) is an integer and divisible by p1, we should have that NK/Q(x) is
divisible by p. Thus it should be divisible by p2 as well. But by the way we selected x,
we have x /∈ σ−1p2 for every σ ∈ G! So σ(x) /∈ p2 for any σ, which is a contradiction.

Theorem 54.4.3 (Inertial degree and ramiﬁcation indices are all equal)
Assume K/Q is Galois. Then for any rational prime p we have

p · OK = (p1p2 . . . pg)e

for some e, where the pi are distinct prime ideals with the same inertial degree f .
Hence

[K : Q] = ef g.

Proof. To see that the inertial degrees are equal, note that each σ induces an isomorphism

OK/p ∼= OK/σ(p).

Because the action is transitive, all fi are equal.

Exercise 54.4.4. Using the fact that σ ∈ Gal(K/Q), show that
σimg(p · OK) = p · σimg(OK) = p · OK.

So for every σ, we have that p · OK =(cid:81) pei

all ei are equal.

i =(cid:81)(σpi)ei. Since the action is transitive,

Let’s see an illustration of this.

54 Ramiﬁcation theory

507

Example 54.4.5 (Factoring 5 in a Galois/non-Galois extension)
Let p = 5 be a prime.

(a) Let E = Q( 3√2). One can show that OE = Z[ 3√2], so we use the Factoring
Algorithm on the minimal polynomial x3 − 2. Since x3 − 2 ≡ (x− 3)(x2 + 3x + 9)
(mod 5) is the irreducible factorization, we have that

(5) = (5, 3√2 − 3)(5, 3√4 + 3 3√2 + 9)

which have inertial degrees 1 and 2, respectively. The fact that this is not uniform
reﬂects that E is not Galois.

(b) Now let K = Q( 3√2, ω), which is the splitting ﬁeld of x3 − 2 over Q; now K is

Galois. It turns out that

OK = Z[ε] where

ε is a root of t6 + 3t5 − 5t3 + 3t + 1.

(this takes a lot of work to obtain, so we won’t do it here). Modulo 5 this has an
irreducible factorization (x2 + x + 2)(x2 + 3x + 3)(x2 + 4x + 1) (mod 5), so by
the Factorization Algorithm,

(5) = (5, ε2 + ε + 2)(5, ε2 + 3ε + 3)(5, ε2 + 4ε + 1).

This time all inertial degrees are 2, as the theorem predicts for K Galois.

§54.5 (Optional) Decomposition and inertia groups

Let p be a rational prime. Thus

p · OK = (p1 . . . pg)e

and all the pi have inertial degree f . Let p denote a choice of the pi.

We can look at both the ﬁelds OK/p and Z/p = Fp. Naturally, since OK/p is a ﬁnite

ﬁeld we can view it as a ﬁeld extension of OK. So we can get the diagram

⊃

Z

(p)

Fp.

At the far right we have ﬁnite ﬁeld extensions, which we know are really well behaved.
So we ask:

How are Gal ((OK/p)/Fp) and Gal(K/Q) related?

Absurdly enough, there is an explicit answer: it’s just the stabilizer of p, at least
when p is unramiﬁed.
Deﬁnition 54.5.1. Let Dp ⊆ Gal(K/Q) be the stabilizer of p, that is

We say Dp is the decomposition group of p.

Dp := {σ ∈ Gal(K/Q) | σp = p} .

⊃

OK

p

OK/p ∼= Fpf

K

Q

508

Napkin, by Evan Chen (v1.5.20190718)

Then, every σ ∈ Dp induces an automorphism of OK/p by

α (cid:55)→ σ(α)

(mod p).

So there’s a natural map

Dp

θ−→ Gal ((OK/p)/Fp)

by declaring θ(σ) to just be “σ (mod p)”. The fact that σ ∈ Dp (i.e. σ ﬁxes p) ensures
this map is well-deﬁned.

Theorem 54.5.2 (Decomposition group and Galois group)

Deﬁne θ as above. Then

 θ is surjective, and

 its kernel is a group of order e, the ramiﬁcation index.

In particular, if p is unramiﬁed then Dp ∼= Gal ((OK/p)/Fp).

(The proof is not hard, but a bit lengthy and in my opinion not very enlightening.)

If p is unramiﬁed, then taking modulo p gives Dp ∼= Gal ((OK/p)/Fp).
But we know exactly what Gal ((OK/p)/Fp) is! We already have OK/p ∼= Fpf , and

the Galois group is

Gal ((OK/p)/Fp) ∼= Gal(cid:0)Fpf /Fp(cid:1) ∼= (cid:104)x (cid:55)→ xp(cid:105) ∼= Z/fZ.

Dp ∼= Z/fZ

So

as well.

Let’s now go back to

Dp

θ−→ Gal ((OK/p) : Fp) .

The kernel of θ is called the inertia group and denoted Ip ⊆ Dp; it has order e.
This gives us a pretty cool sequence of subgroups {1} ⊆ I ⊆ D ⊆ G where G is the
Galois group (I’m dropping the p-subscripts now). Let’s look at the corresponding ﬁxed
ﬁelds via the Fundamental theorem of Galois theory. Picture:

p ⊆ OK ⊆ K 

-

{1}

Ramify

KI

Inert

KD

Split

I

e

f

D

g

(p) ⊆ Z ⊆ Q



- G

54 Ramiﬁcation theory

Something curious happens:

509

 When (p) is lifted into KD it splits completely into g unramiﬁed primes. Each of

these has inertial degree 1.

 When the primes in KD are lifted to KI , they remain inert, and now have inertial

degree f .

 When then lifted to K, they ramify with exponent e (but don’t split at all).

Picture: In other words, the process of going from 1 to ef g can be very nicely broken
into the three steps above. To draw this in the picture, we get

(p)

-

p1 . . . pg

-

p1 . . . pg

- (p1 . . . pg)e

{fi} :

1, . . . , 1

f, . . . , f

f, . . . , f

Q

KD

Split

Inert

KI

Ramify

K

In any case, in the “typical” case that there is no ramiﬁcation, we just have KI = K.

§54.6 Tangential remark: more general Galois extensions

All the discussion about Galois extensions carries over if we replace K/Q by some diﬀerent
Galois extension K/F . Instead of a rational prime p breaking down in OK, we would
have a prime ideal p of F breaking down as

p · OL = (P1 . . . Pg)e

in OL and then all results hold verbatim. (The Pi are primes in L above p.) Instead of
Fp we would have OF /p.
The reason I choose to work with F = Q is that capital Gothic P ’s (P) look really
terrifying.

§54.7 A few harder problems to think about

more prob-
more prob-
lems
lems

Problem 54A†. Prove that no rational prime p can remain inert in K = Q( 3√2, ω), the
splitting ﬁeld of x3 − 2. How does this generalize?

55 The Frobenius element

Throughout this chapter K/Q is a Galois extension with Galois group G, p is an

unramiﬁed rational prime in K, and p is a prime above it. Picture:

⊃

OK

p

OK/p ∼= Fpf

K

Q

If p is unramiﬁed, then one can show there is a unique σ ∈ Gal(L/K) such that σ(α) ≡ αp
(mod p) for every prime p.

⊃

Z

(p)

Fp

§55.1 Frobenius elements

Prototypical example for this section: Frobp in Z[i] depends on p (mod 4).

Here is the theorem statement again:

Theorem 55.1.1 (The Frobenius element)
Assume K/Q is Galois with Galois group G. Let p be a rational prime unramiﬁed in
K, and p a prime above it. There is a unique element Frobp ∈ G with the property
that

It is called the Frobenius element at p, and has order f .

Frobp(α) ≡ αp

(mod p).

The uniqueness part is pretty important: it allows us to show that a given σ ∈ Gal(L/K)
is the Frobenius element by just observing that it satisﬁes the above functional equation.

Let’s see an example of this:

Example 55.1.2 (Frobenius elements of the Gaussian integers)
Let’s actually compute some Frobenius elements for K = Q(i), which has OK = Z[i].
This is a Galois extension, with G = (Z/2Z)×, corresponding to the identity and
complex conjugation.

If p is an odd prime with p above it, then Frobp is the unique element such that

in Z[i]. In particular,

(a + bi)p ≡ Frobp(a + bi)

(mod p)

Frobp(i) = ip =(cid:40)i

p ≡ 1
−i p ≡ 3

(mod 4)
(mod 4).

From this we see that Frobp is the identity when p ≡ 1 (mod 4) and Frobp is complex
conjugation when p ≡ 3 (mod 4).

511

512

Napkin, by Evan Chen (v1.5.20190718)

Note that we really only needed to compute Frobp on i. If this seems too good to be
true, a philosophical reason is “freshman’s dream” where (x + y)p ≡ xp + yp (mod p)
(and hence mod p). So if σ satisﬁes the functional equation on generators, it satisﬁes the
functional equation everywhere.

We also have an important lemma:

Lemma 55.1.3 (Order of the Frobenius element)
Let Frobp be a Frobenius element from an extension K/Q. Then the order of p is
equal to the inertial degree fp. In particular, (p) splits completely in OK if and only
if Frobp = id.

Exercise 55.1.4. Prove this lemma as by using the fact that OK/p is the ﬁnite ﬁeld of
order fp, and the Frobenius element is just x (cid:55)→ xp on this ﬁeld.

Let us now prove the main theorem. This will only make sense in the context of

decomposition groups, so readers which skipped that part should omit this proof.

Proof of existence of Frobenius element. The entire theorem is just a rephrasing of the
fact that the map θ deﬁned in the last section is an isomorphism when p is unramiﬁed.
Picture:

In here we can restrict our attention to Dp since we need to have σ(α) ≡ 0 (mod p) when
α ≡ 0 (mod p). Thus we have the isomorphism

Dp

θ−→ Gal ((OK/p)/Fp) .

But we already know Gal ((OK/p)/Fp), according to the string of isomorphisms

Gal ((OK/p)/Fp) ∼= Gal(cid:0)Fpf /Fp(cid:1) ∼= (cid:104)T = x (cid:55)→ xp(cid:105) ∼= Z/fZ.

So the unique such element is the pre-image of T under θ.

§55.2 Conjugacy classes

Now suppose p1 and p2 are two primes above an unramiﬁed rational prime p. Then we
can deﬁne Frobp1 and Frobp2. Since the Galois group acts transitively, we can select
σ ∈ Gal(K/Q) be such that

σ(p1) = p2.

G=Gal(K/Q)DpFrobpTT2(cid:10)T|Tf=1(cid:11)θ(Frobp)=Tθ∼=513

55 The Frobenius element

We claim that

Frobp2 = σ ◦ Frobp1 ◦ σ−1.

Note that this is an equation in G.

Question 55.2.1. Prove this.

More generally, for a given unramiﬁed rational prime p, we obtain:

Theorem 55.2.2 (Conjugacy classes in Galois groups)

The set

{Frobp | p above p}

is one of the conjugacy classes of G.

Proof. We’ve used the fact that G = Gal(K/Q) is transitive to show that Frobp1 and
Frobp2 are conjugate if they both lie above p; hence it’s contained in some conjugacy
class. So it remains to check that for any p, σ, we have σ ◦ Frobp ◦ σ−1 = Frobp(cid:48) for some
p(cid:48). For this, just take p(cid:48) = σp. Hence the set is indeed a conjugacy class.

In summary,

Frobp is determined up to conjugation by the prime p from which p arises.

So even though the Gothic letters look scary, the content of Frobp really just comes from
the more friendly-looking rational prime p.

Example 55.2.3 (Frobenius elements in Q( 3√2, ω))
K = Q( 3√2, ω) be the splitting ﬁeld of

With those remarks, here is a more involved example of a Frobenius map. Let

t3 − 2 = (t − 3√2)(t − ω 3√2)(t − ω2 3√2).
Thus K/Q is Galois. We’ve seen in an earlier example that

OK = Z[ε] where

ε is a root of t6 + 3t5 − 5t3 + 3t + 1.

Let’s consider the prime 5 which factors (trust me here) as

(5) = (5, ε2 + ε + 2)(5, ε2 + 3ε + 3)(5, ε2 + 4ε + 1) = p1p2p3.

Note that all the prime ideals have inertial degree 2. Thus Frobpi will have order 2
for each i.

Note that

Gal(K/Q) = permutations of { 3√2, ω 3√2, ω2 3√2} ∼= S3.

In this S3 there are 3 elements of order three: ﬁxing one root and swapping the other
two. These correspond to each of Frobp1, Frobp2, Frobp3.

In conclusion, the conjugacy class {Frobp1, Frobp2, Frobp3} associated to (5) is the

cycle type (•)(• •) in S3.

514

Napkin, by Evan Chen (v1.5.20190718)

§55.3 Chebotarev density theorem

Natural question: can we represent every conjugacy class in this way? In other words, is
every element of G equal to Frobp for some p?

Miraculously, not only is the answer “yes”, but in fact it does so in the nicest way

possible: the Frobp’s are “equally distributed” when we pick a random p.

Theorem 55.3.1 (Chebotarev density theorem over Q)
Let C be a conjugacy class of G = Gal(K/Q). The density of (unramiﬁed) primes p
such that {Frobp | p above p} = C is exactly |C| /|G|. In particular, for any σ ∈ G
there are inﬁnitely many rational primes p with p above p so that Frobp = σ.

By density, I mean that the proportion of primes p ≤ x that work approaches |C|
as
|G|
x → ∞. Note that I’m throwing out the primes that ramify in K. This is no issue, since
the only primes that ramify are those dividing ∆K, of which there are only ﬁnitely many.
In other words, if I pick a random prime p and look at the resulting conjugacy class,
it’s a lot like throwing a dart at G: the probability of hitting any conjugacy class depends
just on the size of the class.

Remark 55.3.2 — Happily, this theorem (and preceding discussion) also works if
we replace K/Q with any Galois extension K/F ; in that case we replace “p over p”
with “P over p”. In that case, we use N(p) ≤ x rather than p ≤ x as the way to
deﬁne density.

§55.4 Example: Frobenius elements of cyclotomic ﬁelds

Let q be a prime, and consider L = Q(ζq), with q a primitive qth root of unity. You
should recall from various starred problems that

 ∆L = ±qq−2,
 OL = Z[ζq], and
 The map

σn : L → L by ζq (cid:55)→ ζ n

q

G37.5%37.5%C1C2C318.75%C46.25%55 The Frobenius element

515

is an automorphism of L whenever gcd(n, q) = 1, and depends only on n (mod q).
In other words, the automorphisms of L/Q just shuﬄe around the qth roots of
unity. In fact the Galois group consists exactly of the elements {σn}, namely

As a group,

Gal(L/Q) = {σn | n (cid:54)≡ 0

(mod q)}.

Gal(L/Q) = (Z/qZ)× ∼= Z/(q − 1)Z.

This is surprisingly nice, because elements of Gal(L/Q) look a lot like Frobenius
elements already. Speciﬁcally:

Lemma 55.4.1 (Cyclotomic Frobenius elements)
In the cyclotomic setting L = Q(ζq), let p be a rational unramiﬁed prime and p
above it. Then

Frobp = σp.

Proof. Observe that σp satisﬁes the functional equation (check on generators). Done by
uniqueness.

Question 55.4.2. Conclude that a rational prime p splits completely in OL if and only if
p ≡ 1 (mod m).

§55.5 Frobenius elements behave well with restriction

Let L/Q and K/Q be Galois extensions, and consider the setup
P ............- FrobP ∈ Gal(L/Q)

⊇

L

K

Q

⊇

⊇

p .............- Frobp ∈ Gal(K/Q)

(p)

Here p is above (p) and P is above p. We may deﬁne

and want to know how these are related.

Frobp : K → K and FrobP : L → L

Theorem 55.5.1 (Restrictions of Frobenius elements)
Assume L/Q and K/Q are both Galois. Let P and p be unramiﬁed as above. Then
FrobP(cid:22)K = Frobp, i.e. for every α ∈ K,

Frobp(α) = FrobP(α).

516

Proof. We know

from the deﬁnition.

Napkin, by Evan Chen (v1.5.20190718)

FrobP(α) ≡ αp

(mod P) ∀α ∈ OL

Question 55.5.2. Deduce that

FrobP(α) ≡ αp

(mod p) ∀α ∈ OK.

(This is weaker than the previous statement in two ways!)

Thus FrobP restricted to OK satisﬁes the characterizing property of Frobp.
In short, the point of this section is that

Frobenius elements upstairs restrict to Frobenius elements downstairs.

§55.6 Application: Quadratic reciprocity

We now aim to prove:

Theorem 55.6.1 (Quadratic reciprocity)

Let p and q be distinct odd primes. Then

(cid:18) p
q(cid:19)(cid:18) q

p(cid:19) = (−1)

p−1
2 · q−1
2 .

(See, e.g. [Le] for an exposition on quadratic reciprocity, if you’re not familiar with it.)

§55.6.i Step 1: Setup

For this proof, we ﬁrst deﬁne

L = Q(ζq)

where ζq is a primitive qth root of unity. Then L/Q is Galois, with Galois group G.

Question 55.6.2. Show that G has a unique subgroup H of order two.

In fact, we can describe it exactly: viewing G ∼= (Z/qZ)×, we have

H = {σn | n quadratic residue mod q} .

By the fundamental theorem of Galois Theory, there ought to be a degree 2 extension of
Q inside Q(ζq) (that is, a quadratic ﬁeld). Call it Q(√q∗), for q∗ squarefree:

L = Q(ζq)  -

{1}

q−1
2

K = Q(√q∗)

 -

H

2



Q

- G

55 The Frobenius element

517

Exercise 55.6.3. Note that if a rational prime (cid:96) ramiﬁes in K, then it ramiﬁes in L. Use
this to show that

∗

q

∗

Together these determine the value of q∗.

= ±q and q

≡ 1

(mod 4).

(Actually, it is true in general ∆K divides ∆L in a tower L/K/Q.)

§55.6.ii Step 2: Reformulation

Now we are going to prove:

Theorem 55.6.4 (Quadratic reciprocity, equivalent formulation)

For distinct odd primes p, q we have

q(cid:19) =(cid:18) q∗
p(cid:19) .
(cid:18) p
p (cid:17) = (−1)
Exercise 55.6.5. Using the fact that (cid:16)−1

quadratic reciprocity as we know it.

p−1
2 , show that this is equivalent to

We look at the rational prime p in Z. Either it splits into two in K or is inert; either
way let p be a prime factor in the resulting decomposition (so p is either p · OK in the
inert case, or one of the primes in the split case). Then let P be above p. It could
possibly also split in K: the picture looks like

OL = Z[ζq] ⊃ P ..........- Z[ζp]/P ∼= Fpf
OK = Z[ 1+√q∗
] ⊇ p ................- Fp or Fp2
⊇ (p) ......................- Fp

Z

2

Question 55.6.6. Why is p not ramiﬁed in either K or L?

§55.6.iii Step 3: Introducing the Frobenius

Now, we take the Frobenius

We claim that

σp = FrobP ∈ Gal(L/Q).

FrobP ∈ H ⇐⇒ p splits in K.

To see this, note that FrobP is in H if and only if it acts as the identity on K. But
FrobP(cid:22)K is Frobp! So

Finally note that Frobp has order 1 if p splits (p has inertial degree 1) and order 2 if p is
inert. This completes the proof of the claim.

FrobP ∈ H ⇐⇒ Frobp = idK.

518

Napkin, by Evan Chen (v1.5.20190718)

§55.6.iv Finishing up

We already know by Lemma 55.4.1 that FrobP = σp ∈ H if and only if p is a quadratic
residue. On the other hand,
p(cid:17) = 1. (Use

Exercise 55.6.7. Show that p splits in OK = Z[ 1
the factoring algorithm. You need the fact that p (cid:54)= 2 here.)

2 (1 + √q∗)] if and only if(cid:16) q∗

In other words

(cid:18) p
q(cid:19) = 1 ⇐⇒ σp ∈ H ⇐⇒ p splits in Z(cid:104) 1

2 (1 +(cid:112)q∗)(cid:105) ⇐⇒ (cid:18) q∗

p(cid:19) = 1.

This completes the proof.

§55.7 Frobenius elements control factorization

Prototypical example for this section: Frobp controlled the splitting of p in the proof of
quadratic reciprocity; the same holds in general.

In the proof of quadratic reciprocity, we used the fact that Frobenius elements behaved

well with restriction in order to relate the splitting of p with properties of Frobp.

In fact, there is a much stronger statement for any intermediate ﬁeld Q ⊆ E ⊆ K
which works even if E/Q is not Galois. It relies on the notion of a factorization pattern.
Here is how it goes.

Set n = [E : Q], and let p be a rational prime unramiﬁed in K. Then p can be broken

in E as

p · OE = p1p2 . . . pg

with inertial degrees f1, . . . , fg: (these inertial degrees might be diﬀerent since E/Q isn’t
Galois). The numbers f1 + ··· + fg = n form a partition of the number n. For example,
in the quadratic reciprocity proof we had n = 2, with possible partitions 1 + 1 (if p split)
and 2 (if p was inert). We call this the factorization pattern of p in E.

Next, we introduce a Frobenius FrobP above (p), all the way in K; this is an element

of G = Gal(K/Q). Then let H be the group corresponding to the ﬁeld E. Diagram:

K 

-

{1}

FrobP

E 

- H

n

n

p1 . . . pg

f1 + ··· + fg = n



Q

- G

(p)

Then FrobP induces a permutation of the n left cosets gH by left multiplication (after
all, FrobP is an element of G too!). Just as with any permutation, we may look at the
resulting cycle decomposition, which has a natural “cycle structure”: a partition of n.

55 The Frobenius element

519

The theorem is that these coincide:

Theorem 55.7.1 (Frobenius elements control decomposition)
Let Q ⊆ E ⊆ K an extension of number ﬁelds and assume K/Q is Galois (though
E/Q need not be). Pick an unramiﬁed rational prime p; let G = Gal(K/Q) and H
the corresponding intermediate subgroup. Finally, let P be a prime above p in K.
Then the factorization pattern of p in E is given by the cycle structure of FrobP

acting on the left cosets of H.

Often, we take E = K, in which case this is just asserting that the decomposition of the
prime p is controlled by a Frobenius element over it.

An important special case is when E = Q(α), because as we will see it is let us
determine how the minimal polynomial of α factors modulo p. To motivate this, let’s go
back a few chapters and think about the Factoring Algorithm.

Let α be an algebraic integer and f its minimal polynomial (of degree n). Set E = Q(α)
(which has degree n over Q). Suppose we’re lucky enough that OE = Z[α], i.e. that E is
monogenic. Then we know by the Factoring Algorithm, to factor any p in E, all we have
to do is factor f modulo p, since if f = f e1

(mod p) then we have

1 . . . f eg

g

(p) =(cid:89)i

pi =(cid:89)i

(fi(α), p)ei.

This gives us complete information about the ramiﬁcation indices and inertial degrees;

the ei are the ramiﬁcation indices, and deg fi are the inertial degrees (since OE/pi ∼=
Fp[X]/(fi(X))).
In particular, if p is unramiﬁed then all the ei are equal to 1, and we get

n = deg f = deg f1 + deg f2 + ··· + deg fg.

Once again we have a partition of n; we call this the factorization pattern of f modulo
p. So, to see the factorization pattern of an unramiﬁed p in OE, we just have to know
the factorization pattern of the f (mod p).

Turning this on its head, if we want to know the factorization pattern of f (mod p),
we just need to know how p decomposes. And it turns out these coincide even without
the assumption that E is monogenic.

g1Hg2Hg3H×g×g×g3g=FrobPg4Hg5Hg6Hg7H×g×g×g×g4n=7=3+4520

Napkin, by Evan Chen (v1.5.20190718)

Theorem 55.7.2 (Frobenius controls polynomial factorization)
Let α be an algebraic integer with minimal polynomial f , and let E = Q(α). Then
for any prime p unramiﬁed in the splitting ﬁeld K of f , the following coincide:

(i) The factorization pattern of p in E.

(ii) The factorization pattern of f (mod p).
(iii) The cycle structure associated to the action of FrobP ∈ Gal(K/Q) on the roots

of f , where P is above p in K.

Example 55.7.3 (Factoring x3 − 2 (mod 5))
Let α = 3√2 and f = x3 − 2, so E = Q( 3√2). Set p = 5 and let ﬁnally, let
K = Q( 3√2, ω) be the splitting ﬁeld. Setup:

K = Q( 3√2, ω)

2

E = Q( 3√2)

3

Q

P

p

(5)

x3 − 2 = (x − 3√2)(x − 3√2ω)(x − 3√2ω2)

x3 − 2 = (x − 3√2)(x2 + 3√2x + 3√4)

x3 − 2 irreducible over Q

The three claimed objects now all have shape 2 + 1:

(i) By the Factoring Algorithm, we have (5) = (5, 3√2 − 3)(5, 9 + 3 3√2 + 3√4).
(ii) We have x3 − 2 ≡ (x − 3)(x2 + 3x + 9) (mod 5).
(iii) We saw before that FrobP = (•)(• •).

Sketch of Proof. Letting n = deg f . Let H be the subgroup of G = Gal(K/Q) corre-
sponding to E, so [G : E] = n. Pictorially, we have

K

{1}

E = Q(α)

Q

H

G

P

p

(p)

We claim that (i), (ii), (iii) are all equivalent to

55 The Frobenius element

521

(iv) The pattern of the action of FrobP on the G/H.

In other words we claim the cosets correspond to the n roots of f in K. Indeed H is just
the set of τ ∈ G such that τ (α) = α, so there’s a bijection between the roots and the
cosets G/H by τ H (cid:55)→ τ (α). Think of it this way: if G = Sn, and H = {τ : τ (1) = 1},
then G/H has order n!/(n − 1)! = n and corresponds to the elements {1, . . . , n}. So
there is a natural bijection from (iii) to (iv).
The fact that (i) is in bijection to (iv) was the previous theorem, Theorem 55.7.1. The

correspondence (i) ⇐⇒ (ii) is a fact of Galois theory, so we omit the proof here.

All this can be done in general with Q replaced by F ; for example, in [Le02].

§55.8 Example application: IMO 2003 problem 6

As an example of the power we now have at our disposal, let’s prove:

Problem 6. Let p be a prime number. Prove that there
exists a prime number q such that for every integer n, the
number np − p is not divisible by q.

We will show, much more strongly, that there exist inﬁnitely many primes q such that
X p − p is irreducible modulo q.
Solution. Okay! First, we draw the tower of ﬁelds

where K is the splitting ﬁeld of f (x) = xp − p. Let E = Q( p√p) for brevity and note it
has degree [E : Q] = p. Let G = Gal(K/Q).

Q ⊆ Q( p√p) ⊆ K

Question 55.8.1. Show that p divides the order of G. (Look at E.)

Hence by Cauchy’s theorem (Problem 17A(cid:63), which is a purely group-theoretic fact)
we can ﬁnd a σ ∈ G of order p. By Chebotarev, there exist inﬁnitely many rational
(unramiﬁed) primes q (cid:54)= p and primes Q ⊆ OK above q such that FrobQ = σ. (Yes, that’s
an uppercase Gothic Q. Sorry.)

We claim that all these q work.
By Theorem 55.7.2, the factorization of f (mod q) is controlled by the action of
σ = FrobQ on the roots of f . But σ has prime order p in G! So all the lengths in the
cycle structure have to divide p. Thus the possible factorization patterns of f are

p = 1 + 1 + ··· + 1

p times

or p = p.

So we just need to rule out the p = 1 + ··· + 1 case now: this only happens if f breaks
into linear factors mod q. Intuitively this edge case seems highly unlikely (are we really
so unlucky that f factors into linear factors when we want it to be irreducible?). And
indeed this is easy to see: this means that σ ﬁxes all of the roots of f in K, but that
means σ ﬁxes K altogether, and hence is the identity of G, contradiction.

(cid:124)

(cid:123)(cid:122)

(cid:125)

522

Napkin, by Evan Chen (v1.5.20190718)

Remark 55.8.2 — In fact K = Q( p√p, ζp), and |G| = p(p − 1). With a little more
group theory, we can show that in fact the density of primes q that work is 1
p .

§55.9 A few harder problems to think about

Problem 55A. Show that for an odd prime p,

(cid:18) 2
p(cid:19) = (−1)

1

8 (p2−1).

Problem 55B. Let f be a nonconstant polynomial with integer coeﬃcients. Suppose f
(mod p) splits completely into linear factors for all suﬃciently large primes p. Show that
f splits completely into linear factors.

Problem 55C† (Dirichlet’s theorem on arithmetic progressions). Let a and m be
relatively prime positive integers. Show that the density of primes p ≡ a (mod m) is
exactly

1

φ(m) .

Problem 55D. Let n be an odd integer which is not a prime power. Show that the nth
cyclotomic polynomial is not irreducible modulo any rational prime.

Problem 55E (Putnam 2012 B6). Let p be an odd prime such that p ≡ 2 (mod 3). Let
π be a permutation of Fp by π(x) = x3 (mod p). Show that π is even if and only if p ≡ 3
(mod 4).

56 Bonus: A Bit on Artin Reciprocity

In this chapter, I’m going to state some big theorems of global class ﬁeld theory and use
them to deduce the Kronecker-Weber plus Hilbert class ﬁelds. No proofs, but hopefully
still appreciable. For experts: this is global class ﬁeld theory, without ideles.

Here’s the executive summary: let K be a number ﬁeld. Then all abelian extensions
L/K can be understood using solely information intrinsic to K: namely, the ray class
groups (generalizing ideal class groups).

§56.1 Inﬁnite primes
Prototypical example for this section: Q(√−5) has a complex inﬁnite prime, Q(√5) has

two real inﬁnite ones.

Let K be a number ﬁeld of degree n and signature (r, s). We know what a prime ideal
of OK is, but we now allow for the so-called inﬁnite primes, which I’ll describe using the
embeddings.1 Recall there are n embeddings σ : K → C, which consist of

 r real embeddings where im σ ⊆ R, and
 s pairs of conjugate complex embeddings.

Hence r + 2s = n. The ﬁrst class of embeddings form the real inﬁnite primes, while
the complex inﬁnite primes are the second type. We say K is totally real (resp
totally complex) if all its inﬁnite primes are real (resp complex).

Example 56.1.1 (Examples of inﬁnite primes)

 Q has a single real inﬁnite prime. We often write it as ∞.
 Q(√−5) has a single complex inﬁnite prime, and no real inﬁnite primes. Hence
 Q(√5) has two real inﬁnite primes, and no complex inﬁnite primes. Hence

totally complex.

totally real.

§56.2 Modular arithmetic with inﬁnite primes

A modulus is a formal product

m =(cid:89)p

pν(p)

where the product runs over all primes, ﬁnite and inﬁnite. (Here ν(p) is a nonnegative
integer, of which only ﬁnitely many are nonzero.) We also require that

 ν(p) = 0 for any complex inﬁnite prime p, and

 ν(p) ≤ 1 for any real inﬁnite prime p.

1This is not really the right deﬁnition; the “correct” way to think of primes, ﬁnite or inﬁnite, is in terms

of valuations. But it’ll be suﬃcient for me to state the theorems I want.

523

524

Napkin, by Evan Chen (v1.5.20190718)

Obviously, every m can be written as m = m0m∞ by separating the ﬁnite from the (real)
inﬁnite primes.

We say a ≡ b (mod m) if
 If p is a ﬁnite prime, then a ≡ b (mod pν(p)) means exactly what you think it

should mean: a − b ∈ pν(p).

 If p is a real inﬁnite prime σ : K → R, then a ≡ b (mod p) means that σ(a/b) > 0.
Of course, a ≡ b (mod m) means a ≡ b modulo each prime power in m. With this, we
can deﬁne a generalization of the class group:

Deﬁnition 56.2.1. Let m be a modulus of a number ﬁeld K.

 Let IK(m) to be the set of all fractional ideals of K which are relatively prime to

m, which is an abelian group.

 Let PK(m) be the subgroup of IK(m) generated by

This is sometimes called a “ray” of principal ideals.

(cid:8)αOK | α ∈ K× and α ≡ 1

(mod m)(cid:9) .

Finally deﬁne the ray class group of m to be CK(m) = IK(m)/PK(m).

This group is known to always be ﬁnite. Note the usual class group is CK(1).
One last deﬁnition that we’ll use right after Artin reciprocity:

Deﬁnition 56.2.2. A congruence subgroup of m is a subgroup H with

PK(m) ⊆ H ⊆ IK(m).

Thus CK(m) is a group which contains a lattice of various quotients IK(m)/H, where

H is a congruence subgroup.

This deﬁnition takes a while to get used to, so here are examples.

Example 56.2.3 (Ray class groups in Q, ﬁnite modulus)
Consider K = Q with inﬁnite prime ∞. Then

 If we take m = 1 then IQ(1) is all fractional ideals, and PQ(1) is all principal
fractional ideals. Their quotient is the usual class group of Q, which is trivial.

 Now take m = 8. Thus IQ(8) ∼=(cid:8) a
PQ(8) ∼=(cid:110) a

b

b Z | a/b ≡ 1, 3, 5, 7 (mod 8)(cid:9). Moreover
Z | a/b ≡ 1

(mod 8)(cid:111) .

You might at ﬁrst glance think that the quotient is thus (Z/8Z)×. But the
issue is that we are dealing with ideals: speciﬁcally, we have

7Z = −7Z ∈ PQ(8)

because −7 ≡ 1 (mod 8). So actually, we get

CQ(8) ∼= {1, 3, 5, 7 mod 8} /{1, 7 mod 8} ∼= (Z/4Z)×.

More generally,

CQ(m) = (Z/mZ)×/{±1}.

56 Bonus: A Bit on Artin Reciprocity

525

Example 56.2.4 (Ray class groups in Q, inﬁnite moduli)
Consider K = Q with inﬁnite prime ∞ again.
PQ(∞) =(cid:110) a

Z | a/b > 0(cid:111) .

b

 Now take m = ∞. As before IQ(∞) = Q×. Now, by deﬁnition we have

At ﬁrst glance you might think this was Q>0, but the same behavior with
ideals shows in fact PQ(∞) = Q×. So in this case, PQ(∞) still has all principal
fractional ideals. Therefore, CQ(∞) is still trivial.
b Z | a/b ≡ 1, 3, 5, 7 (mod 8)(cid:9).
 Finally, let m = 8∞. As before IQ(8∞) ∼= (cid:8) a

Now in this case:

PQ(8∞) ∼=(cid:110) a

b

Z | a/b ≡ 1

(mod 8) and a/b > 0(cid:111) .

This time, we really do have −7Z /∈ PQ(8∞): we have 7 (cid:54)≡ 1 (mod 8) and also
−8 < 0. So neither of the generators of 7Z are in PQ(8∞). Thus we ﬁnally
obtain

CQ(8∞) ∼= {1, 3, 5, 7 mod 8} /{1 mod 8} ∼= (Z/8Z)×

with the bijection CQ(8∞) → (Z/8Z)× given by aZ (cid:55)→ |a| (mod 8).

More generally,

CQ(m∞) = (Z/mZ)×.

§56.3 Inﬁnite primes in extensions

I want to emphasize that everything above is intrinsic to a particular number ﬁeld K.
After this point we are going to consider extensions L/K but it is important to keep in
mind the distinction that the concept of modulus and ray class group are objects deﬁned
solely from K rather than the above L.

Now take a Galois extension L/K of degree m. We already know prime ideals p of K
break into a product of prime ideals P of K in L in a nice way, so we want to do the
same thing with inﬁnite primes. This is straightforward: each of the n inﬁnite primes
σ : K → C lifts to m inﬁnite primes τ : L → C, by which I mean the diagram

L

τ

C

σ

K

commutes. Hence like before, each inﬁnite prime σ of K has m inﬁnite primes τ of L
which lie above it.

For a real prime σ of K, any of the resulting τ above it are complex, we say that the
prime σ ramiﬁes in the extension L/K. Otherwise it is unramiﬁed in L/K. An inﬁnite
prime of K is always unramiﬁed in L/K. In this way, we can talk about an unramiﬁed
Galois extension L/K: it is one where all primes (ﬁnite or inﬁnite) are unramiﬁed.

526

Napkin, by Evan Chen (v1.5.20190718)

Example 56.3.1 (Ramiﬁcation of ∞)
Let ∞ be the real inﬁnite prime of Q.

 ∞ is ramiﬁed in Q(√−5)/Q.
 ∞ is unramiﬁed in Q(√5)/Q.

Note also that if K is totally complex then any extension L/K is unramiﬁed.

§56.4 Frobenius element and Artin symbol

Recall the key result:

Theorem 56.4.1 (Frobenius element)
Let L/K be a Galois extension. If p is a prime unramiﬁed in K, and P a prime
above it in L. Then there is a unique element of Gal(L/K), denoted FrobP, obeying

FrobP(α) ≡ αN p

(mod P)

∀α ∈ OL.

Example 56.4.2 (Example of Frobenius elements)
Let L = Q(i), K = Q. We have Gal(L/K) ∼= Z/2Z.

If p is an odd prime with P above it, then FrobP is the unique element such that

in Z[i]. In particular,

(a + bi)p ≡ FrobP(a + bi)

(mod P)

FrobP(i) = ip =(cid:40)i

p ≡ 1
−i p ≡ 3

(mod 4)
(mod 4).

From this we see that FrobP is the identity when p ≡ 1 (mod 4) and FrobP is
complex conjugation when p ≡ 3 (mod 4).

Example 56.4.3 (Cyclotomic Frobenius element)
Generalizing previous example, let L = Q(ζ) and K = Q, with ζ an mth root of
unity. It’s well-known that L/K is unramiﬁed outside ∞ and prime factors of m.
Moreover, the Galois group Gal(L/K) is (Z/mZ)×: the Galois group consists of
elements of the form

and Gal(L/K) = {σn | n ∈ (Z/mZ)×}.

Then it follows just like before that if p (cid:45) n is prime and P is above p

σn : ζ (cid:55)→ ζ n

FrobP(x) = σp.

An important property of the Frobenius element is its order is related to the decompo-

sition of p in the higher ﬁeld L in the nicest way possible:

56 Bonus: A Bit on Artin Reciprocity

527

Lemma 56.4.4 (Order of the Frobenius element)
The Frobenius element FrobP ∈ Gal(L/K) of an extension L/K has order equal to
the inertial degree of P, that is,

In particular, FrobP = id if and only if p splits completely in L/K.

ord FrobP = f (P | p).

Proof. We want to understand the order of the map T : x (cid:55)→ xN p on the ﬁeld OK/P.
But the latter is isomorphic to the splitting ﬁeld of X N P − X in Fp, by Galois theory of
ﬁnite ﬁelds. Hence the order is logN p(N P) = f (P | p).

Exercise 56.4.5. Deduce from this that the rational primes which split completely in Q(ζ)
are exactly those with p ≡ 1 (mod m). Here ζ is an mth root of unity.

The Galois group acts transitively among the set of P above a given p, so that we have

Frobσ(P) = σ ◦ (Frobp) ◦ σ−1.

Thus FrobP is determined by its underlying p up to conjugation.

In class ﬁeld theory, we are interested in abelian extensions, i.e. those for which
Gal(L/K) is Galois. Here the theory becomes extra nice: the conjugacy classes have size
one.

Deﬁnition 56.4.6. Assume L/K is an abelian extension. Then for a given unramiﬁed
prime p in K, the element FrobP doesn’t depend on the choice of P. We denote the
resulting FrobP by the Artin symbol,

(cid:18) L/K
p (cid:19) .

The deﬁnition of the Artin symbol is written deliberately to look like the Legendre

symbol. To see why:

Example 56.4.7 (Legendre symbol subsumed by Artin symbol)
Suppose we want to understand (2/p) ≡ 2
element

p−1
2 where p > 2 is prime. Consider the

(cid:32)Q(√2)/Q
p(cid:17) is the usual Legendre symbol, and P is above p in Q(√2). Thus the

√2 ≡(cid:18) 2

p(cid:19)√2

(mod P)

≡ 2

p−1
2

·

pZ

where (cid:16) 2

Artin symbol generalizes the quadratic Legendre symbol.

It is uniquely determined by where it sends a. But in fact we have

(cid:33) ∈ Gal(Q(√2)/Q).

pZ

(cid:32)Q(√2)/Q
(cid:33)(cid:16)√2(cid:17) ≡(cid:16)√2(cid:17)p

528

Napkin, by Evan Chen (v1.5.20190718)

Example 56.4.8 (Cubic Legendre symbol subsumed by Artin symbol)
Similarly, it also generalizes the cubic Legendre symbol. To see this, assume θ is

primary in K = Q(√−3) = Q(ω) (thus OK = Z[ω] is Eisenstein integers). Then for

example

(cid:32) K( 3√2)/K
θOK (cid:33)(cid:16) 3√2(cid:17) ≡(cid:16) 3√2(cid:17)N (θ)

where P is above p in K( 3√2).

N θ−1

3

≡ 2

√2 ≡(cid:18) 2
θ(cid:19)3

·

3√2.

(mod P)

§56.5 Artin reciprocity

Now, we further capitalize on the fact that Gal(L/K) is abelian. For brevity, in what
follows let Ram(L/K) denote the primes of K (either ﬁnite or inﬁnite) which ramify in
L.

Deﬁnition 56.5.1. Let L/K be an abelian extension and let m be divisible by every
prime in Ram(L/K). Then since L/K is abelian we can extend the Artin symbol
multiplicatively to a map

(cid:18) L/K
• (cid:19) : IK(m) (cid:16) Gal(L/K).

This is called the Artin map, and it is surjective (for example by Chebotarev Density).
Thus we denote its kernel by

In particular we have Gal(L/K) ∼= IK(m)/H(L/K, m).
We can now present the long-awaited Artin reciprocity theorem.

H(L/K, m) ⊆ IK(m).

Theorem 56.5.2 (Artin reciprocity)

Let L/K be an abelian extension. Then there is a modulus f = f(L/K), divisible
by exactly the primes of Ram(L/K), such that: for any modulus m divisible by all
primes of Ram(L/K), we have

PK(m) ⊆ H(L/K, m) ⊆ IK(m)

if and only if

f | m.

We call f the conductor of L/K.

So the conductor f plays a similar role to the discriminant (divisible by exactly the primes
which ramify), and when m is divisible by the conductor, H(L/K, m) is a congruence
subgroup.

Here’s the reason this is called a “reciprocity” theorem. Recalling that CK(f) =

IK(f)/PK(f), the above theorem tells us we get a sequence of maps

IK(f)

CK(f)

Gal(L/K)

(cid:17)

(cid:16) L/K•

∼=

IK(f)/H(L/K, f)

56 Bonus: A Bit on Artin Reciprocity

529

Consequently:

For primes p ∈ IK(f), (cid:16) L/K

p (cid:17) depends only on “p (mod f)”.

Let’s see how this result relates to quadratic reciprocity.

Example 56.5.3 (Artin reciprocity implies quadratic reciprocity)
The big miracle of quadratic reciprocity states that: for a ﬁxed (squarefree) a, the

Legendre symbol(cid:16) a
Let L = Q(√a), K = Q. Then we’ve already seen that the Artin symbol

p(cid:17) should only depend the residue of p modulo something. Let’s

see why Artin reciprocity tells us this a priori.

(cid:18)Q(√a)/Q

•

(cid:19)

is the correct generalization of the Legendre symbol. Thus, Artin reciprocity tells us

that there is a conductor f = f(Q(√a)/Q) such that(cid:16) Q(√a)/Q

residue of p modulo f, which is what we wanted.

p

(cid:17) depends only on the

Here is an example along the same lines.

Example 56.5.4 (Cyclotomic ﬁeld)
Let ζ be a primitive mth root of unity. For primes p, we know that Frobp ∈
Gal(Q(ζ)/Q) is “exactly” p (mod m). Let’s translate this idea into the notation of
Artin reciprocity.

We are going to prove

H(Q(ζ)/Q, m∞) = PQ(m∞) =(cid:110) a

b

Z | a/b ≡ 1

(mod m)(cid:111) .

It’s well-known Q(ζ)/Q is unramiﬁed outside ﬁnite primes dividing m, so that the

This is the generic example of achieving the lower bound in Artin reciprocity. It also
implies that f(Q(ζ)/Q) | m∞.
(cid:16) Q(ζ)/Q
Artin symbol is deﬁned on IK(m). Now the Artin map is given by

(cid:17)

•

IQ(m)

p

Gal(Q(ζ)/Q)

(x (cid:55)→ xp)

∼=

(Z/mZ)×

p (mod m).

So we see that the kernel of this map is trivial, i.e. it is given by the identity of the
Galois group, corresponding to 1 (mod m). On the other hand, we’ve also computed
PQ(m∞) already, so we have the desired equality.

In fact, we also have the following “existence theorem”: every congruence subgroup

appears uniquely once we ﬁx m.

530

Napkin, by Evan Chen (v1.5.20190718)

Theorem 56.5.5 (Takagi existence theorem)

Fix K and let m be a modulus. Consider any congruence subgroup H, i.e.

Then H = H(L/K, m) for a unique abelian extension L/K.

PK(m) ⊆ H ⊆ IK(m).

Finally, such subgroups reverse inclusion in the best way possible:

Lemma 56.5.6 (Inclusion-reversing congruence subgroups)

Fix a modulus m. Let L/K and M/K be abelian extensions and suppose m is
divisible by the conductors of L/K and M/K. Then

L ⊆ M if and only if H(M/K, m) ⊆ H(L/K, m).

Here by L ⊆ M we mean that L is isomorphic to some subﬁeld of M .
Sketch of proof. Let us ﬁrst prove the equivalence with m ﬁxed. In one direction, assume
L ⊆ M ; one can check from the deﬁnitions that the diagram

(cid:17)

(cid:16) M/K•
(cid:17)
(cid:16) L/K•

IK(m)

Gal(M/K)

Gal(L/K)

commutes, because it suﬃces to verify this for prime powers, which is just saying that
Frobenius elements behave well with respect to restriction. Then the inclusion of kernels
follows directly. The reverse direction is essentially the Takagi existence theorem.

Note that we can always take m to be the product of conductors here.

To ﬁnish, here is a quote from Emil Artin on his reciprocity law:

I will tell you a story about the Reciprocity Law. After my thesis, I had the
idea to deﬁne L-series for non-abelian extensions. But for them to agree with
the L-series for abelian extensions, a certain isomorphism had to be true. I
could show it implied all the standard reciprocity laws. So I called it the
General Reciprocity Law and tried to prove it but couldn’t, even after many
tries. Then I showed it to the other number theorists, but they all laughed at
it, and I remember Hasse in particular telling me it couldn’t possibly be true.

Still, I kept at it, but nothing I tried worked. Not a week went by — for three
years! — that I did not try to prove the Reciprocity Law. It was discouraging,
and meanwhile I turned to other things. Then one afternoon I had nothing
special to do, so I said, ‘Well, I try to prove the Reciprocity Law again.’ So
I went out and sat down in the garden. You see, from the very beginning I
had the idea to use the cyclotomic ﬁelds, but they never worked, and now I
suddenly saw that all this time I had been using them in the wrong way —
and in half an hour I had it.

56 Bonus: A Bit on Artin Reciprocity

531

§56.6 A few harder problems to think about

Problem 56A† (Kronecker-Weber theorem). Let L be an abelian extension of Q. Then
L is contained in a cyclic extension Q(ζ) where ζ is an mth root of unity (for some m).

Problem 56B† (Hilbert class ﬁeld). Let K be any number ﬁeld. Then there exists a
unique abelian extension E/K which is unramiﬁed at all primes (ﬁnite or inﬁnite) and
such that

 E/K is the maximal such extension by inclusion.

 Gal(E/K) is isomorphic to the class group of E.

 A prime p of K splits completely in E if and only if it is a principal ideal of OK.

We call E the Hilbert class ﬁeld of K.

XV

Algebraic Topology I: Homotopy

Part XV: Contents

57 Some topological constructions

535
57.1 Spheres . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 535
57.2 Quotient topology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 535
57.3 Product topology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 536
57.4 Disjoint union and wedge sum . . . . . . . . . . . . . . . . . . . . . . . . . . . . 537
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 538
57.5 CW complexes
57.6 The torus, Klein bottle, RPn, CPn . . . . . . . . . . . . . . . . . . . . . . . . . . 539
. . . . . . . . . . . . . . . . . . . . . . . . 545
57.7 A few harder problems to think about

58 Fundamental groups

547
58.1 Fusing paths together . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 547
58.2 Fundamental groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 548
. . . . . . . . . . . . . . . . . . . . . . . . . . 552
58.3 Fundamental groups are functorial
58.4 Higher homotopy groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 553
58.5 Homotopy equivalent spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 554
58.6 The pointed homotopy category . . . . . . . . . . . . . . . . . . . . . . . . . . . 556
. . . . . . . . . . . . . . . . . . . . . . . . 557
58.7 A few harder problems to think about

59 Covering projections

559
. . . . . . . . . . . . . . . . . . . . . . . 559
59.1 Even coverings and covering projections
59.2 Lifting theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 561
59.3 Lifting correspondence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 563
59.4 Regular coverings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 564
59.5 The algebra of fundamental groups . . . . . . . . . . . . . . . . . . . . . . . . . . 566
. . . . . . . . . . . . . . . . . . . . . . . . 568
59.6 A few harder problems to think about

57 Some topological constructions

In this short chapter we brieﬂy describe some common spaces and constructions in

topology that we haven’t yet discussed.

§57.1 Spheres

Recall that

is the surface of an n-sphere while

Sn =(cid:8)(x0, . . . , xn) | x2
Dn+1 =(cid:8)(x0, . . . , xn) | x2

0 + ··· + x2

0 + ··· + x2

n = 1(cid:9) ⊂ Rn+1
n ≤ 1(cid:9) ⊂ Rn+1

is the corresponding closed ball (So for example, D2 is a disk in a plane while S1 is the
unit circle.)

Exercise 57.1.1. Show that the open ball Dn \ Sn−1 is homeomorphic to Rn.

In particular, S0 consists of two points, while D1 can be thought of as the interval

[−1, 1].

§57.2 Quotient topology

Prototypical example for this section: Dn/Sn−1 = Sn, or the torus.

Given a space X, we can identify some of the points together by any equivalence
relation ∼; for an x ∈ X we denote its equivalence class by [x]. Geometrically, this is the
space achieved by welding together points equivalent under ∼.

Formally,

Deﬁnition 57.2.1. Let X be a topological space, and ∼ an equivalence relation on the
points of X. Then X/∼ is the space whose

 Points are equivalence classes of X, and

 U ⊆ X/∼ is open if and only if {x ∈ X such that [x] ∈ U} is open in X.
As far as I can tell, this deﬁnition is mostly useless for intuition, so here are some

examples.

535

S0D1D2S1536

Napkin, by Evan Chen (v1.5.20190718)

Example 57.2.2 (Interval modulo endpoints)
Suppose we take D1 = [−1, 1] and quotient by the equivalence relation which identiﬁes
the endpoints −1 and 1. (Formally, x ∼ y ⇐⇒ (x = y) or {x, y} = {−1, 1}.) In
that case, we simply recover S1:

Observe that a small open neighborhood around −1 ∼ 1 in the quotient space
corresponds to two half-intervals at −1 and 1 in the original space D1. This should
convince you the deﬁnition we gave is the right one.

Example 57.2.3 (More quotient spaces)
Convince yourself that:

 Generalizing the previous example, Dn modulo its boundary Sn−1 is Sn.

 Given a square ABCD, suppose we identify segments AB and DC together.
Then we get a cylinder. (Think elementary school, when you would tape up
pieces of paper together to get cylinders.)

 In the previous example, if we also identify BC and DA together, then we get
a torus. (Imagine taking our cylinder and putting the two circles at the end
together.)

 Let X = R, and let x ∼ y if y − x ∈ Z. Then X/∼ is S1 as well.

One special case that we did above:

Deﬁnition 57.2.4. Let A ⊆ X. Consider the equivalence relation which identiﬁes all
the points of A with each other while leaving all remaining points inequivalent. (In other
words, x ∼ y if x = y or x, y ∈ A.) Then the resulting quotient space is denoted X/A.

So in this notation,

Dn/Sn−1 = Sn.

Abuse of Notation 57.2.5. Note that I’m deliberately being sloppy, and saying
“Dn/Sn−1 = Sn” or “Dn/Sn−1 is Sn”, when I really ought to say “Dn/Sn−1 is homeo-
morphic to Sn”. This is a general theme in mathematics: objects which are homoeomor-
phic/isomorphic/etc. are generally not carefully distinguished from each other.

§57.3 Product topology

Prototypical example for this section: R × R is R2, S1 × S1 is the torus.
Deﬁnition 57.3.1. Given topological spaces X and Y , the product topology on X×Y
is the space whose

−1−1D1S1≈D1/∼−1∼157 Some topological constructions

537

 Points are pairs (x, y) with x ∈ X, y ∈ Y , and
 Topology is given as follows: the basis of the topology for X × Y is U × V , for

U ⊆ X open and V ⊆ Y open.

Remark 57.3.2 — It is not hard to show that, in fact, one need only consider basis
elements for U and V . That is to say,

{U × V | U, V basis elements for X, Y }

We really do need to ﬁddle with the basis: in R × R, an open unit disk better be

is also a basis for X × Y .
open, despite not being of the form U × V .
This does exactly what you think it would.

Example 57.3.3 (The unit square)
Let X = [0, 1] and consider X × X. We of course expect this to be the unit square.
Pictured below is an open set of X × X in the basis.

Exercise 57.3.4. Convince yourself this basis gives the same topology as the product metric
on X × X. So this is the “right” deﬁnition.

Example 57.3.5 (More product spaces)
(a) R × R is the Euclidean plane.
(b) S1 × [0, 1] is a cylinder.
(c) S1 × S1 is a torus! (Why?)

§57.4 Disjoint union and wedge sum

Prototypical example for this section: S1 ∨ S1 is the ﬁgure eight.

The disjoint union of two spaces is geometrically exactly what it sounds like: you just

imagine the two spaces side by side. For completeness, here is the formal deﬁnition.

U×VUV538

Napkin, by Evan Chen (v1.5.20190718)

Deﬁnition 57.4.1. Let X and Y be two topological spaces. The disjoint union,
denoted X (cid:113) Y , is deﬁned by

 The points are the disjoint union X (cid:113) Y , and
 A subset U ⊆ X (cid:113) Y is open if and only if U ∩ X and U ∩ Y are open.
Exercise 57.4.2. Show that the disjoint union of two nonempty spaces is disconnected.

More interesting is the wedge sum, where two topological spaces X and Y are fused

together only at a single base point.
Deﬁnition 57.4.3. Let X and Y be topological spaces, and x0 ∈ X and y0 ∈ Y be
points. We deﬁne the equivalence relation ∼ by declaring x0 ∼ y0 only. Then the wedge
sum of two spaces is deﬁned as

X ∨ Y = (X (cid:113) Y )/∼.

Example 57.4.4 (S1 ∨ S1 is a ﬁgure eight)
Let X = S1 and Y = S1, and let x0 ∈ X and y0 ∈ Y be any points. Then X ∨ Y is
a “ﬁgure eight”: it is two circles fused together at one point.

Abuse of Notation 57.4.5. We often don’t mention x0 and y0 when they are understood
(or irrelevant). For example, from now on we will just write S1 ∨ S1 for a ﬁgure eight.

Remark 57.4.6 — Annoyingly, in LATEX \wedge gives ∧ instead of ∨ (which is
\vee). So this really should be called the “vee product”, but too late.

§57.5 CW complexes

Using this construction, we can start building some spaces. One common way to do so is
using a so-called CW complex. Intuitively, a CW complex is built as follows:

 Start with a set of points X 0.

 Deﬁne X 1 by taking some line segments (copies of D1) and fusing the endpoints

(copies of S0) onto X 0.

 Deﬁne X 2 by taking copies of D2 (a disk) and welding its boundary (a copy of S1)

onto X 1.

 Repeat inductively up until a ﬁnite stage n; we say X is n-dimensional.

The resulting space X is the CW-complex. The set X k is called the k-skeleton of X.
Each Dk is called a k-cell; it is customary to denote it by ek
α where α is some index. We
say that X is ﬁnite if only ﬁnitely many cells were used.

Abuse of Notation 57.5.1. Technically, most sources (like [Ha02]) allow one to construct
inﬁnite-dimensional CW complexes. We will not encounter any such spaces in the Napkin.

57 Some topological constructions

539

Example 57.5.2 (D2 with 2 + 2 + 1 and 1 + 1 + 1 cells)
(a) First, we start with X 0 having two points e0
a and e0
b . Then, we join them with
c and e1
two 1-cells D1 (green), call them e1
d. The endpoints of each 1-cell (the
copy of S0) get identiﬁed with distinct points of X 0; hence X 1 ∼= S1. Finally,
we take a single 2-cell e2 (yellow) and weld it in, with its boundary ﬁtting into
the copy of S1 that we just drew. This gives the ﬁgure on the left.

(b) In fact, one can do this using just 1 + 1 + 1 = 3 cells. Start with X 0 having a
single point e0. Then, use a single 1-cell e1, fusing its two endpoints into the
single point of X 0. Then, one can ﬁt in a copy of S1 as before, giving D2 as on
the right.

Example 57.5.3 (Sn as a CW complex)
(a) One can obtain Sn (for n ≥ 1) with just two cells. Namely, take a single point

e0 for X 0, and to obtain Sn take Dn and weld its entire boundary into e0.

We already saw this example in the beginning with n = 2, when we saw that the
sphere S2 was the result when we fuse the boundary of a disk D2 together.

(b) Alternatively, one can do a “hemisphere” construction, by constructing Sn
inductively using two cells in each dimension. So S0 consists of two points, then
S1 is obtained by joining these two points by two segments (1-cells), and S2 is
obtained by gluing two hemispheres (each a 2-cell) with S1 as its equator.

Deﬁnition 57.5.4. Formally, for each k-cell ek
boundary Sk−1

and weld it onto X k−1 via an attaching map Sk−1

α we want to add to X k, we take its

α

α → X k−1. Then

X k = X k−1 (cid:113)(cid:32)(cid:97)α

α(cid:33) /∼

ek

where ∼ identiﬁes each boundary point of ek
§57.6 The torus, Klein bottle, RPn, CPn

α with its image in X k−1.

We now present four of the most import examples of CW complexes.

§57.6.i The torus

The torus can be formed by taking a square and identifying the opposite edges in the
same direction: if you walk oﬀ the right edge, you re-appear at the corresponding point
in on the left edge. (Think Asteroids from Atari!)

e0ae0be1ce1de2e0e1e2540

Napkin, by Evan Chen (v1.5.20190718)

Thus the torus is (R/Z)2 ∼= S1 × S1.

Note that all four corners get identiﬁed together to a single point. One can realize the
torus in 3-space by treating the square as a sheet of paper, taping together the left and
right (red) edges to form a cylinder, then bending the cylinder and fusing the top and
bottom (blue) edges to form the torus.

The torus can be realized as a CW complex with

 A 0-skeleton consisting of a single point,

Image from [To]

 A 1-skeleton consisting of two 1-cells e1

a, e1

b , and

 A 2-skeleton with a single 2-cell e2, whose circumference is divided into four parts,
and welded onto the 1-skeleton “via aba−1b−1”. This means: wrap a quarter of the
circumference around e1
b , then the third quarter
around e1
b again in
the opposite direction as before.

a but in the opposite direction, and the fourth quarter around e1

a, then another quarter around e1

e1ae1be0e257 Some topological constructions

541

We say that aba−1b−1 is the attaching word; this shorthand will be convenient later
on.

§57.6.ii The Klein bottle

The Klein bottle is deﬁned similarly to the torus, except one pair of edges is identiﬁed
in the opposite manner, as shown.

Unlike the torus one cannot realize this in 3-space without self-intersecting. One can
tape together the red edges as before to get a cylinder, but to then fuse the resulting
blue circles in opposite directions is not possible in 3D. Nevertheless, we often draw a
picture in 3-dimensional space in which we tacitly allow the cylinder to intersect itself.

Image from [In; Fr]

Like the torus, the Klein bottle is realized as a CW complex with

 One 0-cell,

542

Napkin, by Evan Chen (v1.5.20190718)

 Two 1-cells e1

a and e1

b , and

 A single 2-cell attached this time via the word abab−1.

§57.6.iii Real projective space

Let’s start with n = 2. The space RP2 is obtained if we reverse both directions of the
square from before, as shown.

However, once we do this the fact that the original polygon is a square is kind of
irrelevant; we can combine a red and blue edge to get the single purple edge. Equivalently,
one can think of this as a circle with half its circumference identiﬁed with the other half:

The resulting space should be familiar to those of you who do projective (Euclidean)

geometry. Indeed, there are several possible geometric interpretations:

 One can think of RP2 as the set of lines through the origin in R3, with each line

being a point in RP2.
Of course, we can identify each line with a point on the unit sphere S2, except
for the property that two antipodal points actually correspond to the same line,
so that RP2 can be almost thought of as “half a sphere”. Flattening it gives the
picture above.

 Imagine R2, except augmented with “points at inﬁnity”. This means that we add
some points “inﬁnitely far away”, one for each possible direction of a line. Thus in
RP2, any two lines indeed intersect (at a Euclidean point if they are not parallel,
and at a point at inﬁnity if they do).
This gives an interpretation of RP2, where the boundary represents the line at
inﬁnity through all of the points at inﬁnity. Here we have used the fact that R2
and interior of D2 are homeomorphic.

Exercise 57.6.1. Observe that these formulations are equivalent by considering the plane
z = 1 in R3, and intersecting each line in the ﬁrst formulation with this plane.

We can also express RP2 using coordinates: it is the set of triples (x : y : z) of real

numbers not all zero up to scaling, meaning that

(x : y : z) = (λx : λy : λz)

RP2RP257 Some topological constructions

543

for any λ (cid:54)= 0. Using the “lines through the origin in R3” interpretation makes it clear
why this coordinate system gives the right space. The points at inﬁnity are those with
z = 0, and any point with z (cid:54)= 0 gives a Cartesian point since

y
z
z , y
hence we can think of it as the Cartesian point ( x

(x : y : z) =(cid:16) x

z

:

: 1(cid:17)

z ).

In this way we can actually deﬁne real-projective n-space, RPn for any n, as either
(i) The set of lines through the origin in Rn+1,

(ii) Using n + 1 coordinates as above, or

(iii) As Rn augmented with points at inﬁnity, which themselves form a copy of RPn−1.

As a possibly helpful example, we give all three pictures of RP1.

Example 57.6.2 (Real projective 1-Space)
RP1 can be thought of as S1 modulo the relation the antipodal points are identiﬁed.
Projecting onto a tangent line, we see that we get a copy of R plus a single point at
inﬁnity, corresponding to the parallel line (drawn in cyan below).

Thus, the points of RP1 have two forms:
 (x : 1), which we think of as x ∈ R (in dark red above), and
 (1 : 0), which we think of as 1/0 = ∞, corresponding to the cyan line above.

So, we can literally write

Note that RP1 is also the boundary of RP2. In fact, note also that topologically we
have

RP1 = R ∪ {∞}.

RP1 ∼= S1

since it is the “real line with endpoints fused together”.

Since RPn is just “Rn (or Dn) with RPn−1 as its boundary”, we can construct RPn as a
CW complex inductively. Note that RPn thus consists of one cell in each dimension.

S1~0010.36R∞0544

Napkin, by Evan Chen (v1.5.20190718)

Example 57.6.3 (RPn as a cell complex)
(a) RP0 is a single point.
(b) RP1 ∼= S1 is a circle, which as a CW complex is a 0-cell plus a 1-cell.
(c) RP2 can be formed by taking a 2-cell and wrapping its perimeter twice around a

copy of RP1.

§57.6.iv Complex projective space

The complex projective space CPn is deﬁned like RPn with coordinates, i.e.

under scaling; this time zi are complex. As before, CPn can be thought of as Cn
augmented with some points at inﬁnity (corresponding to CPn−1).

(z0 : z1 : ··· : zn)

Example 57.6.4 (Complex projective space)
(a) CP0 is a single point.

(b) CP1 is C plus a single point at inﬁnity (“complex inﬁnity” if you will). That

means as before we can think of CP1 as

CP1 = C ∪ {∞}.

So, imagine taking the complex plane and then adding a single point to encompass
the entire boundary. The result is just sphere S2.

Here is a picture of CP1 with its coordinate system, the Riemann sphere.

57 Some topological constructions

545

Remark 57.6.5 (For Euclidean geometers) — You may recognize that while RP2 is
the setting for projective geometry, inversion about a circle is done in CP1 instead.
When one does an inversion sending generalized circles to generalized circles, there
is only one point at inﬁnity: this is why we work in CPn.

Like RPn, CPn is a CW complex, built inductively by taking Cn and welding its

boundary onto CPn−1 The diﬀerence is that as topological spaces,

Cn ∼= R2n ∼= D2n.

Thus, we attach the cells D0, D2, D4 and so on inductively to construct CPn. Thus we
see that

CPn consists of one cell in each even dimension.

§57.7 A few harder problems to think about

Problem 57A. Show that a space X is Hausdorﬀ if and only if the diagonal {(x, x) |
x ∈ X} is closed in the product space X × X.
Problem 57B. Realize the following spaces as CW complexes:

(a) M¨obius strip.

(b) R.

(c) Rn.

Problem 57C†. Show that a ﬁnite CW complex is compact.

58 Fundamental groups

Topologists can’t tell the diﬀerence between a coﬀee cup and a doughnut. So how do

you tell anything apart?

This is a very hard question to answer, but one way we can try to answer it is to ﬁnd
some invariants of the space. To draw on the group analogy, two groups are clearly not
isomorphic if, say, they have diﬀerent orders, or if one is simple and the other isn’t, etc.
We’d like to ﬁnd some similar properties for topological spaces so that we can actually
tell them apart.

Two such invariants for a space X are

 Deﬁning homology groups H1(X), H2(X), . . .

 Deﬁning homotopy groups π1(X), π2(X), . . .

Homology groups are hard to deﬁne, but in general easier to compute. Homotopy groups
are easier to deﬁne but harder to compute.

This chapter is about the fundamental group π1.

§58.1 Fusing paths together

Recall that a path in a space X is a function [0, 1] → X. Suppose we have paths γ1 and
γ2 such that γ1(1) = γ2(0). We’d like to fuse1 them together to get a path γ1 ∗ γ2. Easy,
right?

We unfortunately do have to hack the deﬁnition a tiny bit. In an ideal world, we’d
have a path γ1 : [0, 1] → X and γ2 : [1, 2] → X and we could just merge them together
to get γ1 ∗ γ2 : [0, 2] → X. But the “2” is wrong here. The solution is that we allocate
[0, 1

2 , 1] for the second path; we run “twice as fast”.

2 ] for the ﬁrst path and [ 1

1Almost everyone else in the world uses “gluing” to describe this and other types of constructs. But I
was traumatized by Elmer’s glue when I was in high school because I hated the stupid “make a poster”
projects and hated having to use glue on them. So I refuse to talk about “gluing” paths together,
referring instead to “fusing” them together, which sounds cooler anyways.

547

Xγ1(0)γ1(1)=γ2(0)γ2(1)γ1γ2548

Napkin, by Evan Chen (v1.5.20190718)

Deﬁnition 58.1.1. Given two paths γ1, γ2 : [0, 1] → X such that γ1(1) = γ2(0), we
deﬁne a path γ1 ∗ γ2 : [0, 1] → X by

(γ1 ∗ γ2)(t) =(cid:40)γ1(2t)

γ2(2t − 1)

0 ≤ t ≤ 1
2 ≤ t ≤ 1.

2

1

This hack unfortunately reveals a second shortcoming: this “product” is not associative.
2 , 1] are the times

If we take (γ1 ∗ γ2) ∗ γ3 for some suitable paths, then [0, 1
allocated for γ1, γ2, γ3.

2 ] and [ 1

4 ], [ 1

4 , 1

Question 58.1.2. What are the times allocated for γ1 ∗ (γ2 ∗ γ3)?

But I hope you’ll agree that even though this operation isn’t associative, the reason it
fails to be associative is kind of stupid. It’s just a matter of how fast we run in certain
parts.

So as long as we’re fusing paths together, we probably don’t want to think of [0, 1] itself
too seriously. And so we only consider everything up to (path) homotopy equivalence.
(Recall that two paths α and β are homotopic if there’s a path homotopy F : [0, 1]2 → X
between them, which is a continuous deformation from α to β.) It is deﬁnitely true that

(γ1 ∗ γ2) ∗ γ3 (cid:39) γ1 ∗ (γ2 ∗ γ3) .

It is also true that if α1 (cid:39) α2 and β1 (cid:39) β2 then α1 ∗ β1 (cid:39) α2 ∗ β2.
Naturally, homotopy is an equivalence relation, so paths γ lives in some “homotopy
type”, the equivalence classes under (cid:39). We’ll denote this [γ]. Then it makes sense to talk
about [α] ∗ [β]. Thus, we can think of ∗ as an operation on homotopy classes.

§58.2 Fundamental groups

Prototypical example for this section: π1(R2) is trivial and π1(S1) ∼= Z.

011412013412γ1γ2γ3γ1γ2γ3γ1∗(γ2∗γ3)(γ1∗γ2)∗γ358 Fundamental groups

549

At this point I’m a little annoyed at keeping track of endpoints, so now I’m going to

specialize to a certain type of path.

Deﬁnition 58.2.1. A loop is a path with γ(0) = γ(1).

Hence if we restrict our attention to paths starting at a single point x0, then we can
stop caring about endpoints and start-points, since everything starts and stops at x0.
We even have a very canonical loop: the “do-nothing” loop2 given by standing at x0 the
whole time.

Deﬁnition 58.2.2. Denote the trivial “do-nothing loop” by 1. A loop γ is nulhomo-
topic if it is homotopic to 1; i.e. γ (cid:39) 1.

For homotopy of loops, you might visualize “reeling in” the loop, contracting it to a

single point.

Example 58.2.3 (Loops in S2 are nulhomotopic)
As the following picture should convince you, every loop in the simply connected
space S2 is nulhomotopic.

(Starting with the purple loop, we contract to the red-brown point.)

Hence to show that spaces are simply connected it suﬃces to understand the loops of

that space. We are now ready to provide:

2Fatty.

Xx0γ550

Napkin, by Evan Chen (v1.5.20190718)

Deﬁnition 58.2.4. The fundamental group of X with basepoint x0, denoted π1(X, x0),
is the set of homotopy classes

{[γ] | γ a loop at x0}

equipped with ∗ as a group operation.

It might come as a surprise that this has a group structure. For example, what is the

inverse? Let’s deﬁne it now.
Deﬁnition 58.2.5. Given a path α : [0, 1] → X we can deﬁne a path α

α(t) = α(1 − t).

In eﬀect, this “runs α backwards”. Note that α starts at the endpoint of α and ends at
the starting point of α.

Exercise 58.2.6. Show that for any path α, α ∗ α is homotopic to the “do-nothing” loop
at α(0). (Draw a picture.)

Let’s check it.

Proof that this is a group structure. Clearly ∗ takes two loops at x0 and spits out a loop
at x0. We also already took the time to show that ∗ is associative. So we only have to
check that (i) there’s an identity, and (ii) there’s an inverse.

 We claim that the identity is the “do-nothing” loop 1 we described above. The

reader can check that for any γ,

 For a loop γ, recall again we deﬁne its “backwards” loop γ by

γ (cid:39) γ ∗ 1 = 1 ∗ γ.

Then we have γ ∗ γ = γ ∗ γ = 1.
Hence π1(X, x0) is actually a group.

γ(t) = γ(1 − t).

Before going any further I had better give some examples.

Example 58.2.7 (Examples of fundamental groups)
Note that proving the following results is not at all trivial. For now, just try to see
intuitively why the claimed answer “should” be correct.

(a) The fundamental group of C is the trivial group: in the plane, every loop is

nulhomotopic. (Proof: imagine it’s a piece of rope and reel it in.)

(b) On the other hand, the fundamental group of C − {0} (meteor example from
earlier) with any base point is actually Z! We won’t be able to prove this for a
while, but essentially a loop is determined by the number of times that it winds
around the origin – these are so-called winding numbers. Think about it!

(c) Similarly, we will soon show that the fundamental group of S1 (the boundary of

the unit circle) is Z.

Oﬃcially, I also have to tell you what the base point is, but by symmetry in these
examples, it doesn’t matter.

58 Fundamental groups

551

Here is the picture for C \ {0}, with the hole exaggerated as the meteor from Section 7.7.

Question 58.2.8. Convince yourself that the fundamental group of S1 is Z, and understand
why we call these “winding numbers”. (This will be the most important example of a
fundamental group in later chapters, so it’s crucial you ﬁgure it out now.)

Example 58.2.9 (The ﬁgure eight)
Consider a ﬁgure eight S1 ∨ S1, and let x0 be the center. Then

π1(S1 ∨ S1, x0) ∼= (cid:104)a, b(cid:105)

is the free group generated on two letters. The idea is that one loop of the eight is a,
and the other loop is b, so we expect π1 to be generated by this loop a and b (and
its inverses a and b). These loops don’t talk to each other.

Recall that in graph theory, we usually assume our graphs are connected, since otherwise
we can just consider every connected component separately. Likewise, we generally want
to restrict our attention to path-connected spaces, since if a space isn’t path-connected
then it can be broken into a bunch of “path-connected components”. (Can you guess
how to deﬁne this?) Indeed, you could imagine a space X that consists of the objects on
my desk (but not the desk itself): π1 of my phone has nothing to do with π1 of my mug.
They are just totally disconnected, both ﬁguratively and literally.

But on the other hand we claim that in a path-connected space, the groups are very

related!

Theorem 58.2.10 (Fundamental groups don’t depend on basepoint)
Let X be a path-connected space. Then for any x1 ∈ X and x2 ∈ X, we have

π1(X, x1) ∼= π1(X, x2).

Before you read the proof, see if you can guess the isomorphism based just on the picture
below.

C\{0}x0ab552

Napkin, by Evan Chen (v1.5.20190718)

Proof. Let α be any path from x1 to x2 (possible by path-connectedness), and let α be
its reverse. Then we can construct a map

π1(X, x1) → π1(X, x2) by [γ] (cid:55)→ [α ∗ γ ∗ α].

In other words, given a loop γ at x1, we can start at x2, follow α to x1, run γ, then run
along α home to x2. Hence this is a map which builds a loop of π1(X, x2) from every
loop at π1(X, x1). It is a homomorphism of the groups just because

(α ∗ γ1 ∗ α) ∗ (α ∗ γ2 ∗ α) = α ∗ γ1 ∗ γ2 ∗ α

as α ∗ α is nulhomotopic.

Similarly, there is a homomorphism

π1(X, x2) → π1(X, x1) by [γ] (cid:55)→ [α ∗ γ ∗ α].

As these maps are mutual inverses, it follows they must be isomorphisms. End of
story.

This is a bigger reason why we usually only care about path-connected spaces.

Abuse of Notation 58.2.11. For a path-connected space X we will often abbreviate
π1(X, x0) to just π1(X), since it doesn’t matter which x0 ∈ X we pick.

Finally, recall that we originally deﬁned “simply connected” as saying that any two
paths with matching endpoints were homotopic. It’s possible to weaken this condition
and then rephrase it using fundamental groups.

Exercise 58.2.12. Let X be a path-connected space. Prove that X is simply connected
if and only if π1(X) is the trivial group. (One direction is easy; the other is a little trickier.)

This is the “usual” deﬁnition of simply connected.

§58.3 Fundamental groups are functorial

One quick shorthand I will introduce to clean up the discussion:

Deﬁnition 58.3.1. By f : (X, x0) → (Y, y0), we will mean that f : X → Y is a
continuous function of spaces which also sends the point x0 to y0.

Xx1x2α/α58 Fundamental groups

553

the fundamental groups of X and Y .

Let X and Y be topological spaces and f : (X, x0) → (Y, y0). We now want to relate
Recall that a loop γ in (X, x0) is a map γ : [0, 1] → X with γ(0) = γ(1) = x0. Then if

we consider the composition

[0, 1]

γ

−→ (X, x0)

f

−→ (Y, y0)

then we get straight-away a loop in Y at y0! Let’s call this loop f(cid:93)γ.

Lemma 58.3.2 (f(cid:93) is homotopy invariant)
If γ1 (cid:39) γ2 are path-homotopic, then in fact

f(cid:93)γ1 (cid:39) f(cid:93)γ2.

Proof. Just take the homotopy h taking γ1 to γ2 and consider f ◦ h.

It’s worth noting at this point that if X and Y are homeomorphic, then their funda-
mental groups are all isomorphic. Indeed, let f : X → Y and g : Y → X be mutually
inverse continuous maps. Then one can check that f(cid:93) : π1(X, x0) → π1(Y, y0) and
g(cid:93) : π1(Y, y0) → π1(X, x0) are inverse maps between the groups (assuming f (x0) = y0
and g(y0) = x0).

§58.4 Higher homotopy groups

Why the notation π1 for the fundamental group? And what are π2, . . . ? The answer lies
in the following rephrasing:

Question 58.4.1. Convince yourself that a loop is the same thing as a continuous function
S1 → X.

It turns out we can deﬁne homotopy for things other than paths. Two functions
f, g : Y → X are homotopic if there exists a continuous function Y × [0, 1] → X which
continuously deforms f to g. So everything we did above was just the special case Y = S1.
For general n, the group πn(X) is deﬁned as the homotopy classes of the maps
Sn → X. The group operation is a little harder to specify. You have to show that Sn
is homeomorphic to [0, 1]n with some endpoints fused together; for example S1 is [0, 1]
with 0 fused to 1. Once you have these cubes, you can merge them together on a face.
(Again, I’m being terribly imprecise, deliberately.)

For n (cid:54)= 1, πn behaves somewhat diﬀerently than π1. (You might not be surprised, as
Sn is simply connected for all n ≥ 2 but not when n = 1.) In particular, it turns out
that πn(X) is an abelian group for all n ≥ 2.

Let’s see some examples.

Example 58.4.2 (πn(Sn) ∼= Z)
As we saw, π1(S1) ∼= Z; given the base circle S1, we can wrap a second circle around
it as many times as we want. In general, it’s true that πn(Sn) ∼= Z.

554

Napkin, by Evan Chen (v1.5.20190718)

Example 58.4.3 (πn(Sm) ∼= {1} when n < m)
We saw that π1(S2) ∼= {1}, because a circle in S2 can just be reeled in to a point. It
turns out that similarly, any smaller n-dimensional sphere can be reeled in on the
surface of a bigger m-dimensional sphere. So in general, πn(Sm) is trivial for n < m.

However, beyond these observations, the groups behave quite weirdly. Here is a table of
πn(Sm) for 1 ≤ m ≤ 8 and 2 ≤ n ≤ 10, so you can see what I’m talking about. (Taken
from Wikipedia.)

2
{1}
Z

3
{1}
Z
Z

πn(Sm)
m = 1
2
3
4
5
6
7
8

5
{1}

4
6
{1}
{1}
Z/2Z Z/2Z Z/12Z
Z/2Z Z/2Z Z/12Z
Z/2Z
Z/2Z

Z

Z/2Z

Z

Z

7
{1}
Z/2Z
Z/2Z

Z/2Z
Z/2Z

Z

Z × Z/12Z (Z/2Z)2 Z/2Z × Z/2Z Z/24Z × Z/3Z

8
{1}
Z/2Z
Z/2Z

Z/24Z
Z/2Z
Z/2Z

Z

9
{1}
Z/3Z
Z/3Z

Z/2Z
Z/24Z
Z/2Z
Z/2Z

10
{1}
Z/15Z
Z/15Z

Z/2Z
{1}
Z/24Z
Z/2Z

Actually, it turns out that if you can compute πn(Sm) for every m and n, then you
can essentially compute any homotopy classes. Thus, computing πn(Sm) is sort of a lost
cause in general, and the mixture of chaos and pattern in the above table is a testament
to this.

§58.5 Homotopy equivalent spaces

Prototypical example for this section: A disk is homotopy equivalent to a point, an annulus
is homotopy equivalent to S1.

Up to now I’ve abused notation and referred to “path homotopy” as just “homotopy”
for two paths. I will unfortunately continue to do so (and so any time I say two paths
are homotopic, you should assume I mean “path-homotopic”). But let me tell you what
the general deﬁnition of homotopy is ﬁrst.

Deﬁnition 58.5.1. Let f, g : X → Y be continuous functions. A homotopy is a
continuous function F : X × [0, 1] → Y , which we’ll write Fs(x) for s ∈ [0, 1], x ∈ X,
such that

If such a function exists, then f and g are homotopic.

F0(x) = f (x) and F1(x) = g(x) for all x ∈ X.

Intuitively this is once again “deforming f to g”. You might notice this is almost
exactly the same deﬁnition as path-homotopy, except that f and g are any functions
instead of paths, and hence there’s no restriction on keeping some “endpoints” ﬁxed
through the deformation.

This homotopy can be quite dramatic:

Example 58.5.2
The zero function z (cid:55)→ 0 and the identity function z (cid:55)→ z are homotopic as functions
C → C. The necessary deformation is

[0, 1] × C → C by (t, z) (cid:55)→ tz.

58 Fundamental groups

555

I bring this up because I want to deﬁne:

Deﬁnition 58.5.3. Let X and Y be continuous spaces. They are homotopy equivalent
if there exist functions f : X → Y and g : Y → X such that
(i) f ◦ g : X → X is homotopic to the identity map on X, and
(ii) g ◦ f : Y → Y is homotopic to the identity map on Y .
If a topological space is homotopy equivalent to a point, then it is said to be contractible.

Question 58.5.4. Why are two homeomorphic spaces also homotopy equivalent?

Intuitively, you can think of this as a more generous form of stretching and bending

than homeomorphism: we are allowed to compress huge spaces into single points.

Example 58.5.5 (C is contractible)
Consider the topological spaces C and the space consisting of the single point {0}.
We claim these spaces are homotopy equivalent (can you guess what f and g are?)
Indeed, the two things to check are
(i) C → {0} (cid:44)→ C by z (cid:55)→ 0 (cid:55)→ 0 is homotopy equivalent to the identity on C,

which we just saw, and

(ii) {0} (cid:44)→ C → {0} by 0 (cid:55)→ 0 (cid:55)→ 0, which is the identity on {0}.
Here by (cid:44)→ I just mean → in the special case that the function is just an “inclusion”.

Remark 58.5.6 — C cannot be homeomorphic to a point because there is no
bijection of sets between them.

Example 58.5.7 (C \ {0} is homotopy equivalent to S1)
Consider the topological spaces C \ {0}, the punctured plane, and the circle S1
viewed as a subset of S1. We claim these spaces are actually homotopy equivalent!
The necessary functions are the inclusion

S1 (cid:44)→ C \ {0}

and the function

.

z
|z|
You can check that these satisfy the required condition.

C \ {0} → S1 by z (cid:55)→

Remark 58.5.8 — On the other hand, C\{0} cannot be homeomorphic to S1. One
can make S1 disconnected by deleting two points; the same is not true for C \ {0}.

556

Napkin, by Evan Chen (v1.5.20190718)

Example 58.5.9 (Disk = Point, Annulus = Circle.)
By the same token, a disk is homotopic to a point; an annulus is homotopic to a
circle. (This might be a little easier to visualize, since it’s ﬁnite.)

I bring these up because it turns out that

Algebraic topology can’t distinguish between homotopy equivalent spaces.

More precisely,

Theorem 58.5.10 (Homotopy equivalent spaces have isomorphic fundamental groups)
Let X and Y be path-connected, homotopy-equivalent spaces. Then πn(X) ∼= πn(Y )
for every positive integer n.

Proof. Let γ : [0, 1] → X be a loop. Let f : X → Y and g : Y → X be maps witnessing
that X and Y are homotopy equivalent (meaning f ◦ g and g ◦ f are each homotopic to
the identity). Then the composition

[0, 1]

γ

−→ X

f

−→ Y

is a loop in Y and hence f induces a natural homomorphism π1(X) → π1(Y ). Similarly
g induces a natural homomorphism π1(Y ) → π1(X). The conditions on f and g now say
exactly that these two homomorphisms are inverse to each other, meaning the maps are
isomorphisms.

In particular,

Question 58.5.11. What are the fundamental groups of contractible spaces?

That means, for example, that algebraic topology can’t tell the following homotopic

subspaces of R2 apart.

♀ ♂

§58.6 The pointed homotopy category

This section is meant to be read by those who know some basic category theory. Those of
you that don’t should come back after reading Chapters 60 and 61. Those of you that do
will enjoy how succinctly we can summarize the content of this chapter using categorical
notions.

Deﬁnition 58.6.1. The pointed homotopy category hTop∗ is deﬁned as follows.

 Objects: pairs (X, x0) of spaces with a distinguished basepoint, and

 Morphisms: homotopy classes of continuous functions (X, x0) → (Y, y0).
In particular, two path-connected spaces are isomorphic in this category exactly when
they are homotopy equivalent. Then we can summarize many of the preceding results as
follows:

58 Fundamental groups

557

Theorem 58.6.2 (Functorial interpretation of fundamental groups)

There is a functor

sending

π1 : hTop∗ → Grp

(X, x0)

-

π1(X, x0)

f

?
(Y, y0)

f(cid:93)

?

- π1(Y, y0)

This implies several things, like

 The functor bundles the information of f(cid:93), including the fact that it respects

composition. In the categorical language, f(cid:93) is π1(f ).

 Homotopic spaces have isomorphic fundamental group (since the spaces are iso-
morphic in hTop, and functors preserve isomorphism by Theorem 61.2.8). In fact,
you’ll notice that the proofs of Theorem 61.2.8 and Theorem 58.5.10 are secretly
identical to each other.

 If maps f, g : (X, x0) → (Y, y0) are homotopic, then f(cid:93) = g(cid:93). This is basically

Lemma 58.3.2

Remark 58.6.3 — In fact, π1(X, x0) is the set of arrows (S1, 1) → (X, x0) in hTop∗,
so this is actually a covariant Yoneda functor (Example 61.2.6), except with target
Grp instead of Set.

§58.7 A few harder problems to think about

Problem 58A (Harmonic fan). Exhibit a subspace X of the metric space R2 which is
path-connected but for which a point p can be found such that any r-neighborhood of p
with r < 1 is not path-connected.
Problem 58B† (Special case of Seifert-van Kampen). Let X be a topological space.
Suppose U and V are connected open subsets of X, with X = U ∪ V , so that U ∩ V is
nonempty and path-connected.

Prove that if π1(U ) = π1(V ) = {1} then π1(X) = 1.

Remark 58.7.1 — The Seifert–van Kampen theorem generalizes this for π1(U )
and π1(V ) any groups; it gives a formula for calculating π1(X) in terms of π1(U ),
π1(V ), π1(U ∩ V ). The proof is much the same.
possible to write S1 = U ∪ V for U ∩ V connected.

Unfortunately, this does not give us a way to calculate π1(S1), because it is not

Problem 58C (RMM 2013). Let n ≥ 2 be a positive integer. A stone is placed at
each vertex of a regular 2n-gon. A move consists of selecting an edge of the 2n-gon and
swapping the two stones at the endpoints of the edge. Prove that if a sequence of moves
swaps every pair of stones exactly once, then there is some edge never used in any move.

558

Napkin, by Evan Chen (v1.5.20190718)

(This last problem doesn’t technically have anything to do with the chapter, but the

“gut feeling” which motivates the solution is very similar.)

59 Covering projections

A few chapters ago we talked about what a fundamental group was, but we didn’t
actually show how to compute any of them except for the most trivial case of a simply
connected space. In this chapter we’ll introduce the notion of a covering projection, which
will let us see how some of these groups can be found.

§59.1 Even coverings and covering projections

Prototypical example for this section: R covers S1.

What we want now is a notion where a big space E, a “covering space”, can be

projected down onto a base space B in a nice way. Here is the notion of “nice”:

Deﬁnition 59.1.1. Let p : E → B be a continuous function. Let U be an open set of
B. We call U evenly covered (by p) if ppre(U ) is a disjoint union of open sets (possibly
inﬁnite) such that p restricted to any of these sets is a homeomorphism.

Picture:

Image from [Wo]

All we’re saying is that U is evenly covered if its pre-image is a bunch of copies of it.
(Actually, a little more: each of the pancakes is homeomorphic to U , but we also require
that p is the homeomorphism.)

Deﬁnition 59.1.2. A covering projection p : E → B is a surjective continuous map
such that every base point b ∈ B has an open neighborhood U (cid:51) b which is evenly covered
by p.

Exercise 59.1.3 (On requiring surjectivity of p). Let p : E → B be satisfying this deﬁnition,
except that p need not be surjective. Show that the image of p is a connected component of
B. Thus if B is connected and E is nonempty, then p : E → B is already surjective. For
this reason, some authors omit the surjectivity hypothesis as usually B is path-connected.

Here is the most stupid example of a covering projection.

559

560

Napkin, by Evan Chen (v1.5.20190718)

Example 59.1.4 (Tautological covering projection)
formally, E = B × {1, . . . , n}
Let’s take n disconnected copies of any space B:
with the discrete topology on {1, . . . , n}. Then there exists a tautological covering
projection E → B by (x, m) (cid:55)→ x; we just project all n copies.

This is a covering projection because every open set in B is evenly covered.

This is not really that interesting because B × [n] is not path-connected.

A much more interesting example is that of R and S1.

Example 59.1.5 (Covering projection of S1)
Take p : R → S1 by θ (cid:55)→ e2πiθ. This is essentially wrapping the real line into a single
helix and projecting it down.

Missing

ﬁgure

helix

We claim this is a covering projection. Indeed, consider the point 1 ∈ S1 (where we
view S1 as the unit circle in the complex plane). We can draw a small open neighborhood
of it whose pre-image is a bunch of copies in R.

Note that not all open neighborhoods work this time: notably, U = S1 does not work

because the pre-image would be the entire R.

Example 59.1.6 (Covering of S1 by itself)
The map S1 → S1 by z (cid:55)→ z3 is also a covering projection. Can you see why?

−2−1012RpS1159 Covering projections

561

Example 59.1.7 (Covering projections of C \ {0})
For those comfortable with complex arithmetic,
(a) The exponential map exp : C → C \ {0} is a covering projection.
(b) For each n, the nth power map −n : C \ {0} → C \ {0} is a covering projection.

§59.2 Lifting theorem

Prototypical example for this section: R covers S1.

Now here’s the key idea: we are going to try to interpret loops in B as paths in R. This
is often much simpler. For example, we had no idea how to compute the fundamental
group of S1, but the fundamental group of R is just the trivial group. So if we can
interpret loops in S1 as paths in R, that might (and indeed it does!) make computing
π1(S1) tractable.
Deﬁnition 59.2.1. Let γ : [0, 1] → B be a path and p : E → B a covering projection.
A lifting of γ is a path ˜γ : [0, 1] → E such that p ◦ ˜γ = γ.

Picture:

˜γ

[0, 1]

γ

E

-

p

?
B

-

Example 59.2.2 (Typical example of lifting)
Take p : R → S1 ⊆ C by θ (cid:55)→ e2πiθ (so S1 is considered again as the unit circle).
Consider the path γ in S1 which starts at 1 ∈ C and wraps around S1 once,
counterclockwise, ending at 1 again. In symbols, γ : [0, 1] → S1 by t (cid:55)→ e2πit.
Then one lifting ˜γ is the path which walks from 0 to 1. In fact, for any integer n,
walking from n to n + 1 works.

Similarly, the counterclockwise path from 1 ∈ S1 to −1 ∈ S1 has a lifting: for

some integer n, the path from n to n + 1
2 .

−1012R˜γpS11γp(0)=1p(1)=1562

Napkin, by Evan Chen (v1.5.20190718)

The above is the primary example of a lifting. It seems like we have the following
structure: given a path γ in B starting at b0, we start at any point in the ﬁber ppre(b0).
(In our prototypical example, B = S1, b0 = 1 ∈ C and that’s why we start at any integer
n.) After that we just trace along the path in B, and we get a corresponding path in
E.

Question 59.2.3. Take a path γ in S1 with γ(0) = 1 ∈ C. Convince yourself that once we
select an integer n ∈ R, then there is exactly one lifting starting at n.

It turns out this is true more generally.

Theorem 59.2.4 (Lifting paths)
Suppose γ : [0, 1] → B is a path with γ(0) = b0, and p : (E, e0) → (B, b0) is a
covering projection. Then there exists a unique lifting ˜γ : [0, 1] → E such that
˜γ(0) = e0.

Proof. For every point b ∈ B, consider an evenly covered open neighborhood Ub in B.
Then the family of open sets

{γpre(Ub) | b ∈ B}

is an open cover of [0, 1]. As [0, 1] is compact we can take a ﬁnite subcover. Thus we can
chop [0, 1] into ﬁnitely many disjoint closed intervals [0, 1] = I1 (cid:116) I2 (cid:116) ··· (cid:116) IN in that
order, such that for every Ik, γimg(Ik) is contained in some Ub.
We’ll construct ˜γ interval by interval now, starting at I1. Initially, place a robot at
e0 ∈ E and a mouse at b0 ∈ B. For each interval Ik, the mouse moves around according
to however γ behaves on Ik. But the whole time it’s in some evenly covered Uk; the fact
that p is a covering projection tells us that there are several copies of Uk living in E.
Exactly one of them, say Vk, contains our robot. So the robot just mimics the mouse
until it gets to the end of Ik. Then the mouse is in some new evenly covered Uk+1, and
we can repeat.

The theorem can be generalized to a diagram

(E, e0)
-

˜f

p

f -

?
(B, b0)

(Y, y0)

where Y is some general path-connected space, as follows.

Theorem 59.2.5 (General lifting criterion)
Let f : (Y, y0) → (B, b0) be continuous and consider a covering projection p :
(E, e0) → (B, b0). (As usual, Y , B, E are path-connected.) Then a lifting ˜f with
˜f (y0) = e0 exists if and only if

f img
∗

(π1(Y, y0)) ⊆ pimg
∗

(π1(E, e0)),

i.e. the image of π1(Y, y0) under f is contained in the image of π1(E, e0) under p
(both viewed as subgroups of π1(B, b0)). If this lifting exists, it is unique.

59 Covering projections

563

As p is injective, we actually have pimg
interested in the actual elements, not just the isomorphism classes of the groups.

(π1(E, e0)) ∼= π1(E, e0). But in this case we are

∗

Question 59.2.6. What happens if we put Y = [0, 1]?

Remark 59.2.7 (Lifting homotopies) — Here’s another cool special case: Recall
that a homotopy can be encoded as a continuous function [0, 1] × [0, 1] → X. But
[0, 1] × [0, 1] is also simply connected. Hence given a homotopy γ1 (cid:39) γ2 in the base
space B, we can lift it to get a homotopy ˜γ1 (cid:39) ˜γ2 in E.

Another nice application of this result is Chapter 33.

§59.3 Lifting correspondence

Prototypical example for this section: (R, 0) covers (S1, 1).

Let’s return to the task of computing fundamental groups. Consider a covering

projection p : (E, e0) → (B, b0).
A loop γ can be lifted uniquely to ˜γ in E which starts at e0 and ends at some point e
in the ﬁber ppre(b0). You can easily check that this e ∈ E does not change if we pick a
diﬀerent path γ(cid:48) homotopic to ˜γ.

Question 59.3.1. Look at the picture in Example 59.2.2.

Put one ﬁnger at 1 ∈ S1, and one ﬁnger on 0 ∈ R. Trace a loop homotopic to γ in
S1 (meaning, you can go backwards and forwards but you must end with exactly one full
counterclockwise rotation) and follow along with the other ﬁnger in R.

Convince yourself that you have to end at the point 1 ∈ R.

Thus every homotopy class of a loop at b0 (i.e. an element of π1(B, b0)) can be
associated with some e in the ﬁber of b0. The below proposition summarizes this and
more.

Proposition 59.3.2
Let p : (E, e0) → (B, b0) be a covering projection. Then we have a function of sets

Φ : π1(B, b0) → ppre(b0)

by [γ] (cid:55)→ ˜γ(1), where ˜γ is the unique lifting starting at e0. Furthermore,

 If E is path-connected, then Φ is surjective.

 If E is simply connected, then Φ is injective.

Question 59.3.3. Prove that E path-connected implies Φ is surjective. (This is really
oﬀensively easy.)

Proof. To prove the proposition, we’ve done everything except show that E simply
connected implies Φ injective. To do this suppose that γ1 and γ2 are loops such that
Φ([γ1]) = Φ([γ2]).

564

Napkin, by Evan Chen (v1.5.20190718)

Applying lifting, we get paths ˜γ1 and ˜γ2 both starting at some point e0 ∈ E and ending
at some point e1 ∈ E. Since E is simply connected that means they are homotopic, and
we can write a homotopy F : [0, 1] × [0, 1] → E which unites them. But then consider
the composition of maps

You can check this is a homotopy from γ1 to γ2. Hence [γ1] = [γ2], done.

[0, 1] × [0, 1] F−→ E

p

−→ B.

This motivates:

Deﬁnition 59.3.4. A universal cover of a space B is a covering projection p : E → B
where E is simply connected (and in particular path-connected).

Abuse of Notation 59.3.5. When p is understood, we sometimes just say E is the
universal cover.

Example 59.3.6 (Fundamental group of S1)
Let’s return to our standard p : R → S1. Since R is simply connected, this is a
universal cover of S1. And indeed, the ﬁber of any point in S1 is a copy of the
integers: naturally in bijection with loops in S1.

You can show (and it’s intuitively obvious) that the bijection

Φ : π1(S1) ↔ Z

is in fact a group homomorphism if we equip Z with its additive group structure Z.
Since it’s a bijection, this leads us to conclude π1(S1) ∼= Z.

§59.4 Regular coverings

Prototypical example for this section: R → S1 comes from n · x = n + x

Here’s another way to generate some coverings. Let X be a topological space and G a

group acting on its points. Thus for every g, we get a map X → X by

x (cid:55)→ g · x.

We require that this map is continuous1 for every g ∈ G, and that the stabilizer of each
point in X is trivial. Then we can consider a quotient space X/G deﬁned by fusing any
points in the same orbit of this action. Thus the points of X/G are identiﬁed with the
orbits of the action. Then we get a natural “projection”

by simply sending every point to the orbit it lives in.

X → X/G

Deﬁnition 59.4.1. Such a projection is called regular. (Terrible, I know.)

1Another way of phrasing this: the action, interpreted as a map G × X → X, should be continuous,

where G on the left-hand side is interpreted as a set with the discrete topology.

59 Covering projections

565

Example 59.4.2 (R → S1 is regular)
Let G = Z, X = R and deﬁne the group action of G on X by

n · x = n + x

You can then think of X/G as “real numbers modulo 1”, with [0, 1) a complete set
of representatives and 0 ∼ 1.

So we can identify X/G with S1 and the associated regular projection is just our
usual exp : θ (cid:55)→ e2iπθ.

Example 59.4.3 (The torus)
Let G = Z×Z and X = R2, and deﬁne the group action of G on X by (m, n)·(x, y) =
(m + x, n + y). As [0, 1)2 is a complete set of representatives, you can think of it as
a unit square with the edges identiﬁed. We obtain the torus S1 × S1 and a covering
projection R2 → S1 × S1.

Example 59.4.4 (RP2)

Let G = Z/2Z =(cid:10)T | T 2 = 1(cid:11) and let X = S2 be the surface of the sphere, viewed
as a subset of R3. We’ll let G act on X by sending T · (cid:126)x = −(cid:126)x; hence the orbits are
pairs of opposite points (e.g. North and South pole).
Let’s draw a picture of a space. All the orbits have size two: every point below the
equator gets fused with a point above the equator. As for the points on the equator,
we can take half of them; the other half gets fused with the corresponding antipodes.
Now if we ﬂatten everything, you can think of the result as a disk with half its
boundary: this is RP2 from before. The resulting space has a name: real projective
2-space, denoted RP2.

This gives us a covering projection S2 → RP2 (note that the pre-image of a

suﬃciently small patch is just two copies of it on S2.)

011323R/G0=11323S1RP2566

Napkin, by Evan Chen (v1.5.20190718)

Example 59.4.5 (Fundamental group of RP2)
As above, we saw that there was a covering projection S2 → RP2. Moreover the
ﬁber of any point has size two. Since S2 is simply connected, we have a natural
bijection π1(RP2) to a set of size two; that is,

This can only occur if π1(RP2) ∼= Z/2Z, as there is only one group of order two!

(cid:12)(cid:12)π1(RP2)(cid:12)(cid:12) = 2.

Question 59.4.6. Show each of the continuous maps x (cid:55)→ g · x is in fact a homeomorphism.
(Name its continuous inverse).

§59.5 The algebra of fundamental groups

Prototypical example for this section: S1, with fundamental group Z.

Next up, we’re going to turn functions between spaces into homomorphisms of funda-

mental groups.

Let X and Y be topological spaces and f : (X, x0) → (Y, y0). Recall that we deﬁned a

group homomorphism

f(cid:93) : π1(X, x0) → π1(Y0, y0) by [γ] (cid:55)→ [f ◦ γ].

More importantly, we have:

Proposition 59.5.1
Let p : (E, e0) → (B, b0) be a covering projection of path-connected spaces. Then
the homomorphism p(cid:93) : π1(E, e0) → π1(B, b0) is injective. Hence pimg
(π1(E, e0)) is
an isomorphic copy of π1(E, e0) as a subgroup of π1(B, b0).

(cid:93)

Proof. We’ll show ker p(cid:93) is trivial. It suﬃces to show if γ is a nulhomotopic loop in B
then its lift is nulhomotopic.

By deﬁnition, there’s a homotopy F : [0, 1] × [0, 1] → B taking γ to the constant loop
1B. We can lift it to a homotopy ˜F : [0, 1] × [0, 1] → E that establishes ˜γ (cid:39) ˜1B. But 1E
is a lift of 1B (duh) and lifts are unique.

Example 59.5.2 (Subgroups of Z)
Let’s look at the space S1 with fundamental group Z. The group Z has two types of
subgroups:

 The trivial subgroup. This corresponds to the canonical projection R → S1,
since π1(R) is the trivial group (R is simply connected) and hence its image in
Z is the trivial group.

 nZ for n ≥ 1. This is given by the covering projection S1 → S1 by z (cid:55)→ zn.

The image of a loop in the covering S1 is a “multiple of n” in the base S1.

59 Covering projections

567

It turns out that these are the only covering projections of Sn by path-connected
spaces: there’s one for each subgroup of Z. (We don’t care about disconnected spaces
because, again, a covering projection via disconnected spaces is just a bunch of unrelated
“good” coverings.) For this statement to make sense I need to tell you what it means for
two covering projections to be equivalent.
Deﬁnition 59.5.3. Fix a space B. Given two covering projections p1 : E1 → B and
p2 : E2 → B a map of covering projections is a continuous function f : E1 → E2
such that p2 ◦ f = p1.

f

E1

-

E2

p
1

p2

-

?
B

Then two covering projections p1 and p2 are isomorphic if there are f : E1 → E2 and
g : E2 → E1 such that f ◦ g = idE1 and g ◦ f = idE2.

Remark 59.5.4 (For category theorists) — The set of covering projections forms a
category in this way.

It’s an absolute miracle that this is true more generally: the greatest triumph of
covering spaces is the following result. Suppose a space X satisﬁes some nice conditions,
like:
Deﬁnition 59.5.5. A space X is called locally connected if for each point x ∈ X and
open neighborhood V of it, there is a connected open set U with x ∈ U ⊆ V .
Deﬁnition 59.5.6. A space X is semi-locally simply connected if for every point
x ∈ X there is an open neighborhood U such that all loops in U are nulhomotopic. (But
the contraction need not take place in U .)

Example 59.5.7 (These conditions are weak)
Pretty much every space I’ve shown you has these two properties. In other words,
they are rather mild conditions, and you can think of them as just saying “the space
is not too pathological”.

Then we get:

Theorem 59.5.8 (Group theory via covering spaces)

Suppose B is a locally connected, semi-locally simply connected space. Then:

 Every subgroup H ⊆ π1(B) corresponds to exactly one covering projection

p : E → B with E path-connected (up to isomorphism).
(Speciﬁcally, H is the image of π1(E) in π1(B) through p(cid:93).)

 Moreover, the normal subgroups of π1(B) correspond exactly to the regular

covering projections.

568

Napkin, by Evan Chen (v1.5.20190718)

Hence it’s possible to understand the group theory of π1(B) completely in terms of the
covering projections.

Moreover, this is how the “universal cover” gets its name: it is the one corresponding
to the trivial subgroup of π1(B). Actually, you can show that it really is universal in
the sense that if p : E → B is another covering projection, then E is in turn covered
by the universal space. More generally, if H1 ⊆ H2 ⊆ G are subgroups, then the space
corresponding to H2 can be covered by the space corresponding to H1.

§59.6 A few harder problems to think about

problems
problems

XVI

Category Theory

Part XVI: Contents

60 Objects and morphisms

571
60.1 Motivation: isomorphisms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 571
60.2 Categories, and examples thereof . . . . . . . . . . . . . . . . . . . . . . . . . . . 571
60.3 Special objects in categories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 575
60.4 Binary products . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 576
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 580
60.5 Monic and epic maps
. . . . . . . . . . . . . . . . . . . . . . . . 581
60.6 A few harder problems to think about

61 Functors and natural transformations

583
61.1 Many examples of functors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 583
61.2 Covariant functors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 584
61.3 Contravariant functors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 586
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 588
61.4 Equivalence of categories
61.5 (Optional) Natural transformations . . . . . . . . . . . . . . . . . . . . . . . . . . 588
61.6 (Optional) The Yoneda lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . 590
. . . . . . . . . . . . . . . . . . . . . . . . 592
61.7 A few harder problems to think about

62 Limits in categories (TO DO)

593
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 593
62.1 Equalizers
62.2 Pullback squares (TO DO) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 594
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 594
62.3 Limits
. . . . . . . . . . . . . . . . . . . . . . . . 594
62.4 A few harder problems to think about

63 Abelian categories

595
63.1 Zero objects, kernels, cokernels, and images . . . . . . . . . . . . . . . . . . . . . . 595
. . . . . . . . . . . . . . . . . . . . . . . . . . . 596
63.2 Additive and abelian categories
63.3 Exact sequences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 598
63.4 The Freyd-Mitchell embedding theorem . . . . . . . . . . . . . . . . . . . . . . . 599
63.5 Breaking long exact sequences . . . . . . . . . . . . . . . . . . . . . . . . . . . . 600
. . . . . . . . . . . . . . . . . . . . . . . . 601
63.6 A few harder problems to think about

60 Objects and morphisms

I can’t possibly hope to do category theory any justice in these few chapters; thus I’ll
just give a very high-level overview of how many of the concepts we’ve encountered so far
can be re-cast into categorical terms. So I’ll say what a category is, give some examples,
then talk about a few things that categories can do. For my examples, I’ll be drawing
from all the previous chapters; feel free to skip over the examples corresponding to things
you haven’t seen.

If you’re interested in category theory (like I was!), perhaps in what surprising results

are true for general categories, I strongly recommend [Le14].

§60.1 Motivation: isomorphisms

From earlier chapters let’s recall the deﬁnition of an isomorphism of two objects:

 Two groups G and H are isomorphic if there was a bijective homomorphism:
equivalently, we wanted homomorphisms φ : G → H and ψ : H → G which were
mutual inverses, meaning φ ◦ ψ = idH and ψ ◦ φ = idG.

 Two metric (or topological) spaces X and Y are isomorphic if there is a continuous

bijection f : X → Y such that f−1 is also continuous.

 Two vector spaces V and W are isomorphic if there is a bijection T : V → W which
is a linear map. Again, this can be re-cast as saying that T and T −1 are linear
maps.

 Two rings R and S are isomorphic if there is a bijective ring homomorphism φ;

again, we can re-cast this as two mutually inverse ring homomorphisms.

In each case we have some collections of objects and some maps, and the isomorphisms
can be viewed as just maps. Let’s use this to motivate the deﬁnition of a general category.

§60.2 Categories, and examples thereof

Prototypical example for this section: Grp is possibly the most natural example.

Deﬁnition 60.2.1. A category A consists of:

 A class of objects, denoted obj(A).
 For any two objects A1, A2 ∈ obj(A), a class of arrows (also called morphisms
or maps) between them. We’ll denote the set of these arrows by HomA(A1, A2).
 For any A1, A2, A3 ∈ obj(A), if f : A1 → A2 is an arrow and g : A2 → A3 is an

arrow, we can compose these arrows to get an arrow g ◦ f : A1 → A3.

571

572

Napkin, by Evan Chen (v1.5.20190718)

We can represent this in a commutative diagram

A1

f - A2

h

g

-

?
A3

where h = g ◦ f . The composition operation ◦ is part of the data of A; it must be
associative. In the diagram above we say that h factors through A2.

 Finally, every object A ∈ obj(A) has a special identity arrow idA; you can guess

what it does.1

Abuse of Notation 60.2.2. From now on, by A ∈ A we’ll mean A ∈ obj(A).
Abuse of Notation 60.2.3. You can think of “class” as just “set”. The reason we
can’t use the word “set” is because of some paradoxical issues with collections which are
too large; Cantor’s Paradox says there is no set of all sets. So referring to these by “class”
is a way of sidestepping these issues.

Now and forever I’ll be sloppy and assume all my categories are locally small, meaning
that HomA(A1, A2) is a set for any A1, A2 ∈ A. So elements of A may not form a set,
but the set of morphisms between two given objects will always assumed to be a set.

Let’s formalize the motivation we began with.

Example 60.2.4 (Basic examples of categories)
(a) There is a category of groups Grp. The data is

 The objects of Grp are the groups.

 The arrows of Grp are the homomorphisms between these groups.
 The composition ◦ in Grp is function composition.

(b) In the same way we can conceive a category CRing of (commutative) rings.

(c) Similarly, there is a category Top of topological spaces, whose arrows are the

continuous maps.

(d) There is a category Top∗ of topological spaces with a distinguished basepoint;
that is, a pair (X, x0) where x0 ∈ X. Arrows are continuous maps f : X → Y
with f (x0) = y0.

(e) Similarly, there is a category Vectk of vector spaces (possibly inﬁnite-dimensional)
over a ﬁeld k, whose arrows are the linear maps. There is even a category FDVectk
of ﬁnite-dimensional vector spaces.

(f) We have a category Set of sets, where the arrows are any maps.

And of course, we can now deﬁne what an isomorphism is!
1To be painfully explicit: if f : A(cid:48) → A is an arrow then idA ◦ f = f ; similarly, if g : A → A(cid:48) is an arrow

then g ◦ idA = g.

60 Objects and morphisms

573

Deﬁnition 60.2.5. An arrow A1
−→ A1
such that f ◦ g = idA2 and g ◦ f = idA1. In that case we say A1 and A2 are isomorphic,
hence A1 ∼= A2.

−→ A2 is an isomorphism if there exists A2

f

g

Remark 60.2.6 — Note that in Set, X ∼= Y ⇐⇒ |X| = |Y |.

Question 60.2.7. Check that every object in a category is isomorphic to itself. (This is
oﬀensively easy.)

More importantly, this deﬁnition should strike you as a little impressive. We’re able to
deﬁne whether two groups (rings, spaces, etc.) are isomorphic solely by the functions
between the objects. Indeed, one of the key themes in category theory (and even algebra)
is that

One can learn about objects by the functions between them. Category
theory takes this to the extreme by only looking at arrows, and ignoring
what the objects themselves are.

But there are some trickier interesting examples of categories.

Example 60.2.8 (Posets are categories)
Let P be a partially ordered set. We can construct a category P for it as follows:

 The objects of P are going to be the elements of P.
 The arrows of P are deﬁned as follows:

– For every object p ∈ P , we add an identity arrow idp, and
– For any pair of distinct objects p ≤ q, we add a single arrow p → q.

There are no other arrows.

 There’s only one way to do the composition. What is it?

For example, for the poset P on four objects {a, b, c, d} with a ≤ b and a ≤ c ≤ d, we get:

ida

a

a ≤ b

a ≤ c

idb

b

a ≤ d

c

idc

c ≤ d

idd

d

574

Napkin, by Evan Chen (v1.5.20190718)

This illustrates the point that

The arrows of a category can be totally diﬀerent from functions.

In fact, in a way that can be made precise, the term “concrete category” refers to one
where the arrows really are “structure-preserving maps between sets”, like Grp, Top, or
CRing.

Question 60.2.9. Check that no two distinct objects of a poset are isomorphic.

Here’s a second quite important example of a non-concrete category.

Example 60.2.10 (Important: groups are one-Object categories)
A group G can be interpreted as a category G with one object ∗, all of whose arrows
are isomorphisms.

1 = ida

g4

g2

∗

g3

As [Le14] says:

The ﬁrst time you meet the idea that a group is a kind of category, it’s
tempting to dismiss it as a coincidence or a trick. It’s not: there’s real
content. To see this, suppose your education had been shuﬄed and you
took a course on category theory before ever learning what a group was.
Someone comes to you and says:

“There are these structures called ‘groups’, and the idea is this: a group
is what you get when you collect together all the symmetries of a given
thing.”

“What do you mean by a ‘symmetry’ ?” you ask.

“Well, a symmetry of an object X is a way of transforming X or mapping
X into itself, in an invertible way.”

“Oh,” you reply, “that’s a special case of an idea I’ve met before. A category
is the structure formed by lots of objects and mappings between them
– not necessarily invertible. A group’s just the very special case where
you’ve only got one object, and all the maps happen to be invertible.”

Exercise 60.2.11. Verify the above! That is, show that the data of a one-object category
with all isomorphisms is the same as the data of a group.

Finally, here are some examples of categories you can make from other categories.

60 Objects and morphisms

575

Example 60.2.12 (Deriving categories)
(a) Given a category A, we can construct the opposite category Aop, which is

the same as A but with all arrows reversed.

(b) Given categories A and B, we can construct the product category A × B as
follows: the objects are pairs (A, B) for A ∈ A and B ∈ B, and the arrows from
(A1, B1) to (A2, B2) are pairs

What do you think the composition and identities are?

g

−→ B2(cid:17) .

(cid:16)A1

f

−→ A2, B1

§60.3 Special objects in categories

Prototypical example for this section: Set has initial object ∅ and ﬁnal object {∗}. An
element of S corresponds to a map {∗} → S.

Certain objects in categories have special properties. Here are a couple exam-

ples.

Example 60.3.1 (Initial object)
An initial object of A is an object Ainit ∈ A such that for any A ∈ A (possibly
A = Ainit), there is exactly one arrow from Ainit to A. For example,

(a) The initial object of Set is the empty set ∅.
(b) The initial object of Grp is the trivial group {1}.
(c) The initial object of CRing is the ring Z (recall that ring homomorphisms R → S

map 1R to 1S).

(d) The initial object of Top is the empty space.

(e) The initial object of a partially ordered set is its smallest element, if one exists.

We will usually refer to “the” initial object of a category, since:

Exercise 60.3.2 (Important!). Show that any two initial objects A1, A2 of A are uniquely
isomorphic meaning there is a unique isomorphism between them.

Remark 60.3.3 — In mathematics, we usually neither know nor care if two objects
are actually equal or whether they are isomorphic. For example, there are many
competing ways to deﬁne R, but we still just refer to it as “the” real numbers.

Thus when we deﬁne categorical notions, we would like to check they are unique
up to isomorphism. This is really clean in the language of categories, and deﬁnitions
often cause objects to be unique up to isomorphism for elegant reasons like the
above.

One can take the “dual” notion, a terminal object.

576

Napkin, by Evan Chen (v1.5.20190718)

Example 60.3.4 (Terminal object)
A terminal object of A is an object Aﬁnal ∈ A such that for any A ∈ A (possibly
A = Aﬁnal), there is exactly one arrow from A to Aﬁnal. For example,

(a) The terminal object of Set is the singleton set {∗}. (There are many singleton

sets, of course, but as sets they are all isomorphic!)

(b) The terminal object of Grp is the trivial group {1}.
(c) The terminal object of CRing is the zero ring 0. (Recall that ring homomorphisms

R → S must map 1R to 1S).

(d) The terminal object of Top is the single-point space.

(e) The terminal object of a partially ordered set is its maximal element, if one

exists.

Again, terminal objects are unique up to isomorphism. The reader is invited to repeat
the proof from the preceding exercise here. However, we can illustrate more strongly the
notion of duality to give a short proof.

Question 60.3.5. Verify that terminal objects of A are equivalent to initial objects of Aop.
Thus terminal objects of A are unique up to isomorphism.

In general, one can consider in this way the dual of any categorical notion: properties
of A can all be translated to dual properties of Aop (often by adding the preﬁx “co” in
front).
One last neat construction: suppose we’re working in a concrete category, meaning
(loosely) that the objects are “sets with additional structure”. Now suppose you’re sick
of maps and just want to think about elements of these sets. Well, I won’t let you do
that since you’re reading a category theory chapter, but I will oﬀer you some advice:

 In Set, arrows from {∗} to S correspond to elements of S.
 In Top, arrows from {∗} to X correspond to points of X.
 In Grp, arrows from Z to G correspond to elements of G.

 In CRing, arrows from Z[x] to R correspond to elements of R.

and so on. So in most concrete categories, you can think of elements as functions from
special sets to the set in question. In each of these cases we call the object in question a
free object.

§60.4 Binary products

Prototypical example for this section: X×Y in most concrete categories is the set-theoretic
product.

The “universal property” is a way of describing objects in terms of maps in such a way
that it deﬁnes the object up to unique isomorphism (much the same as the initial and
terminal objects).

To show how this works in general, let me give a concrete example. Suppose I’m in a
category – let’s say Set for now. I have two sets X and Y , and I want to construct the

60 Objects and morphisms

577

Cartesian product X × Y as we know it. The philosophy of category theory dictates that
I should talk about maps only, and avoid referring to anything about the sets themselves.
How might I do this?

Well, let’s think about maps into X × Y . The key observation is that

f

A function A

−→ X × Y amounts to a pair of functions (A

−→ X, A h−→ Y ).
Put another way, there is a natural projection map X × Y (cid:16) X and X × Y (cid:16) Y :

g

X × Y

--

X

π X

π

Y

--
Y

(We have to do this in terms of projection maps rather than elements, because category
theory forces us to talk about arrows.) Now how do I add A to this diagram? The point
−→ X × Y and pairs (g, h) of functions.

is that there is a bijection between functions A

f

g

−→ X and A h−→ Y there is a unique function A

f

−→ X × Y .

Thus for every pair A

But X × Y is special in that it is “universal”: for any other set A, if you give me
functions A → X and A → Y , I can use it build a unique function A → X × Y . Picture:

A ........∃!f .-

g

X × Y

h

π X

π

Y

-
--

X

--
-
Y

We can do this in any general category, deﬁning a so-called product.

Deﬁnition 60.4.1. Let X and Y be objects in any category A. The product consists
of an object X × Y and arrows πX , πY to X and Y (thought of as projection). We
−→ X, A h−→ Y , there is a unique function
require that for any object A and arrows A

g

f

A

−→ X × Y such that the diagram

A ........∃!f .-

g

X × Y

h

π X

π

Y

-
--

X

--
-
Y

commutes.

Abuse of Notation 60.4.2. Strictly speaking, the product should consist of both the
object X×Y and the projection maps πX and πY . However, if πX and πY are understood,
then we often use X × Y to refer to the object, and refer to it also as the product.

Products do not always exist; for example, take a category with just two objects and

no non-identity morphisms. Nonetheless:

578

Napkin, by Evan Chen (v1.5.20190718)

Proposition 60.4.3 (Uniqueness of products)

When they exist, products are unique up to isomorphism: given two products P1
and P2 of X and Y there is an isomorphism between the two objects.

Proof. This is very similar to the proof that initial objects are unique up to unique
isomorphism. Consider two such objects P1 and P2, and the associated projection maps.
So, we have a diagram

- -

X
66





1
π

X

π

1

X

π2
X

P1

-

P2

f

-

P1

g

π

1

Y

-

-

π2
Y
?? 
Y

1 Y
π

There are unique morphisms f and g between P1 and P2 that make the entire diagram
commute, according to the universal property.

On the other hand, look at g ◦ f and focus on just the outer square. Observe that g ◦ f
is a map which makes the outer square commute, so by the universal property of P1 it is
the only one. But idP1 works as well. Thus idP1 = g ◦ f . Similarly, f ◦ g = idP2 so f and
g are isomorphisms.

Abuse of Notation 60.4.4. Actually, this is not really the morally correct theorem;
since we’ve only showed the objects P1 and P2 are isomorphic and have not made any
assertion about the projection maps. But I haven’t (and won’t) deﬁne isomorphism of
the entire product, and so in what follows if I say “P1 and P2 are isomorphic” I really
just mean the objects are isomorphic.

Exercise 60.4.5. In fact, show the products are unique up to unique isomorphism: the f
and g above are the only isomorphisms between the objects P1 and P2.

The nice fact about this “universal property” mindset is that we don’t have to give
explicit constructions; assuming existence, the “universal property” allows us to bypass all
this work by saying “the object with these properties is unique up to unique isomorphism”,
thus we don’t need to understand the internal workings of the object to use its properties.

Of course, that’s not to say we can’t give concrete examples.

60 Objects and morphisms

579

Example 60.4.6 (Examples of products)
(a) In Set, the product of two sets X and Y is their Cartesian product X × Y .
(b) In Grp, the product of G, H is the group product G × H.
(c) In Vectk, the product of V and W is V ⊕ W .
(d) In CRing, the product of R and S is appropriately the ring product R × S.
(e) Let P be a poset interpreted as a category. Then the product of two objects x

and y is the greatest lower bound; for example,

 If the poset is (R,≤) then it’s min{x, y}.
 If the poset is the subsets of a ﬁnite set by inclusion, then it’s x ∩ y.
 If the poset is the positive integers ordered by division, then it’s gcd(x, y).

Of course, we can deﬁne products of more than just one object. Consider a set of
objects (Xi)i∈I in a category A. We deﬁne a cone on the Xi to be an object A with
some “projection” maps to each Xi. Then the product is a cone P which is “universal”
in the same sense as before: given any other cone A there is a unique map A → P making
the diagram commute. In short, a product is a “universal cone”.

The picture of this is

A

!∃f
?
P

 
X1


X2

-

-
-
-

X3

See also Problem 60C.

-

-

-

-

X4

object X + Y together with maps X

One can also do the dual construction to get a coproduct: given X and Y , it’s the
ιY−→ X + Y (that’s Greek iota,
−→ A, Y h−→ A there is a unique

think inclusion) such that for any object A and maps X
f for which

ιX−→ X + Y and Y

g

X

Y

ι

X

Y
ι

-

X + Y
-

g

h

!∃f

-

-
-

A

commutes. We’ll leave some of the concrete examples as an exercise this time, for
example:

580

Napkin, by Evan Chen (v1.5.20190718)

Exercise 60.4.7. Describe the coproduct in Set.

Predictable terminology: a coproduct is a universal cocone.

Spoiler alert later on: this construction can be generalized vastly to so-called “limits”,

and we’ll do so later on.

§60.5 Monic and epic maps

The notion of “injective” doesn’t make sense in an arbitrary category since arrows need
not be functions. The correct categorical notion is:

Deﬁnition 60.5.1. A map X
tive diagram

f

−→ Y is monic (or a monomorphism) if for any commuta-

A

g -

- X

h

f - Y

we must have g = h. In other words, f ◦ g = f ◦ h =⇒ g = h.

Question 60.5.2. Verify that in a concrete category, injective =⇒ monic.

Question 60.5.3. Show that the composition of two monic maps is monic.

In most but not all situations, the converse is also true.

Exercise 60.5.4. Show that in Set, Grp, CRing, monic implies injective. (Take A = {∗},
A = {1}, A = Z[x].)

More generally, as we said before there are many categories with a “free” object that you
can use to think of as elements. An element of a set is a function 1 → S, and element
of a ring is a function Z[x] → R, et cetera. In all these categories, the deﬁnition of
monic literally reads “f is injective on HomA(A, X)”. So in these categories, “monic”
and “injective” coincide.

That said, here is the standard counterexample. An additive abelian group G = (G, +)
is called divisible if for every x ∈ G and n ∈ Z there exists y ∈ G with ny = x. Let
DivAbGrp be the category of such groups.

Exercise 60.5.5. Show that the projection Q → Q/Z is monic but not injective.

Of course, we can also take the dual notion.

Deﬁnition 60.5.6. A map X
diagram

f

−→ Y is epic (or an epimorphism) if for any commutative

X

f - Y

g -

- A

h

we must have g = h. In other words, g ◦ f = h ◦ f =⇒ g = h.

This is kind of like surjectivity, although it’s a little farther than last time. Note that

in concrete categories, surjective =⇒ epic.

60 Objects and morphisms

581

Exercise 60.5.7. Show that in Set, Grp, Ab, Vectk, Top, the notions of epic and surjective
coincide. (For Set, take A = {0, 1}.)

However, there are more cases where it fails. Most notably:

Example 60.5.8 (Epic but not surjective)
(a) In CRing, for instance, the inclusion Z (cid:44)→ Q is epic (and not surjective).. Indeed,
if two homomorphisms Q → A agree on every integer then they agree everywhere
(why?),

(b) In the category of Hausdorﬀ topological spaces (every two points have disjoint

open neighborhoods), in fact epic ⇐⇒ dense image (like Q (cid:44)→ R).

Thus failures arise when a function f : X → Y can be determined by just some of
the points of X.

§60.6 A few harder problems to think about

Problem 60A. In the category Vectk of k-vector spaces (for a ﬁeld k), what are the
initial and terminal objects?

Problem 60B†. What is the coproduct X + Y in the categories Set, Vectk, and a poset?

Problem 60C. In any category A where all products exist, show that

(X × Y ) × Z ∼= X × (Y × Z)

where X, Y , Z are arbitrary objects. (Here both sides refer to the objects, as in Abuse
of Notation 60.4.2.)

Problem 60D. Consider a category A with a zero object, meaning an object which
is both initial and terminal. Given objects X and Y in A, prove that the projection
X × Y → X is epic.

61 Functors and natural

transformations

Functors are maps between categories; natural transformations are maps between

functors.

§61.1 Many examples of functors

Prototypical example for this section: Forgetful functors; fundamental groups; −∨.

Here’s the point of a functor:

Pretty much any time you make an object out of another object, you get
a functor.

Before I give you a formal deﬁnition, let me list (informally) some examples. (You’ll
notice some of them have opposite categories Aop appearing in places. Don’t worry about
those for now; you’ll see why in a moment.)

 Given a group G (or vector space, ﬁeld, . . . ), we can take its underlying set S; this

is a functor from Grp → Set.

 Given a set S we can consider a vector space with basis S; this is a functor from

Set → Vect.

 Given a vector space V we can consider its dual space V ∨. This is a functor

Vectop

k → Vectk.

 Tensor products give a functor from Vectk × Vectk → Vectk.
 Given a set S, we can build its power set, giving a functor Set → Set.
 In algebraic topology, we take a topological space X and build several groups
H1(X), π1(X), etc. associated to it. All these group constructions are functors
Top → Grp.

 Sets of homomorphisms: let A be a category.

– Given two vector spaces V1 and V2 over k, we construct the abelian group of

linear maps V1 → V2. This is a functor from Vectop

k × Vectk → AbGrp.

– More generally for any category A we can take pairs (A1, A2) of objects and
obtain a set HomA(A1, A2). This turns out to be a functor Aop × A → Set.
– The above operation has two “slots”. If we “pre-ﬁll” the ﬁrst slots, then we
get a functor A → Set. That is, by ﬁxing A ∈ A, we obtain a functor (called
H A) from A → Set by sending A(cid:48) ∈ A to HomA(A, A(cid:48)). This is called the
covariant Yoneda functor (explained later).

– As we saw above, for every A ∈ A we obtain a functor H A : A → Set. It turns
out we can construct a category [A, Set] whose elements are functors A → Set;
in that case, we now have a functor Aop → [A, Set].

583

584

Napkin, by Evan Chen (v1.5.20190718)

§61.2 Covariant functors

Prototypical example for this section: Forgetful/free functors, . . .

Category theorists are always asking “what are the maps?”, and so we can now think

about maps between categories.
Deﬁnition 61.2.1. Let A and B be categories. Of course, a functor F takes every
object of A to an object of B. In addition, though, it must take every arrow A1
−→ A2 to
an arrow F (A1)

−−−→ F (A2). You can picture this as follows.

F (f )

f

A1

B1 = F (A1)

A (cid:51)

F -
........................

f

F (f )

?
A2

?
B2

= F (A2)

∈ B

(I’ll try to use dotted arrows for functors, which cross diﬀerent categories, for emphasis.)
It needs to satisfy the “naturality” requirements:

 Identity arrows get sent to identity arrows: for each identity arrow idA, we have

F (idA) = idF (A).

 The functor respects composition:

if A1

F (g ◦ f ) = F (g) ◦ F (f ).

f

−→ A2

g

−→ A3 are arrows in A, then

So the idea is:
Whenever we naturally make an object A ∈ A into an object of B ∈ B,
there should usually be a natural way to transform a map A1 → A2 into
a map B1 → B2.

Let’s see some examples of this.

Example 61.2.2 (Free and forgetful functors)
Note that these are both informal terms, and don’t have a rigid deﬁnition.

(a) We talked about a forgetful functor earlier, which takes the underlying set of

a category like Vectk. Let’s call it U : Vectk → Set.
Now, given a map T : V1 → V2 in Vectk, there is an obvious U (T ) : U (V1) →
U (V2) which is just the set-theoretic map corresponding to T .
Similarly there are forgetful functors from Grp, CRing, etc., to Set. There is even
a forgetful functor CRing → Grp: send a ring R to the abelian group (R, +). The
common theme is that we are “forgetting” structure from the original category.

(b) We also talked about a free functor in the example. A free functor F : Set →
Vectk can be taken by considering F (S) to be the vector space with basis S.
Now, given a map f : S → T , what is the obvious map F (S) → F (T )? Simple:
take each basis element s ∈ S to the basis element f (s) ∈ T .
Similarly, we can deﬁne F : Set → Grp by taking the free group generated by a
set S.

61 Functors and natural transformations

585

Remark 61.2.3 — There is also a notion of “injective” and “surjective” for functors
(on arrows) as follows. A functor F : A → B is faithful (resp. full) if for any A1, A2,
F : HomA(A1, A2) → HomB(F A1, F A2) is injective (resp. surjective).a
We can use this to give an exact deﬁnition of concrete category: it’s a category
with a faithful (forgetful) functor U : A → Set.
aAgain, experts might object that HomA(A1, A2) or HomB(F A1, F A2) may be proper classes

instead of sets, but I am assuming everything is locally small.

Example 61.2.4 (Functors from G)
Let G be a group and G = {∗} be the associated one-object category.
(a) Consider a functor F : G → Set, and let S = F (∗). Then the data of F

corresponds to putting a group action of G on S.

(b) Consider a functor F : G → FDVectk, and let V = F (∗) have dimension n. Then
the data of F corresponds to embedding G as a subgroup of the n × n matrices
(i.e. the linear maps V → V ). This is one way groups historically arose; the
theory of viewing groups as matrices forms the ﬁeld of representation theory.

(c) Let H be a group and construct H the same way. Then functors G → H

correspond to homomorphisms G → H.

Exercise 61.2.5. Check the above group-based functors work as advertised.

Here’s a more involved example. If you ﬁnd it confusing, skip it and come back after

reading about its contravariant version.

Example 61.2.6 (Covariant Yoneda functor)
Fix an A ∈ A. For a category A, deﬁne the covariant Yoneda functor H A : A →
Set by deﬁning

H A(A1) := HomA(A, A1) ∈ Set.

Hence each A1 is sent to the arrows from A to A1; so H A describes how A sees
the world.

f

Now we want to specify how H A behaves on arrows. For each arrow A1

−→ A2, we
need to specify Set-map HomA(A, A1) → Hom(A, A2); in other words, we need to
−→ A1 to an arrow A → A2. There’s only one reasonable way to do
send an arrow A
this: take the composition

p

In other words, HA(f ) is p (cid:55)→ f ◦ p. In still other words, HA(f ) = f ◦ −; the − is a
slot for the input to go into.

A

p

−→ A1

f

−→ A2.

As another example:

Question 61.2.7. If P and Q are posets interpreted as categories, what does a functor
from P to Q represent?
Now, let me explain why we might care. Consider the following “obvious” fact: if G
and H are isomorphic groups, then they have the same size. We can formalize it by

586

Napkin, by Evan Chen (v1.5.20190718)

saying: if G ∼= H in Grp and U : Grp → Set is the forgetful functor (mapping each group
to its underlying set), then U (G) ∼= U (H). The beauty of category theory shows itself:
this in fact works for any functors and categories, and the proof is done solely through
arrows:

Theorem 61.2.8 (Functors preserve isomorphism)
If A1 ∼= A2 are isomorphic objects in A and F : A → B is a functor then F (A1) ∼=
F (A2).

Proof. Try it yourself! The picture is:

A (cid:51)

A1

6

B1 = F (A1)

6

F-
......

f

g

F (f )

F (g)

?
A2

?
B2

= F (A2)

∈ B

You’ll need to use both key properties of functors: they preserve composition and the
identity map.

This will give us a great intuition in the future, because

(i) Almost every operation we do in our lifetime will be a functor, and

(ii) We now know that functors take isomorphic objects to isomorphic objects.

Thus, we now automatically know that basically any “reasonable” operation we do will
preserve isomorphism (where “reasonable” means that it’s a functor). This is super
convenient in algebraic topology, for example; see Theorem 58.6.2, where we get for free
that homotopic spaces have isomorphic fundamental groups.

Remark 61.2.9 — This lets us construct a category Cat whose objects are categories
and arrows are functors.

§61.3 Contravariant functors

Prototypical example for this section: Dual spaces, contravariant Yoneda functor, etc.

Now I have to explain what the opposite categories were doing earlier. In all the
previous examples, we took an arrow A1 → A2, and it became an arrow F (A1) → F (A2).
Sometimes, however, the arrow in fact goes the other way: we get an arrow F (A2) →
F (A1) instead. In other words, instead of just getting a functor A → B we ended up
with a functor Aop → B.

These functors have a name:

Deﬁnition 61.3.1. A contravariant functor from A to B is a functor F : Aop → B.
(Note that we do not write “contravariant functor F : A → B”, since that would be
confusing; the function notation will always use the correct domain and codomain.)

61 Functors and natural transformations

587

Pictorially:

A (cid:51)

A1

B1 = F (A1)

6

F -
........................

f

F (f )

?
A2

B2

= F (A2)

∈ B

For emphasis, a usual functor is often called a covariant functor. (The word “functor”
with no adjective always refers to covariant.)

Let’s see why this might happen.

Example 61.3.2 (V (cid:55)→ V ∨ is contravariant)
Consider the functor Vectk → Vectk by V (cid:55)→ V ∨.
If we were trying to specify a covariant functor, we would need, for every linear
map T : V1 → V2, a linear map T ∨ : V ∨1 → V ∨2 . But recall that V ∨1 = Hom(V1, k)
and V ∨2 = Hom(V2, k): there’s no easy way to get an obvious map from left to right.
However, there is an obvious map from right to left: given ξ2 : V2 → k, we can
easily give a map from V1 → k: just compose with T ! In other words, there is a very
natural map V ∨2 → V ∨1 according to the composition

V1

T - V2

ξ2 - k

In summary, a map T : V1 → V2 induces naturally a map T ∨ : V ∨2 → V ∨1
opposite direction. So the contravariant functor looks like:

in the

V1

T

V ∨1
6
T ∨

∨

-
........................

−

?
V2

V ∨2

We can generalize the example above in any category by replacing the ﬁeld k with any

chosen object A ∈ A.

Example 61.3.3 (Contravariant Yoneda functor)
The contravariant Yoneda functor on A, denoted HA : Aop → Set, is used to
describe how objects of A see A. For each X ∈ A it puts
HA(X) := HomA(X, A) ∈ Set.

For X

f

−→ Y in A, the map HA(f ) sends each arrow Y

p

−→ A ∈ HomA(Y, A) to

X

f

−→ Y

p

−→ A ∈ HomA(X, A)

as we did above. Thus HA(f ) is an arrow from HomA(Y, A) → HomA(X, A). (Note
the ﬂipping!)

fully faithful
fully faithful
and essen-
and essen-
tially surjec-
tially surjec-
tive
tive

588

Napkin, by Evan Chen (v1.5.20190718)

Exercise 61.3.4. Check now the claim that Aop × A → Set by (A1, A2) (cid:55)→ Hom(A1, A2) is
in fact a functor.

§61.4 Equivalence of categories

§61.5 (Optional) Natural transformations

We made categories to keep track of objects and maps, then went a little crazy and asked
“what are the maps between categories?” to get functors. Now we’ll ask “what are the
maps between functors?” to get natural transformations.

It might sound terrifying that we’re drawing arrows between functors, but this is
actually an old idea. Recall that given two paths α, β : [0, 1] → X, we built a path-
homotopy by “continuously deforming” the path α to β; this could be viewed as a
function [0, 1] × [0, 1] → X. The deﬁnition of a natural transformation is similar: we
want to pull F to G along a series of arrows in the target space B.
Deﬁnition 61.5.1. Let F, G : A → B be two functors. A natural transformation α
from F to G, denoted

A

F
 α

G

B

consists of, for each A ∈ A an arrow αA ∈ HomB(F (A), G(A)), which is called the
component of α at A. Pictorially, it looks like this:

F (A) ∈ B
-

F

αA

?

....................
....................

G

-

A (cid:51)

A

These αA are subject to the “naturality” requirement that for any A1

f

−→ A2, the diagram

G(A) ∈ B

F (A1)

F (f )- F (A2)

αA1
?
G(A1)

αA2
?
- G(A2)

G(f )

commutes.

The arrow αA represents the path that F (A) takes to get to G(A) (just as in a path-
homotopy from α to β each point α(t) gets deformed to the point β(t) continuously). A
picture might help: consider

'
'
7
7

61 Functors and natural transformations

589

Here A is the small category with three elements and two non-identity arrows f , g (I’ve
omitted the identity arrows for simplicity). The images of A under F and G are the blue
and green “subcategories” of B. Note that B could potentially have many more objects
and arrows in it (grey). The natural transformation α (red) selects an arrow of B from
each F (A) to the corresponding G(A), dragging the entire image of F to the image of
G. Finally, we require that any diagram formed by the blue, red, and green arrows is
commutative (naturality), so the natural transformation is really “natural”.

There is a second equivalent deﬁnition that looks much more like the homotopy.

Deﬁnition 61.5.2. Let 2 denote the category generated by a poset with two elements
0 ≤ 1, that is,

id0

0

id1

1

0 ≤ 1

Then a natural transformation A

F
 α

G

B is just a functor α : A× 2 → B satisfying

α(A, 0) = F (A), α(f, 0) = F (f )

and α(A, 1) = G(A), α(f, 1) = G(f ).

More succinctly, α(−, 0) = F , α(−, 1) = G.

The proof that these are equivalent is left as a practice problem.
Naturally, two natural transformations α : F → G and β : G → H can get composed.

F (A)

-

αA

?
G(A)

G -
A (cid:51) A ............

F

..............................
...............................

H

βA

-

?
H(A)

A1A2A3fgAF(A1)F(A2)F(A3)F(f)F(g)G(A1)G(A2)G(A3)G(f)G(g)αA1αA2αA3FGαB'
'
7
7

590

Napkin, by Evan Chen (v1.5.20190718)

Now suppose α is a natural transformation such that αA is an isomorphism for each A.

In this way, we can construct an inverse arrow βA to it.
F (A) ∈ B
-
..................
6
...................

A (cid:51) A

αA

βA

F

G

-

?

G(A) ∈ B

In this case, we say α is a natural isomorphism. We can then say that F (A) ∼= G(A)
naturally in A. (And β is an isomorphism too!) This means that the functors F and G
are “really the same”: not only are they isomorphic on the level of objects, but these
isomorphisms are “natural”. As a result of this, we also write F ∼= G to mean that the
functors are naturally isomorphic.

This is what it really means when we say that “there is a natural / canonical isomor-
phism”. For example, I claimed earlier (in Problem 15A(cid:63)) that there was a canonical
isomorphism (V ∨)∨ ∼= V , and mumbled something about “not having to pick a basis”
and “God-given”. Category theory, amazingly, lets us formalize this: it just says that
(V ∨)∨ ∼= id(V ) naturally in V ∈ FDVectk. Really, we have a natural transformation

FDVectk

id
 ε
∨)∨

(−

FDVectk .

where the component εV is given by v (cid:55)→ evv (as discussed earlier, the fact that it is an
isomorphism follows from the fact that V and (V ∨)∨ have equal dimensions and εV is
injective).

§61.6 (Optional) The Yoneda lemma

Now that I have natural transformations, I can deﬁne:

Deﬁnition 61.6.1. The functor category of two categories A and B, denoted [A,B],
is deﬁned as follows:

 The objects of [A,B] are (covariant) functors F : A → B, and
 The morphisms are natural transformations α : F → G.

Question 61.6.2. When are two objects in the functor category isomorphic?

With this, I can make good on the last example I mentioned at the beginning:

Exercise 61.6.3. Construct the following functors:
 A → [Aop, Set] by A (cid:55)→ HA, which we call H•.
 Aop → [A, Set] by A (cid:55)→ H A, which we call H•.

Notice that we have opposite categories either way; even if you like H A because it is
covariant, the map H• is contravariant. So for what follows, we’ll prefer to use H•.
Aop → Set which are already “built” in to the category A. In light of this, we deﬁne:

The main observation now is that given a category A, H• provides some special functors

,
,
2
2

Natural transformations Aop



HA
 α

X

Set → X(A)

61 Functors and natural transformations

591

Deﬁnition 61.6.4. A presheaf X is just a contravariant functor Aop → Set. It is called
representable if X ∼= HA for some A.

In other words, when we think about representable, the question we’re asking is:

What kind of presheaves are already “built in” to the category A?

One way to get at this question is: given a presheaf X and a particular HA, we can look
at the set of natural transformations α : X =⇒ HA, and see if we can learn anything
about it. In fact, this set can be written explicitly:

Theorem 61.6.5 (Yoneda lemma)
Let A be a category, pick A ∈ A, and let HA be the contravariant Yoneda functor.
Let X : Aop → Set be a contravariant functor. Then the map

deﬁned by α (cid:55)→ αA(idA) ∈ X(A) is an isomorphism of Set (i.e. a bijection). Moreover,
if we view both sides of the equality as functors

then this isomorphism is natural.

Aop × [Aop, Set] → Set

This might be startling at ﬁrst sight. Here’s an unsatisfying explanation why this
might not be too crazy: in category theory, a rule of thumb is that “two objects of the
same type that are built naturally are probably the same”. You can see this theme
when we deﬁned functors and natural transformations, and even just compositions. Now
to look at the set of natural transformations, we took a pair of elements A ∈ A and
X ∈ [Aop, Set] and constructed a set of natural transformations. Is there another way we
can get a set from these two pieces of information? Yes: just look at X(A). The Yoneda
lemma is telling us that our heuristic still holds true here.

Some consequences of the Yoneda lemma are recorded in [Le14]. Since this chapter is
already a bit too long, I’ll just write down the statements, and refer you to [Le14] for the
proofs.

1. As we mentioned before, H• provides a functor

A → [Aop, Set].

It turns out this functor is in fact fully faithful ; it quite literally embeds the category
A into the functor category on the right (much like Cayley’s theorem embeds every
group into a permutation group).

2. If X, Y ∈ A then

HX ∼= HY ⇐⇒ X ∼= Y ⇐⇒ H X ∼= H Y .

To see why this is expected, consider A = Grp for concreteness. Suppose A, X, Y
are groups such that HX (A) ∼= HY (A) for all A. For example,

)
)
5
5

592

Napkin, by Evan Chen (v1.5.20190718)

 If A = Z, then |X| = |Y |.
 If A = Z/2Z, then X and Y have the same number of elements of order 2.
 . . .

Each A gives us some information on how X and Y are similar, but the whole
natural isomorphism is strong enough to imply X ∼= Y .

3. Consider the functor U : Grp → Set. It can be represented by HZ, in the sense that

HomGrp(Z, G) ∼= U (G)

by

φ (cid:55)→ φ(1).

That is, elements of G are in bijection with maps Z → G, determined by the image
of +1 (or −1 if you prefer). So a representation of U was determined by looking at
Z and picking +1 ∈ U (Z).
The generalization of this is a follows: let A be a category and X : A → Set a
covariant functor. Then a representation H A ∼= X consists of an object A ∈ A
and an element u ∈ X(A) satisfying a certain condition. You can read this oﬀ the
condition1 if you know what the inverse map is in Theorem 61.6.5. In the above
situation, X = U , A = Z and u = ±1.

§61.7 A few harder problems to think about

Problem 61A. Show that the two deﬁnitions of natural transformation (one in terms
of A × 2 → B and one in terms of arrows F (A)
Problem 61B. Let A be the category of ﬁnite sets whose arrows are bijections between
sets. For A ∈ A, let F (A) be the set of permutations of A and let G(A) be the set of
orderings on A.2

αA−−→ G(A)) are equivalent.

(a) Extend F and G to functors A → Set.
(b) Show that F (A) ∼= G(A) for every A, but this isomorphism is not natural.
Problem 61C (Proving the Yoneda lemma). In the context of Theorem 61.6.5:

(a) Prove that the map described is in fact a bijection. (To do this, you will probably

have to explicitly write down the inverse map.)

(b) Prove that the bijection is indeed natural. (This is long-winded, but not diﬃcult;

from start to ﬁnish, there is only one thing you can possibly do.)

1Just for completeness, the condition is: For all A(cid:48) ∈ A and x ∈ X(A(cid:48)), there’s a unique f : A → A(cid:48)
2 A permutation is a bijection A → A, and an ordering is a bijection {1, . . . , n} → A, where n is the

with (Xf )(u) = x.

size of A.

62 Limits in categories (TO DO)

We saw near the start of our category theory chapter the nice construction of products
by drawing a bunch of arrows. It turns out that this concept can be generalized immensely,
and I want to give a you taste of that here.

write intro-
write intro-
duction
duction

To run this chapter, we follow the approach of [Le14].

§62.1 Equalizers

Prototypical example for this section: The equalizer of f, g : X → Y is the set of points
with f (x) = g(x).

Given two sets X and Y , and maps X

f,g

−−→ Y , we deﬁne their equalizer to be

{x ∈ X | f (x) = g(x)} .

We would like a categorical way of deﬁning this, too.

Consider two objects X and Y with two maps f and g between them. Stealing a page

from [Le14], we call this a fork:

X

f -

- Y

g

A cone over this fork is an object A and arrows over X and Y which make the diagram
commute, like so.

A

q

?
X

f

◦q

=

g

◦q

-

f -

- Y

g

Eﬀectively, the arrow over Y is just forcing f ◦ q = g ◦ q. In any case, the equalizer of f
and g is a “universal cone” over this fork: it is an object E and a map E e−→ X such that
for each A

q

−→ X the diagram

A

!∃h
?
E

g

--

f -
-

Y

q

 e


X

commutes for a unique A h−→ E. In other words, any map A

−→ X as above must factor
uniquely through E. Again, the dotted arrows can be omitted, and as before equalizers
may not exist. But when they do exist:

q

593

594

Napkin, by Evan Chen (v1.5.20190718)

Exercise 62.1.1. If E

e

−→ X and E(cid:48) e(cid:48)

−→ X are equalizers, show that E ∼= E(cid:48).

Example 62.1.2 (Examples of equalizers)
(a) In Set, given X

f,g

−−→ Y the equalizer E can be realized as E = {x | f (x) = g(x)},
with the inclusion e : E (cid:44)→ X as the morphism. As usual, by abuse we’ll often
just refer to E as the equalizer.

(b) Ditto in Top, Grp. One has to check that the appropriate structures are preserved

(e.g. one should check that {φ(g) = ψ(g) | g ∈ G} is a group).

(c) In particular, given a homomorphism φ : G → H, the inclusion ker φ (cid:44)→ G is an

equalizer for the fork G → H by φ and the trivial homomorphism.

According to (c) equalizers let us get at the concept of a kernel if there is a distinguished
“trivial map”, like the trivial homomorphism in Grp. We’ll ﬂesh this idea out in the chapter
on abelian categories.

§62.2 Pullback squares (TO DO)

write me
write me

Great example: diﬀerentiable functions on (−3, 1) and (−1, 3)

Example 62.2.1

§62.3 Limits

We’ve deﬁned cones over discrete sets of Xi and over forks. It turns out you can also
deﬁne a cone over any general diagram of objects and arrows; we specify a projection
from A to each object and require that the projections from A commute with the arrows
in the diagram. (For example, a cone over a fork is a diagram with two edges and two
arrows.) If you then demand the cone be universal, you have the extremely general
deﬁnition of a limit. As always, these are unique up to unique isomorphism. We can
also deﬁne the dual notion of a colimit in the same way.

§62.4 A few harder problems to think about

Problem 62A(cid:63) (Equalizers are monic). Show that the equalizer of any fork is monic.

pushout square gives tenor product
p-adic
relative Chinese remainder theorem!!

63 Abelian categories

In this chapter I’ll translate some more familiar concepts into categorical language;
this will require some additional assumptions about our category, culminating in the
deﬁnition of a so-called “abelian category”. Once that’s done, I’ll be able to tell you
what this “diagram chasing” thing is all about.

Throughout this chapter, “(cid:44)→” will be used for monic maps and “(cid:16)” for epic maps.

§63.1 Zero objects, kernels, cokernels, and images

Prototypical example for this section: In Grp, the trivial group and homomorphism are
the zero objects and morphisms. If G, H are abelian then the cokernel of φ : G → H is
H/ im φ.

A zero object of a category is an object 0 which is both initial and terminal; of
course, it’s unique up to unique isomorphism. For example, in Grp the zero object is the
trivial group, in Vectk it’s the zero-dimensional vector space consisting of one point, and
so on.

Question 63.1.1. Show that Set and Top don’t have zero objects.

For the rest of this chapter, all categories will have zero objects.

In a category A with zero objects, any two objects A and B thus have a distinguished

morphism

A → 0 → B

which is called the zero morphism and also denoted 0. For example, in Grp this is the
trivial homomorphism.

We can now deﬁne:

Deﬁnition 63.1.2. Consider a map A
this map and the map A 0−→ B. Thus, it’s a map ker f : Ker f (cid:44)→ A such that

−→ B. The kernel is deﬁned as the equalizer of

f

Ker f

∩

ker f

?
A

0

f

-

- B

commutes, and moreover any other map with the same property factors uniquely through
Ker A (so it is universal with this property). By Problem 62A(cid:63), ker f is a monic morphism,
which justiﬁes the use of “(cid:44)→”.

Notice that we’re using ker f to represent the map and Ker f to represent the object

Similarly, we deﬁne the cokernel, the dual notion:

595

596

Napkin, by Evan Chen (v1.5.20190718)

Deﬁnition 63.1.3. Consider a map A
Coker f such that

f

−→ B. The cokernel of f is a map coker f : B (cid:16)

A

f -

B

0

coker f

-

??
Coker f

commutes, and moreover any other map with the same property factors uniquely through
Coker f (so it is universal with this property). Thus it is the “coequalizer” of this map

and the map A 0−→ B. By the dual of Problem 62A(cid:63), coker f is an epic morphism, which
justiﬁes the use of “(cid:16)”.

Think of the cokernel of a map A

f

−→ B as “B modulo the image of f ”, e.g.

Example 63.1.4 (Cokernels)

Consider the map Z/6Z → D12 =(cid:10)r, s | r6 = s2 = 1, rs = sr−1(cid:11). Then the cokernel
of this map in Grp is D12/(cid:104)r(cid:105) ∼= Z/2Z.

This doesn’t always work out quite the way we want since in general the image of a
homomorphism need not be normal in the codomain. Nonetheless, we can use this to
deﬁne:

Deﬁnition 63.1.5. The image of A
Ker(coker f ). This gives a unique map im f : A → Im f .

−→ B is the kernel of coker f . We denote Im f =

f

When it exists, this coincides with our concrete notion of “image”. Picture:

A

f - B

-

0

∃
!

-

⊂
Im f

c

o

k

e

r

f

-

-

-

0-

Coker f

Note that by universality of Im f , we ﬁnd that there is a unique map im f : A → Im f
that makes the entire diagram commute.

§63.2 Additive and abelian categories

Prototypical example for this section: Ab, Vectk, or more generally ModR.

We can now deﬁne the notion of an additive and abelian category, which are the types

of categories where this notion is most useful.

Deﬁnition 63.2.1. An additive category A is one such that:
 A has a zero object, and any two objects have a product.

63 Abelian categories

597

 More importantly: every HomA(A, B) forms an abelian group (written additively)

such that composition distributes over addition:

(g + h) ◦ f = g ◦ f + h ◦ f

and f ◦ (g + h) = f ◦ g + f ◦ h.

The zero map serves as the identity element for each group.

Deﬁnition 63.2.2. An abelian category A is one with the additional properties that
for any morphism A

f

−→ B,

 The kernel and cokernel exist, and

 The morphism factors through the image so that im(f ) is epic.

So, this yields a diagram

Ker(f ) ⊂ ker(f )- A

im(f )-- Im(f ) ⊂

- B

coker(f )-- Coker(f ).

Example 63.2.3 (Examples of abelian categories)
(a) Vectk, Ab are abelian categories, where f + g takes its usual meaning.

(b) Generalizing this, the category ModR of R-modules is abelian.

(c) Grp is not even additive, because there is no way to assign a commutative

addition to pairs of morphisms.

In general, once you assume a category is abelian, all the properties you would want of

these kernels, cokernels, . . . that you would guess hold true. For example,

Proposition 63.2.4 (Monic ⇐⇒ trival kernel)
A map A
and only if its cokernel is B → 0.

f

−→ B is monic if and only if its kernel is 0 → A. Dually, A

f

−→ B is epic if

Proof. The easy direction is:

Exercise 63.2.5. Show that if A
in non-abelian categories.)

f

−→ B is monic, then 0 → A is a kernel. (This holds even

Of course, since kernels are unique up to isomorphism, monic =⇒ 0 kernel. On the
−→ B. For this we can exploit the group
other hand, assume that 0 → A is a kernel of A
structure of the underlying homomorphisms now. Assume the diagram

f

Z

g -

- A

h

f -

B

commutes. Then (g − h) ◦ f = g ◦ f − h ◦ f = 0, and we’ve arrived at a commutative
diagram.

Z

g−h

?
A

................................

0

-

-

B

f

598

Napkin, by Evan Chen (v1.5.20190718)

But since 0 → A is a kernel it follows that g−h factors through 0, so g−h = 0 =⇒ g = h,
which is to say that f is monic.

Proposition 63.2.6 (Isomorphism ⇐⇒ monic and epic)
In an abelian category, a map is an isomorphism if and only if it is monic and epic.

Proof. Omitted, because the Mitchell embedding theorem presented later implies this.

§63.3 Exact sequences

Prototypical example for this section: 0 → G → G × H → H → 0 is exact.

Exact sequences will seem exceedingly unmotivated until you learn about homology
groups, which is one of the most natural places that exact sequences appear. In light of
this, it might be worth trying to read the chapter on homology groups simultaneously
with this one.

First, let me state the deﬁnition for groups, to motivate the general categorical deﬁnition.

A sequence of groups

G0

f1−→ G1

f2−→ G2

f3−→ . . .

fn−→ Gn

is exact at Gk if the image of fk is the kernel of fk+1. We say the entire sequence is exact
if it’s exact at k = 1, . . . , n − 1.

Example 63.3.1 (Exact sequences)

(a) The sequence

0 → Z/3Z ×5

(cid:44)→ Z/15Z (cid:16) Z/5Z → 0

is exact. Actually, 0 → G (cid:44)→ G × H (cid:16) H → 0 is exact in general. (Here 0

denotes the trivial group.)

(b) For groups, the map 0 → A → B is exact if and only if A → B is injective.
(c) For groups, the map A → B → 0 is exact if and only if A → B is surjective.

Now, we want to mimic this deﬁnition in a general abelian category A. So, let’s write
−→ C is exact. First, we had better have that g ◦ f = 0,
−→ B
down a criterion for when A
which encodes the fact that im(f ) ⊆ ker(g). Adding in all the relevant objects, we get
the commutative diagram below.

f

g

A

f

0

- C
-
6

g

im f

?? ⊂
Im f

-

-

B



ι
-
........................

∃!

0

⊃

Ker g

Here the map A (cid:16) Im f is epic since we are assuming A is an abelian category. So, we

have that

0 = (g ◦ ι) ◦ im f = g ◦ (ι ◦ im f ) = g ◦ f = 0

63 Abelian categories

599

but since im f is epic, this means that g ◦ ι = 0. So there is a unique map Im f → Ker g,
and we require that this diagram commutes. In short,

Deﬁnition 63.3.2. Let A be an abelian category. The sequence

··· → An−1

fn−→ An

fn+1−−−→ An+1 → . . .

is exact at An if fn◦fn+1 = 0 and the canonical map Im fn → Ker fn+1 is an isomorphism.
The entire sequence is exact if it is exact at each Ai. (For ﬁnite sequences we don’t
impose condition on the very ﬁrst and very last object.)

Exercise 63.3.3. Show that, as before, 0 → A → B is exact ⇐⇒ A → B is monic.

§63.4 The Freyd-Mitchell embedding theorem

We now introduce the Freyd-Mitchell embedding theorem, which essentially says that
any abelian category can be realized as a concrete one.

Deﬁnition 63.4.1. A category is small if obj(A) is a set (as opposed to a class), i.e.
there is a “set of all objects in A”. For example, Set is not small because there is no set
of all sets.

Theorem 63.4.2 (Freyd-Mitchell embedding theorem)
Let A be a small abelian category. Then there exists a ring R (with 1 but possibly
non-commutative) and a full, faithful, exact functor onto the category of left R-
modules.

Here a functor is exact if it preserves exact sequences. This theorem is good because it
means

You can basically forget about all the weird deﬁnitions that work in any
abelian category.

Any time you’re faced with a statement about an abelian category, it suﬃces to just
prove it for a “concrete” category where injective/surjective/kernel/image/exact/etc.
agree with your previous notions. A proof by this means is sometimes called diagram
chasing.

Remark 63.4.3 — The “small” condition is a technical obstruction that requires
the objects A to actually form a set. I’ll ignore this distinction, because one can
almost always work around it by doing enough set-theoretic technicalities.

For example, let’s prove:

600

Napkin, by Evan Chen (v1.5.20190718)

Lemma 63.4.4 (Short ﬁve lemma)

In an abelian category, consider the commutative diagram

0

0

- A ⊂

p - B

q -- C

- 0

∼= α
?
⊂
A(cid:48)

-

β

?
B(cid:48)

p(cid:48) -

∼= γ
?
C(cid:48)

q(cid:48) --

-

0

and assume the top and bottom rows are exact. If α and γ are isomorphisms, then
so is β.

Proof. We prove that β is epic (with a similar proof to get monic). By the embedding
theorem we can treat the category as R-modules over some R. This lets us do a so-
called “diagram chase” where we move elements around the picture, using the concrete
interpretation of our category as R-modules.

Let b(cid:48) be an element of B(cid:48). Then q(cid:48)(b(cid:48)) ∈ C(cid:48), and since γ is surjective, we have a c

such that γ(c) = b(cid:48), and ﬁnally a b ∈ B such that q(b) = c. Picture:

b ∈ B

q -

c ∈ C

∼= γ
?

q(cid:48)- c(cid:48) ∈ C(cid:48)

β

?
b(cid:48) ∈ B(cid:48)

Now, it is not necessarily the case that β(b) = b(cid:48). However, since the diagram commutes
we at least have that

q(cid:48)(b(cid:48)) = q(cid:48)(β(b))

so b(cid:48) − β(b) ∈ Ker q(cid:48) = Im p(cid:48), and there is an a(cid:48) ∈ A(cid:48) such that p(cid:48)(a(cid:48)) = b(cid:48) − β(b); use α
now to lift it to a ∈ A. Picture:
a ∈ A

b ∈ B

?
a(cid:48) ∈ A(cid:48)

-

b(cid:48) − β(b) ∈ B(cid:48) -

0 ∈ C(cid:48)

Then, we have

β(b + q(a)) = βb + βpa = βb + p(cid:48)αa = βb + (b(cid:48) − βb) = b(cid:48)

so b(cid:48) ∈ Im β which completes the proof that β(cid:48) is surjective.

§63.5 Breaking long exact sequences

Prototypical example for this section: First isomorphism theorem.

In fact, it turns out that any exact sequence breaks into short exact sequences. This

relies on:

63 Abelian categories

601

Proposition 63.5.1 (“First isomorphism theorem” in abelian categories)

Let A

f

−→ B be an arrow of an abelian category. Then there is an exact sequence

0 → Ker f

ker f

−−−→ A

im f

−−→ Im f → 0.

Example 63.5.2

Let’s analyze this theorem in our two examples of abelian categories:

(a) In the category of abelian groups, this is basically the ﬁrst isomorphism theorem.

(b) In the category Vectk, this amounts to the rank-nullity theorem, Theorem 9.7.7.

Thus, any exact sequence can be broken into short exact sequences, as

0

0

-

...........
.........-

Cn+2

0

...........-
............

fn -

An+1

fn+1 -

-

. . .

...........-
..........
fn−1 -

-

An

0

-

...........
Cn
..........-
- An−1
.........-
Cn−1
...........
0

-

. . .
...........
...........-

0

-

-

..........
...........-

0

.........-
...........
0

-

Cn+1

where Ck = im fk−1 = ker fk for every k.

§63.6 A few harder problems to think about

Problem 63A (Five lemma). In an abelian category, consider the commutative diagram

A

α

??
A(cid:48)

p -

B

q -

C

r -

D

s -

∼= β
?
- B(cid:48)

p(cid:48)

γ

?
- C(cid:48)

q(cid:48)

∼= δ
?
- D(cid:48)

r(cid:48)

?
- E(cid:48)

s(cid:48)

E
∩

ε

where the ﬁrst and second rows are exact. Prove that if α is epic, ε is monic, and β, δ
are isomorphisms, then γ is an isomorphism as well. Thus this is a stronger version of
the short ﬁve lemma.

Problem 63B(cid:63) (Snake lemma). In an abelian category, consider the diagram

A

f - B

g -- C

- 0

a

?
⊂
A(cid:48)

-

b

?
B(cid:48)

-

f(cid:48)

c

?
C(cid:48)

-

g(cid:48)

0

602

Napkin, by Evan Chen (v1.5.20190718)

where the ﬁrst and second rows are exact sequences. Prove that there is an exact sequence

Ker a → Ker b → Ker c → Coker a → Coker b → Coker c.

XVII

Algebraic Topology II: Homology

Part XVII: Contents

64 Singular homology

605
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 605
64.1 Simplices and boundaries
64.2 The singular homology groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . 607
64.3 The homology functor and chain complexes . . . . . . . . . . . . . . . . . . . . . . 610
64.4 More examples of chain complexes . . . . . . . . . . . . . . . . . . . . . . . . . . 614
. . . . . . . . . . . . . . . . . . . . . . . . 615
64.5 A few harder problems to think about

65 The long exact sequence

617
65.1 Short exact sequences and four examples . . . . . . . . . . . . . . . . . . . . . . . 617
. . . . . . . . . . . . . . . . . . . . . 619
65.2 The long exact sequence of homology groups
65.3 The Mayer-Vietoris sequence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 621
. . . . . . . . . . . . . . . . . . . . . . . . 626
65.4 A few harder problems to think about

66 Excision and relative homology

627
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 627
66.1 The long exact sequences
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 628
66.2 The category of pairs
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 629
66.3 Excision
66.4 Some applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 630
66.5 Invariance of dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 631
. . . . . . . . . . . . . . . . . . . . . . . . 632
66.6 A few harder problems to think about

67 Bonus: Cellular homology

633
67.1 Degrees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 633
67.2 Cellular chain complex . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 634
67.3 The cellular boundary formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . 637
. . . . . . . . . . . . . . . . . . . . . . . . 639
67.4 A few harder problems to think about

68 Singular cohomology

641
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 641
68.1 Cochain complexes
68.2 Cohomology of spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 642
. . . . . . . . . . . . . . . . . . . . . . . . . . 643
68.3 Cohomology of spaces is functorial
68.4 Universal coeﬃcient theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 644
. . . . . . . . . . . . . . . . . . . . . 645
68.5 Example computation of cohomology groups
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 646
68.6 Relative cohomology groups
. . . . . . . . . . . . . . . . . . . . . . . . 647
68.7 A few harder problems to think about

69 Application of cohomology

649
69.1 Poincar´e duality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 649
69.2 de Rham cohomology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 649
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 650
69.3 Graded rings
69.4 Cup products . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 652
. . . . . . . . . . . . . . . . . . . . . . . . . . 654
69.5 Relative cohomology pseudo-rings
69.6 Wedge sums . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 654
69.7 K¨unneth formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 656
. . . . . . . . . . . . . . . . . . . . . . . . 657
69.8 A few harder problems to think about

64 Singular homology

Now that we’ve deﬁned π1(X), we turn our attention to a second way of capturing
the same idea, H1(X). We’ll then deﬁne Hn(X) for n ≥ 2. The good thing about the
Hn groups is that, unlike the πn groups, they are much easier to compute in practice.
The downside is that their deﬁnition will require quite a bit of setup, and the “algebraic”
part of “algebraic topology” will become a lot more technical.

§64.1 Simplices and boundaries

Prototypical example for this section: ∂[v0, v1, v2] = [v0, v1] − [v0, v2] + [v1, v2].

First things ﬁrst:

Deﬁnition 64.1.1. The standard n-simplex, denoted ∆n, is deﬁned as

{(x0, x1, . . . xn) | xi ≥ 0, x0 + ··· + xn = 1} .

Hence it’s the convex hull of some vertices [v0, . . . , vn]. Note that we keep track of the
order v0, . . . , vn of the vertices, for reasons that will soon become clear.

Given a topological space X, a singular n-simplex is a map σ : ∆n → X.

Example 64.1.2 (Singular simplices)
(a) Since ∆0 = [v0] is just a point, a singular 0-simplex X is just a point of X.

(b) Since ∆1 = [v0, v1] is an interval, a singular 1-simplex X is just a path in X.

(c) Since ∆2 = [v0, v1, v2] is an equilateral triangle, a singular 2-simplex X looks a

“disk” in X.

Here is a picture of all three in a space X:

The arrows aren’t strictly necessary, but I’ve included them to help keep track of
the “order” of the vertices; this will be useful in just a moment.

605

Xσ0v0v1σ1v0v1v2σ2606

Napkin, by Evan Chen (v1.5.20190718)

Now we’re going to do something much like when we were talking about Stokes’
theorem: we’ll put a boundary ∂ operator on the singular n-simplices. This will give us

a formal linear sums of n-simplices(cid:80)k akσk, which we call an n-chain.

In that case,

Deﬁnition 64.1.3. Given a singular n-simplex σ with vertices [v0, . . . , vn], note that for
every i we have an (n − 1) simplex [v0, . . . , vi−1, vi+1, . . . , vn]. The boundary operator
∂ is then deﬁned by

(−1)i [v0, . . . , vi−1, vi+1, . . . , vn] .

The boundary operator then extends linearly to n-chains:

∂(σ) :=(cid:88)i
∂(cid:32)(cid:88)k

akσk(cid:33) :=(cid:88) ak∂(σk).

By convention, a 0-chain has empty boundary.

Example 64.1.4 (Boundary operator)
Consider the chains depicted in Example 64.1.2. Then

(a) ∂σ0 = 0.
(b) ∂(σ1) = [v1] − [v0]: it’s the “diﬀerence” of the 0-chain corresponding to point v1

and the 0-chain corresponding to point v0.

(c) ∂(σ2) = [v0, v1]− [v0, v2] + [v1, v2]; i.e. one can think of it as the sum of the three

oriented arrows which make up the “sides” of σ2.

(d) Notice that if we take the boundary again, we get

∂(∂(σ2)) = ∂([v0, v1]) − ∂([v0, v2]) + ∂([v1, v2])

= ([v1] − [v0]) − ([v2] − [v0]) + ([v2] − [v1])
= 0.

The fact that ∂2 = 0 is of course not a coincidence.

Theorem 64.1.5 (∂2 = 0)
For any chain c, ∂(∂(c)) = 0.

Proof. Essentially identical to Problem 44B: this is just a matter of writing down a bunch

of(cid:80) signs. Diligent readers are welcome to try the computation.

Remark 64.1.6 — The eerie similarity between the chains used to integrate dif-
ferential forms and the chains in homology is not a coincidence. The de Rham
cohomology, discussed much later, will make the relation explicit.

64 Singular homology

607

§64.2 The singular homology groups

Prototypical example for this section: Probably Hn(Sm), especially the case m = n = 1.

Let X be a topological space, and let Cn(X) be the free abelian group of n-chains of
X that we deﬁned earlier. Our work above gives us a boundary operator ∂, so we have a
sequence of maps

. . . ∂−→ C3(X) ∂−→ C2(X) ∂−→ C1(X) ∂−→ C0(X) ∂−→ 0

(here I’m using 0 to for the trivial group, which is standard notation for abelian groups.)
We’ll call this the singular chain complex.

Now, how does this let us detect holes in the space? To see why, let’s consider an

annulus, with a 1-chain c drawn in red:

Notice that

∂c = ([v1] − [v0]) − ([v2] − [v0]) + ([v2] − [v1]) = 0

and so we can say this 1-chain c is a “cycle”, because it has trivial boundary. However, c
is not itself the boundary of any 2-chain, because of the hole in the center of the space —
it’s impossible to “ﬁll in” the interior of c! So, we have detected the hole by the algebraic
fact that

c ∈ ker(cid:16)C1(X) ∂−→ C0(X)(cid:17)

but

c /∈ im(cid:16)C2(X) ∂−→ C1(X)(cid:17) .

Indeed, if the hole was not present then this statement would be false.

We can capture this idea in any dimension, as follows.

Deﬁnition 64.2.1. Let

. . . ∂−→ C2(X) ∂−→ C1(X) ∂−→ C0(X) ∂−→ 0

as above. We say that c ∈ Cn(X) is:

 a cycle if c ∈ ker(cid:16)Cn(X) ∂−→ Cn−1(X)(cid:17), and
 a boundary if c ∈ im(cid:16)Cn+1(X) ∂−→ Cn(X)(cid:17).

Denote the cycles and boundaries by Zn(X), Bn(X) ⊆ Cn(X), respectively.

Xv0v1v2608

Napkin, by Evan Chen (v1.5.20190718)

Question 64.2.2. Just to get you used to the notation: check that Bn and Zn are themselves
abelian groups, and that Bn(X) ⊆ Zn(X) ⊆ Cn(X).

The key point is that we can now deﬁne:

Deﬁnition 64.2.3. The nth homology group Hn(X) is deﬁned as

Hn(X) := Zn(X)/Bn(X).

Example 64.2.4 (The zeroth homology group)
Let’s compute H0(X) for a topological space X. We take C0(X), which is just formal
linear sums of points of X.

space C0(X): that is, every point is a “cycle”.

First, we consider the kernel of ∂ : C0(X) → 0, so the kernel of ∂ is the entire
Now, what is the boundary? The main idea is that [b] − [a] = 0 if and only if
there’s a 1-chain which connects a to b, i.e. there is a path from a to b. In particular,

X path connected =⇒ H0(X) ∼= Z.

More generally, we have

Proposition 64.2.5 (Homology groups split into path-connected components)

If X =(cid:83)α Xα is a decomposition into path-connected components, then we have

Hn(X) ∼=(cid:77)α

Hn(Xα).

In particular, if X has r path-connected components, then H0(X) ∼= Z⊕r.

(If it’s surprising to see Z⊕r, remember that an abelian group is the same thing as a
Z-module, so the notation G⊕ H is customary in place of G× H when G, H are abelian.)

Now let’s investigate the ﬁrst homology group.

Theorem 64.2.6 (Hurewicz theorem)
Let X be path-connected. Then H1(X) is the abelianization of π1(X, x0).

We won’t prove this but you can see it roughly from the example. The group H1(X)
captures the same information as π1(X, x0): a cycle (in Z1(X)) corresponds to the same
thing as the loops we studied in π1(X, x0), and the boundaries (in B1(X), i.e. the things
we mod out by) are exactly the nulhomotopic loops in π1(X, x0). The diﬀerence is that
H1(X) allows loops to commute, whereas π1(X, x0) does not.

64 Singular homology

609

Example 64.2.7 (The ﬁrst homology group of the annulus)
To give a concrete example, consider the annulus X above. We found a chain c that
wrapped once around the hole of X. The point is that in fact,

H1(X) = (cid:104)c(cid:105) ∼= Z

which is to say the chains c, 2c, . . . are all not the same in H1(X), but that any other
1-chain is equivalent to one of these. This captures the fact that X is really just S1.

Example 64.2.8 (An explicit boundary in S1)
In X = S1, let a be the uppermost point and b the lowermost point. Let c be the
simplex from a to b along the left half of the circle, and d the simplex from a to b
along the right half. Finally, let γ be the simplex which represents a loop γ from a to
itself, wrapping once counterclockwise around S1. We claim that in H 1(S1) we have

which geometrically means that c − d represents wrapping once around the circle
(which is of course what we expect).

γ = c − d

Indeed this can be seen from the picture above, where we have drawn a 2-simplex
whose boundary is exactly γ − c + d. The picture is somewhat metaphorical: in
reality v0 = v1 = a, and the entire 2-simplex is embedded in S1. This is why singular
homology is so-called: the images of the simplex can sometimes look quite “singular”.

Example 64.2.9 (The ﬁrst homology group of the ﬁgure eight)
Consider X8 (see Example 58.2.9). Both homology and homotopy see the two loops
in X8, call them a and b. The diﬀerence is that in π1(X8, x0), these two loops are
not allowed to commute: we don’t have ab (cid:54)= ba, because the group operation in π1
is “concatenate paths” But in the homology group H1(X) the way we add a and b
is to add them formally, to get the 1-chain a + b. So

H1(X) ∼= Z⊕2 while π1(X, x0) = (cid:104)a, b(cid:105) .

v0=av1=av2=bγcd610

Napkin, by Evan Chen (v1.5.20190718)

Example 64.2.10 (The homology groups of S2)
Consider S2, the two-dimensional sphere. Since it’s path connected, we have
H0(S2) = Z. We also have H1(S2) = 0, for the same reason that π1(S2) is trivial as
well. On the other hand we claim that

H2(S2) ∼= Z.

The elements of H2(S2) correspond to wrapping S2 in a tetrahedral bag (or two bags,
or three bags, etc.). Thus, the second homology group lets us detect the spherical
cavity of S2.

Actually, more generally it turns out that we will have

Hn(Sm) ∼=(cid:40)Z n = m or n = 0

otherwise.

0

Example 64.2.11 (Contractible spaces)
Given any contractible space X, it turns out that

Hn(X) ∼=(cid:40)Z n = 0

0

otherwise.

The reason is that, like homotopy groups, it turns out that homology groups are
homotopy invariant. (We’ll prove this next section.) So the homology groups of
contractible X are the same as those of a one-point space, which are those above.

Example 64.2.12 (Homology groups of the torus)
While we won’t be able to prove it for a while, it turns out that

Hn(S1 × S1) ∼=

otherwise.

n = 0, 2

Z
Z⊕2 n = 1
0

The homology group at 1 corresponds to our knowledge that π1(S1 × S1) ∼= Z2 and

the homology group at 2 detects the “cavity” of the torus.

This is fantastic and all, but how does one go about actually computing any homology
groups? This will be a rather long story, and we’ll have to do a signiﬁcant amount of
both algebra and geometry before we’re really able to compute any homology groups. In
what follows, it will often be helpful to keep track of which things are purely algebraic
(work for any chain complex), and which parts are actually stating something which is
geometrically true.

§64.3 The homology functor and chain complexes

As I mentioned before, the homology groups are homotopy invariant. This will be a
similar song and dance as the work we did to create a functor π1 : hTop∗ → Grp. Rather

64 Singular homology

611

than working slowly and pulling away the curtain to reveal the category theory at the
end, we’ll instead start with the category theory right from the start just to save some
time.

Deﬁnition 64.3.1. The category hTop is deﬁned as follows:

 Objects: topological spaces.

 Morphisms: homotopy classes of morphisms X → Y .

In particular, X and Y are isomorphic in hTop if and only if they are homotopic.

You’ll notice this is the same as hTop∗, except without the basepoints.

Theorem 64.3.2 (Homology is a functor hTop → Grp)
For any particular n, Hn is a functor hTop → Grp. In particular,

 Given any map f : X → Y , we get an induced map f∗ : Hn(X) → Hn(Y ).
 For two homotopic maps f, g : X → Y , f∗ = g∗.
 Two homotopic spaces X and Y have isomorphic homology groups: if f : X →

Y is a homotopy then f∗ : Hn(X) → Hn(Y ) is an isomorphism.

 (Insert your favorite result about functors here.)

In order to do this, we have to describe how to take a map f : X → Y and obtain a
map Hn(f ) : Hn(X) → Hn(Y ). Then we have to show that this map doesn’t depend on
the choice of homotopy. (This is the analog of the work we did with f(cid:93) before.) It turns
out that this time around, proving this is much more tricky, and we will have to go back
to the chain complex C•(X) that we built at the beginning.

§64.3.i Algebra of chain complexes

Let’s start with the algebra. First, I’ll deﬁne the following abstraction of the complex
to any sequence of abelian groups. Actually, though, it works in any category (not just
AbGrp). The strategy is as follows: we’ll deﬁne everything that we need completely
abstractly, then show that the geometry concepts we want correspond to this setting.

Deﬁnition 64.3.3. A chain complex is a sequence of groups An and maps

. . . ∂−→ An+1

∂−→ An

∂−→ An−1

∂−→ . . .

such that the composition of any two adjacent maps is the zero morphism. We we usually
denote this by A•.

The nth homology group Hn(A•) is deﬁned as ker(An → An−1)/ im(An+1 → An).

Cycles and boundaries are deﬁned in the same way as before.

Obviously, this is just an algebraic generalization of the structure we previously looked

at, rid of all its original geometric context.

612

Napkin, by Evan Chen (v1.5.20190718)

Deﬁnition 64.3.4. A morphism of chain complexes f : A• → B• is a sequence of
maps fn for every n such that the diagram

. . .

∂A -

An+1

∂A -

An

∂A -

An−1

∂A -

. . .

fn+1

fn

fn−1

?
∂B - Bn+1

?
∂B - Bn

. . .

?

∂B - Bn−1

∂B - . . .

commutes. Under this deﬁnition, the set of chain complexes becomes a category, which
we denote Cmplx.

Note that given a morphism of chain complexes f : A• → B•, every cycle in An gets

sent to a cycle in Bn, since the square

An

fn

?
Bn

∂A - An−1

?

- Bn−1

∂B

commutes. Similarly, every boundary in An gets sent to a boundary in Bn. Thus,

Every map of f : A• → B• gives a map f∗ : Hn(A) → Hn(B) for every n.

Exercise 64.3.5. Interpret Hn as a functor Cmplx → Grp.

Next, we want to deﬁne what it means for two maps f and g to be homotopic. Here’s

the answer:

Deﬁnition 64.3.6. Let f, g : A• → B•. Suppose that one can ﬁnd a map Pn : An →
Bn+1 for every n such that

gn − fn = ∂B ◦ Pn + Pn−1 ◦ ∂A

Then P is a chain homotopy from f to g and f and g are chain homotopic.

We can draw a picture to illustrate this (warning: the diagonal dotted arrows do NOT

commute with all the other arrows):

. . .

∂A - An+1

............ P n

g−f
?
∂B - Bn+1

∂B -

. . .

∂A - An
............

∂A - An−1
..........
.......... P n− 1

g−f
?
Bn−1

∂B -

g−f
?
Bn

∂A - . . .

∂B - . . .

The deﬁnition is that in each slanted “parallelogram”, the g − f arrow is the sum of the
two compositions along the sides.

64 Singular homology

613

Remark 64.3.7 — This equation should look terribly unmotivated right now, aside
from the fact that we are about to show it does the right algebraic thing.
Its
derivation comes from the geometric context that we have deferred until the next
section, where “homotopy” will naturally give “chain homotopy”.

Now, the point of this deﬁnition is that

Proposition 64.3.8 (Chain homotopic maps induce the same map on homology groups)
Let f, g : A• → B• be chain homotopic maps A• → B•. Then the induced maps
f∗, g∗ : Hn(A•) → Hn(B•) coincide for each n.

Proof. It’s equivalent to show g−f gives the zero map on homology groups, In other words,
we need to check that every cycle of An becomes a boundary of Bn under g − f .

Question 64.3.9. Verify that this is true.

§64.3.ii Geometry of chain complexes

Now let’s ﬁll in the geometric details of the picture above. First:

Lemma 64.3.10 (Map of space =⇒ map of singular chain complexes)
Each f : X → Y induces a map Cn(X) → Cn(Y ).

Proof. Take the composition

∆n σ−→ X

f

−→ Y.

In other words, a path in X becomes a path in Y , et cetera. (It’s not hard to see that
the squares involving ∂ commute; check it if you like.)

Now, what we need is to show that if f, g : X → Y are homotopic, then they are chain
homotopic. To produce a chain homotopy, we need to take every n-simplex X to an
(n + 1)-chain in Y , thus deﬁning the map Pn.

Let’s think about how we might do this. Let’s take the n-simplex σ : ∆n → X and
feed it through f and g; pictured below is a 1-simplex σ (i.e. a path in X) which has been
mapped into the space Y . Homotopy means the existence of a map F : X × [0, 1] → Y
such that F (−, 0) = f and F (−, 1) = g, parts of which I’ve illustrated below with grey
arrows in the image for Y .

Yv0v1f(σ)w0w1g(σ)Fv0v1w0w1∆1×[0,1]X×[0,1]Fσ×id614

Napkin, by Evan Chen (v1.5.20190718)

This picture suggests how we might proceed: we want to create a 2-chain on Y given
the 1-chains we’ve drawn. The homotopy F provides us with a “square” structure on Y ,
i.e. the square bounded by v0, v1, w1, w0. We split this up into two triangles; and that’s
our 2-chain.

We can make this formal by taking ∆1 × [0, 1] (which is a square) and splitting it
into two triangles. Then, if we apply σ × id, we’ll get an 2-chain in X × [0, 1], and
then ﬁnally applying F will map everything into our space Y . In our example, the ﬁnal
image is the 2-chain, consisting of two triangles, which in our picture can be written as
[v0, w0, w1] − [v0, v1, w1]; the boundaries are given by the red, green, grey.
Pn as follows. Set vi = f (xi) and wi = g(xi) for each i. Then, we let

More generally, for an n-simplex φ = [x0, . . . , xn] we deﬁne the so-called prism operator

Pn(φ) :=

n(cid:88)i=0
(−1)i(F ◦ (φ × id)) [v0, . . . , vi, wi, . . . , wn] .

This is just the generalization of the construction above to dimensions n > 1; we split
∆n × [0, 1] into n + 1 simplices, map it into X by φ × id and then push the whole thing
into Y . The (−1)i makes sure that the “diagonal” faces all cancel oﬀ with each other.

We now claim that for every σ,

∂Y (Pn(σ)) = g(σ) − f (σ) − Pn−1(∂X σ).

In the picture, ∂Y ◦ Pn is the boundary of the entire prism (in the ﬁgure, this becomes
the red, green, and grey lines, not including diagonal grey, which is cancelled out). The
g − f is the green minus the red, and the Pn−1 ◦ ∂X represents the grey edges of the
prism (not including the diagonal line from v1 to w0). Indeed, one can check (just by

writing down several(cid:80) signs) that the above identity holds.

So that gives the chain homotopy from f to g, completing the proof of Theorem 64.3.2.

§64.4 More examples of chain complexes

We now end this chapter by providing some more examples of chain complexes, which
we’ll use in the next chapter to ﬁnally compute topological homology groups.

Example 64.4.1 (Reduced homology groups)
Suppose X is a (nonempty) topological space. One can augment the standard
singular complex as follows: do the same thing as before, but augment the end by
adding a Z, as shown:

··· → C1(X) → C0(X) ε−→ Z → 0

just a formal sum of points!) We denote this augmented singular chain complex

Here ε is deﬁned by ε((cid:80) nipi) =(cid:80) ni for points pi ∈ X. (Recall that a 0-chain is
by (cid:101)C•(X).

Question 64.4.2. What’s the homology of the above chain at Z? (Hint: you need X
nonempty.)

Deﬁnition 64.4.3. The homology groups of the augmented chain complex are called

the reduced homology groups (cid:101)Hn(X) of the space X.

64 Singular homology

615

Obviously (cid:101)Hn(X) ∼= Hn(X) for n > 0. But when n = 0, the map H0(X) → Z by ε has
kernel (cid:101)H0(X), thus H0(X) ∼= (cid:101)H0(X) ⊕ Z.

This is usually just an added convenience. For example, it means that if X is contractible,
then all its reduced homology groups vanish, and thus we won’t have to keep fussing
with the special n = 0 case.

Question 64.4.4. Given the claim earlier about Hn(Sm), what should (cid:101)Hn(Sm) be?

Example 64.4.5 (Relative chain groups)
Suppose X is a topological space, and A ⊆ X a subspace. We can “mod out” by A
by deﬁning

Cn(X, A) := Cn(X)/Cn(A)

for every n. Thus chains contained entirely in A are trivial.

Then, the usual ∂ on Cn(X) generates a new chain complex

. . . ∂−→ Cn+1(X, A) ∂−→ Cn(X, A) ∂−→ Cn−1(X, A) ∂−→ . . . .

This is well-deﬁned since ∂ takes Cn(A) into Cn−1(A).

Deﬁnition 64.4.6. The homology groups of the relative chain complex are the relative
homology groups and denoted Hn(X, A).

One na¨ıve guess is that this might equal Hn(X)/Hn(A). This is not true and in general
doesn’t even make sense; if we take X to be R2 and A = S1 a circle inside it, we have
H1(X) = H1(R2) = 0 and H1(S1) = Z.

Another guess is that Hn(X, A) might just be (cid:101)Hn(X/A). This will turn out to be true

for most reasonable spaces X and A, and we will discuss this when we reach the excision
theorem.

Example 64.4.7 (Mayer-Vietoris sequence)
Suppose a space X is covered by two open sets U and V . We can deﬁne Cn(U + V )
as follows: it consists of chains such that each simplex is either entirely contained in
U , or entirely contained in V .

Of course, ∂ then deﬁnes another chain complex

. . . ∂−→ Cn+1(U + V ) ∂−→ Cn(U + V ) ∂−→ Cn−1(U + V ) ∂−→ . . . .

So once again, we can deﬁne homology groups for this complex; we denote them by
Hn(U + V ). Miraculously, it will turn out that Hn(U + V ) ∼= Hn(X).

§64.5 A few harder problems to think about

Problem 64A. For n ≥ 1 show that the composition
Sn−1 (cid:44)→ Dn F−→ Sn−1

cannot be the identity map on Sn−1 for any F .
Problem 64B (Brouwer ﬁxed point theorem). Use the previous problem to prove that
any continuous function f : Dn → Dn has a ﬁxed point.

65 The long exact sequence

In this chapter we introduce the key fact about chain complexes that will allow us to

compute the homology groups of any space: the so-called “long exact sequence”.

For those that haven’t read about abelian categories: a sequence of morphisms of

abelian groups

is exact if the image of any arrow is equal to the kernel of the next arrow. In particular,

··· → Gn+1 → Gn → Gn−1 → . . .

 The map 0 → A → B is exact if and only if A → B is injective.
 the map A → B → 0 is exact if and only if A → B is surjective.

(On that note: what do you call a chain complex whose homology groups are all trivial?)

A short exact sequence is one of the form 0 → A (cid:44)→ B (cid:16) C → 0.
§65.1 Short exact sequences and four examples

Prototypical example for this section: Relative sequence and Mayer-Vietoris sequence.
Let A = AbGrp. Recall that we deﬁned a morphism of chain complexes in A already.

Deﬁnition 65.1.1. Suppose we have a map of chain complexes

It is said to be short exact if each row of the diagram below is short exact.

0 → A•

f

−→ B•

g

−→ C• → 0

0

0

0

...

...

...

∂A

∂B

∂C

?
- An+1

?
⊂ fn+1- Bn+1

?
gn+1-- Cn+1

∂A

∂B

∂C

?
- An

⊂

?
fn - Bn

?
gn -- Cn

∂A

∂B

∂C

?

- An−1

?

⊂ fn−1- Bn−1

?

gn−1-- Cn−1

∂A

?
...

∂B

?
...

∂C

?
...

- 0

- 0

- 0

617

618

Napkin, by Evan Chen (v1.5.20190718)

Example 65.1.2 (Mayer-Vietoris short exact sequence and its augmentation)
Let X = U ∪ V be an open cover. For each n consider

Cn(U ∩ V ) ⊂- Cn(U ) ⊕ Cn(V ) -- Cn(U + V )

c

- (c,−c)

(c, d)

-

c + d

One can easily see (by taking a suitable basis) that the kernel of the latter map is
exactly the image of the ﬁrst map. This generates a short exact sequence

0 → C•(U ∩ V ) (cid:44)→ C•(U ) ⊕ C•(V ) (cid:16) C•(U + V ) → 0.

Example 65.1.3 (Augmented Mayer-Vietoris sequence)
We can augment each of the chain complexes in the Mayer-Vietoris sequence as well,
by appending

0

0

-

C0(U ∩ V ) ⊂-

C0(U ) ⊕ C0(V )

--

C0(U + V )

-

0

ε

??
- Z

ε⊕ε
??
- Z ⊕ Z

ε

?
- Z

- 0

to the bottom of the diagram. In other words we modify the above into

0 → (cid:101)C•(U ∩ V ) (cid:44)→ (cid:101)C•(U ) ⊕ (cid:101)C•(V ) (cid:16) (cid:101)C•(U + V ) → 0

where (cid:101)C• is the chain complex deﬁned in Deﬁnition 64.4.3.

Example 65.1.4 (Relative chain short exact sequence)
Since Cn(X, A) := Cn(X)/Cn(A), we have a short exact sequence

for every space X and subspace A. This can be augmented: we get

0 → C•(A) (cid:44)→ C•(X) (cid:16) C•(X, A) → 0
0 → (cid:101)C•(A) (cid:44)→ (cid:101)C•(X) (cid:16) C•(X, A) → 0

by adding the ﬁnal row

0

0

- C0(A) ⊂ - C0(X) -- C0(X, A)

- 0

ε

??
- Z

ε

??
- Z

id

?
- 0

- 0.

65 The long exact sequence

619

§65.2 The long exact sequence of homology groups

−→ B•
Consider a short exact sequence 0 → A•
induced maps of homology groups, i.e. we have

f

g

−→ C• → 0. Now, we know that we get

...

...

...

Hn+1(A•)

Hn(A•)

Hn−1(A•)

f∗-

Hn+1(B•)
f∗- Hn(B•)
f∗- Hn−1(B•)

g∗-

Hn+1(C•)
g∗- Hn(C•)
g∗- Hn−1(C•)

...

...

...

But the theorem is that we can string these all together, taking each Hn+1(C•) to Hn(A•).

Theorem 65.2.1 (Short exact =⇒ long exact)
Let 0 → A•
like. Then there is an exact sequence

−→ B•

f

g

−→ C• → 0 be any short exact sequence of chain complexes we

- Hn+2(C•)
g∗- Hn+1(C•)
g∗- Hn(C•)
g∗- Hn−1(C•)

. . .

∂

∂

Hn(B•)

∂

-

∂

. . .

Hn−1(A•)

Hn−1(B•)

Hn+1(A•)

Hn+1(B•)


-
f∗
-
f∗

-
f∗


Hn(A•)

Hn−2(A•)

This is called a long exact sequence of homology groups.

Proof. A very long diagram chase, valid over any abelian category. (Alternatively, it’s
actually possible to use the snake lemma twice.)

Remark 65.2.2 — The map ∂ : Hn(C•) → Hn−1(A•) can be written explicitly as
follows. Recall that Hn is “cycles modulo boundaries”, and consider the sub-diagram

Bn

gn --

Cn

∂B

∂C

?

⊂ fn−1- Bn−1

An−1

?

-- Cn−1

gn−1

620

Napkin, by Evan Chen (v1.5.20190718)

We need to take every cycle in Cn to a cycle in An−1. (Then we need to check a ton
of “well-deﬁned” issues, but let’s put that aside for now.)
Suppose c ∈ Cn is a cycle (so ∂C(c) = 0). By surjectivity, there is a b ∈ Bn with
gn(b) = c, which maps down to ∂B(b). Now, the image of ∂B(b) under gn−1 is zero
by commutativity of the square, and so we can pull back under fn−1 to get a unique
element of An−1 (by exactness at Bn−1).

In summary: we go “left, down, left” to go from c to a:

b

gn - c

∂B

?
fn−1- ∂B(b)

a

gn−1

∂C

?
- 0

Exercise 65.2.3. Check quickly that the recovered a is actually a cycle, meaning ∂A(a) = 0.
(You’ll need another row, and the fact that ∂2

B = 0.)

The ﬁnal word is that:

Short exact sequences of chain complexes give long exact sequences of
homology groups.

In particular, let us take the four examples given earlier.

Example 65.2.4 (Mayer-Vietoris long exact sequence, provisional version)
The Mayer-Vietoris ones give, for X = U ∪ V an open cover,

··· → Hn(U ∩ V ) → Hn(U ) ⊕ Hn(V ) → Hn(U + V ) → Hn−1(U ∩ V ) → . . . .

and its reduced version

··· → (cid:101)Hn(U ∩ V ) → (cid:101)Hn(U ) ⊕ (cid:101)Hn(V ) → (cid:101)Hn(U + V ) → (cid:101)Hn−1(U ∩ V ) → . . . .

This version is “provisional” because in the next section we will replace Hn(U + V )

and (cid:101)Hn(U + V ) with something better. As for the relative homology sequences, we

Theorem 65.2.5 (Long exact sequence for relative homology)
Let X be a space, and let A ⊆ X be a subspace. There are long exact sequences

··· → Hn(A) → Hn(X) → Hn(X, A) → Hn−1(A) → . . . .

have:

and

··· → (cid:101)Hn(A) → (cid:101)Hn(X) → Hn(X, A) → (cid:101)Hn−1(A) → . . . .

The exactness of these sequences will give tons of information about Hn(X) if only
we knew something about what Hn(U + V ) or Hn(X, A) looked like. This is the purpose
of the next chapter.

65 The long exact sequence

621

§65.3 The Mayer-Vietoris sequence

Prototypical example for this section: The computation of Hn(Sm) by splitting Sm into
two hemispheres.

Now that we have done so much algebra, we need to invoke some geometry. There
are two major geometric results in the Napkin. One is the excision theorem, which we
discuss next chapter. The other we present here, which will let us take advantage of the
Mayer-Vietoris sequence. The proofs are somewhat involved and are thus omitted; see
[Ha02] for details.

The ﬁrst theorem is that the notation Hn(U + V ) that we have kept until now is

redundant, and can be replaced with just Hn(X):

Theorem 65.3.1 (Open cover homology theorem)
Consider the inclusion ι : C•(U + V ) (cid:44)→ C•(X). Then ι induces an isomorphism

Hn(U + V ) ∼= Hn(X).

Remark 65.3.2 — In fact, this is true for any open cover (even uncountable), not
just those with two covers U ∪ V . But we only state the special case with two open
sets, because this is what is needed for Example 65.1.2.

So, Example 65.1.2 together with the above theorem implies, after replacing all the
Hn(U + V )’s with Hn(X)’s:

Theorem 65.3.3 (Mayer-Vietoris long exact sequence)
If X = U ∪ V is an open cover, then we have long exact sequences

··· → Hn(U ∩ V ) → Hn(U ) ⊕ Hn(V ) → Hn(X) → Hn−1(U ∩ V ) → . . . .

and

··· → (cid:101)Hn(U ∩ V ) → (cid:101)Hn(U ) ⊕ (cid:101)Hn(V ) → (cid:101)Hn(X) → (cid:101)Hn−1(U ∩ V ) → . . . .

At long last, we can compute the homology groups of the spheres.

Theorem 65.3.4 (The homology groups of Sm)
For integers m and n,

(cid:101)Hn(Sm) ∼=(cid:40)Z n = m

0

otherwise.

The generator (cid:101)Hn(Sn) is an n-cell which covers Sn exactly once (for example, the
generator for (cid:101)H1(S1) is a loop which wraps around S1 once).

Proof. This one’s fun, so I’ll only spoil the case m = 1, and leave the rest to you.
Decompose the circle S1 into two arcs U and V , as shown:

622

Napkin, by Evan Chen (v1.5.20190718)

Each of U and V is contractible, so all their reduced homology groups vanish. Moreover,
U ∩ V is homotopy equivalent to two points, hence

0

otherwise.

(cid:101)Hn(U ∩ V ) ∼=(cid:40)Z n = 0
→ (cid:101)Hn(S1) ∂−→ (cid:101)Hn−1(U ∩ V ) → (cid:101)Hn−1(U ) ⊕ (cid:101)Hn−1(V )
(cid:125)

(cid:123)(cid:122)

(cid:124)

=0

Now consider again the segment of the short exact sequence

=0

(cid:124)

(cid:123)(cid:122)

··· → (cid:101)Hn(U ) ⊕ (cid:101)Hn(V )
(cid:125)
From this we derive that (cid:101)Hn(S1) is Z for n = 1 and 0 elsewhere.
It remains to analyze the generators of (cid:101)H1(S1). Note that the isomorphism was given
C1(U ) ⊕ C1(V ) - C1(U + V )

by the connecting homomorphism ∂, which is given by a “left, down, left” procedure
(Remark 65.2.2) in the diagram

→ . . . .

∂⊕∂

C0(U ∩ V ) - C0(U ) ⊕ C0(V ).

?

Mark the points a and b as shown in the two disjoint paths of U ∩ V .

Then a − b is a cycle which represents a generator of H0(U ∩ V ). We can ﬁnd the
pre-image of ∂ as follows: letting c and d be the chains joining a and b, with c contained
in U , and d contained in V , the diagram completes as

(c, d)

- c − d

?

a − b - (a − b, a − b)

In other words ∂(c − d) = a − b, so c − d is a generator for (cid:101)H 1(S1).
Thus we wish to show that c − d is (in H 1(S1)) equivalent to the loop γ wrapping

around S1 once, counterclockwise. This was illustrated in Example 64.2.8.

S1VUS1abcd65 The long exact sequence

623

Thus, the key idea in Mayer-Vietoris is that

Mayer-Vietoris lets us compute Hn(X) by splitting X into two open sets.

Here are some more examples.

Proposition 65.3.5 (The homology groups of the ﬁgure eight)
Let X = S1 ∧ S1 be the ﬁgure eight. Then

(cid:101)Hn(X) ∼=(cid:40)Z⊕2 n = 1

0

otherwise.

The generators for (cid:101)H1(X) are the two loops of the ﬁgure eight.

Proof. Again, for simplicity we work with reduced homology groups. Let U be the “left”
half of the ﬁgure eight plus a little bit of the right, as shown below.

The set V is deﬁned symmetrically. In this case U ∩ V is contractible, while each of U
and V is homotopic to S1.

Thus, we can read a segment of the long exact sequence as

→ . . . .

··· → (cid:101)Hn(U ∩ V )
(cid:125)

(cid:123)(cid:122)

(cid:124)

=0

→ (cid:101)Hn(U ) ⊕ (cid:101)Hn(V ) → (cid:101)Hn(X) → (cid:101)Hn−1(U ∩ V )
(cid:125)

(cid:123)(cid:122)

(cid:124)

=0

theorem.

So we get that (cid:101)Hn(X) ∼= (cid:101)Hn(S1) ⊕ (cid:101)Hn(S1), The claim about the generators follows
from the fact that, according to the isomorphism above, the generators of (cid:101)Hn(X) are
the generators of (cid:101)Hn(U ) and (cid:101)Hn(V ), which we described geometrically in the last

Up until now, we have been very fortunate that we have always been able to make
certain parts of the space contractible. This is not always the case, and in the next
example we will have to actually understand the maps in question to complete the
solution.

Proposition 65.3.6 (Homology groups of the torus)
Let X = S1 × S1 be the torus. Then

(cid:101)Hn(X) =

Z⊕2 n = 1
Z
n = 2
0
otherwise.

Proof. To make our diagram look good on 2D paper, we’ll represent the torus as a square
with its edges identiﬁed, though three-dimensionally the picture makes sense as well.
Consider U (shaded light orange) and V (shaded green) as shown. (Note that V is
connected due to the identiﬁcation of the left and right (blue) edges, even if it doesn’t
look connected in the picture).

US1∧S1624

Napkin, by Evan Chen (v1.5.20190718)

In the three dimensional picture, U and V are two cylinders which together give the torus.
This time, U and V are each homotopic to S1, and the intersection U ∩ V is the disjoint

union of two circles: thus (cid:101)H1(U ∩V ) ∼= Z⊕Z, and H0(U ∩V ) ∼= Z⊕2 =⇒ (cid:101)H0(U ∩V ) ∼= Z.

For n ≥ 3, we have

··· → (cid:101)Hn(U ∩ V )
(cid:125)

(cid:123)(cid:122)

(cid:124)

=0

→ (cid:101)Hn(U ) ⊕ (cid:101)Hn(V ) → (cid:101)Hn(X) → (cid:101)Hn−1(U ∩ V )
(cid:125)

(cid:123)(cid:122)

(cid:124)

=0

and so Hn(X) ∼= 0 for n ≥ 3. Also, we have H0(X) ∼= Z since X is path-connected. So it
remains to compute H2(X) and H1(X).
Let’s ﬁnd H2(X) ﬁrst. We ﬁrst consider the segment

→ . . . .

··· → (cid:101)H2(U ) ⊕ (cid:101)H2(V )
(cid:125)

(cid:123)(cid:122)

(cid:124)

=0

→ (cid:101)H2(X)

δ

(cid:44)−→ (cid:101)H1(U ∩ V )
(cid:124)
(cid:125)

(cid:123)(cid:122)

∼=Z⊕Z

φ

−→ (cid:101)H1(U ) ⊕ (cid:101)H1(V )
(cid:125)
(cid:124)

(cid:123)(cid:122)

∼=Z⊕Z

→ . . .

Unfortunately, this time it’s not immediately clear what (cid:101)H2(X) because we only have

one zero at the left. In order to do this, we have to actually ﬁgure out what the maps δ
and φ look like. Note that, as we’ll see, φ isn’t an isomorphism even though the groups
are isomorphic.

The presence of the zero term has allowed us to make the connecting map δ injective.

arrow φ inserted. To ﬁgure out what ker φ is, we have to think back to how the map
C•(U ∩ V ) → C•(U ) ⊕ C•(V ) was constructed: it was c (cid:55)→ (c,−c). So the induced maps

First, (cid:101)H2(X) is isomorphic to the image of of δ, which is exactly the kernel of the
of homology groups is actually what you would guess: a 1-cycle z in (cid:101)H1(U ∩ V ) gets sent
(z,−z) in (cid:101)H1(U ) ⊕ (cid:101)H1(V ).
In particular, consider the two generators z1 and z2 of (cid:101)H1(U ∩ V ) = Z⊕Z, i.e. one cycle
zi wraps around the ith one once.) Moreover, let αU denote a generator of (cid:101)H1(U ) ∼= Z,

in each connected component of U ∩ V . (To clarify: U ∩ V consists of two “wristbands”;
and αV a generator of H2(U ) ∼= Z. Then we have that

z1 (cid:55)→ (αU ,−αV )

and

z2 (cid:55)→ (αU ,−αV ).

(The signs may diﬀer on which direction you pick for the generators; note that Z has two
possible generators.) We can even format this as a matrix:

φ =(cid:20) 1

−1 −1(cid:21) .

1

UVV65 The long exact sequence

625

And we observe φ(z1 − z2) = 0, meaning this map has nontrivial kernel! That is,

ker φ = (cid:104)z1 − z2(cid:105) ∼= Z.

ψ

φ

. . .

−→ (cid:101)H1(U ) ⊕ (cid:101)H1(V )
(cid:124)
(cid:125)

Thus, (cid:101)H2(X) ∼= im δ ∼= ker φ ∼= Z. We’ll also note that im φ is the set generated by
(αU ,−αV ); (in particular im φ ∼= Z and the quotient by im φ is Z too).
The situation is similar with (cid:101)H1(X): this time, we have
∂(cid:16) (cid:101)H0(U ∩ V )
→ (cid:101)H0(U ) ⊕ (cid:101)H0(V )
(cid:124)
(cid:123)(cid:122)
(cid:125)
(cid:124)
(cid:125)
ker ∂ ∼= im ψ ∼=(cid:16)(cid:101)H1(U ) ⊕ (cid:101)H1(V )(cid:17) / ker ψ
∼=(cid:16)(cid:101)H1(U ) ⊕ (cid:101)H1(V )(cid:17) / im φ

and so we know that the connecting map ∂ is surjective, hence im ∂ ∼= Z. Now, we also
have

→ (cid:101)H1(X)

→ . . .

(cid:123)(cid:122)

(cid:123)(cid:122)

∼=Z⊕Z

∼= Z

∼=Z

=0

by what we knew about im φ already. To ﬁnish oﬀ we need some algebraic tricks. The
ﬁrst is Proposition 63.5.1, which gives us a short exact sequence

0 → ker ∂(cid:124)(cid:123)(cid:122)(cid:125)

∼=im ψ∼=Z

(cid:44)→ (cid:101)H1(X) (cid:16) im ∂(cid:124)(cid:123)(cid:122)(cid:125)∼=Z

→ 0.

You should satisfy yourself that (cid:101)H1(X) ∼= Z ⊕ Z is the only possibility, but we’ll prove

this rigorously with Lemma 65.3.7.

Note that the previous example is of a diﬀerent attitude than the previous ones, because
we had to ﬁgure out what the maps in the long exact sequence actually were to even
compute the groups. In principle, you could also ﬁgure out all the isomorphisms in the

previous proof and explicitly compute the generators of (cid:101)H1(S1 × S1), but to avoid getting

Finally, to fully justify the last step, we present:

bogged down in detail I won’t do so here.

Lemma 65.3.7 (Splitting lemma)

For a short exact sequence 0 → A
are equivalent:

f

−→ B

g

−→ C → 0 of abelian groups, the following

f

p

−→ B
(a) There exists p : B → A such that A
(b) There exists s : C → B such that C s−→ B
(c) There is an isomorphism from B to A ⊕ C such that the diagram

−→ A is the identity.
−→ C is the identity.

g

0

- A

⊂
⊂

-

B
6

f

g

∼=

-

?
A ⊕ C

--

--

C

-

0

commutes. (The maps attached to A ⊕ C are the obvious ones.)

In particular, (b) holds anytime C is free.

626

Napkin, by Evan Chen (v1.5.20190718)

In these cases we say the short exact sequence splits. The point is that

An exact sequence which splits let us obtain B given A and C.

done.

In particular, for C = Z or any free abelian group, condition (b) is necessarily true.

So, once we obtained the short exact sequence 0 → Z → (cid:101)H1(X) → Z → 0, we were

Remark 65.3.8 — Unfortunately, not all exact sequences split: An example of a
short exact sequence which doesn’t split is

0 → Z/2Z ×2

(cid:44)−→ Z/4Z (cid:16) Z/2Z → 0

since it is not true that Z/4Z ∼= Z/2Z ⊕ Z/2Z.

Remark 65.3.9 — The splitting lemma is true in any abelian category. The “direct
sum” is the colimit of the two objects A and C.

§65.4 A few harder problems to think about

Problem 65A. Complete the proof of Theorem 65.3.4, i.e. compute Hn(Sm) for all m
and n. (Try doing m = 2 ﬁrst, and you’ll see how to proceed.)
Problem 65B. Compute the reduced homology groups of Rn with p ≥ 1 points removed.
Problem 65C(cid:63). Let n ≥ 1 and k ≥ 0 be integers. Compute Hk(Rn, Rn \ {0}).
Problem 65D(cid:63) (Triple long exact sequence). Let A ⊆ B ⊆ X be subspaces. Show that
there is a long exact sequence

··· → Hn(B, A) → Hn(X, A) → Hn(X, B) → Hn−1(B, A) → . . . .

Problem 65E(cid:63) (Klein bottle). Show that the reduced homology groups of the Klein
bottle K are given by

(cid:101)Hn(K) =(cid:40)Z ⊕ Z/2Z n = 1

0

otherwise.

66 Excision and relative homology

We have already seen how to use the Mayer-Vietoris sequence: we started with a

sequence

··· → Hn(U ∩ V ) → Hn(U ) ⊕ Hn(V ) → Hn(U + V ) → Hn−1(U ∩ V ) → . . .

and its reduced version, then appealed to the geometric fact that Hn(U + V ) ∼= Hn(X).
This allowed us to algebraically make computations on Hn(X).

In this chapter, we turn our attention to the long exact sequence associated to the

chain complex

0 → Cn(A) (cid:44)→ Cn(X) (cid:16) Cn(X, A) → 0.

The setup will look a lot like the previous two chapters, except in addition to Hn :
hTop → Grp we will have a functor Hn : hPairTop → Grp which takes a pair (X, A) to
Hn(X, A). Then, we state (again without proof) the key geometric result, and use this
to make deductions.

§66.1 The long exact sequences

Recall Theorem 65.2.5, which says that the sequences

··· → Hn(A) → Hn(X) → Hn(X, A) → Hn−1(A) → . . . .

and

are long exact. By Problem 65D(cid:63) we even have a long exact sequence

··· → (cid:101)Hn(A) → (cid:101)Hn(X) → Hn(X, A) → (cid:101)Hn−1(A) → . . .

··· → Hn(B, A) → Hn(X, A) → Hn(X, B) → Hn−1(B, A) → . . . .
for A ⊆ B ⊆ X. An application of the second long exact sequence above gives:

Lemma 66.1.1 (Homology relative to contractible spaces)
Let X be a topological space, and let A ⊆ X be contractible. For all n,

Hn(X, A) ∼= (cid:101)Hn(X).

Proof. Since A is contractible, we have (cid:101)Hn(A) = 0 for every n. For each n there’s a

segment of the long exact sequence given by

··· → (cid:101)Hn(A)
(cid:124) (cid:123)(cid:122) (cid:125)=0

→ (cid:101)Hn(X) → Hn(X, A) → (cid:101)Hn−1(A)
(cid:123)(cid:122)
(cid:125)

(cid:124)

→ . . . .

=0

So since 0 → (cid:101)Hn(X) → Hn(X, A) → 0 is exact, this means Hn(X, A) ∼= (cid:101)Hn(X).

In particular, the theorem applies if A is a single point. The case A = ∅ is also worth

noting. We compile these results into a lemma:

627

628

Napkin, by Evan Chen (v1.5.20190718)

Lemma 66.1.2 (Relative homology generalizes absolute homology)
Let X be any space, and ∗ ∈ X a point. Then for all n,

Hn(X,{∗}) ∼= (cid:101)Hn(X)

and

Hn(X, ∅) = Hn(X).

§66.2 The category of pairs

Since we now have an Hn(X, A) instead of just Hn(X), a natural next step is to create a
suitable category of pairs and give ourselves the same functorial setup as before.
Deﬁnition 66.2.1. Let ∅ (cid:54)= A ⊆ X and ∅ (cid:54)= B ⊆ X be subspaces, and consider a map
f : X → Y . If f img(A) ⊆ B we write

f : (X, A) → (Y, B).

We say f is a map of pairs, between the pairs (X, A) and (Y, B).

Deﬁnition 66.2.2. We say that f, g : (X, A) → (Y, B) are pair-homotopic if they are
“homotopic through maps of pairs”.
More formally, a pair-homotopy f, g : (X, A) → (Y, B) is a map F : [0, 1] × X → Y ,
which we’ll write as Ft(X), such that F is a homotopy of the maps f, g : X → Y and
each Ft is itself a map of pairs.

Thus, we naturally arrive at two categories:

 PairTop, the category of pairs of toplogical spaces, and

 hPairTop, the same category except with maps only equivalent up to homotopy.

Deﬁnition 66.2.3. As before, we say pairs (X, A) and (Y, B) are pair-homotopy
equivalent if they are isomorphic in hPairTop. An isomorphism of hPairTop is a pair-
homotopy equivalence.

We can do the same song and dance as before with the prism operator to ob-

tain:

Lemma 66.2.4 (Induced maps of relative homology)

We have a functor

Hn : hPairTop → Grp.

That is, if f : (X, A) → (Y, B) then we obtain an induced map

f∗ : Hn(X, A) → Hn(Y, B).
and if two such f and g are pair-homotopic then f∗ = g∗.

Now, we want an analog of contractible spaces for our pairs: i.e. pairs of spaces (X, A)

such that Hn(X, A) = 0. The correct deﬁnition is:

Deﬁnition 66.2.5. Let A ⊆ X. We say that A is a deformation retract of X if there
is a map of pairs r : (X, A) → (A, A) which is a pair homotopy equivalence.

66 Excision and relative homology

629

Example 66.2.6 (Examples of deformation retracts)

(a) If a single point p is a deformation retract of a space X, then X is contractible,
since the retraction r : X → {∗} (when viewed as a map X → X) is homotopic
to the identity map idX : X → X.

(b) The punctured disk D2 \ {0} deformation retracts onto its boundary S1.
(c) More generally, Dn \ {0} deformation retracts onto its boundary Sn−1.
(d) Similarly, Rn \ {0} deformation retracts onto a sphere Sn−1.

Of course in this situation we have that

Hn(X, A) ∼= Hn(A, A) = 0.

Exercise 66.2.7. Show that if A ⊆ V ⊆ X, and A is a deformation retract of V , then
Hn(X, A) ∼= Hn(X, V ) for all n. (Use Problem 65D(cid:63). Solution in next section.)

§66.3 Excision

Now for the key geometric result, which is the analog of Theorem 65.3.1 for our relative
homology groups.

Theorem 66.3.1 (Excision)
Let Z ⊆ A ⊆ X be subspaces such that the closure of Z is contained in the interior
of A. Then the inclusion ι(X \ Z, A\ Z) (cid:44)→ (X, A) (viewed as a map of pairs) induces
an isomorphism of relative homology groups

Hn(X \ Z, A \ Z) ∼= Hn(X, A).

This means we can excise (delete) a subset Z of A in computing the relative homology
groups Hn(X, A). This should intuitively make sense: since we are “modding out by
points in A”, the internals of the point A should not matter so much.

The main application of excision is to decide when Hn(X, A) ∼= (cid:101)Hn(X/A). Answer:

Theorem 66.3.2 (Relative homology =⇒ quotient space)
Let X be a space and A be a subspace such that A is a deformation retract of some
open set V ⊆ X. Then the quotient map q : X → X/A induces an isomorphism

Hn(X, A) ∼= Hn(X/A, A/A) ∼= (cid:101)Hn(X/A).

Proof. By hypothesis, we can consider the following maps of pairs:

r : (V, A) → (A, A)
q : (X, A) → (X/A, A/A)

(cid:98)q : (X − A, V − A) → (X/A − A/A, V /A − A/A).

630

Napkin, by Evan Chen (v1.5.20190718)

Moreover, r is a pair-homotopy equivalence. Considering the long exact sequence of a
triple (which was Problem 65D(cid:63)) we have a diagram

Hn(V, A) - Hn(X, A)

f- Hn(X, V ) - Hn−1(V, A)

∼= r
?

∼= r
?

Hn(A, A)

(cid:124)

=0

(cid:123)(cid:122)

(cid:125)

Hn−1(A, A)

(cid:124)

=0

(cid:123)(cid:122)

(cid:125)

where the isomorphisms arise since r is a pair-homotopy equivalence. So f is an isomor-
phism. Similarly the map

g : Hn(X/A, A/A) → Hn(X/A, V /A)

is an isomorphism.

Now, consider the commutative diagram

Hn(X, A)

f - Hn(X, V )  Excise

q∗

?

Hn(X/A, A/A)

g- Hn(X/A, V /A) Excise

Hn(X − A, V − A)

(cid:98)q∗ ∼=

?

Hn(X/A − A/A, V /A − A/A)

and observe that the rightmost arrow (cid:98)q∗ is an isomorphism, because outside of A the
map(cid:98)q is the identity. We know f and g are isomorphisms, as are the two arrows marked

with “Excise” (by excision). From this we conclude that q∗ is an isomorphism. Of course
we already know that homology relative to a point is just the relative homology groups
(this is the important case of Lemma 66.1.1).

§66.4 Some applications

One nice application of excision is to compute (cid:101)Hn(X ∨ Y ).

Theorem 66.4.1 (Homology of wedge sums)
Let X and Y be spaces with basepoints x0 ∈ X and y0 ∈ Y , and assuming each
point is a deformation retract of some open neighborhood. Then for every n we have

(cid:101)Hn(X ∨ Y ) = (cid:101)Hn(X) ⊕ (cid:101)Hn(Y ).

Proof. Apply Theorem 66.3.2 with the subset {x0, y0} of X (cid:113) Y ,

(cid:101)Hn(X ∨ Y ) ∼= (cid:101)Hn((X (cid:113) Y )/{x0, y0}) ∼= Hn(X (cid:113) Y,{x0, y0})
∼= Hn(X,{x0}) ⊕ Hn(Y,{y0})
∼= (cid:101)Hn(X) ⊕ (cid:101)Hn(Y ).

Another application is to give a second method of computing Hn(Sm). To do this, we

will prove that

for any n, m > 1. However,

(cid:101)Hn(Sm) ∼= (cid:101)Hn−1(Sm−1)

66 Excision and relative homology

631

 (cid:101)H0(Sn) is Z for n = 0 and 0 otherwise.
 (cid:101)Hn(S0) is Z for m = 0 and 0 otherwise.

So by induction on min{m, n} we directly obtain that

(cid:101)Hn(Sm) ∼=(cid:40)Z m = n

0

otherwise

which is what we wanted.

To prove the claim, let’s consider the exact sequence formed by the pair X = D2 and

A = S1.

Example 66.4.2 (The long exact sequence for (X, A) = (D2, S1))
Consider D2 (which is contractible) with boundary S1. Clearly S1 is a deformation
retraction of D2 \ {0}, and if we fuse all points on the boundary together we get
D2/S1 ∼= S2. So we have a long exact sequence





- (cid:101)H2(S2)
- (cid:101)H1(S2)
- (cid:101)H0(S2)
(cid:124) (cid:123)(cid:122) (cid:125)=0

(cid:101)H2(S1) - (cid:101)H2(D2)
(cid:124) (cid:123)(cid:122) (cid:125)=0
(cid:101)H1(S1) -
(cid:101)H1(D2)
(cid:124) (cid:123)(cid:122) (cid:125)=0
(cid:101)H0(S1) -
(cid:101)H0(D2)
(cid:124) (cid:123)(cid:122) (cid:125)=0
(cid:101)H2(S2) = (cid:101)H1(S1),
(cid:101)H3(S2) = (cid:101)H2(S1),
(cid:101)Hn(Sm) ∼= (cid:101)Hn−1(Sm−1), which is the desired conclusion.

From this diagram we read that

§66.5 Invariance of dimension

. . . ,

More generally, the exact sequence for the pair (X, A) = (Dm, Sm−1) shows that

(cid:101)H1(S2) = (cid:101)H0(S1).

Here is one last example of an application of excision.

Deﬁnition 66.5.1. Let X be a space and p ∈ X a point. The kth local homology
group of p at X is deﬁned as

Hk(X, X \ {p}).

Note that for any open neighborhood U of p, we have by excision that

Hk(X, X \ {p}) ∼= Hk(U, U \ {p}).

Thus this local homology group only depends on the space near p.

632

Napkin, by Evan Chen (v1.5.20190718)

Theorem 66.5.2 (Invariance of dimension, Brouwer 1910)
Let U ⊆ Rn and V ⊆ Rm be nonempty open sets. If U and V are homeomorphic,
then m = n.

Proof. Consider a point x ∈ U and its local homology groups. By excision,

Hk(Rn, Rn \ {x}) ∼= Hk(U, U \ {x}).

But since Rn \ {x} is homotopic to Sn−1, the long exact sequence of Theorem 65.2.5 tells
us that

Hk(Rn, Rn \ {x}) ∼=(cid:40)Z k = n

0

otherwise.

Analogously, given y ∈ V we have

Hk(Rm, Rm \ {y}) ∼= Hk(V, V \ {y}).

If U ∼= V , we thus deduce that

Hk(Rn, Rn \ {x}) ∼= Hk(Rm, Rm \ {y})

for all k. This of course can only happen if m = n.

§66.6 A few harder problems to think about

Problem 66A. Let X = S1 × S1 and Y = S1 ∨ S1 ∨ S2. Show that

Hn(X) ∼= Hn(Y )

for every integer n.

Problem 66B (Hatcher §2.1 exercise 18). Consider Q ⊂ R. Compute (cid:101)H1(R, Q).

Problem 66C(cid:63). What are the local homology groups of a topological n-manifold?

Problem 66D. Let

X = {(x, y) | x ≥ 0} ⊆ R2

denote the half-plane. What are the local homology groups of points in X?

67 Bonus: Cellular homology

We now introduce cellular homology, which essentially lets us compute the homology

groups of any CW complex we like.

§67.1 Degrees

Prototypical example for this section: z (cid:55)→ zd has degree d.

For any n > 0 and map f : Sn → Sn, consider

f∗ : Hn(Sn)

(cid:124) (cid:123)(cid:122) (cid:125)∼=Z

→ Hn(Sn)
(cid:124) (cid:123)(cid:122) (cid:125)∼=Z

which must be multiplication by some constant d. This d is called the degree of f ,
denoted deg f .

Question 67.1.1. Show that deg(f ◦ g) = deg(f ) deg(g).

Example 67.1.2 (Degree)
(a) For n = 1, the map z (cid:55)→ zk (viewing S1 ⊆ C) has degree k.
(b) A reﬂection map (x0, x1, . . . , xn) (cid:55)→ (−x0, x1, . . . , xn) has degree −1; we won’t

prove this, but geometrically this should be clear.

(c) The antipodal map x (cid:55)→ −x has degree (−1)n+1 since it’s the composition of

n + 1 reﬂections as above. We denote this map by −id.

Obviously, if f and g are homotopic, then deg f = deg g. In fact, a theorem of Hopf
says that this is a classifying invariant: anytime deg f = deg g, we have that f and g are
homotopic.

One nice application of this:

Theorem 67.1.3 (Hairy ball theorem)
If n > 0 is even, then Sn doesn’t have a continuous ﬁeld of nonzero tangent vectors.

Proof. If the vectors are nonzero then WLOG they have norm 1; that is for every
x we have an orthogonal unit vector v(x). Then we can construct a homotopy map
F : Sn × [0, 1] → Sn by

(x, t) (cid:55)→ (cos πt)x + (sin πt)v(x).

which gives a homotopy from id to −id. So deg(id) = deg(−id), which means 1 = (−1)n+1
so n must be odd.

Of course, the one can construct such a vector ﬁeld whenever n is odd. For example,
when n = 1 such a vector ﬁeld is drawn below.

633

634

Napkin, by Evan Chen (v1.5.20190718)

§67.2 Cellular chain complex

Before starting, we state:

Lemma 67.2.1 (CW homology groups)

Let X be a CW complex. Then

Hk(X n, X n−1) ∼=(cid:40)Z⊕#n-cells of X k = n

0

otherwise.

and

Hk(X n) ∼=(cid:40)Hk(X) k ≤ n − 1

0

k ≥ n + 1.

Proof. The ﬁrst part is immediate by noting that (X n, X n−1) is a good pair and X n/X n−1
is a wedge sum of two spheres. For the second part, ﬁx k and note that, as long as
n ≤ k − 1 or n ≥ k + 2,

(cid:124)

=0

(cid:123)(cid:122)

So we have isomorphisms

Hk+1(X n, X n−1)

→ Hk(X n−1) → Hk(X n) → Hk(X n, X n−1)
(cid:125)

(cid:123)(cid:122)

(cid:124)

=0

.

(cid:125)

and

Hk(X k−1) ∼= Hk(X k−2) ∼= . . . ∼= Hk(X 0) = 0

Hk(X k+1) ∼= Hk(X k+2) ∼= . . . ∼= Hk(X).

So, we know that the groups Hk(X k, X k−1) are super nice: they are free abelian with

basis given by the cells of X. So, we give them a name:

Deﬁnition 67.2.2. For a CW complex X, we deﬁne

Cellsk(X) = Hk(X k, X k−1)

where Cells0(X) = H0(X 0, ∅) = H0(X 0) by convention. So Cellsk(X) is an abelian
group with basis given by the k-cells of X.

S167 Bonus: Cellular homology

635

Now, using Cellsk = Hk(X k, X k−1) let’s use our long exact sequence and try to string

together maps between these. Consider the following diagram.

H3(X 2)

(cid:124) (cid:123)(cid:122) (cid:125)=0

0
?

H3(X 3)

Cells4(X)

∂4 -

...........

d

4...........

-

0
?

Cells3(X)

...........

-
0

--

H3(X 4)
∼=H3(X)

(cid:124) (cid:123)(cid:122) (cid:125)

d

3...........

-

⊂ -

Cells2(X)

∂2 -

...........

H2(X 1)

(cid:124) (cid:123)(cid:122) (cid:125)=0

∂3

?

H2(X 2)

-

0

??
H2(X 3)
∼=H2(X)

(cid:124) (cid:123)(cid:122) (cid:125)

0
?

H2(X 3, X 2)

=0

(cid:123)(cid:122)

(cid:125)

(cid:124)

H3(X 4, X 3)

=0

(cid:123)(cid:122)

(cid:125)

(cid:124)

H1(X 0)

(cid:124) (cid:123)(cid:122) (cid:125)=0

0
?

H1(X 1)

∩

d

2...........

-

?

Cells1(X)

...........

-
0

--

H1(X 2)
∼=H1(X)

(cid:124) (cid:123)(cid:122) (cid:125)

(cid:124)

H1(X 2, X 1)

=0

(cid:123)(cid:122)

(cid:125)

d

1...........

-

⊂ -

Cells0(X)

∂0 -

. . .

H0(∅)

(cid:124) (cid:123)(cid:122) (cid:125)=0

∂1

?

H0(X 0)

-

0

??
H0(X 1)
∼=H0(X)

(cid:124) (cid:123)(cid:122) (cid:125)

0
?

H0(X 1, X 0)

=0

(cid:123)(cid:122)

(cid:125)

(cid:124)

The idea is that we have taken all the exact sequences generated by adjacent skeletons,
and strung them together at the groups Hk(X k), with half the exact sequences being
laid out vertically and the other half horizontally.

In that case, composition generates a sequence of dotted maps between the Hk(X k, X k−1)

as shown.

Question 67.2.3. Show that the composition of two adjacent dotted arrows is zero.

So from the diagram above, we can read oﬀ a sequence of arrows

. . .

d5−→ Cells4(X)

d4−→ Cells3(X)

d3−→ Cells2(X)

d2−→ Cells1(X)

d1−→ Cells0(X)

d0−→ 0.

This is a chain complex, called the cellular chain complex; as mentioned before before
all the homology groups are free, but these ones are especially nice because for most

636

Napkin, by Evan Chen (v1.5.20190718)

reasonable CW complexes, they are also ﬁnitely generated (unlike the massive C•(X)
that we had earlier). In other words, the Hk(X k, X k−1) are especially nice “concrete”
free groups that one can actually work with.

The other reason we care is that in fact:

Theorem 67.2.4 (Cellular chain complex gives Hn(X))
The kth homology group of the cellular chain complex is isomorphic to Hk(X).

Proof. Follows from the diagram; Problem 67D.

A nice application of this is to deﬁne the Euler characteristic of a ﬁnite CW complex

X. Of course we can write

χ(X) =(cid:88)n

(−1)n · #(n-cells of X)

which generalizes the familiar V −E +F formula. However, this deﬁnition is unsatisfactory
because it depends on the choice of CW complex, while we actually want χ(X) to only
depend on the space X itself (and not how it was built). In light of this, we prove
that:

Theorem 67.2.5 (Euler characteristic via Betti numbers)

For any ﬁnite CW complex X we have

χ(X) =(cid:88)n

(−1)n rank Hn(X).

Thus χ(X) does not depend on the choice of CW decomposition. The numbers

bn = rank Hn(X)

are called the Betti numbers of X. In fact, we can use this to deﬁne χ(X) for any
reasonable space; we are happy because in the (frequent) case that X is a CW complex,

Proof. We quote the fact that if 0 → A → B → C → D → 0 is exact then rank B +
rank D = rank A + rank C. Then for example the row

- H2(X 2) ⊂ - H2(X 2, X 1)

0

H2(X 1)

(cid:124) (cid:123)(cid:122) (cid:125)=0

from the cellular diagram gives

∂2- H1(X 1) -- H1(X 2)
∼=H1(X)

(cid:124) (cid:123)(cid:122) (cid:125)

- H1(X 2, X 1)

0

(cid:125)

(cid:124)

=0

(cid:123)(cid:122)

#(2-cells) + rank H1(X) = rank H2(X 2) + rank H1(X 1).

More generally,

#(k-cells) + rank Hk−1(X) = rank Hk(X k) + rank Hk−1(X k−1)

which holds also for k = 0 if we drop the H−1 terms (since #0-cells = rank H0(X 0) is
obvious). Multiplying this by (−1)k and summing across k ≥ 0 gives the conclusion.

67 Bonus: Cellular homology

637

Example 67.2.6 (Examples of Betti numbers)
(a) The Betti numbers of Sn are b0 = bn = 1, and zero elsewhere. The Euler

characteristic is 1 + (−1)n.

(b) The Betti numbers of a torus S1 × S1 are b0 = 1, b1 = 2, b2 = 1, and zero

elsewhere. Thus the Euler characteristic is 0.

(c) The Betti numbers of CPn are b0 = b2 = ··· = b2n = 1, and zero elsewhere.

Thus the Euler characteristic is n + 1.

(d) The Betti numbers of the Klein bottle are b0 = 1, b1 = 1 and zero elsewhere.
Thus the Euler characteristic is 0, the same as the sphere (also since their CW
structures use the same number of cells).

One notices that in the “nice” spaces Sn, S1 × S1 and CPn there is a nice symmetry
in the Betti numbers, namely bk = bn−k. This is true more generally; see Poincar´e
duality and Problem 69A†.

§67.3 The cellular boundary formula

In fact, one can describe explicitly what the maps dn are. Recalling that Hk(X k, X k−1)
has a basis the k-cells of X, we obtain:

Theorem 67.3.1 (Cellular boundary formula for k = 1)
For k = 1,

d1 : Cells1(X) → Cells0(X)

is just the boundary map.

Theorem 67.3.2 (Cellular boundary for k > 1)
Let k > 1 be a positive integer. Let ek be an k-cell, and let {ek−1
(k − 1)-cells of X. Then

β

}β denote all

is given on basis elements by

dk : Cellsk(X) → Cellsk−1(X)

dk(ek) =(cid:88)β

dβek−1

β

where dβ is be the degree of the composed map

Sk−1 = ∂Dk
β

attach−−−−→ X k−1 (cid:16) Sk−1

β

.

Here the ﬁrst arrow is the attaching map for ek and the second arrow is the quotient
of collapsing X k−1 \ ek−1

to a point.

β

This gives us an algorithm for computing homology groups of a CW complex:

638

Napkin, by Evan Chen (v1.5.20190718)

 Construct the cellular chain complex, where Cellsk(X) is Z⊕#k-cells.

 d1 : Cells1(X) → Cells0(X) is just the boundary map (so d1(e1) is the diﬀerence of

the two endpoints).

 For any k > 1, we compute dk : Cellsk(X) → Cellsk−1(X) on basis elements as

follows. Repeat the following for each k-cell ek:

, compute the degree of the boundary of ek welded

β

– For every k − 1 cell ek−1
onto the boundary of ek−1
– Then dk(ek) =(cid:80)β dβek−1

β

β

.

, say dβ.

 Now we have the maps of the cellular chain complex, so we can compute the

homologies directly (by taking the quotient of the kernel by the image).

We can use this for example to compute the homology groups of the torus again, as

well as the Klein bottle and other spaces.

Example 67.3.3 (Cellular homology of a torus)
a, e1
Consider the torus built from e0, e1
the word aba−1b−1. For example, X 1 is

b and e2 as before, where e2 is attached via

The cellular chain complex is

0

- Ze2

d2- Ze1

a ⊕ Ze1

b

d1 - Ze0

d0 - 0

Now apply the cellular boundary formulas:

 Recall that d1 was the boundary formula. We have d1(e1

a) = e0 − e0 = 0 and

similarly d1(e1

b ) = 0. So d1 = 0.

 For d2, consider the image of the boundary e2 on e1

a, once around e1

once around e1
and again around e1
the degree of the map is 0. So d2(e2) has no e1
e1
b coeﬃcient, hence d2 = 0.

b . Once we collapse the entire e1

b , again around e1

a. Around X 1, it wraps
a (in the opposite direction),
b to a point, we see that
a coeﬃcient. Similarly, it has no

Thus

d1 = d2 = 0.

So at every map in the complex, the kernel of the map is the whole space while the
image is {0}. So the homology groups are Z, Z⊕2, Z.

e1ae1be067 Bonus: Cellular homology

639

Example 67.3.4 (Cellular homology of the Klein bottle)
Let X be a Klein bottle. Consider cells e0, e1
b and e2 as before, but this time e2 is
attached via the word abab−1. So d1 is still zero, but this time we have d2(e2) = 2e1
a
instead (why?). So our diagram looks like

a, e1

0

- Ze2

e2

d2- Ze1

d1 - Ze0

d0 -

0

b

a ⊕ Ze1
- 2e1

a

So we get that H0(X) ∼= Z, but

ea
1

eb
1

- 0

- 0

H1(X) ∼= Z ⊕ Z/2Z

this time (it is Z⊕2 modulo a copy of 2Z). Also, ker d2 = 0, and so now H2(X) = 0.

§67.4 A few harder problems to think about

Problem 67A†. Let n be a positive integer. Show that

Hk(CPn) ∼=(cid:40)Z k = 0, 2, 4, . . . , 2n

otherwise.

0

Problem 67B. Show that a non-surjective map f : Sn → Sn has degree zero.
Problem 67C (Moore spaces). Let G1, G2, . . . , GN be a sequence of ﬁnitely generated
abelian groups. Construct a space X such that

(cid:101)Hn(X) ∼=(cid:40)Gn

0

1 ≤ n ≤ N
otherwise.

Problem 67D. Prove Theorem 67.2.4, showing that the homology groups of X coincide
with the homology groups of the cellular chain complex.

Problem 67E†. Let n be a positive integer. Show that

Hk(RPn) ∼=

if k = 0 or k = n ≡ 1

Z
Z/2Z if k is odd and 0 < k < n
0

otherwise.

(mod 2)

68 Singular cohomology

Here’s one way to motivate this chapter. It turns out that:

 Hn(CP2) ∼= Hn(S2 ∨ S4) for every n.
 Hn(CP3) ∼= Hn(S2 × S4) for every n.

This is unfortunate, because if possible we would like to be able to tell these spaces apart
(as they are in fact not homotopy equivalent), but the homology groups cannot tell the
diﬀerence between them.

In this chapter, we’ll deﬁne a cohomology group H n(X) and H n(Y ). In fact, the H n’s
are completely determined by the Hn’s by the so-called universal coeﬃcient theorem.
However, it turns out that one can take all the cohomology groups and put them together

to form a cohomology ring H•. We will then see that H•(X) (cid:54)∼= H•(Y ) as rings.

§68.1 Cochain complexes

Deﬁnition 68.1.1. A cochain complex A• is algebraically the same as a chain complex,
except that the indices increase. So it is a sequence of abelian groups

. . . δ−→ An−1 δ−→ An δ−→ An+1 δ−→ . . . .

such that δ2 = 0. Notation-wise, we’re now using subscripts, and use δ rather ∂. We
deﬁne the cohomology groups by

H n(A•) = ker(cid:16)An δ−→ An+1(cid:17) / im(cid:16)An−1 δ−→ An(cid:17) .

Example 68.1.2 (de Rham cohomology)
We have already met one example of a cochain complex: let M be a smooth manifold
and Ωk(M ) be the additive group of k-forms on M . Then we have a cochain complex

0 d−→ Ω0(M ) d−→ Ω1(M ) d−→ Ω2(M ) d−→ . . . .

The resulting cohomology is called de Rham cohomology, described later.

Aside from de Rham’s cochain complex, the most common way to get a cochain
complex is to dualize a chain complex. Speciﬁcally, pick an abelian group G; note
that Hom(−, G) is a contravariant functor, and thus takes every chain complex

∂−→ An

. . . ∂−→ An+1

∂−→ . . .
into a cochain complex: letting An = Hom(An, G) we obtain
. . . δ−→ An−1 δ−→ An δ−→ An+1 δ−→ . . . .
∂−→ A

∂−→ An−1

f

−→ G.

f

where δ(An

−→ G) = An+1
special notation to them.

These are the cohomology groups we study most in algebraic topology, so we give a

641

642

Napkin, by Evan Chen (v1.5.20190718)

Deﬁnition 68.1.3. Given a chain complex A• of abelian groups and another group G,
we let

denote the cohomology groups of the dual cochain complex A• obtained by applying
Hom(−, G). In other words, H n(A•; G) = H n(A•).

H n(A•; G)

§68.2 Cohomology of spaces

Prototypical example for this section: C0(X; G) all functions X → G while H 0(X) are
those functions X → G constant on path components.

The case of interest is our usual geometric situation, with C•(X).

Deﬁnition 68.2.1. For a space X and abelian group G, we deﬁne C•(X; G) to be the
dual to the singular chain complex C•(X), called the singular cochain complex of X;
its elements are called cochains.

Then we deﬁne the cohomology groups of the space X as

H n(X; G) := H n(C•(X); G) = Hn(C•(X; G)).

Remark 68.2.2 — Note that if G is also a ring (like Z or R), then H n(X; G) is not
only an abelian group but actually a G-module.

Example 68.2.3 (C0(X; G), C1(X; G), and H 0(X; G))
Let X be a topological space and consider C•(X).

 C0(X) is the free abelian group on X, and C0(X) = Hom(C0(X), G). So a

0-cochain is a function that takes every point of X to an element of G.

 C1(X) is the free abelian group on 1-simplices in X. So C1(X) needs to take

every 1-simplex to an element of G.

Let’s now try to understand δ : C0(X) → C1(X). Given a 0-cochain φ ∈ C0(X), i.e.
a homomorphism φ : C0(X) → G, what is δφ : C1(X) → G? Answer:

δφ : [v0, v1] (cid:55)→ φ([v0]) − φ([v1]).

Hence, elements of ker(C0 δ−→ C1) ∼= H 0(X; G) are those cochains that are constant
on path-connected components.

In particular, much like H0(X), we have

if X has r path-connected components (where r is ﬁnite1).

H 0(X) ∼= G⊕r

To the best of my knowledge, the higher cohomology groups H n(X; G) (or even the

cochain groups Cn(X; G) = Hom(Cn(X), G)) are harder to describe concretely.

1Something funny happens if X has inﬁnitely many path-connected components: say X =(cid:96)
an inﬁnite indexing set. In this case we have H0(X) =(cid:76)
α G while H 0(X) =(cid:81)
These are actually diﬀerent for inﬁnite indexing sets. For general modules(cid:76)

we get a direct sum while for cohomology we get a direct product.

α Xα over
α G. For homology

α Mα is deﬁned to

68 Singular cohomology

643

Abuse of Notation 68.2.4. In this chapter the only cochain complexes we will consider
are dual complexes as above. So, any time we write a chain complex A• it is implicitly
given by applying Hom(−, G) to A•.

§68.3 Cohomology of spaces is functorial

We now check that the cohomology groups still exhibit the same nice functorial behavior.
First, let’s categorize the previous results we had:

Question 68.3.1. Deﬁne CoCmplx the category of cochain complexes.

Exercise 68.3.2. Interpret Hom(−, G) as a contravariant functor from

Hom(−, G) : Cmplxop → CoCmplx.

This means in particular that given a chain map f : A• → B•, we naturally obtain a dual
map f∨ : B•

→ A•.

Question 68.3.3. Interpret H n : CoCmplx → Grp as a functor. Compose these to get a
contravariant functor H n(−; G) : Cmplxop → Grp.

Then in exact analog to our result that Hn : hTop → Grp we have:

Theorem 68.3.4 (H n(−; G) : hTopop → Grp)
For every n, H n(−; G) is a contravariant functor from hTopop to Grp.

Proof. The idea is to leverage the work we already did in constructing the prism operator
earlier. First, we construct the entire sequence of functors from Topop → Grp:

Topop

C•- Cmplxop Hom(−;G)- CoCmplx

H n - Grp

X

f

?
Y

C•(X)

-

f(cid:93)

?
C•(Y )

C•(X; G)

6

H n(X; G)

6

-

f (cid:93)

-

f∗

C•(Y ; G)

H n(Y ; G).

Here f (cid:93) = (f(cid:93))∨, and f∗ is the resulting induced map on homology groups of the cochain
complex.

So as before all we have to show is that f (cid:39) g, then f∗ = g∗. Recall now that there is
a prism operator such that f(cid:93) − g(cid:93) = P ∂ + ∂P . If we apply the entire functor Hom(−; G)
we get that f (cid:93) − g(cid:93) = δP ∨ + P ∨δ where P ∨ : Cn+1(Y ; G) → Cn(X; G). So f (cid:93) and g(cid:93) are
chain homotopic thus f∗ = g∗.
only ever deﬁned M ⊕ N and extended it to ﬁnite direct sums.) No such restriction holds for(cid:81)

only allow to have ﬁnitely many zero terms. (This was never mentioned earlier in the Napkin, since I
α Gα
a product of groups. This corresponds to the fact that C0(X) is formal linear sums of 0-chains (which,
like all formal sums, are ﬁnite) from the path-connected components of G. But a cochain of C 0(X) is
a function from each path-connected component of X to G, where there is no restriction.

644

Napkin, by Evan Chen (v1.5.20190718)

§68.4 Universal coeﬃcient theorem

We now wish to show that the cohomology groups are determined up to isomorphism by
the homology groups: given Hn(A•), we can extract H n(A•; G). This is achieved by the
universal coeﬃcient theorem.

Theorem 68.4.1 (Universal coeﬃcient theorem)
Let A• be a chain complex of free abelian groups, and let G be another abelian
group. Then there is a natural short exact sequence

0 → Ext(Hn−1(A•), G) → H n(A•; G) h−→ Hom(Hn(A•), G) → 0.

In addition, this exact sequence is split so in particular

H n(C•; G) ∼= Ext(Hn−1(A•, G)) ⊕ Hom(Hn(A•), G).

Fortunately, in our case of interest, A• is C•(X) which is by deﬁnition free.

There are two things we need to explain, what the map h is and the map Ext is.
It’s not too hard to guess how

h : H n(A•; G) → Hom(Hn(A•), G)

is deﬁned. An element of H n(A•; G) is represented by a function which sends a cycle in
An to an element of G. The content of the theorem is to show that h is surjective with
kernel Ext(Hn−1(A•), G).

What about Ext? It turns out that Ext(−, G) is the so-called Ext functor, deﬁned
as follows. Let H be an abelian group, and consider a free resolution of H, by which
we mean an exact sequence

. . .

f2−→ F1

f1−→ F0

f0−→ H → 0

with each Fi free. Then we can apply Hom(−, G) to get a cochain complex

. . .

f∨
2←−− Hom(F1, G)

f∨
1←−− Hom(F0, G)

f∨
0←−− Hom(H, G) ← 0.

but this cochain complex need not be exact (in categorical terms, Hom(−, G) does not
preserve exactness). We deﬁne

Ext(H, G) := ker(f∨2 )/ im(f∨1 )

and it’s a theorem that this doesn’t depend on the choice of the free resolution. There’s
a lot of homological algebra that goes into this, which I won’t take the time to discuss;
but the upshot of the little bit that I did include is that the Ext functor is very easy to
compute in practice, since you can pick any free resolution you want and compute the
above.

Lemma 68.4.2 (Computing the Ext functor)
For any abelian groups G, H, H(cid:48) we have
(a) Ext(H ⊕ H(cid:48), G) = Ext(H, G) ⊕ Ext(H(cid:48), G).
(b) Ext(H, G) = 0 for H free, and

(c) Ext(Z/nZ, G) = G/nG.

68 Singular cohomology

645

Proof. For (a), note that if ··· → F1 → F0 → H → 0 and ··· → F (cid:48)1 → F (cid:48)0 → F (cid:48)0 → H(cid:48) →
0 are free resolutions, then so is F1 ⊕ F (cid:48)1 → F0 ⊕ F (cid:48)0 → H ⊕ H(cid:48) → 0.

For (b), note that 0 → H → H → 0 is a free resolution.
Part (c) follows by taking the free resolution

0 → Z ×n−−→ Z → Z/nZ → 0

and applying Hom(−, G) to it.

Question 68.4.3. Finish the proof of (c) from here.

Question 68.4.4. Some Ext practice: compute Ext(Z⊕2015, G) and Ext(Z/30Z, Z/4Z).

§68.5 Example computation of cohomology groups

Prototypical example for this section: Possibly H n(Sm).

The universal coeﬃcient theorem gives us a direct way to compute any cohomology

groups, provided we know the homology ones.

Example 68.5.1 (Cohomolgy groups of Sm)
It is straightforward to compute H n(Sm) now: all the Ext terms vanish since Hn(Sm)
is always free, and hence we obtain that

H n(Sm) ∼= Hom(Hn(Sm), G) ∼=(cid:40)G n = m, n = 0

otherwise.

0

Example 68.5.2 (Cohomolgy groups of torus)
This example has no nonzero Ext terms either, since this time H n(S1 × S1) is always
free. So we obtain

H n(S1 × S1) ∼= Hom(Hn(S1 × S1), G).

Since Hn(S1 × S1) is Z, Z⊕2, Z in dimensions n = 1, 2, 1 we derive that

H n(S1 × S1) ∼=(cid:40)G

n = 0, 2

G⊕2 n = 1.

From these examples one might notice that:

Lemma 68.5.3 (0th homology groups are just duals)
For n = 0 and n = 1, we have

H n(X; G) ∼= Hom(Hn(X), G).

Proof. It’s already been shown for n = 0. For n = 1, notice that H0(X) is free, so the
Ext term vanishes.

646

Napkin, by Evan Chen (v1.5.20190718)

Example 68.5.4 (Cohomolgy groups of Klein bottle)
This example will actually have Ext term. Recall that if K is a Klein Bottle then its
homology groups are Z in dimension n = 0 and Z ⊕ Z/2Z in n = 1, and 0 elsewhere.
For n = 0, we again just have H 0(K; G) ∼= Hom(Z, G) ∼= G. For n = 1, the Ext
term is Ext(H0(K), G) ∼= Ext(Z, G) = 0 so

H 1(K; G) ∼= Hom(Z ⊕ Z/2Z, G) ∼= G ⊕ Hom(Z/2Z, G).

We have that Hom(Z/2Z, G) is the subgroup of elements of order 2 in G (and 0 ∈ G).

But for n = 2, we have our ﬁrst interesting Ext group: the exact sequence is

0 → Ext(Z ⊕ Z/2Z, G) → H 2(X; G) → H2(X)
(cid:124) (cid:123)(cid:122) (cid:125)=0

→ 0.

Thus, we have

H 2(X; G) ∼= (Ext(Z, G) ⊕ Ext(Z/2Z, G)) ⊕ 0 ∼= G/2G.

All the higher groups vanish. In summary:

H n(X; G) ∼=

G
n = 0
G ⊕ Hom(Z/2Z, G) n = 1
n = 2
G/2G
n ≥ 3.
0



§68.6 Relative cohomology groups

One can also deﬁne relative cohomology groups in the obvious way: dualize the chain
complex

to obtain a cochain complex

. . . ∂−→ C1(X, A) ∂−→ C0(X, A) → 0

We can take the cohomology groups ofthis.

. . . δ←− C1(X, A; G) δ←− C0(X, A; G) ← 0.

Deﬁnition 68.6.1. The groups thus obtained are the relative cohomology groups
are denoted H n(X, A; G).

In addition, we can deﬁne reduced cohomology groups as well. One way to do it is to

take the augmented singular chain complex

. . . ∂−→ C1(X) ∂−→ C0(X) ε−→ Z → 0

and dualize it to obtain

Since the Z we add is also free, the universal coeﬃcient theorem still applies. So this will
give us reduced cohomology groups.

However, since we already deﬁned the relative cohomology groups, it is easiest to

simply deﬁne:

. . . δ←− C1(X; G) δ←− C0(X; G) ε∨

← 0.

←− Hom(Z, G)
(cid:125)
(cid:124)

(cid:123)(cid:122)

∼=G

68 Singular cohomology

647

Deﬁnition 68.6.2. The reduced cohomology groups of a nonempty space X, de-

noted (cid:101)H n(X; G), are deﬁned to be H n(X,{∗}; G) for some point ∗ ∈ X.

§68.7 A few harder problems to think about

Problem 68A(cid:63) (Wedge product cohomology). For any G and n we have

Problem 68B†. Prove that for a ﬁeld F of characteristic zero and a space X with
ﬁnitely generated homology groups:

(cid:101)H n(X ∨ Y ; G) ∼= (cid:101)H n(X; G) ⊕ (cid:101)H n(Y ; G).

H k(X, F ) ∼= (Hk(X))∨ .

Thus over ﬁelds cohomology is the dual of homology.

Problem 68C (Z/2Z-cohomology of RPn). Prove that

m = 0, or m is odd and m = n

H m(RPn, Z/2Z) ∼=

Z
Z/2Z 0 < m < n and m is odd
0

otherwise.

69 Application of cohomology

In this ﬁnal chapter on topology, I’ll state (mostly without proof) some nice properties
of cohomology groups, and in particular introduce the so-called cup product. For an
actual treatise on the cup product, see [Ha02] or [Ma13a].

§69.1 Poincar´e duality

First cool result: you may have noticed symmetry in the (co)homology groups of “nice”
spaces like the torus or Sn. In fact this is predicted by:

Theorem 69.1.1 (Poincar´e duality)

If M is a smooth oriented compact n-manifold, then we have a natural isomorphism

for every k. In particular, H k(M ) = 0 for k > n.

H k(M ; Z) ∼= Hn−k(M )

So for smooth oriented compact manifolds, cohomology and homology groups are not so
diﬀerent.

From this follows the symmetry that we mentioned when we ﬁrst deﬁned the Betti

numbers:

Corollary 69.1.2 (Symmetry of Betti numbers)

Let M be a smooth oriented compact n-manifold, and let bk denote its Betti number.
Then

bk = bn−k.

Proof. Problem 69A†.

§69.2 de Rham cohomology

We now reveal the connection between diﬀerential forms and singular cohomology.

Let M be a smooth manifold. We are interested in the homology and cohomology

groups of M . We specialize to the case G = R, the additive group of real numbers.

Question 69.2.1. Check that Ext(H, R) = 0 for any ﬁnitely generated abelian group H.

Thus, with real coeﬃcients the universal coeﬃcient theorem says that

H k(M ; R) ∼= Hom(Hk(M ), R) = (Hk(M ))∨

where we view Hk(X) as a real vector space. So, we’d like to get a handle on either
Hk(M ) or H k(M ; R).

649

650

Napkin, by Evan Chen (v1.5.20190718)

Consider the cochain complex

0 → Ω0(M ) d−→ Ω1(M ) d−→ Ω2(M ) d−→ Ω3(M ) d−→ . . .

and let H k
closed forms modulo the exact forms.

dR(M ) denote its cohomology groups. Thus the de Rham cohomology is the

Cochain : Cocycle : Coboundary = k-form : Closed form : Exact form.

The whole punch line is:

Theorem 69.2.2 (de Rham’s theorem)

For any smooth manifold M , we have a natural isomorphism

H k(M ; R) ∼= H k

dR(M ).

So the theorem is that the real cohomology groups of manifolds M are actually just given
by the behavior of diﬀerential forms. Thus,

One can metaphorically think of elements of cohomology groups as G-
valued diﬀerential forms on the space.

Why does this happen? In fact, we observed already behavior of diﬀerential forms
which reﬂects holes in the space. For example, let M = S1 be a circle and consider the
angle form α (see Example 43.5.4). The from α is closed, but not exact, because it is
possible to run a full circle around S1. So the failure of α to be exact is signaling that
H1(S1) ∼= Z.

§69.3 Graded rings

Prototypical example for this section: Polynomial rings are commutative graded rings,
while Λ•(V ) is anticommutative.

In the de Rham cohomology, the diﬀerential forms can interact in another way: given

a k-form α and an (cid:96)-form β, we can consider a (k + (cid:96))-form

α ∧ β.

So we can equip the set of forms with a “product”, satisfying β ∧ α = (−1)k(cid:96)α ∧ β This
is a special case of a more general structure:

Deﬁnition 69.3.1. A graded pseudo-ring R is an abelian group

R =(cid:77)d≥0

Rd

where R0, R1, . . . , are abelian groups, with an additional associative binary operation
× : R → R. We require that if r ∈ Rd and s ∈ Re, we have rs ∈ Rd+e. Elements of an
Rd are called homogeneous elements; if r ∈ Rd and r (cid:54)= 0, we write |r| = d.

Note that we do not assume commutativity. In fact, these “rings” may not even have

an identity 1. We use other words if there are additional properties:

69 Application of cohomology

651

Deﬁnition 69.3.2. A graded ring is a graded pseudo-ring with 1. If it is commutative
we say it is a commutative graded ring.

Deﬁnition 69.3.3. A graded (pseudo-)ring R is anticommutative if for any homoge-
neous r and s we have

rs = (−1)|r||s|sr.

To summarize:

Flavors of graded rings

No Assumption
Anticommutative

Commutative

Need not have 1

graded pseudo-ring

Must have a 1

graded ring

anticommutative pseudo-ring

anticommutative ring

commutative graded ring

Example 69.3.4 (Examples of graded rings)
(a) The ring R = Z[x] is a commutative graded ring, with the dth component

being the multiples of xd.

(b) The ring R = Z[x, y, z] is a commutative graded ring, with the dth component

being the abelian group of homogeneous degree d polynomials (and 0).

(c) Let V be a vector space, and consider the abelian group

Λ•(V ) =(cid:77)d≥0

Λd(V ).

For example, e1 + (e2 ∧ e3) ∈ Λ•(V ), say. We endow Λ•(V ) with the product ∧,
which makes it into an anticommutative ring.

(d) Consider the set of diﬀerential forms of a manifold M , say

Ω•(M ) =(cid:77)d≥0

Ωd(M )

endowed with the product ∧. This is an anticommutative ring.

All four examples have a multiplicative identity.

Let’s return to the situation of Ω•(M ). Consider again the de Rham cohomology groups
dR(M ), whose elements are closed forms modulo exact forms. We claim that:

H k

Lemma 69.3.5 (Wedge product respects de Rham cohomology)

The wedge product induces a map

∧ : H k

dR(M ) × H (cid:96)

dR(M ) → H k+(cid:96)

dR (M ).

Proof. First, we recall that the operator d satisﬁes

d(α ∧ β) = (dα) ∧ β + α ∧ (dβ).

Now suppose α and β are closed forms. Then from the above, α ∧ β is clearly closed.
Also if α is closed and β = dω is exact, then α ∧ β is exact, from the identity

d(α ∧ ω) = dα ∧ ω + α ∧ dω = α ∧ β.

652

Napkin, by Evan Chen (v1.5.20190718)

Similarly if α is exact and β is closed then α ∧ β is exact. Thus it makes sense to take
the product modulo exact forms, giving the theorem above.

Therefore, we can obtain a anticommutative ring

H•dR(M ) =(cid:77)k≥0

H k

dR(M )

with ∧ as a product, and 1 ∈ Λ0(R) = R as the identity

§69.4 Cup products

Inspired by this, we want to see if we can construct a similar product on(cid:76)k≥0 H k(X; R)

for any topological space X and ring R (where R is commutative with 1 as always). The
way to do this is via the cup product.

Then this gives us a way to multiply two cochains, as follows.

Deﬁnition 69.4.1. Suppose φ ∈ Ck(X; R) and ψ ∈ C(cid:96)(X; R). Then we can deﬁne their
cup product φ (cid:94) ψ ∈ Ck+(cid:96)(X; R) to be

(φ (cid:94) ψ)([v0, . . . , vk+(cid:96)]) = φ ([v0, . . . , vk]) · ψ ([vk, . . . , vk+(cid:96)])

where the multiplication is in R.

Question 69.4.2. Assuming R has a 1, which 0-cochain is the identity for (cid:94)?

First, we prove an analogous result as before:

Lemma 69.4.3 (δ with cup products)
We have δ(φ (cid:94) ψ) = δφ (cid:94) ψ + (−1)kφ (cid:94) δψ.
Proof. Direct(cid:80) computations.

Thus, by the same routine we used for de Rham cohomology, we get an induced map

(cid:94): H k(X; R) × H (cid:96)(X; R) → H k+(cid:96)(X; R).

We then deﬁne the singular cohomology ring whose elements are ﬁnite sums in

H•(X; R) =(cid:77)k≥0

H k(X; R)

and with multiplication given by (cid:94). Thus it is a graded ring (with 1R ∈ R the identity)
and is in fact anticommutative:

Proposition 69.4.4 (Cohomology is anticommutative)
H•(X; R) is an anticommutative ring, meaning φ (cid:94) ψ = (−1)k(cid:96)ψ (cid:94) φ.

For a proof, see [Ha02, Theorem 3.11, pages 210-212]. Moreover, we have the de Rham
isomorphism

69 Application of cohomology

653

Theorem 69.4.5 (de Rham extends to ring isomorphism)

For any smooth manifold M , the isomorphism of de Rham cohomology groups to
singular cohomology groups in facts gives an isomorphism

of anticommutative rings.

H•(M ; R) ∼= H•dR(M )

Therefore, if “diﬀerential forms” are the way to visualize the elements of a cohomology

group, the wedge product is the correct way to visualize the cup product.

We now present (mostly without proof) the cohomology rings of some common spaces.

Example 69.4.6 (Cohomology of torus)
The cohomology ring H•(S1×S1; Z) of the torus is generated by elements |α| = |β| = 1
which satisfy the relations α (cid:94) α = β (cid:94) β = 0, and α (cid:94) β = −β (cid:94) α. (It also
includes an identity 1.) Thus as a Z-module it is

H•(S1 × S1; Z) ∼= Z ⊕ [αZ ⊕ βZ] ⊕ (α (cid:94) β)Z.

This gives the expected dimensions 1 + 2 + 1 = 4. It is anti-commutative.

Example 69.4.7 (Cohomology ring of Sn)
Consider Sn for n ≥ 1. The nontrivial cohomology groups are given by H 0(Sn; Z) ∼=
H n(Sn; Z) ∼= Z. So as an abelian group

H•(Sn; Z) ∼= Z ⊕ αZ

where α is the generator of H n(Sn, Z).

Now, observe that |α (cid:94) α| = 2n, but since H 2n(Sn; Z) = 0 we must have

α (cid:94) α = 0. So even more succinctly,

H•(Sn; Z) ∼= Z[α]/(α2).

Confusingly enough, this graded ring is both commutative and anti-commutative.
The reason is that α (cid:94) α = 0 = −(α (cid:94) α).

654

Napkin, by Evan Chen (v1.5.20190718)

Example 69.4.8 (Cohmology ring of real and complex projective space)
It turns out that

H•(RPn; Z/2Z) ∼= Z/2Z[α]/(αn+1)

H•(CPn; Z) ∼= Z[β]/(βn+1)

where |α| = 1 is a generator of H 1(RPn; Z/2Z) and |β| = 2 is a generator of
H 2(CPn; Z).
Confusingly enough, both graded rings are commutative and anti-commutative. In
the ﬁrst case it is because we work in Z/2Z, for which 1 = −1, so anticommutative
is actually equivalent to commutative. In the second case, all nonzero homogeneous
elements have degree 2.

§69.5 Relative cohomology pseudo-rings

For A ⊆ X, one can also deﬁne a relative cup product

H k(X, A; R) × H (cid:96)(X, A; R) → H k+(cid:96)(X, A; R).

After all, if either cochain vanishes on chains in A, then so does their cup product.
This lets us deﬁne relative cohomology pseudo-ring and reduced cohomology
pseudo-ring (by A = {∗}), say

H k(X, A; R)

H•(X, A; R) =(cid:77)k≥0
(cid:101)H•(X; R) =(cid:77)k≥0 (cid:101)H k(X; R).

These are both anticommutative pseudo-rings. Indeed, often we have (cid:101)H 0(X; R) = 0

and thus there is no identity at all.
Once again we have functoriality:

Theorem 69.5.1 (Cohomology (pseudo-)rings are functorial)

Fix a ring R (commutative with 1). Then we have functors

H•(−; R) : hTopop → GradedRings

H•(−,−; R) : hPairTopop → GradedPseudoRings.

Unfortunately, unlike with (co)homology groups, it is a nontrivial task to determine
the cup product for even nice spaces like CW complexes. So we will not do much in the
way of computation. However, there is a little progress we can make.

§69.6 Wedge sums

Our goal is to now compute (cid:101)H•(X ∨ Y ). To do this, we need to deﬁne the product of

two graded pseudo-rings:

69 Application of cohomology

655

Deﬁnition 69.6.1. Let R and S be two graded pseudo-rings. The product pseudo-
ring R × S is the graded pseudo-ring deﬁned by taking the underlying abelian group
as

R ⊕ S =(cid:77)d≥0

(Rd ⊕ Sd).

Multiplication comes from R and S, followed by declaring r · s = 0 for r ∈ R, s ∈ S.

Note that this is just graded version of the product ring deﬁned in Example 4.3.8.

Exercise 69.6.2. Show that if R and S are graded rings (meaning they have 1R and 1S),
then so is R × S.

Now, the theorem is that:

Theorem 69.6.3 (Cohomology pseudo-rings of wedge sums)

We have

as graded pseudo-rings.

(cid:101)H•(X ∧ Y ; R) ∼= (cid:101)H•(X; R) × (cid:101)H•(Y ; R)

This allows us to resolve the ﬁrst question posed at the beginning. Let X = CP2 and

Y = S2 ∨ S4. We have that

H•(CP2; Z) ∼= Z[α]/(α3).
Hence this is a graded ring generated by there elements:

 1, in dimension 0.

 α, in dimension 2.

 α2, in dimension 4.

Next, consider the reduced cohomology pseudo-ring

Thus the absolute cohomology ring H•(S2 ∨ S4 Z) is a graded ring also generated by
three elements.

(cid:101)H•(S2 ∨ S4; Z) ∼= (cid:101)H•(S2; Z) ⊕ (cid:101)H•(S4; Z).

 1, in dimension 0 (once we add back in the 0th dimension).

 a2, in dimension 2 (from H•(S2; Z)).
 a4, in dimension 4 (from H•(S4; Z)).

Each graded component is isomorphic, like we expected. However, in the former, the
product of two degree 2 generators is

α · α = α2.

In the latter, the product of two degree 2 generators is

a2 · a2 = a2

2 = 0

since a2 (cid:94) a2 = 0 ∈ H•(S2; Z).

Thus S2 ∨ S4 and CP2 are not homotopy equivalent.

656

Napkin, by Evan Chen (v1.5.20190718)

§69.7 K¨unneth formula

We now wish to tell apart the spaces S2 × S4 and CP3. In order to do this, we will need
a formula for H n(X × Y ; R) in terms of H n(X; R) and H n(Y ; R). Thus formulas are
called K¨unneth formulas. In this section we will only use a very special case, which
involves the tensor product of two graded rings.

Deﬁnition 69.7.1. Let A and B be two graded rings which are also R-modules (where
R is a commutative ring with 1). We deﬁne the tensor product A ⊗R B as follows. As
an abelian group, it is

A ⊗R B =(cid:77)d≥0(cid:32) d(cid:77)k=0

Ak ⊗R Bd−k(cid:33) .

The multiplication is given on basis elements by

(a1 ⊗ b1) (a2 ⊗ b2) = (a1a2) ⊗ (b1b2).

Of course the multiplicative identity is 1A ⊗ 1B.

Now let X and Y be topological spaces, and take the product: we have a diagram

X × Y

π

Y

π X

-
Y



X

where πX and πY are projections. As H k(−; R) is functorial, this gives induced maps

π∗X : H k(X × Y ; R) → H k(X; R)
π∗Y : H k(X × Y ; R) → H k(Y ; R)

for every k.

By using this, we can deﬁne a so-called cross product.

Deﬁnition 69.7.2. Let R be a ring, and X and Y spaces. Let πX and πY be the
projections of X × Y onto X and Y . Then the cross product is the map

H•(X; R) ⊗R H•(Y ; R) ×−→ H•(X × Y ; R)

acting on cocycles as follows: φ × ψ = π∗X (φ) (cid:94) π∗Y (ψ).

This is just the most natural way to take a k-cycle on X and an (cid:96)-cycle on Y , and

create a (k + (cid:96))-cycle on the product space X × Y .

Theorem 69.7.3 (K¨unneth formula)
Let X and Y be CW complexes such that H k(Y ; R) is a ﬁnitely generated free
R-module for every k. Then the cross product is an isomorphism of anticommutative
rings

H•(X; R) ⊗R H•(Y ; R) → H•(X × Y ; R).

69 Application of cohomology

657

In any case, this ﬁnally lets us resolve the question set out at the beginning. We saw that

Hn(CP3) ∼= Hn(S2×S4) for every n, and thus it follows that H n(CP3; Z) ∼= H n(S2×S4; Z)

too.

But now let us look at the cohomology rings. First, we have

H•(CP3; Z) ∼= Z[α]/(α3) ∼= Z ⊕ αZ ⊕ α2Z ⊕ α3Z

where |α| = 2; hence this is a graded ring generated by

 1, in degree 0.

 α, in degree 2.

 α2, in degree 4.

 α3, in degree 6.

Now let’s analyze

H•(S2 × S4; Z) ∼= Z[β]/(β2) ⊗ Z[γ]/(γ2).

It is thus generated thus by the following elements:

 1 ⊗ 1, in degree 0.
 β ⊗ 1, in degree 2.
 1 ⊗ γ, in degree 4.
 β ⊗ γ, in degree 6.

Again in each dimension we have the same abelian group. But notice that if we square
β ⊗ 1 we get

(β ⊗ 1)(β ⊗ 1) = β2 ⊗ 1 = 0.

Yet the degree 2 generator of H•(CP3; Z) does not have this property. Hence these two
graded rings are not isomorphic.

So it follows that CP3 and S2 × S4 are not homotopy equivalent.

§69.8 A few harder problems to think about

Problem 69A† (Symmetry of Betti numbers by Poincar´e duality). Let M be a smooth
oriented compact n-manifold, and let bk denote its Betti number. Prove that bk = bn−k.
Problem 69B. Show that RPn is not orientable for even n.
Problem 69C. Show that RP3 is not homotopy equivalent to RP2 ∨ S3.
Problem 69D. Show that Sm ∨ Sn is not a deformation retract of Sm × Sn for any
m, n ≥ 1.

XVIII

Algebraic Geometry I: Classical

Varieties

Part XVIII: Contents

661
70 Aﬃne varieties
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 661
70.1 Aﬃne varieties
70.2 Naming aﬃne varieties via ideals . . . . . . . . . . . . . . . . . . . . . . . . . . . 662
70.3 Radical ideals and Hilbert’s Nullstellensatz . . . . . . . . . . . . . . . . . . . . . . 663
70.4 Pictures of varieties in A1
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 664
. . . . . . . . . . . . . . . . . 665
70.5 Prime ideals correspond to irreducible aﬃne varieties
70.6 Pictures in A2 and A3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 666
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 667
70.7 Maximal ideals
70.8 Motivating schemes with non-radical ideals . . . . . . . . . . . . . . . . . . . . . . 668
. . . . . . . . . . . . . . . . . . . . . . . . 668
70.9 A few harder problems to think about

71 Aﬃne varieties as ringed spaces

669
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 669
71.1 Synopsis
71.2 The Zariski topology on An . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 669
71.3 The Zariski topology on aﬃne varieties . . . . . . . . . . . . . . . . . . . . . . . . 671
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 672
71.4 Coordinate rings
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 673
71.5 The sheaf of regular functions
. . . . . . . . . . . . . . . . . . . . . 674
71.6 Regular functions on distinguished open sets
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 675
71.7 Baby ringed spaces
. . . . . . . . . . . . . . . . . . . . . . . . 676
71.8 A few harder problems to think about

72 Projective varieties

677
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 677
72.1 Graded rings
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 678
72.2 The ambient space
72.3 Homogeneous ideals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 680
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 681
72.4 As ringed spaces
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 682
72.5 Examples of regular functions
. . . . . . . . . . . . . . . . . . . . . . . . 683
72.6 A few harder problems to think about

73 Bonus: B´ezout’s theorem

685
73.1 Non-radical ideals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 685
. . . . . . . . . . . . . . . . . . . . . . . 686
73.2 Hilbert functions of ﬁnitely many points
73.3 Hilbert polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 688
73.4 B´ezout’s theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 690
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 690
73.5 Applications
. . . . . . . . . . . . . . . . . . . . . . . . 691
73.6 A few harder problems to think about

74 Morphisms of varieties

693
. . . . . . . . . . . . . . . . . . . . . . 693
74.1 Deﬁning morphisms of baby ringed spaces
. . . . . . . . . . . . . . . . . . . . . . . . . . 694
74.2 Classifying the simplest examples
. . . . . . . . . . . . . . . . . . . . . . . . 696
74.3 Some more applications and examples
74.4 The hyperbola eﬀect . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 697
. . . . . . . . . . . . . . . . . . . . . . . . 699
74.5 A few harder problems to think about

70 Aﬃne varieties

In this chapter we introduce aﬃne varieties. We introduce them in the context of
coordinates, but over the course of the other chapters we’ll gradually move away from
this perspective to viewing varieties as “intrinsic objects”, rather than embedded in
coordinates.

For simplicity, we’ll do almost everything over the ﬁeld of complex numbers, but the

discussion generalizes to any algebraically closed ﬁeld.

§70.1 Aﬃne varieties

Prototypical example for this section: V(y − x2) is a parabola in A2.
Deﬁnition 70.1.1. Given a set of polynomials S ⊆ C[x1, . . . , xn] (not necessarily ﬁnite
or even countable), we let V(S) denote the set of points vanishing on all the polynomials
in S. Such a set is called an aﬃne variety. It lives in n-dimensional aﬃne space,
denoted An (to distinguish it from projective space later).

For example, a parabola is the zero locus of the polynomial V(y − x2). Picture:

Example 70.1.2 (Examples of aﬃne varieties)
These examples are in two-dimensional space A2, whose points are pairs (x, y).

(a) A straight line can be thought of as V(Ax + By + C).
(b) A parabola as above can be pictured as V(y − x2).
(c) A hyperbola might be the zero locus of the polynomial V(xy − 1).
(d) The two axes can be thought of as V(xy); this is the set of points such that

x = 0 or y = 0.

(e) A point (x0, y0) can be thought of as V(x − x0, y − y0).
(f) The entire space A2 can be thought of as V(0).
(g) The empty set is the zero locus of the constant polynomial 1, that is V(1).

661

yxV(y−x2)A2662

Napkin, by Evan Chen (v1.5.20190718)

§70.2 Naming aﬃne varieties via ideals

Prototypical example for this section: V(I) is a parabola, where I = (y − x2).

As you might have already noticed, a variety can be named by V(−) in multiple ways.

For example, the set of solutions to

x = 3 and y = 4

is just the point (3, 4). But this is also the set of solutions to

x = 3 and y = x + 1.

So, for example

{(3, 4)} = V(x − 3, y − 4) = V(x − 3, y − x − 1).

That’s a little annoying, because in an ideal1 world we would have one name for every
variety. Let’s see if we can achieve this.

A partial solution is to use ideals rather than small sets. That is, consider the ideal

I = (x − 3, y − 4) = {p(x, y) · (x − 3) + q(x, y) · (y − 4) | p, q ∈ C[x, y]}

and look at V(I).

Question 70.2.1. Convince yourself that V(I) = {(3, 4)}.

So rather than writing V(x − 3, y − 4) it makes sense to think about this as V (I), where
I = (x − 3, y − 4) is the ideal generated by the two polynomials x − 3 and y − 4. This is
an improvement because

Question 70.2.2. Check that (x − 3, y − x − 1) = (x − 3, y − 4).

Needless to say, this pattern holds in general.

Question 70.2.3. Let {fi} be a set of polynomials, and consider the ideal I generated by
these {fi}. Show that V({fi}) = V(I).

Thus we will only consider V(I) when I is an ideal. Of course, frequently our ideals

are generated by one or two polynomials, which leads to:

Abuse of Notation 70.2.4. Given a set of polynomials f1, . . . , fm we let V(f1, . . . , fm)
be shorthand for V ((f1, . . . , fm)). In other words we let V(f1, . . . , fm) abbreviate V(I),
where I is the ideal I = (f1, . . . , fm).

This is where the Noetherian condition really shines: it guarantees that every ideal
I ⊆ C[x1, . . . , xn] can be written in the form above with ﬁnitely many polynomials,
because it is ﬁnitely generated. (The fact that C[x1, . . . , xn] is Noetherian follows from
the Hilbert basis theorem, which is Theorem 4.8.5). This is a relief, because dealing with
inﬁnite sets of polynomials is not much fun.

1Pun not intended but left for amusement value.

70 Aﬃne varieties

663

§70.3 Radical ideals and Hilbert’s Nullstellensatz

Prototypical example for this section: (cid:112)(x2) = (x) in C[x],(cid:112)(12) = (6) in Z.

You might ask whether the name is unique now: that is, if V(I) = V(J), does it follow
that I = J? The answer is unfortunately no: the counterexample can be found in just
A1. It is

V(x) = V(x2).

In other words, the set of solutions to x = 0 is the same as the set of solutions to x2 = 0.
Well, that’s stupid. We want an operation which takes the ideal (x2) and makes it into

the ideal (x). The way to do so is using the radical of an ideal.

Deﬁnition 70.3.1. Let R be a ring. The radical of an ideal I ⊆ R, denoted √I, is

deﬁned by

√I = {r ∈ R | rm ∈ I for some integer m ≥ 1} .

If I = √I, we say the ideal I itself is radical.
For example,(cid:112)(x2) = (x). You may like to take the time to verify that √I is actually

an ideal.

Remark 70.3.2 (Number theoretic motivation) — This is actually the same as the
notion of “radical” in number theory. In Z, the radical of an ideal (n) corresponds
to just removing all the duplicate prime factors, so for example

(cid:112)(12) = (6).

In particular, if you try to take(cid:112)(6), you just get (6) back; you don’t squeeze out

This is actually true more generally, and there is a nice corresponding alternate

any new prime factors.

deﬁnition: for any ideal I, we have

√I = (cid:92)I⊆p prime

p.

Although we could prove this now, it will be proved later in Theorem 77.4.2, when
we ﬁrst need it.

Here are the immediate properties you should know.

Proposition 70.3.3 (Properties of radical)

In any ring:

 If I is an ideal, then √I is always a radical ideal.

 Prime ideals are radical.

 For I ⊆ C[x1, . . . , xn] we have V(I) = V(√I).

Proof. These are all obvious.

 If f m ∈ √I then f mn ∈ I, so f ∈ √I.

664

Napkin, by Evan Chen (v1.5.20190718)

 If f n ∈ p for a prime p, then either f ∈ p or f n−1 ∈ p, and in the latter case we

may continue by induction.

 We have f (x1, . . . , xn) = 0 if and only if f (x1, . . . , xn)m = 0 for some integer m.
The last bit makes sense: you would never refer to x = 0 as x2 = 0, and hence we would
always want to call V(x2) just V(x). With this, we obtain a theorem called Hilbert’s
Nullstellensatz.

Theorem 70.3.4 (Hilbert’s Nullstellensatz)
Given an aﬃne variety V = V(I), the set of polynomials which vanish on all points
of V is precisely √I. Thus if I and J are ideals in C[x1, . . . , xn], then

V(I) = V(J) if and only if √I = √J.

In other words

Radical ideals in C[x1, . . . , xn] correspond exactly to n-dimensional aﬃne
varieties.

The proof of Hilbert’s Nullestellensatz will be given in Problem 70C; for now it is worth
remarking that it relies essentially on the fact that C is algebraically closed. For example,
it is false in R[x], with (x2 + 1) being a maximal ideal with empty vanishing set.

§70.4 Pictures of varieties in A1
Prototypical example for this section: Finite sets of points (in fact these are the only
nontrivial examples).

Let’s ﬁrst draw some pictures. In what follows I’ll draw C as a straight line. . . sorry.
First of all, let’s look at just the complex line A1. What are the various varieties on it?

For starters, we have a single point 9 ∈ C, generated by (x − 9).

Another example is the point 4. And in fact, if we like we can get an ideal consisting

of just these two points; consider V ((x − 4)(x − 9)).

In general, in A1 you can get ﬁnitely many points {a1, . . . , an} by just taking

V ((x − a1)(x − a2) . . . (x − an)) .

On the other hand, you can’t get the set {0, 1, 2, . . .} as an aﬃne variety; the only
polynomial vanishing on all those points is the zero polynomial. In fact, you can convince
yourself that these are the only aﬃne varieties, with two exceptions:

 The entire line A1 is given by V(0), and
 The empty set is given by V(1).

V(x−9)9A1V((x−4)(x−9))49A170 Aﬃne varieties

665

Exercise 70.4.1. Show that these are the only varieties of A1. (Let V(I) be the variety
and pick a 0 (cid:54)= f ∈ I.)

As you might correctly guess, we have:

Theorem 70.4.2 (Intersections and unions of varieties)

(a) The intersection of aﬃne varieties (even inﬁnitely many) is an aﬃne variety.

(b) The union of ﬁnitely many aﬃne varieties is an aﬃne variety.

In fact we have

V(Iα) = V(cid:32)(cid:88)α

(cid:92)α

Iα(cid:33)

and

V(Ik) = V(cid:32) n(cid:92)k=1

n(cid:91)k=1

Ik(cid:33) .

You are welcome to prove this easy result yourself.

Remark 70.4.3 — Part (a) is a little misleading in that the sum I + J need not

be radical: take for example I = (y − x2) and J = (y) in C[x, y], where x ∈ √I + J
and x /∈ I + J. But in part (b) for radical ideals I and J, the intersection I ∩ J is
radical.

§70.5 Prime ideals correspond to irreducible aﬃne varieties

Prototypical example for this section: (xy) corresponds to the union of two lines in A2.
Note that most of the aﬃne varieties of A1, like {4, 9}, are just unions of the simplest
“one-point” ideals. To ease our classiﬁcation, we can restrict our attention to the case of
irreducible varieties:

Deﬁnition 70.5.1. A variety V is irreducible if it cannot be written as the union of
two proper sub-varieties V = V1 ∪ V2.
Abuse of Notation 70.5.2. Warning:
deﬁnition of variety.

in other literature, irreducible is part of the

Example 70.5.3 (Irreducible varieties of A1)
The irreducible varieties of A1 are:

 the empty set V(1),
 a single point V(x − a), and
 the entire line A1 = V(0).

Example 70.5.4 (The union of two axes)
Let’s take a non-prime ideal in C[x, y], such as I = (xy). Its vanishing set V(I) is
the union of two lines x = 0 and y = 0. So V(I) is reducible.

666

In general:

Napkin, by Evan Chen (v1.5.20190718)

Theorem 70.5.5 (Prime ⇐⇒ irreducible)
Let I be a radical ideal, and V = V(I) a nonempty variety. Then I is prime if and
only if V is irreducible.

Proof. First, assume V is irreducible; we’ll show I is prime. Let f, g ∈ C[x1, . . . , xn] so
that f g ∈ I. Then V is a subset of the union V(f ) ∪ V(g); actually, V = (V ∩ V(f )) ∪
(V ∩ V(g)). Since V is irreducible, we may assume V = V ∩ V(f ), hence f vanishes on
all of V . So f ∈ I.

The reverse direction is similar.

§70.6 Pictures in A2 and A3

Prototypical example for this section: Various curves and hypersurfaces.

With this notion, we can now draw pictures in “complex aﬃne plane”, A2. What are

the irreducible aﬃne varieties in it?

As we saw in the previous discussion, naming irreducible aﬃne varieties in A2 amounts

to naming the prime ideals of C[x, y]. Here are a few.

 The ideal (0) is prime. V(0) as usual corresponds to the entire plane.
 The ideal (x − a, y − b) is prime, since C[x, y]/(x − a, y − b) ∼= C is an integral
domain. (In fact, since C is a ﬁeld, the ideal (x − a, y − b) is maximal ). The
vanishing set of this is V(x − a, y − b) = {(a, b)} ∈ C2, so these ideals correspond
to a single point.

 Let f (x, y) be an irreducible polynomial, like y − x2. Then (f ) is a prime ideal!

Here V(I) is a “degree one curve”.

By using some polynomial algebra (again you’re welcome to check this; Euclidean

algorithm), these are in fact the only prime ideals of C[x, y]. Here’s a picture.

As usual, you can make varieties which are just unions of these irreducible ones. For
example, if you wanted the variety consisting of a parabola y = x2 plus the point (20, 15)
you would write

V(cid:0)(y − x2)(x − 20), (y − x2)(y − 15)(cid:1) .

yxV(y−x2)V(x−1,y+2)70 Aﬃne varieties

667

The picture in A3 is harder to describe. Again, you have points V(x − a, y − b, z − c)
corresponding to be zero-dimensional points (a, b, c), and two-dimensional surfaces V(f )
for each irreducible polynomial f (for example, x + y + z = 0 is a plane). But there are
more prime ideals, like V(x, y), which corresponds to the intersection of the planes x = 0
and y = 0: this is the one-dimensional z-axis. It turns out there is no reasonable way to
classify the “one-dimensional” varieties; they correspond to “irreducible curves”.

Thus, as Ravi Vakil [Va17] says: the purely algebraic question of determining the prime

ideals of C[x, y, z] has a fundamentally geometric answer.

§70.7 Maximal ideals

Prototypical example for this section: All maximal ideals are (x1 − a1, . . . , xn − an).

We begin by noting:

Proposition 70.7.1 (V(−) is inclusion reversing)
If I ⊆ J then V(I) ⊇ V(J). Thus V(−) is inclusion-reversing.

Question 70.7.2. Verify this.

Thus, bigger ideals correspond to smaller varieties. As the above pictures might have
indicated, the smallest varieties are single points. Moreover, as you might guess from the
name, the biggest ideals are the maximal ideals. As an example, all ideals of the form

(x1 − a1, . . . , xn − an)

are maximal, since the quotient

C[x1, . . . , xn]/ (x1 − a1, . . . , xn − an) ∼= C

is a ﬁeld. The question is: are all maximal ideals of this form?

The answer is in the aﬃrmative.

Theorem 70.7.3 (Weak Nullestellensatz, phrased with maximal ideals)
Every maximal ideal of C[x1, . . . , xn] is of the form (x1 − a1, . . . , xn − an).

The proof of this is surprisingly pernicious, so we won’t include it here; see [Va17, §7.4.3].
Again this uses the fact that C is algebraically closed. (For example (x2 + 1) is a maximal
ideal of R[x].) Thus:

Over C, maximal ideals correspond to single points.

Consequently, our various ideals over C correspond to various ﬂavors of aﬃne varieties:

Algebraic ﬂavor Geometric ﬂavor

radical ideal
prime ideal

maximal ideal

any ideal

aﬃne variety

irreducible variety

single point
(scheme?)

There’s one thing I haven’t talked about: what’s the last entry?

668

Napkin, by Evan Chen (v1.5.20190718)

§70.8 Motivating schemes with non-radical ideals

One of the most elementary motivations for the scheme is that we would like to use them
to count multiplicity. That is, consider the intersection

V(y − x2) ∩ V(y) ⊆ A2

This is the intersection of the parabola with the tangent x-axis, this is the green dot
below.

Unfortunately, as a variety, it is just a single point! However, we want to think of this
as a “double point”: after all, in some sense it has multiplicity 2. You can detect this
when you look at the ideals:

(y − x2) + (y) = (x2, y)
and thus, if we blithely ignore taking the radical, we get

C[x, y]/(x2, y) ∼= C[ε]/(ε2).

So the ideals in question are noticing the presence of a double point.

In order to encapsulate this, we need a more reﬁned object than a variety, which (at
the end of the day) is just a set of points; it’s not possible using topology along to encode
more information (there is only one topology on a single point!). This reﬁned object is
the scheme.

§70.9 A few harder problems to think about

Problem 70A. Show that a real aﬃne variety V ⊆ An
form V(f ).
Problem 70B (Complex varieties can’t be empty). Prove that if I is a proper ideal in
C[x1, . . . , xn] then V(I) (cid:54)= ∅.
Problem 70C. Show that Hilbert’s Nullstellensatz in n dimensions follows from the
Weak Nullstellensatz. (This solution is called the Rabinowitsch Trick.)

R can always be written in the

some actual
some actual
computation
computation
here would
here would
be good
be good

yxV(y−x2)A271 Aﬃne varieties as ringed spaces

As in the previous chapter, we are working only over aﬃne varieties in C for simplicity.

§71.1 Synopsis

Group theory was a strange creature in the early 19th century. During the 19th century,
a group was literally deﬁned as a subset of GL(n) or of Sn. Indeed, the word “group”
hadn’t been invented yet. This may sound ludicrous, but it was true – Sylow developed
his theorems without this notion. Only much later was the abstract deﬁnition of a group
given, an abstract set G which was independent of any embedding into Sn, and an object
in its own right.

We are about to make the same type of change for our aﬃne varieties. Rather than
thinking of them as an object locked into an ambient space An we are instead going to
try to make them into an object in their own right. Speciﬁcally, for us an aﬃne variety
will become a topological space equipped with a ring of functions for each of its open
sets: this is why we call it a ringed space.

The bit about the topological space is not too drastic. The key insight is the addition

of the ring of functions. For example, consider the double point from last chapter.

As a set, it is a single point, and thus it can have only one possible topology. But the

addition of the function ring will let us tell it apart from just a single point.

This construction is quite involved, so we’ll proceed as follows: we’ll deﬁne the structure
bit by bit onto our existing aﬃne varieties in An, until we have all the data of a ringed
space. In later chapters, these ideas will grow up to become the core of modern algebraic
geometry: the scheme.

§71.2 The Zariski topology on An
Prototypical example for this section: In A1, closed sets are ﬁnite collections of points. In
A2, a nonempty open set is the whole space minus some ﬁnite collection of curves/points.
We begin by endowing a topological structure on every variety V . Since our aﬃne
varieties (for now) all live in An, all we have to do is put a suitable topology on An, and
then just view V as a subspace.

However, rather than putting the standard Euclidean topology on An, we put a much

more bizarre topology.

669

yxV(y−x2)A2670

Napkin, by Evan Chen (v1.5.20190718)

Deﬁnition 71.2.1. In the Zariski topology on An, the closed sets are those of the
form

Of course, the open sets are complements of such sets.

V(I)

where

I ⊆ C[x1, . . . , xn].

Example 71.2.2 (Zariski topology on A1)
Let us determine the open sets of A1, which as usual we picture as a straight line
(ignoring the fact that C is two-dimensional).

Since C[x] is a principal ideal domain, rather than looking at V(I) for every
I ⊆ C[x], we just have to look at V(f ) for a single f . There are a few ﬂavors of
polynomials f :

 The zero polynomial 0 which vanishes everywhere: this implies that the entire

space A1 is a closed set.

 The constant polynomial 1 which vanishes nowhere. This implies that ∅ is a

closed set.

 A polynomial c(x − t1)(x − t2) . . . (x − tn) of degree n. It has n roots, and so

{t1, . . . , tn} is a closed set.

Hence the closed sets of A1 are exactly all of A1 and ﬁnite sets of points (including
∅). Consequently, the open sets of A1 are

 ∅, and

 A1 minus a ﬁnite collection (possibly empty) of points.

Thus, the picture of a “typical” open set A1 might be

It’s everything except a few marked points!

Example 71.2.3 (Zariski topology on A2)
Similarly, in A2, the interesting closed sets are going to consist of ﬁnite unions
(possibly empty) of

 Closed curves, like V(y − x2) (which is a parabola), and
 Single points, like V(x − 3, y − 4) (which is the point (3, 4)).

Of course, the entire space A2 = V(0) and the empty set ∅ = V(1) are closed sets.
Thus the nonempty open sets in A2 consist of the entire plane, minus a ﬁnite
collection of points and one-dimensional curves.

Question 71.2.4. Draw a picture (to the best of your artistic ability) of a “typical” open
set in A2.

All this is to say

A171 Aﬃne varieties as ringed spaces

671

The nonempty Zariski open sets are huge.

This is an important diﬀerence than what you’re used to in topology. To be very clear:

 In the past, if I said something like “has so-and-so property in an open neighborhood

of point p”, one thought of this as saying “is true in a small region around p”.

 In the Zariski topology, “has so-and-so property in an open neighborhood of point
p” should be thought of as saying “is true for virtually all points, other than those
on certain curves”.

Indeed, “open neighborhood” is no longer really a accurate description. Nonetheless, in
many pictures to follow, it will still be helpful to draw open neighborhoods as circles.

It remains to verify that as I’ve stated it, the closed sets actually form a topology.

That is, I need to verify brieﬂy that

 ∅ and An are both closed.

 Intersections of closed sets (even inﬁnite) are still closed.

 Finite unions of closed sets are still closed.

Well, closed sets are the same as aﬃne varieties, so we already know this!

§71.3 The Zariski topology on aﬃne varieties

Prototypical example for this section: If V = V(y − x2) is a parabola, then V minus (1, 1)
is open in V . Also, the plane minus the origin is D(x) ∪ D(y).

As we said before, by considering a variety V as a subspace of An it inherits the Zariski
topology. One should think of an open subset of V as “V minus a few Zariski-closed
sets”. For example:

Example 71.3.1 (Open set of a variety)
Let V = V(y − x2) ⊆ A2 be a parabola, and let U = V \ {(1, 1)}. We claim U is
open in V .

Indeed, ˜U = A2 \ {(1, 1)} is open in A2 (since it is the complement of the closed set
V(x − 1, y − 1)), so U = ˜U ∩ V is open in V . Note that on the other hand the set U
is not open in A2.

yxV(y−x2)672

Napkin, by Evan Chen (v1.5.20190718)

We will go ahead and introduce now a deﬁnition that will be very useful later.

Deﬁnition 71.3.2. Given V ⊆ An an aﬃne variety and f ∈ C[x1, . . . , xn], we deﬁne the
distinguished open set D(f ) to be the open set in V of points not vanishing on f :

In [Va17], Vakil suggests remembering the notation D(f ) as “doesn’t-vanish set”.

D(f ) = {p ∈ V | f (p) (cid:54)= 0} = V \ V(f ).

Example 71.3.3 (Examples of (unions of) distinguished open sets)
(a) If V = A1 then D(x) corresponds to a line minus a point.
(b) If V = V(y − x2) ⊆ A2, then D(x − 1) corresponds to the parabola minus (1, 1).
(c) If V = A2, then D(x) ∪ D(y) = A2 \ {(0, 0)} is the punctured plane. You can

show that this set is not distinguished open.

§71.4 Coordinate rings

Prototypical example for this section: If V = V(y − x2) then C[V ] = C[x, y]/(y − x2).

The next thing we do is consider the functions from V to the base ﬁeld C. We restrict
our attention to algebraic (polynomial) functions on a variety V : they should take every
point (a1, . . . , an) on V to some complex number P (a1, . . . , an) ∈ C. For example, a
valid function on a three-dimensional aﬃne variety might be (a, b, c) (cid:55)→ a; we just call
this projection “x”. Similarly we have a canonical projection y and z, and we can create
polynomials by combining them, say x2y + 2xyz.
Deﬁnition 71.4.1. The coordinate ring C[V ] of a variety V is the ring of polynomial
functions on V . (Notation explained next section.)

At ﬁrst glance, we might think this is just C[x1, . . . , xn]. But on closer inspection
we realize that on a given variety, some of these functions are the same. For example,
consider in A2 the parabola V = V(y − x2). Then the two functions

V → C
(x, y) (cid:55)→ x2
(x, y) (cid:55)→ y

are actually the same function! We have to “mod out” by the ideal I which generates V .
This leads us naturally to:

Theorem 71.4.2 (Coordinate rings correspond to ideal)
Let I be a radical ideal, and V = V(I) ⊆ An. Then
C[V ] ∼= C[x1, . . . , xn]/I.

Proof. There’s a natural surjection as above

C[x1, . . . , xn] (cid:16) C[V ]

and the kernel is I.

Thus properties of a variety V correspond to properties of the ring C[V ].

71 Aﬃne varieties as ringed spaces

673

§71.5 The sheaf of regular functions

Prototypical example for this section: Let V = A1, U = V \ {0}. Then 1/x ∈ OV (U ) is
regular on U .

Let V be an aﬃne variety and let C[V ] be its coordinate ring. We want to deﬁne a
notion of OV (U ) for any open set U : the “nice” functions on any open subset. Obviously,
any function in C[V ] will work as a function on OV (U ). However, to capture more of the
structure we want to loosen our deﬁnition of “nice” function slightly by allowing rational
functions.

The chief example is that 1/x should be a regular function on A1 \ {0}. The ﬁrst

natural guess is:

Deﬁnition 71.5.1. Let U ⊆ V be an open set of the variety V . A rational function
on U is a quotient f (x)/g(x) of two elements f and g in C[V ], where we require that
g(x) (cid:54)= 0 for x ∈ U .

However, the deﬁnition is slightly too restrictive; we have to allow for multiple repre-

sentations:
Deﬁnition 71.5.2. Let U ⊆ V be open. We say a function φ : U → C is a regular
function if for every point p ∈ U , we can ﬁnd an open set Up ⊆ U containing p and a
rational function fp/gp on Up such that

φ(x) =

fp(x)
gp(x)

∀x ∈ Up.

In particular, we require gp(x) (cid:54)= 0 on the set Up. We denote the set of all regular
functions on U by OV (U ).

Thus,

φ is regular on U if it is locally a rational function.

This deﬁnition is misleadingly complicated, and the examples should illuminate it
signiﬁcantly. Firstly, in practice, most of the time we will be able to ﬁnd a “global”
representation of a regular function as a quotient, and we will not need to fuss with the
p’s. For example:

Example 71.5.3 (Regular functions)
(a) Any function in f ∈ C[V ] is clearly regular, since we can take gp = 1, fp = f for

every p. So C[V ] ⊆ OV (U ) for any open set U .

(b) Let V = A1, U0 = V \ {0}. Then 1/x ∈ OV (U0) is regular on U .
(c) Let V = A1, U12 = V \ {1, 2}. Then
1

(x − 1)(x − 2) ∈ OV (U12)

is regular on U .

The “local” clause with p’s is still necessary, though.

674

Napkin, by Evan Chen (v1.5.20190718)

Example 71.5.4 (Requiring local representations)
Consider the variety

and the open set U = V \ V(b, d). There is a regular function on U given by

V = V(ab − cd) ⊆ A4
(a, b, c, d) (cid:55)→(cid:40)a/d d (cid:54)= 0

c/b

b (cid:54)= 0.

Clearly these are the “same function” (since ab = cd), but we cannot write “a/d”
or “c/b” to express it because we run into divide-by-zero issues. That’s why in the
deﬁnition of a regular function, we have to allow multiple representations.

In fact, we will see later on that the deﬁnition of a regular function is a special case of
a more general construction called sheaﬁﬁcation, in which “sheaves of functions which
are P ” are transformed into “sheaves of functions which are locally P ”.

§71.6 Regular functions on distinguished open sets

Prototypical example for this section: Regular functions on A1 \ {0} are P (x)/xn.

The division-by-zero, as one would expect, essentially prohibits regular functions on
the entire space V ; i.e. there are no regular functions in OV (V ) that were not already in
C[V ]. Actually, we have a more general result which computes the regular functions on
distinguished open sets:

Theorem 71.6.1 (Regular functions on distinguished open sets)
Let V ⊆ An be an aﬃne variety and D(g) a distinguished open subset of it. Then

OV (D(g)) =(cid:26) f

gn | f ∈ C[V ] and n ∈ Z(cid:27) .

In particular, OV (V ) = OV (D(1)) ∼= C[V ].

The proof of this theorem requires the Nullstellensatz, so it relies on C being algebraically
closed. In fact, a counter-example is easy to ﬁnd if we replace C by R: consider
Proof. Obviously, every function of the form f /gn works, so we want the reverse direction.
This is long, and perhaps should be omitted on a ﬁrst reading.

x2+1 .

1

Here’s the situation. Let U = D(g). We’re given a regular function φ, meaning at
every point p ∈ D(g), there is an open neighborhood Up on which φ can be expressed as
fp/gp (where fp, gp ∈ C[V ]). Then, we want to construct an f ∈ C[V ] and an integer n
such that φ = f /gn.
First, look at a particular Up and fp/gp. Shrink Up to a distinguished open set D(hp).

Then, let (cid:101)fp = fphp and(cid:101)gp = gphp. Thus we have that

is correct on D(hp) ⊆ U ⊆ X.

The upshot of using the modiﬁed fp and gp is that:

(cid:101)fp
(cid:101)gp

(cid:101)fp(cid:101)gq = (cid:101)fq(cid:101)gp

∀p, q ∈ U.

71 Aﬃne varieties as ringed spaces

675

Indeed, it is correct on D(hp)∩D(hq) by deﬁnition, and outside this set both the left-hand
side and right-hand side are zero.

Now, we know that D(g) =(cid:83)p∈U D((cid:101)gp), i.e.
V(g) = (cid:92)p∈U

So by the Nullstellensatz we know that

V((cid:101)gp).

g ∈(cid:113)((cid:101)gp : p ∈ U ) =⇒ ∃n : gn ∈ ((cid:101)gp : p ∈ U ).

where only ﬁnitely many kp are not zero. Now, we claim that

In other words, for some n and kp ∈ C[V ] we have
kp(cid:101)gp
kp(cid:101)fp
kp((cid:101)fp(cid:101)gq −(cid:101)gp(cid:101)fq) = 0.

gn =(cid:88)p
f :=(cid:88)p
f(cid:101)gq − gn(cid:101)fq =(cid:88)p

works. This just observes by noting that for any q ∈ U , we have

This means that the global regular functions are just the same as those in the coordinate
ring: you don’t gain anything new by allowing it to be locally a quotient. (The same
goes for distinguished open sets.)

Example 71.6.2 (Regular functions on distinguished open sets)
(a) As said already, taking g = 1 we recover OV (V ) ∼= C[V ] for any aﬃne variety V .
(b) Let V = A1, U0 = V \ {0}. Then
OV (U0) =(cid:26) P (x)

| P ∈ C[x], n ∈ Z(cid:27) .

xn

So more examples are 1/x and (x + 1)/x3.

Question 71.6.3. Why doesn’t our theorem on regular functions apply to Example 71.5.4?

The regular functions will become of crucial importance once we deﬁne a scheme in

the next chapter.

§71.7 Baby ringed spaces

In summary, given an aﬃne variety V we have:

 A structure of a set of points,

 A structure of a topological space V on these points, and

676

Napkin, by Evan Chen (v1.5.20190718)

 For every open set U ⊆ V , a ring OV (U ). Elements of the rings are functions

U → C.

Let us agree that:

Deﬁnition 71.7.1. A baby ringed space is a topological space X equipped with a
ring OX (U ) for every open set U . It is required that elements of the ring OX (U ) are
functions f : U → C; we call these the regular functions of X on U .

Therefore, aﬃne varieties are baby ringed spaces.

Remark 71.7.2 — This is not a standard deﬁnition. Hehe.

The reason this is called a “baby ringed space” is that in a ringed space, the rings OV (U )
can actually be any rings, but they have to satisfy a set of fairly technical conditions.
When this happens, it’s the OV that does all the work; we think of OV as a type of
functor called a sheaf.
Since we are only studying aﬃne/projective/quasi-projective varieties for the next
chapters, we will just refer to these as baby ringed spaces so that we don’t have to deal
with the entire deﬁnition. The key concept is that we want to think of these varieties
as intrinsic objects, free of any embedding. A baby ringed space is philosophically the
correct thing to do.

Anyways, aﬃne varieties are baby ringed spaces (V,OV ). In the next chapter we’ll
meet projective and quasi-projective varieties, which give more such examples of (baby)
ringed spaces. With these examples in mind, we will ﬁnally lay down the complete
deﬁnition of a ringed space, and use this to deﬁne a scheme.

§71.8 A few harder problems to think about

Problem 71A†. Show that for any n ≥ 1 the Zariski topology of An is not Hausdorﬀ.
Problem 71B†. Let V be an aﬃne variety, and consider its Zariski topology.

(a) Show that the Zariski topology is Noetherian, meaning there is no inﬁnite descending

chain Z1 (cid:41) Z2 (cid:41) Z3 (cid:41) . . . of closed subsets.

(b) Prove that a Noetherian topological space is compact. Hence varieties are topologically

compact.

Problem 71C(cid:63) (Punctured Plane). Let V = A2 and let X = A2 \ {(0, 0)} be the
punctured plane (which is an open set of V ). Compute OV (X).

72 Projective varieties

Having studied aﬃne varieties in An, we now consider CPn. We will also make it into

a baby ringed space in the same way as with An.

§72.1 Graded rings

Prototypical example for this section: C[x0, . . . , xn] is a graded ring.

We ﬁrst take the time to state what a graded ring is, just so that we have this language

to use (now and later).

Deﬁnition 72.1.1. A graded ring R is a ring with the following additional structure:
as an abelian group, it decomposes as

R =(cid:77)d≥0

Rd

where R0, R1, . . . , are abelian groups. The ring multiplication has the property that if
r ∈ Rd and s ∈ Re, we have rs ∈ Rd+e. Elements of an Rd are called homogeneous
elements; we write “d = deg r” to mean “r ∈ Rd”.
We denote by R+ the ideal R \ R0 generated by the homogeneous elements of nonzero
degree, and call it the irrelevant ideal.

Remark 72.1.2 — For experts: all our graded rings are commutative with 1.

Example 72.1.3 (Examples of graded rings)
(a) The ring C[x] is graded by degree: as abelian groups, C[x] ∼= C⊕ xC⊕ x2C⊕ . . . .
(b) More generally, the polynomial ring C[x0, . . . , xn] is graded by degree.

Abuse of Notation 72.1.4. The notation deg r is abusive in the case r = 0; note that
0 ∈ Rd for every d. So it makes sense to talk about “the” degree of r except when r = 0.

We will frequently refer to homogeneous ideals:

Deﬁnition 72.1.5. An ideal I ⊆ C[x0, . . . , xn] is homogeneous if it can be written as
I = (f1, . . . , fm) where each fi is a homogeneous polynomial.

Remark 72.1.6 — If I and J are homogeneous, then so are I + J, IJ, I ∩ J, √I.

Lemma 72.1.7 (Graded quotients are graded too)

Let I be a homogeneous ideal of a graded ring R. Then

R/I =(cid:77)d≥0

Rd/(Rd ∩ I)

realizes R/I as a graded ring.

677

678

Napkin, by Evan Chen (v1.5.20190718)

Since these assertions are just algebra, we omit their proofs here.

Example 72.1.8 (Example of a graded quotient ring)
Let R = C[x, y] and set I = (x3, y2). Let S = R/I. Then

S0 = C
S1 = Cx ⊕ Cy
S2 = Cx2 ⊕ Cxy
S3 = Cx2y
Sd = 0

∀d ≥ 4.

So in fact S = R/I is graded, and is a six-dimensional C-vector space.

§72.2 The ambient space

Prototypical example for this section: Perhaps Vpr(x2 + y2 − z2).

The set of points we choose to work with is CPn this time, which for us can be thought

of as the set of n-tuples

(x0 : x1 : ··· : xn)

not all zero, up to scaling. Equivalently, it is the set of lines through the origin in Cn+1.
Projective space is deﬁned in full in Section 57.6, and you should refer there if you aren’t
familiar with projective space.

The right way to think about it is “An plus points at inﬁnity”:

Deﬁnition 72.2.1. We deﬁne the set

Ui = {(x0 : ··· : xn) | xi (cid:54)= 0} ⊆ CPn.

These are called the standard aﬃne charts.

The name comes from:

Exercise 72.2.2 (Mandatory). Give a natural bijection from Ui to An. Thus we can think
of CPn as the aﬃne set Ui plus “points at inﬁnity”.

Remark 72.2.3 — In fact, these charts Ui make CPn with its usual topology into
a complex manifold with holomorphic transition functions.

Example 72.2.4 (Colloquially, CP1 = A1 ∪ {∞})
The space CP1 consists of pairs (s : t), which you can think of as representing the
complex number z/1. In particular U1 = {(z : 1)} is basically another copy of A1.
There is only one new point, (1 : 0).

However, like before we want to impose a Zariski topology on it. For concreteness,
let’s consider CP2 = {(x0 : x1 : x2)}. We wish to consider zero loci in CP2, just like we
did in aﬃne space, and hence obtain a notion of a projective variety.

72 Projective varieties

679

But this isn’t so easy: for example, the function “x0” is not a well-deﬁned function on
points in CP2 because (x0 : x1 : x2) = (5x0 : 5x1 : 5x2)! So we’d love to consider these
“pseudo-functions” that still have zero loci. These are just the homogeneous polynomials
f , because f is homogeneous of degree d if and only if

f (λx0, . . . , λxn) = λdf (x0, . . . , xn).

In particular, the relation “f (x0, . . . , xn) = 0” is well-deﬁned if F is homogeneous. Thus,
we can say:

Deﬁnition 72.2.5. If f is homogeneous, we can then deﬁne its vanishing locus as

Vpr(f ) = {(x0 : ··· : xn) | f (x0, . . . , xn) = 0} .

makes no sense, since the points (1 : 1 : 1) and (2015 : 2015 : 2015) are the same.

The homogeneous condition is really necessary. For example, to require “x0 − 1 = 0”
It’s trivial to verify that homogeneous polynomials do exactly what we want; hence we

can now deﬁne:

Deﬁnition 72.2.6. A projective variety in CPn is the common zero locus of an
arbitrary collection of homogeneous polynomials in n + 1 variables.

Example 72.2.7 (A conic in CP2, or a cone in C3)
Let’s try to picture the variety

Vpr(x2 + y2 − z2) ⊆ CP2

which consists of the points [x : y : z] such that x2 + y2 = z2. If we view this as
subspace of C3 (i.e. by thinking of CP2 as the set of lines through the origin), then
we get a “cone”:

If we take the standard aﬃne charts now, we obtain:
 At x = 1, we get a hyperbola V(1 + y2 − z2).
 At y = 1, we get a hyperbola V(1 + x2 − z2).
 At z = 1, we get a circle V(x2 + y2 − 1).

That said, over C a hyperbola and circle are the same thing; I’m cheating a little by
drawing C as one-dimensional, just like last chapter.

680

Napkin, by Evan Chen (v1.5.20190718)

Question 72.2.8. Draw the intersection of the cone above with the z = 1 plane, and check
that you do in fact get a circle. (This geometric picture will be crucial later.)

§72.3 Homogeneous ideals

Now, the next thing we want to do is deﬁne Vpr(I) for an ideal I. Of course, we again
run into an issue with things like x0 − 1 not making sense.
The way out of this is to use only homogeneous ideals.

Deﬁnition 72.3.1. If I is a homogeneous ideal, we deﬁne

Vpr(I) = {x | f (x) = 0 ∀f ∈ I}.

Exercise 72.3.2. Show that the notion “f (x) = 0 ∀f ∈ I” is well-deﬁned for a homogeneous
ideal I.

So, we would hope for a Nullstellensatz-like theorem which bijects the homogeneous
radical ideals to projective ideals. Unfortunately:

Example 72.3.3 (Irrelevant ideal)
To crush some dreams and hopes, consider the ideal

I = (x0, x1, . . . , xn).

This is called the irrelevant ideal; it is a homogeneous radical yet Vpr(I) = ∅.

However, other than the irrelevant ideal:

Theorem 72.3.4 (Homogeneous Nullstellensatz)

Let I and J be homogeneous ideals.

(a) If Vpr(I) = Vpr(J) (cid:54)= ∅ then √I = √J.
(b) If Vpr(I) = ∅, then either I = (1) or √I = (x0, x1, . . . , xn).

Thus there is a natural bijection between:

 projective varieties in CPn, and

 homogeneous radical ideals of C[x1, . . . , xn] except for the irrelevant ideal.

Proof. For the ﬁrst part, let V = Vpr(I) and W = Vpr(J) be projective varieties in CPn.
We can consider them as aﬃne varieties in An+1 by using the interpretation of CPn as
lines through the origin in Cn.

Algebraically, this is done by taking the homogeneous ideals I, J ⊆ C[x0, . . . , xn] and
using the same ideals to cut out aﬃne varieties Vaﬀ = V(I) and Waﬀ = V(J) in An+1.
For example, the cone x2 + y2 − z2 = 0 is a conic (a one-dimensional curve) in CP2, but
can also be thought of as a cone (which is a two-dimensional surface) in A3.

Then for (a), we have Vaﬀ = Waﬀ, so √I = √J.
For (b), either Vaﬀ is empty or it is just the origin of An+1, so the Nullstellensatz

implies either I = (1) or √I = (x0, . . . , xn) as desired.

72 Projective varieties

681

Projective analogues of Theorem 70.4.2 (on intersections and unions of varieties) hold
verbatim for projective varieties as well.

§72.4 As ringed spaces

Prototypical example for this section: The regular functions on CP1 minus a point are
exactly those of the form P (s/t).

Now, let us make every projective variety V into a baby ringed space. We already

have the set of points, a subset of CPn.

The topology is deﬁned as follows.

Deﬁnition 72.4.1. We endow CPn with the Zariski topology by declaring the sets of
the form Vpr(I), where I is a homogeneous ideal, to be the closed sets.
distinguished open sets D(f ) are V \ Vpr(f ).

Every projective variety V then inherits the Zariski topology from its parent CPn. The

Thus every projective variety V is now a topological space. It remains to endow it
with a sheaf of regular functions OV . To do this we have to be a little careful. In the
aﬃne case we had a nice little ring of functions, the coordinate ring C[x0, . . . , xn]/I, that
we could use to provide the numerator and denominators. So, it seems natural to then
deﬁne:

Deﬁnition 72.4.2. The homogeneous coordinate ring of a projective variety V =
Vpr(I) ⊆ CPn, where I is homogeneous radical, is deﬁned as the ring

C[V ] = C[x0, . . . , xn]/I.

However, when we deﬁne a rational function we must impose a new requirement that

the numerator and denominator are the same degree.

Deﬁnition 72.4.3. Let U ⊆ V be an open set of a projective variety V . A rational
function φ on a projective variety V is a quotient f /g, where f, g ∈ C[V ], and f and
g are homogeneous of the same degree, and Vpr(g) ∩ U = ∅. In this way we obtain a
function φ : U → C.

Example 72.4.4 (Examples of rational functions)
Let V = CP1 have coordinates (s : t).

(a) If U = V , then constant functions c/1 are the only rational functions on U .

(b) Now let U1 = V \ {(1 : 0)}. Then, an example of a regular function is

+ 9.

s2 + 9t2

t2

t(cid:17)2
=(cid:16) s

If we think of U1 as C (i.e. CP1 minus an inﬁnity point, hence like A1) then
really this is just the function x2 + 9.

Then we can repeat the same deﬁnition as before:

682

Napkin, by Evan Chen (v1.5.20190718)

Deﬁnition 72.4.5. Let U ⊆ V be an open set of a projective variety V . We say a
function φ : U → C is a regular function if for every point p, we can ﬁnd an open set
Up containing p and a rational function fp/gp on Up such that

φ(x) =

fp(x)
gp(x)

∀x ∈ Up.

In particular, we require Up ∩ Vpr(gp) = ∅. We denote the set of all regular functions on
U by OV (U ).

Of course, the rational functions from the previous example are examples of regular
functions as well. This completes the deﬁnition of a projective variety V as a baby ringed
space.

§72.5 Examples of regular functions

Naturally, I ought to tell you what the regular functions on distinguished open sets are;
this is an analog to Theorem 71.6.1 from last time.

Theorem 72.5.1 (Regular functions on distinguished open sets for projective varieties)
Let V be a projective variety, and let g ∈ C[V ] be homogeneous of positive degree
(thus g is nonconstant). Then

OV (D(g)) =(cid:26) f

gr | f ∈ C[V ] homogeneous of degree r deg g(cid:27) .

What about the case g = 1? A similar result holds, but we need the assumption that V
is irreducible.

Deﬁnition 72.5.2. A projective variety V is irreducible if it can’t be written as the
union of two proper (projective) sub-varieties.

Theorem 72.5.3 (Only constant regular functions on projective space)
Let V be an irreducible projective variety. Then the only regular functions on V are
constant, thus we have

This relies on the fact that C is algebraically closed.

OV (V ) ∼= C.

Proofs of these are omitted for now.

Example 72.5.4 (Irreducibility is needed above)
The reason we need V irreducible is otherwise we could, for example, take V to be

the union of two points; in this case OV (V ) ∼= C⊕2.

Remark 72.5.5 — It might seem strange that OV (D(g)) behaves so diﬀerently
when g = 1. One vague explanation is that in a projective variety, a distinguished
open D(g) looks much like an aﬃne variety if deg g > 0. For example, in CP1 we
have CP1 \ {0} ∼= A1 (where ∼= is used in a sense that I haven’t made precise). Thus

72 Projective varieties

683

the claim becomes related to the corresponding aﬃne result. But if deg g = 0 and
g (cid:54)= 0, then D(g) is the entire projective variety, which does not look aﬃne, and
thus the analogy breaks down.

Example 72.5.6 (Regular functions on CP1)
Let V = CP1, with coordinates (s : t).

(a) By Theorem 72.5.1, if U1 is the standard aﬃne chart omitting the point (1 : 0),

we have OV (U1) =(cid:110) f

tn | deg f = n(cid:111). One can write this as
OV (U1) ∼= {P (s/t) | P ∈ C[x]} ∼= OA1(A1).

This conforms with our knowledge that U1 “looks very much like A1”.

(b) As V is irreducible, OV (V ) = C: there are no nonconstant functions on CP1.

Example 72.5.7 (Regular functions on CP2)

Let CP2 have coordinates (x : y : z) and let U0 = (cid:8)(x : y : 1) ∈ CP2(cid:9) be the

distinguished open set D(z). Then in the same vein,

OCP2(U0) =(cid:26) P (x, y)

zn

| deg P = n(cid:27) ∼= {P (x/z, y/z) | P ∈ C[x, y]} .

§72.6 A few harder problems to think about

Problems:
Problems:

73 Bonus: B´ezout’s theorem

In this chapter we discuss B´ezout’s theorem. It makes precise the idea that two degree
d and e curves in CP2 should intersect at “exactly” de points. (We work in projective
space so e.g. any two lines intersect.)

§73.1 Non-radical ideals

Prototypical example for this section: Tangent to the parabola.

We need to account for multiplicities. So we will whenever possible work with homoge-
neous ideals I, rather than varieties V , because we want to allow the possibility that I is
not radical. Let’s see how we might do so.

For a ﬁrst example, suppose we intersect y = x2 with the line y = 1; or more accurately,
in projective coordinates of CP2, the parabola zy = x2 and y = z. The intersection of
the ideals is

(zy − x2, y − z) = (x2 − z2, y − z) ⊆ C[x, y, z].

So this corresponds to having two points; this gives two intersection points: (1 : 1 : 1)
and (−1 : 1 : 1). Here is a picture of the two varieties in the aﬃne z = 1 chart:

That’s ﬁne, but now suppose we intersect zy = x2 with the line x = 0 instead. Then we
instead get a “double point”:

The corresponding ideal is this time

(zy − x2, y) = (x2, y) ⊆ C[x, y, z].

This ideal is not radical, and when we take (cid:112)(x2, y) = (x, y) we get the ideal which

corresponds to a single projective point (0 : 0 : 1) of CP2. This is why we work with
ideals rather than varieties: we need to tell the diﬀerence between (x2, y) and (x, y).

685

V(y−z)V(zy−x2)CP2V(y)V(zy−x2)CP2686

Napkin, by Evan Chen (v1.5.20190718)

§73.2 Hilbert functions of ﬁnitely many points

Prototypical example for this section: The Hilbert function attached to the double point
(x2, y) is eventually the constant 2.

Deﬁnition 73.2.1. Given a nonempty projective variety V , there is a unique radical
ideal I such that V = Vpr(I). In this chapter we denote it by Irad(V ). For an empty
variety we set Irad(∅) = (1), rather than choosing the irrelevant ideal.
Deﬁnition 73.2.2. Let I ⊆ C[x0, . . . , xn] be homogeneous. We deﬁne the Hilbert
function of I, denoted hI : Z≥0 → Z≥0 by

hI (d) = dimC (C[x0, . . . , xn]/I)d

i.e. hI (d) is the dimension of the dth graded part of C[x0, . . . , xn]/I.
Deﬁnition 73.2.3. If V is a projective variety, we set hV = hIrad(V ), where I is the
radical ideal satisfying V = Vpr(I). If V = ∅, we choose I = (1).

Example 73.2.4 (Examples of Hilbert functions in zero dimensions)
For concreteness, let us use CP2.

(a) If V is the single point (0 : 0 : 1), with ideal Irad(V ) = (x, y), then

C[x, y, z]/(x, y) ∼= C[z] ∼= C ⊕ zC ⊕ z2C ⊕ z3C . . .

which has dimension 1 in all degrees. Consequently, we have

hI (d) ≡ 1.

(b) Now suppose we use the “double point” ideal I = (x2, y). This time, we have

C[x, y, z]/(x2, y) ∼= C[z] ⊕ xC[z]

∼= C ⊕ (xC ⊕ zC) ⊕ (xzC ⊕ z2C) ⊕ (xz2C ⊕ z3C) ⊕ . . . .

From this we deduce that

hI (d) =(cid:40)2 d = 1, 2, 3, . . .

1 d = 0.

(c) Let’s now take the variety V = {(1 : 1 : 1), (−1 : 1 : 1)} consisting of two points,

with Irad(V ) = (x2 − z2, y − z). Then

C[x, y, z]/(x2 − z2, y − z) ∼= C[x, z]/(x2 − z2)

∼= C[z] ⊕ xC[z].

So this example has the same Hilbert function as the previous one.

Abuse of Notation 73.2.5. I’m abusing the isomorphism symbol C[z] ∼= C⊕ zC⊕ z2C
and similarly in other examples. This is an isomorphism only on the level of C-vector
spaces. However, in computing Hilbert functions of other examples I will continue using
this abuse of notation.

73 Bonus: B´ezout’s theorem

687

Example 73.2.6 (Hilbert functions for empty varieties)
Suppose I (cid:40) C[x0, . . . , xn] is an ideal, possibly not radical but such that

Vpr(I) = ∅

hence √I = (x0, . . . , xn) is the irrelevant ideal. Thus there are integers di for i =
0, . . . , n such that xdi
i ∈ I for every i; consequently, hI (d) = 0 for any d > d0 +···+dn.
We summarize this by saying that

hI (d) = 0 for all d (cid:29) 0.

Here the notation d (cid:29) 0 means “all suﬃciently large d”.
From these examples we see that if I is an ideal, then the Hilbert function appears
to eventually be constant, with the desired constant equal to the size of Vpr(I), “with
multiplicity” in the case that I is not radical.
Let’s prove this. Before proceeding we brieﬂy remind the reader of short exact

sequences: a sequence of maps of 0 → V (cid:44)→ W (cid:16) X → 0 is one such that the
im(V (cid:44)→ W ) = ker(W (cid:16) X) (and of course the maps V (cid:44)→ W and W (cid:16) X are injective
and surjective). If V , W , X are ﬁnite-dimensional vector spaces over C this implies that
dim W = dim V + dim X.

Proposition 73.2.7 (Hilbert functions of I ∩ J and I + J)
Let I and J be homogeneous ideals in C[x0, . . . , xn]. Then

hI∩J + hI+J = hI + hJ .

Proof. Consider any d ≥ 0. Let S = C[x0, . . . , xn] for brevity. Then
- [S/(I ∩ J)]d ⊂- [S/I]d ⊕ [S/J]d -- [S/(I + J)]d

0

- 0

f

- (f, f )

(f, g)

- f − g

is a short exact sequence of vector spaces. Therefore, for every d ≥ 0 we have that

dim [S/I]d ⊕ [S/J]d = dim [S/(I ∩ J)]d + dim [S/(I + J)]d

which gives the conclusion.

Example 73.2.8 (Hilbert function of two points in CP1)
In CP1 with coordinate ring C[s, t], consider I = (s) the ideal corresponding to
the point (0 : 1) and J = (t) the ideal corresponding to the point (1 : 0). Then
I ∩ J = (st) is the ideal corresponding to the disjoint union of these two points, while
I + J = (s, t) is the irrelevant ideal. Consequently hI+J (d) = 0 for d (cid:29) 0. Therefore,
we get

hI∩J (d) = hI (d) + hJ (d) for d (cid:29) 0

so the Hilbert function of a two-point projective variety is the constant 2 for d (cid:29) 0.

688

Napkin, by Evan Chen (v1.5.20190718)

This example illustrates the content of the main result:

Theorem 73.2.9 (Hilbert functions of zero-dimensional varieties)
Let V be a projective variety consisting of m points (where m ≥ 0 is an integer).
Then

hV (d) = m for d (cid:29) 0.

Proof. We already did m = 0, so assume m ≥ 1. Let I = Irad(V ) and for k = 1, . . . , m
let Ik = Irad(kth point of V ).

Exercise 73.2.10. Show that hIk (d) = 1 for every d. (Modify Example 73.2.4(a).)

Hence we can proceed by induction on m ≥ 2, with the base case m = 1 already done
above. For the inductive step, we use the projective analogues of Theorem 70.4.2. We
know that hI1∩···∩Im−1(d) = m − 1 for d (cid:29) 0 (this is the ﬁrst m − 1 points; note that
I1 ∩ ··· ∩ Im−1 is radical). To add in the mth point we note that
hI1∩···∩Im(d) = hI1∩...Im−1(d) + hIm(d) − hJ (d)

where J = (I1∩···∩ Im−1) + Im. The ideal J may not be radical, but satisﬁes Vpr(J) = ∅
by an earlier example, hence hJ = 0 for d (cid:29) 0. This completes the proof.

In exactly the same way we can prove that:

Corollary 73.2.11 (hI eventually constant when dimVpr(I) = 0)
Let I be an ideal, not necessarily radical, such that Vpr(I) consists of ﬁnitely many
points. Then the Hilbert hI is eventually constant.

Proof. Induction on the number of points, m ≥ 1. The base case m = 1 was essentially
done in Example 73.2.4(b) and Exercise 73.2.10. The inductive step is literally the same
as in the proof above, except no fuss about radical ideals.

§73.3 Hilbert polynomials

So far we have only talked about Hilbert functions of zero-dimensional varieties, and
showed that they are eventually constant. Let’s look at some more examples.

Example 73.3.1 (Hilbert function of CPn)
The Hilbert function of CPn is

hCPn(d) =(cid:18)d + n
n (cid:19) =

1
n!

(d + n)(d + n − 1) . . . (d + 1)

by a “balls and urns” argument. This is a polynomial of degree n.

73 Bonus: B´ezout’s theorem

689

Example 73.3.2 (Hilbert function of the parabola)
Consider the parabola zy − x2 in CP2 with coordinates C[x, y, z]. Then

C[x, y, z]/(zy − x2) ∼= C[y, z] ⊕ xC[y, z].

A combinatorial computation gives that

h(zy−x2)(0) = 1
h(zy−x2)(1) = 3
h(zy−x2)(2) = 5

Basis 1

Basis x, y, z
Basis xy, xz, y2, yz, z2.

We thus in fact see that h(zy−x2)(d) = 2d − 1.

In fact, this behavior of “eventually polynomial” always works.

Theorem 73.3.3 (Hilbert polynomial)
Let I ⊆ C[x0, . . . , xn] be a homogeneous ideal, not necessarily radical. Then
(a) There exists a polynomial χI such that hI (d) = χI (d) for all d (cid:29) 0.
(b) deg χI = dimVpr(I) (if Vpr(I) = ∅ then χI = 0).
(c) The polynomial m! · χI has integer coeﬃcients.

Proof. The base case was addressed in the previous section.

For the inductive step, consider Vpr(I) with dimension m. Consider a hyperplane H
such that no irreducible component of Vpr(I) is contained inside H (we quote this fact
without proof, as it is geometrically obvious, but the last time I tried to write the proof I
messed up). For simplicity, assume WLOG that H = Vpr(x0).

Let S = C[x0, . . . , xn] again. Now, consider the short exact sequence

0

- [S/I]d−1 ⊂×x0- [S/I]d -- [S/(I + (x0))]d

- 0

f

- f x0

f

-

f.

(The injectivity of the ﬁrst map follows from the assumption about irreducible components
of Vpr(I).) Now exactness implies that

hI (d) − hI (d − 1) = hI+(x0)(d).

The last term geometrically corresponds to Vpr(I) ∩ H; it has dimension m − 1, so by
the inductive hypothesis we know that

hI (d) − hI (d − 1) =

c0dm−1 + c1dm−2 + ··· + cm−1

(m − 1)!

d (cid:29) 0

for some integers c0, . . . , cm−1. Then we are done by the theory of ﬁnite diﬀerences
of polynomials.

690

Napkin, by Evan Chen (v1.5.20190718)

§73.4 B´ezout’s theorem

Deﬁnition 73.4.1. We call χI the Hilbert polynomial of I. If χI is nonzero, we call
the leading coeﬃcient of m!χI the degree of I, which is an integer, denoted deg I.

Of course for projective varieties V we let hV = hIrad(V ).

Example 73.4.2 (Examples of degrees)
(a) If V is a ﬁnite set of n ≥ 1 points, it has degree n.
(b) If I corresponds to a double point, it has degree 2.

(c) CPn has degree 1.

(d) The parabola has degree 2.

Now, you might guess that if f is a homogeneous quadratic polynomial then the degree
of the principal ideal (f ) is 2, and so on. (Thus for example we expect a circle to have
degree 2.) This is true:

Theorem 73.4.3 (B´ezout’s theorem)
Let I be a homogeneous ideal of C[x0, . . . , xn], such that dimVpr(I) ≥ 1. Let
f ∈ C[x0, . . . , xn] be a homogeneous polynomial of degree k which does not vanish
on any irreducible component of Vpr(I). Then

deg (I + (f )) = k deg I.

Proof. Let S = C[x0, . . . , xn] again. This time the exact sequence is

0

- [S/I]d−k ⊂×f- [S/I]d -- [S/(I + (f ))]d

- 0

We leave this olympiad-esque exercise as Problem 73A.

§73.5 Applications

First, we show that the notion of degree is what we expect.

Corollary 73.5.1 (Hypersurfaces: the degree deserves its name)
Let V be a hypersurface, i.e. Irad(V ) = (f ) for f a homogeneous polynomial of
degree k. Then deg V = k.

Proof. Recall deg(0) = deg CPn = 1. Take I = (0) in B´ezout’s theorem.

The common special case in CP2 is:

Corollary 73.5.2 (B´ezout’s theorem for curves)
For any two curves X and Y in CP2 without a common irreducible component,

|X ∩ Y | ≤ deg X · deg Y.

73 Bonus: B´ezout’s theorem

691

Now, we use this to prove Pascal’s theorem.

Theorem 73.5.3 (Pascal’s theorem)
Let A, B, C, D, E, F be six distinct points which lie on a conic C in CP2. Then
the points AB ∩ DE, BC ∩ EF , CD ∩ F A are collinear.

Proof. Let X be the variety equal to the union of the three lines AB, CD, EF , hence
X = Vpr(f ) for some cubic polynomial f (which is the product of three linear ones).
Similarly, let Y = Vpr(g) be the variety equal to the union of the three lines BC, DE,
F A.

Now let P be an arbitrary point on the conic on C , distinct from the six points A, B,

C, D, E, F . Consider the projective variety

V = Vpr(αf + βg)

where the constants α and β are chosen such that P ∈ V .

Question 73.5.4. Show that V also contains the six points A, B, C, D, E, F as well as
the three points AB ∩ DE, BC ∩ EF , CD ∩ F A regardless of which α and β are chosen.
Now, note that |V ∩ C| ≥ 7. But dim V = 3 and dim C = 2. This contradicts B´ezout’s
theorem unless V and C share an irreducible component. This can only happen if V is
the union of a line and conic, for degree reasons; i.e. we must have that

V = C ∪ line.

Finally note that the three intersection points AB ∩ DE, BC ∩ EF and CD ∩ F A do
not lie on C , so they must lie on this line.

§73.6 A few harder problems to think about

Problem 73A. Complete the proof of B´ezout’s theorem from before.

Problem 73B (January TST for IMO 2016). Let ABC be an acute scalene triangle
and let P be a point in its interior. Let A1, B1, C1 be projections of P onto triangle
sides BC, CA, AB, respectively. Find the locus of points P such that AA1, BB1, CC1
are concurrent and ∠P AB + ∠P BC + ∠P CA = 90◦.

ABCDEF74 Morphisms of varieties

In preparation for our work with schemes, we will ﬁnish this part by talking about
morphisms between aﬃne and projective varieties, given that we have taken the time to
deﬁne them.

Idea: we know both aﬃne and projective varieties are special cases of baby ringed
spaces, so in fact we will just deﬁne a morphism between any two baby ringed spaces.

§74.1 Deﬁning morphisms of baby ringed spaces

Prototypical example for this section: See next section.

morphism between them.

Let (X,OX ) and (Y,OY ) be baby ringed spaces, and think about how to deﬁne a
The guiding principle in algebra is that we want morphisms to be functions on
underlying structure, but also respect the enriched additional data on top. To give some
examples from the very beginning of time:

Example 74.1.1 (How to deﬁne a morphism)

 Consider groups. A group G has an underlying set (of elements), which we
then enrich with a multiplication operation. So a homomorphism is a map of
the underlying sets, plus it has to respect the group multiplication.

 Consider R-modules. Each R-module has an underlying abelian group, which
we then enrich with scalar multiplication. So we require that a linear map
respects the scalar multiplication as well, in addition to being a homomorphism
of abelian groups.

 Consider topological spaces. A space X has an underlying set (of points),
which we then enrich with a topology of open sets. So we consider maps of the
set of points which respect the topology (pre-images of open sets are open).

This time, the ringed spaces (X,OX ) have an underlying topological space, which we
have enriched with a structure sheaf. So, we want a continuous map f : X → Y of these
topological spaces, which we then need to respect the sheaf of regular functions.
How might we do this? Well, if we let ψ : Y → C be a regular function, then there’s a
natural way to get then composition gives us a way to write a map X → Y → C. We
then want to require that this is also a regular function.
More generally, we can take any regular function on Y and obtain some function on

X, which we call a pullback. We then require that all the pullbacks are regular on X.

Deﬁnition 74.1.2. Let (X,OX ) and (Y,OY ) be baby ringed spaces. Given a map
f : X → Y and a regular function φ ∈ OY (U ), we deﬁne the pullback of φ, denoted f (cid:93)φ,
to be the composed function

f pre(U )

f - U

φ - C.

The use of the word “pullback” is the same as in our study of diﬀerential forms.

693

694

Napkin, by Evan Chen (v1.5.20190718)

Deﬁnition 74.1.3. Let (X,OX ) and (Y,OY ) be baby ringed spaces. A continuous map
of topological spaces f : X → Y is a morphism if every pullback of a regular function
on Y is a regular function on X.
Two baby ringed spaces are isomorphic if there are mutually inverse morphisms

between them, which we then call isomorphisms.

In particular, the pullback gives us a (reversed) ring homomorphism

f (cid:93) : OY (U ) → OX (f pre(U ))

for every U ; thus our morphisms package a lot of information. Here’s a picture of a
morphism f , and the pullback of φ : U → C (where U ⊆ Y ).

Example 74.1.4 (The pullback of
The map

1

y−25 under t (cid:55)→ t2)

f : X = A1 → Y = A1 by t (cid:55)→ t2

is a morphism of varieties. For example, consider the regular function ϕ = 1
y−25 on
the open set Y \ {25} ⊆ Y . The f -inverse image is X \ {±5}. Thus the pullback is

f (cid:93)ϕ : X \ {±5} → Y \ {25}
x2 − 25

by x (cid:55)→

1

which is regular on X \ {±5}.

§74.2 Classifying the simplest examples

Prototypical example for this section: Theorem 74.2.2; they’re just polynomials.

On a philosophical point, we like the earlier deﬁnition because it adheres to our
philosophy of treating our varieties as intrinsic objects, rather than embedded ones.
However, it is somewhat of a nuisance to actually verify it.

Xfpre(U)YUfφ∈OY(U)Cf♯φ∈OX(fpre(U))74 Morphisms of varieties

So in this section, we will

 classify all the morphisms from Am → An, and
 classify all the morphisms from CPm → CPn.

695

It what follows I will wave my hands a lot in claiming that something is a morphism, since
doing so is mostly detail checking. The theorems which follow will give us alternative
deﬁnitions of morphism which are more coordinate-based and easier to use for actual
computations.

§74.2.i Aﬃne classiﬁcation

Earlier we saw how t (cid:55)→ t2 gives us a map. More generally, given any polynomial P (t),
the map t (cid:55)→ P (t) will work. And in fact, that’s all:

Exercise 74.2.1. Let X = A1, Y = A1. By considering id ∈ OY (Y ), show that no other
regular functions exist.

In fact, let’s generalize the previous exercise:

Theorem 74.2.2 (Regular maps of aﬃne varieties are globally polynomials)
Let X ⊆ Am and Y ⊆ An be aﬃne varieties. Every morphism f : X → Y of varieties
is given by

x = (x1, . . . , xm)

f

(cid:55)−→ (P1(x), . . . , Pn(x))

where P1, . . . , Pn are polynomials.

Proof. It’s not too hard to see that all such functions work, so let’s go the other way. Let
f : X → Y be a morphism.

by the projection (y1, . . . , yn) (cid:55)→ y1. Thus we need f ◦ π1 to be regular on X.

First, remark that f pre(Y ) = X. Now consider the regular function π1 ∈ OY (Y ), given
But for aﬃne varieties OX (X) is just the coordinate ring C[X] and so we know there

is a polynomial P1 such that f ◦ π1 = P1. Similarly for the other coordinates.

§74.2.ii Projective classiﬁcation

Unfortunately, the situation is a little weirder in the projective setting. If X ⊆ CPm and
Y ⊆ CPn are projective varieties, then every function

x = (x0 : x1 : ··· : xm) (cid:55)→ (P0(x) : P1(x) : ··· : Pn(x))

is a valid morphism, provided the Pi are homogeneous of the same degree and don’t all
vanish simultaneously. However if we try to repeat the proof for aﬃne varieties we run
into an issue: there is no π1 morphism. (Would we send (1 : 1) = (2 : 2) to 1 or 2?)

And unfortunately, there is no way to repair this. Counterexample:

696

Napkin, by Evan Chen (v1.5.20190718)

Example 74.2.3 (Projective map which is not globally polynomial)
Let V = Vpr(xy − z2) ⊆ CP2. Then the map

V → CP1 by (x : y : z) (cid:55)→(cid:40)(x : z) x (cid:54)= 0

(z : y)

y (cid:54)= 0

turns out to be a morphism of projective varieties. This is well deﬁned just because
(x : z) = (z : y) if x, y (cid:54)= 0; this should feel reminiscent of the deﬁnition of regular
function.

The good news is that “local” issues are the only limiting factor.

Theorem 74.2.4 (Regular maps of projective varieties are locally polynomials)
Let X ⊆ CPm and Y ⊆ CPn be projective varieties and let f : X → Y be a morphism.
Then at every point p ∈ X there exists an open neighborhood Up (cid:51) p and polynomials
P0, P1, . . . , Pn (which depend on U ) so that

f (x) = (P0(x) : P1(x) : ··· : Pn(x))

∀x = (x0 : ··· : xn) ∈ Up.

Of course the polynomials Pi must be homogeneous of the same degree and cannot
vanish simultaneously on any point of Up.

Example 74.2.5 (Example of an isomorphism)
In fact, the map V = Vpr(xy − z2) → CP1 is an isomorphism. The inverse map
CP1 → V is given by

(s : t) (cid:55)→ (s2 : st : t2).

Thus actually V ∼= CP1.

§74.3 Some more applications and examples

Prototypical example for this section: A1 (cid:44)→ CP1 is a good one.

The previous section complete settles aﬃne varieties to aﬃne varieties, and projective
varieties to projective varieties. However, the deﬁnition we gave at the start of the chapter
works for any baby ringed spaces, and therefore there is still a lot of room to explore.

For example, we can have aﬃne spaces talk to projective ones. Why not? The
power of our pullback-based deﬁnition is that you enable any baby ringed spaces to
communicate, even if they live in diﬀerent places.

Example 74.3.1 (Embedding A1 (cid:44)→ CP1)
Consider a morphism

f : A1 (cid:44)→ CP1 by t (cid:55)→ (t : 1).

This is also a morphism of varieties. (Can you see what the pullbacks look like?)
This reﬂects the fact that CP1 is “A1 plus a point at inﬁnity”.

74 Morphisms of varieties

697

Here is another way you can generate more baby ringed spaces. Given any projective
variety, you can take an open subset of it, and that will itself be a baby ringed space.
We give this a name:

Deﬁnition 74.3.2. A quasi-projective variety is an open set X of a projective variety
V . It is a baby ringed space (X,OX ) too, because for any open set U ⊆ X we simply
deﬁne OX (U ) = OV (U ).

We chose to take open subsets of projective varieties because this will subsume the

aﬃne ones, for example:

Example 74.3.3 (The parabola is quasi-projective)
Consider the parabola V = V(y − x2) ⊂ A2. We take the projective variety W =
Vpr(zy−x2) and look at the standard aﬃne chart D(z). Then there is an isomorphism

V → D(z) ⊆ W
(x, y) (cid:55)→ (x : y : 1)

(x/z, y/z) ←(cid:91) (x : y : z).

Consequently, V is (isomorphic to) an open subset of W , thus we regard it as
quasi-projective.

Missing

ﬁgure

parabola

In general this proof can be readily adapted:

Proposition 74.3.4 (Aﬃne ⊆ quasi-projective)
Every aﬃne variety is isomorphic to a quasi-projective one (i.e. every aﬃne variety
is an open subset of a projective variety).

So quasi-projective varieties generalize both types of varieties we have seen.

§74.4 The hyperbola eﬀect

Prototypical example for this section: A1 \ {0} is even aﬃne

So here is a natural question: are there quasi-projective varieties which are neither
aﬃne nor projective? The answer is yes, but for the sake of narrative I’m going to play
dumb and ﬁnd a non-example, with the actual example being given in the problems.

Our ﬁrst guess might be to take the simplest projective variety, say CP1, and delete a
point (to get an open set). This is quasi-projective, but it’s isomorphic to A1. So instead
we start with the simplest aﬃne variety, say A1, and try to delete a point.

Surprisingly, this doesn’t work.

698

Napkin, by Evan Chen (v1.5.20190718)

Example 74.4.1 (Crucial example: punctured line is isomorphic to hyperbola)
Let X = A1 \ {0} be an quasi-projective variety. We claim that in fact we have an
isomorphism

X ∼= V = V(xy − 1) ⊆ A2

which shows that X is still isomorphic to an aﬃne variety. The maps are

X ↔ V
t (cid:55)→ (t, 1/t)

x ←(cid:91) (x, y).

Intuitively, the “hyperbola y = 1/x” in A2 can be projected onto the x-axis. Here is the
relevant picture.

Actually, deleting any number of points from A1 fails.

If we delete {1, 2, 3}, the
resulting open set is isomorphic as a baby ringed space to V(y(x − 1)(x − 2)(x − 3) − 1),
which colloquially might be called y =

The truth is more general.

1

(x−1)(x−2)(x−3) .

Distinguished open sets of aﬃne varieties are aﬃne.

Here is the exact isomorphism.

Theorem 74.4.2 (Distinguished open subsets of aﬃnes are aﬃne)
Consider X = D(f ) ⊆ V = V(f1, . . . , fm) ⊆ An, where V is an aﬃne variety, and
the distinguished open set X is thought of as a quasi-projective variety. Deﬁne

W = V(f1, . . . , fm, y · f − 1) ⊆ An+1

where y is the (n + 1)st coordinate of An+1.

Then X ∼= W .

For lack of a better name, I will dub this the hyperbola eﬀect, and it will play a
signiﬁcant role later on.

Therefore, if we wish to ﬁnd an example of a quasi-projective variety which is not aﬃne,
one good place to look would be an open set of an aﬃne space which is not distinguished

yxV(xy−1)X74 Morphisms of varieties

699

open. If you are ambitious now, you can try to prove the punctured plane (that is, A2
minus the origin) works. We will see that example once again later in the next chapter,
so you will have a second chance to do so.

§74.5 A few harder problems to think about

Problem 74A. Consider the map

A1 → V(y2 − x3) ⊆ A2 by t (cid:55)→ (t2, t3).

Show that it is a morphism of varieties, but it is not an isomorphism.

Problem 74B†. Show that every projective variety has an open neighborhood which is
isomorphic to an aﬃne variety. In this way, “projective varieties are locally aﬃne”.

Problem 74C. Let V be a aﬃne variety and let W be a irreducible projective variety.
Prove that V ∼= W if and only if V and W are a single point.
Problem 74D (Punctured plane is not aﬃne). Let X = A2 \ {(0, 0)} be an open set of
A2. Let V be any aﬃne variety and let f : X → V be a morphism. Show that f is not
an isomorphism.

XIX

Algebraic Geometry II: Aﬃne

Schemes

Part XIX: Contents

75 Sheaves and ringed spaces

705
75.1 Motivation and warnings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 705
75.2 Pre-sheaves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 705
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 707
75.3 Stalks and germs
75.4 Sheaves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 711
. . . . . . . . . . . . . . . . . . . . 712
75.5 For sheaves, sections “are” sequences of germs
75.6 Sheaﬁﬁcation (optional) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 714
. . . . . . . . . . . . . . . . . . . . . . . . 715
75.7 A few harder problems to think about

76 Localization

717
76.1 Spoilers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 717
76.2 The deﬁnition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 718
76.3 Localization away from an element . . . . . . . . . . . . . . . . . . . . . . . . . . 719
76.4 Localization at a prime ideal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 720
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 722
76.5 Prime ideals of localizations
76.6 Prime ideals of quotients . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 723
76.7 Localization commute with quotients . . . . . . . . . . . . . . . . . . . . . . . . . 723
. . . . . . . . . . . . . . . . . . . . . . . . 725
76.8 A few harder problems to think about

77 Aﬃne schemes: the Zariski topology

727
77.1 Some more advertising . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 727
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 727
77.2 The set of points
77.3 The Zariski topology on the spectrum . . . . . . . . . . . . . . . . . . . . . . . . 729
77.4 On radicals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 732
. . . . . . . . . . . . . . . . . . . . . . . . 733
77.5 A few harder problems to think about

78 Aﬃne schemes: the sheaf

735
78.1 A useless deﬁnition of the structure sheaf . . . . . . . . . . . . . . . . . . . . . . . 735
. . . . . . 736
78.2 The value of distinguished open sets (or: how to actually compute sections)
. . . . . . . . . . . . . . . . . . . . . . . . . . . 738
78.3 The stalks of the structure sheaf
78.4 Local rings and residue ﬁelds: linking germs to values . . . . . . . . . . . . . . . . . 739
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 742
78.5 Recap
. . . . . . . . . . . . . . . . . . . . 742
78.6 Functions are determined by germs, not values
. . . . . . . . . . . . . . . . . . . . . . . . 743
78.7 A few harder problems to think about

79 Interlude: eighteen examples of aﬃne schemes

745
. . . . . . . . . . . . . . . . . . . . . . . . . . . 745
79.1 Example: Spec k, a single point
79.2 Spec C[x], a one-dimensional line . . . . . . . . . . . . . . . . . . . . . . . . . . . 745
79.3 Spec R[x], a one-dimensional line with complex conjugates glued (no fear nullstellensatz)
746
. . . . . . . . . . . . . . . . . . . . . . . . . . . 747
79.4 Spec k[x], over any ground ﬁeld
79.5 Spec Z, a one-dimensional scheme
. . . . . . . . . . . . . . . . . . . . . . . . . . 747
79.6 Spec k[x]/(x2 − x), two points
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 748
79.7 Spec k[x]/(x2), the double point
. . . . . . . . . . . . . . . . . . . . . . . . . . . 748
79.8 Spec k[x]/(x3 − x), a double point and a single point
. . . . . . . . . . . . . . . . . 749
79.9 Spec Z/60Z, a scheme with three points . . . . . . . . . . . . . . . . . . . . . . . . 749
79.10Spec k[x, y], the two-dimensional plane . . . . . . . . . . . . . . . . . . . . . . . . 750
79.11Spec Z[x], a two-dimensional scheme, and Mumford’s picture . . . . . . . . . . . . . . 751
79.12Spec k[x, y]/(y − x2), the parabola . . . . . . . . . . . . . . . . . . . . . . . . . . 752
79.13Spec Z[i], the Gaussian integers (one-dimensional) . . . . . . . . . . . . . . . . . . . 753
. . . . . . . . . . . . . . . . . . . . . . 754
79.14Long example: Spec k[x, y]/(xy), two axes
79.15Spec k[x, x−1], the punctured line (or hyperbola)
. . . . . . . . . . . . . . . . . . . 756
79.16Spec k[x](x), zooming in to the origin of the line . . . . . . . . . . . . . . . . . . . . 757
79.17Spec k[x, y](x,y), zooming in to the origin of the plane . . . . . . . . . . . . . . . . . 758
79.18Spec k[x, y](0) = Spec k(x, y), the stalk above the generic point . . . . . . . . . . . . . 758
. . . . . . . . . . . . . . . . . . . . . . . . 758
79.19A few harder problems to think about

TABLE OF CONTENTS

703

759
80 Morphisms of locally ringed spaces
. . . . . . . . . . . . . . . . . . . . . . . 759
80.1 Morphisms of ringed spaces via sections
. . . . . . . . . . . . . . . . . . . . . . . . 760
80.2 Morphisms of ringed spaces via stalks
80.3 Morphisms of locally ringed spaces . . . . . . . . . . . . . . . . . . . . . . . . . . 761
80.4 A few examples of morphisms between aﬃne schemes . . . . . . . . . . . . . . . . . 762
80.5 The big theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 765
80.6 More examples of scheme morphisms . . . . . . . . . . . . . . . . . . . . . . . . . 767
80.7 A little bit on non-aﬃne schemes . . . . . . . . . . . . . . . . . . . . . . . . . . . 768
80.8 Where to go from here . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 770
. . . . . . . . . . . . . . . . . . . . . . . . 770
80.9 A few harder problems to think about

75 Sheaves and ringed spaces

Most of the complexity of the aﬃne variety V earlier comes from OV . This is a type
of object called a “sheaf”. The purpose of this chapter is to completely deﬁne what this
sheaf is, and just what it is doing.

§75.1 Motivation and warnings

The typical example to keep in mind is a sheaf of “functions with property P ” on a
topological space X: for every open set U , F (U ) gives us the ring of functions on X.
However, we will work very abstractly and only assume F (U ) is a ring, without an
interpretation as “functions”.

Throughout this chapter, I will not only be using algebraic geometry examples, but
also those with X a topological space and F being a sheaf of diﬀerentiable/analytic/etc
functions. One of the nice things about sheaves is that the same abstraction works ﬁne,
so you can train your intuition with both algebraic and analytic examples. In particular,
we can keep drawing open sets U as ovals, even though in the Zariski topology that’s not
what they look like.

The payoﬀ for this abstraction is that it will allow us to deﬁne an arbitrary scheme in
the next chapter. Varieties use C[x1, x2, . . . , xn]/I as their “ring of functions”, and by
using the fully general sheaf we replace this with any commutative ring. In particular,
we could choose C[x]/(x2) and this will give the “multiplicity” behavior that we sought
all along.

§75.2 Pre-sheaves

Prototypical example for this section: The sheaf of holomorphic (or regular, continuous,
diﬀerentiable, constant, whatever) functions.

The proper generalization of our OV is a so-called sheaf of rings. Recall that OV took
open sets of V to rings, with the interpretation that OV (U ) was a “ring of functions”.
§75.2.i Usual deﬁnition

So here is the oﬃcial deﬁnition of a pre-sheaf.

Deﬁnition 75.2.1. For a topological space X let OpenSets(X) denote its open sets of
X.

Deﬁnition 75.2.2. A pre-sheaf of rings on a space X is a function

F : OpenSets(X) → Rings

meaning each open set gets associated with a ring F (U ). Each individual element of
F (U ) is called a section.

It is also equipped with a restriction map for any U1 ⊆ U2; this is a map

The map satisﬁes two axioms:

resU1,U2 : F (U2) → F (U1).

705

706

Napkin, by Evan Chen (v1.5.20190718)

 The map resU,U is the identity, and

 Whenever we have nested subsets

Usmall ⊆ Umed ⊆ Ubig

F (Ubig)

res- F (Umed)

r

e

s

res

?

-

F (Usmall)

the diagram

commutes.

Deﬁnition 75.2.3. An element of F (X) is called a global section.
Abuse of Notation 75.2.4. If s ∈ F (U2) is some section and U1 ⊆ U2, then rather
than write resU1,U2(s) I will write s(cid:22)U1 instead: “s restricted to U1”. This is abuse of
notation because the section s is just an element of some ring, and in the most abstract
of cases may not have a natural interpretation as function.

Here is a way you can picture sections. In all our usual examples, sheaves return
functions an open set U . So, we draw a space X, and an open set U , and then we want to
draw a “function on U ” to represent a section s. Crudely, we will illustrate s by drawing
an xy-plot of a curve, since that is how we were told to draw functions in grade school.

Then, the restriction corresponds to, well, taking just a chunk of the section.

All of this is still a dream, since s in reality is an element of a ring. However, by the end
of this chapter we will be able to make our dream into a reality.

XUs∈F(U)XUVress∈F(V)75 Sheaves and ringed spaces

707

Example 75.2.5 (Examples of pre-sheaves)
(a) For an aﬃne variety V , OV is of course a sheaf, with OV (U ) being the ring of
regular functions on U . The restriction map just says that if U1 ⊆ U2, then a
function s ∈ OV (U2) can also be thought of as a function s(cid:22)U1 ∈ OV (U1), hence
the name “restriction”. The commutativity of the diagram then follows.

(b) Let X ⊆ Rn be an open set. Then there is a sheaf of smooth/diﬀerentiable/etc.
functions on X. In fact, one can do the same construction for any manifold M .
(c) Similarly, if X ⊆ C is open, we can construct a sheaf of holomorphic functions

on X.

In all these examples, the sections s ∈ F (U ) are really functions on the space, but
in general they need not be.

In practice, thinking about the restriction maps might be more confusing than helpful;

it is better to say:

Pre-sheaves should be thought of as “returning the ring of functions with
a property P ”.

§75.2.ii Categorical deﬁnition

If you really like category theory, we can give a second equivalent and shorter deﬁnition.
Despite being a category lover myself, I ﬁnd this deﬁnition less intuitive, but its brevity
helps with remembering the ﬁrst one.

Abuse of Notation 75.2.6. By abuse of notation, OpenSets(X) will also be thought
of as a posetal category by inclusion. Thus ∅ is an initial object and the entire space X
is a terminal object.

Deﬁnition 75.2.7. A pre-sheaf of rings on X is a contravariant functor

F : OpenSets(X)op → Rings.

Exercise 75.2.8. Check that these deﬁnitions are equivalent.

In particular, it is possible to replace Rings with any category we want. We will not need
to do so any time soon, but it’s worth mentioning.

§75.3 Stalks and germs

Prototypical example for this section: Germs of real smooth functions tell you the
derivatives, but germs of holomorphic functions determine the entire function.

As we mentioned, the helpful pictures from the previous section are still just metaphors,
because there is no notion of “value”. With the addition of the words “stalk” and “germ”,
we can actually change that.

Deﬁnition 75.3.1. Let F be a pre-sheaf (of rings). For every point p we deﬁne the
stalk Fp to be the set

{(s, U ) | s ∈ F (U ), p ∈ U}

708

Napkin, by Evan Chen (v1.5.20190718)

modulo the equivalence relation ∼ that

(s1, U1) ∼ (s2, U2) if s1(cid:22)U1∩U2 = s2(cid:22)U1∩U2.

The equivalence classes themselves are called germs.

Deﬁnition 75.3.2. The germ of a given s ∈ F (U ) at a point p is the equivalence class
for (s, U ) ∈ Fp. We denote this by [s]p.

It is rarely useful to think of a germ as an ordered pair, since the set U can get
arbitrarily small. Instead, one should think of a germ as a “shred” of some section near
p. A nice summary for the right mindset might be:

A germ is an “enriched value”; the stalk is the set of possible germs.

Let’s add this to our picture from before. If we insist on continuing to draw our sections
as xy-plots, then above each point p a good metaphor would be a vertical line out from
p. The germ would then be the “enriched value of s at p”. We just draw that as a big
dot in our plot. The main diﬀerence is that the germ is enriched in the sense that the
germ carries information in a small region around p as well, rather than literally just the
point p itself. So accordingly we draw a large dot for [s]p, rather than a small dot at p.

Before going on, we might as well note that the stalks are themselves rings, not just

sets: we can certainly add or subtract enriched values.

Deﬁnition 75.3.3. The stalk Fp can itself be regarded as a ring: for example, addition
is done by

(s1, U1) + (s2, U2) = (s1(cid:22)U1∩U2 + s2(cid:22)U1∩U2, U1 ∩ U2) .

XUs∈F(U)Fp[s]pp75 Sheaves and ringed spaces

709

Example 75.3.4 (Germs of real smooth functions)
Let X = R and let F be the pre-sheaf on X of smooth functions (i.e. F (U ) is the
set of smooth real-valued functions on U ).

Consider a global section, s : R → R (thus s ∈ F (X)) and its germ at 0.

(a) From the germ we can read oﬀ s(0), obviously.

(b) We can also ﬁnd s(cid:48)(0), because the germ carries enough information to compute

the limit limh→0

1

h [s(h) − s(0)].

(c) Similarly, we can compute the second derivative and so on.

(d) However, we can’t read oﬀ, say, s(3) from the germ. For example, consider the

function from Example 29.4.4,

s(x) =(cid:40)e− 1

0

x−1

x > 1
x ≤ 1.

Note s(3) = e− 1
the zero function and s.

2 , but [zero function]0 = [s]0. So germs can’t distinguish between

Example 75.3.5 (Germs of holomorphic functions)
Holomorphic functions are surprising in this respect. Consider the sheaf F on C of
holomorphic functions.

Take s : C → C a global section. Given the germ of s at 0, we can read oﬀ s(0),
s(cid:48)(0), et cetera. The miracle of complex analysis is that just knowing the derivatives
of s at zero is enough to reconstruct all of s: we can compute the Taylor series of s
now. Thus germs of holomorphic functions determine the entire function;
they “carry more information” than their real counterparts.

In particular, we can concretely describe the stalks of the pre-sheaf:

Fp =(cid:88)k≥0

ck(z − p)k convergent near p

.

For example, this includes germs of meromorphic functions, so long as there is no
pole at p itself.

And of course, our algebraic geometry example. This example will matter a lot later,

so we do it carefully now.

Abuse of Notation 75.3.6. Rather than writing (OX )p we will write OX,p.

Theorem 75.3.7 (Stalks of OV )
Let V ⊆ An be a variety, and assume p ∈ V is a point. Then
g | f, g ∈ C[V ], g(p) (cid:54)= 0(cid:27) .

OV,p ∼=(cid:26) f

710

Napkin, by Evan Chen (v1.5.20190718)

Proof. A regular function ϕ on U ⊆ V is supposed to be a function on U that “locally”
is a quotient of two functions in C[V ]. Since we are looking at the stalk near p, though,
the germ only cares up to the choice of representation at p, and so we can go ahead and
write

OV,p =(cid:110)(cid:16) f

g , U(cid:17) | U (cid:51) p, f, g ∈ C[V ], g (cid:54)= 0 on U(cid:111)

modulo the same relation.

Now we claim that the map

OV,p → desired RHS

by

(cid:18) f

g

, U(cid:19) (cid:55)→

f
g

is an isomorphism.

 Injectivity: We are working with complex polynomials, so we know that a rational
function is determined by its behavior on any open neighborhood of p; thus two
germ representatives ( f1
, U2) agree on U1 ∩ U2 if and only if they are
g1
actually the same quotient.

, U1) and ( f2
g2

 Surjectivity: take U = D(g).

OV,a =(cid:26) f (x)

Example 75.3.8 (Stalks of your favorite varieties at the origin)
(a) Let V = A1; then the stalk of OV at each point p ∈ V is
g(x) | g(p) (cid:54)= 0(cid:27) .
x+1 if p (cid:54)= 1, x+7
(b) Let V = A2; then the stalk of OV at the origin is
g(x, y) | g(0, 0) (cid:54)= 0(cid:27) .

OV,(0,0) =(cid:26) f (x, y)

Examples of elements are x2 + 5,

1

x2−9 if p (cid:54)= ±3, and so on.

Examples of elements are x2 + y2,

x3
xy+1 , 13x+37y
x2+8y+2 .

(c) Let V = V(y − x2) ⊆ A2; then the stalk of OV at the origin is

OV,(0,0) =(cid:26) f (x, y)

g(x, y) | f, g ∈ C[x, y]/(y − x2), g(0, 0) (cid:54)= 0(cid:27) .

y

1+x = x2

For example,
1+x denote the same element in the stalk. Actually, you
could give a canonical choice of representative by replacing y with x2 everywhere,
so it would also be correct to write

OV,(0,0) =(cid:26) f (x)

g(x) | g(0) (cid:54)= 0(cid:27)

which is the same as the ﬁrst example.

75 Sheaves and ringed spaces

711

Remark 75.3.9 (Aside for category lovers) — You may notice that Fp seems to be
“all the Fp(U ) coming together”, where p ∈ U . And in fact, Fp(U ) is the categorical
limit of the diagram formed by all the F (U ) such that p ∈ U . This is often written

F (U )

Fp = lim−→U(cid:51)p

Thus we can deﬁne stalks in any category with limits, though to be able to talk
about germs the category needs to be concrete.

§75.4 Sheaves

Prototypical example for this section: Constant functions aren’t sheaves, but locally
constant ones are.

Since we care so much about stalks, which study local behavior, we will impose

additional local conditions on our pre-sheaves. One way to think about this is:

Sheaves are pre-sheaves for which P is a local property.

The formal deﬁnition doesn’t illuminate this as much as the examples do, but sadly I

have to give the deﬁnition ﬁrst for the examples to make sense.

Deﬁnition 75.4.1. A sheaf F is a pre-sheaf obeying two additional axioms: Suppose
U is covered by open sets Uα ⊆ U . Then:

1. (Identity) If s, t ∈ F (U ) are sections, and s(cid:22)Uα = t(cid:22)Uα for all α, then s = t.
2. (Gluing) Consider sections sα ∈ (Uα) for each α. Suppose that

sα(cid:22)Uα∩Uβ = sβ(cid:22)Uα∩Uβ

for each Uα and Uβ. Then we can ﬁnd s ∈ U such that s(cid:22)Uα = sα.

Remark 75.4.2 (For keepers of the empty set) — The above axioms imply F (∅) = 0
(the zero ring). This is not worth worrying about until you actually need it, so you
can forget I said that.

This is best illustrated by picture in the case of just two open sets: consider two open
sets U and V . Then the sheaf axioms are saying something about F (U ∪ V ), F (U ∩ V ),
F (U ) and F (V ).

Then for a sheaf of functions, the axioms are saying that:

 If s and t are functions (with property P ) on U ∩ V and s(cid:22)U = t(cid:22)U , s(cid:22)V = t(cid:22)V ,

then s = t on the entire union. This is clear.

UV712

Napkin, by Evan Chen (v1.5.20190718)

 If s1 is a function with property P on U and s2 is a function with property P on
V , and the two functions agree on the overlap, then one can glue them to obtain a
function s on the whole space: this is obvious, but the catch is that the collated
function needs to have property P as well (i.e. needs to be an element of
F (U ∪ V )). That’s why it matters that P is local.

So you can summarize both of these as saying: any two functions on U and V which
agree on the overlap glue to a unique function on U ∪ V . If you like category theory, you
might remember we alluded to this in Example 62.2.1.

Exercise 75.4.3 (For the categorically inclined). Show that the diagram

F (U ∪ V )

F (U )

F (V )

F (U ∩ V )

is a pullback square.

Now for the examples.

Example 75.4.4 (Examples and non-examples of sheaves)
Note that every example of a stalk we computed in the previous section was of a
sheaf. Here are more details:

(a) Pre-sheaves of arbitrary / continuous / diﬀerentiable / smooth / holomorphic
functions are still sheaves. This is because to verify a function is continuous, one
only needs to look at small open neighborhoods at once.

(b) For a complex variety V , OV is a sheaf, precisely because our deﬁnition was

locally quotients of polynomials.

(c) The pre-sheaf of constant real functions on a space X is not a sheaf in general,
because it fails the gluing axiom. Namely, suppose that U1 ∩ U2 = ∅ are disjoint
open sets of X. Then if s1 is the constant function 1 on U1 while s2 is the
constant function 2 on U2, then we cannot glue these to a constant function on
U1 ∪ U2.

(d) On the other hand, locally constant functions do produce a sheaf. (A function is

locally constant if for every point it is constant on some open neighborhood.)

In fact, the sheaf in (d) is what is called a sheaﬁﬁcation of the pre-sheaf constant
functions, which we deﬁne momentarily.

§75.5 For sheaves, sections “are” sequences of germs

Prototypical example for this section: A real function on U is a sequence of real numbers
f (p) for each p ∈ U satisfying some local condition. Analogously, a section s ∈ F (U ) is
a sequence of germs satisfying some local compatibility condition.

Once we impose the sheaf axioms, our metaphorical picture will actually be more or
less complete. Just as a function was supposed to be a choice of value at each point, a

75 Sheaves and ringed spaces

713

section will be a choice of germ at each stalk.

Example 75.5.1 (Real functions vs. germs)
Let X be a space and let F be the sheaf of smooth functions. Take a section
f ∈ F (U ).

 As a function, f is just a choice of value f (p) ∈ R at every point p, subject to

a local “smooth” condition.

 Let’s now think of f as a sequence of germs. At every point p the germ
[f ]p ∈ Fp gives us the value f (p) as we described above. The germ packages
even more data than this: from the germ [f ]p alone we can for example compute
f(cid:48)(p). Nonetheless we stretch the analogy and think of f as a choice of germ
[f ]p ∈ Fp at each point p.

Thus we can replace the notion of the value f (p) with germ [f ]p. This is useful
because in a general sheaf F , the notion s(p) is not deﬁned while the notion [s]p is.

From the above example it’s obvious that if we know each germ [s]p this should let us

reconstruct the entire section s. Let’s check this from the sheaf axioms:

Exercise 75.5.2 (Sections are determined by stalks). Let F be a sheaf. Consider the
natural map

F (U ) → (cid:89)p∈U

Fp

described above. Show that this map is injective, i.e. the germs of s at every point p ∈ U
determine the section s. (You will need the “identity” sheaf axiom, but not “gluing”.)

However, this map is clearly not surjective! Nonetheless we can describe the image:
we want a sequence of germs (gp)p∈U such that near every germ gp, the germs gq are
“compatible” with gp. We make this precise:

Deﬁnition 75.5.3. Let F be pre-sheaf and let U be an open set. A sequence (gp)p∈U
of germs (with gp ∈ Fp for each p) is said to be compatible if they can be “locally
collated”:

For any p ∈ U there exists an open neighborhood Up (cid:51) p and a section
s ∈ F (Up) on it such that [s]q = gq for each q ∈ Up.

Intuitively, the germs should “collate together” to some section near each individual
point q (but not necessarily to a section on all of U ).

We let the reader check this deﬁnition is what we want:

Exercise 75.5.4. Prove that any choice of compatible germs over U collates together to a
section of U . (You will need the “gluing” sheaf axiom, but not “identity”.)

Putting together the previous two exercise gives:

714

Napkin, by Evan Chen (v1.5.20190718)

Theorem 75.5.5 (Sections “are” just compatible germs)
Let F be a sheaf. There is a natural bijection between

 sections of F (U ), and

 sequences of compatible germs over U .

We draw this in a picture below by drawing several stalks, rather than just one, with
the germs above. The stalks at diﬀerent points need not be related to each other, so
I have drawn the stalks to have diﬀerent heights to signal this. And, the caveat is
that the germs are large enough that germs which are “close to each other” should be
“compatible”.

This is in exact analogy to the way that e.g. a smooth real-valued function on U is
a choice of real number f (p) ∈ R at each point p ∈ U satisfying a local smoothness
condition.
Thus the notion of stalks is what lets us recover the viewpoint that sections are

“functions”. Therefore for theoretical purposes,

With sheaf axioms, sections are sequences of compatible germs.

In particular, this makes restriction morphisms easy to deal with: just truncate the
sequence of germs!

§75.6 Sheaﬁﬁcation (optional)

Prototypical example for this section: The pre-sheaf of locally constant functions becomes
the sheaf of constant functions.

The idea is that if F is the pre-sheaf of “functions with property P ” then we want to
associate a sheaf F sh of “functions which are locally P ”, which makes them into a sheaf.
We have already seen two examples of this:

Example 75.6.1 (Sheaﬁﬁcation)
(a) If X is a topological space, and F is the pre-sheaf of constant functions on open

sets of X, then F sh is the sheaf of locally constant functions.

(b) If V is an aﬃne variety, and F is the pre-sheaf of rational functions, then F sh

is the sheaf of regular functions (which are locally rational).

XU75 Sheaves and ringed spaces

715

The procedure is based on stalks and germs. We saw that for a sheaf, sections
correspond to sequences of compatible germs. For a pre-sheaf, we can still deﬁne stalks
and germs, but their properties will be less nice. But given our initial pre-sheaf F , we
deﬁne the sections of F sh to be sequences of compatible F -germs.

Deﬁnition 75.6.2. The sheaﬁﬁcation F sh of a pre-sheaf F is deﬁned by

F sh(U ) = {sequences of compatible F -germs (gp)p∈U} .

Question 75.6.3. Complete the deﬁnition by describing the restriction morphisms of F sh.

Abuse of Notation 75.6.4. I’ll usually be equally sloppy in the future: when deﬁning
a sheaf F , I’ll only say what F (U ) is, with the restriction morphisms F (U2) → F (U1)
being implicit.

The construction is contrived so that given a section (gp)p∈U ∈ F sh(U ) the germ at a
point p is gp:

Lemma 75.6.5 (Stalks preserved by sheaﬁﬁcation)
Let F be a pre-sheaf and F sh its sheaﬁﬁcation. Then for any point q, there is an
isomorphism

(F sh)q ∼= Fq.

Proof. A germ in (F sh)q looks like ((gp)p∈U , U ), where gp = (sp, Up) are themselves
germs of Fp, and q ∈ U . Then the isomorphism is given by

((gp)p∈U , U ) (cid:55)→ gq ∈ Fq.
The inverse map is given by for each g = (s, U ) ∈ Fq by
g (cid:55)→ ((g)p∈U , U ) ∈ (F sh)q

i.e. the sequence of germs is the constant sequence.

We will use sheaﬁﬁcation in the future to economically construct sheaves. However, in

practice, the details of the construction will often not matter.

§75.7 A few harder problems to think about

Problem 75A. Prove that if F is already a sheaf, then F (U ) ∼= F sh(U ) for every open
set U .

Problem 75B. Let X be a space with two points {p, q} and let F be a sheaf on it.
Suppose Fp = Z/5Z and Fq = Z. Describe F (U ) for each open set U of X, where

(a) X is equipped with the discrete topology.

(b) X is equipped ∅, {p}, {p, q} as the only open sets.

716

Napkin, by Evan Chen (v1.5.20190718)

Problem 75C (Skyscraper sheaf). Let Y be a topological space. Fix p ∈ Y a point,
and R a ring. The skyscraper sheaf is deﬁned by

F (U ) =(cid:40)R p ∈ U

0

otherwise

with restriction maps in the obvious manner. Compute all the stalks of F .

(Possible suggestion: ﬁrst do the case where Y is Hausdorﬀ, where your intuition will
give the right answer. Then do the pathological case where every open set of Y contains
p. Then try to work out the general answer.)
Problem 75D. Let F be a sheaf on a space X and let s ∈ F (X) be a global section.
Deﬁne the support of s as

Show that Z is a closed set of X.

Z = {p ∈ X | [s]p (cid:54)= 0 ∈ Fp} .

76 Localization

Before we proceed on to deﬁning an aﬃne scheme, we will take the time to properly
cover one more algebraic construction that of a localization. This is mandatory because
when we deﬁne a scheme, we will ﬁnd that all the sections and stalks are actually obtained
using this construction.

One silly slogan might be:

Localization is the art of adding denominators.

You may remember that when we were working with aﬃne varieties, there were constantly

expressions of the form(cid:110) f

g | g(p) (cid:54)= 0(cid:111) and the like. The point is that we introduced a

lot of denominators. Localization will give us a concise way of doing this in general.

Notational note: moving forward we’ll prefer to denote rings by A, B, . . . , rather than

R, S, . . . .

§76.1 Spoilers

Here is a preview of things to come, so that you know what you are expecting. Some
things here won’t make sense, but that’s okay, it is just foreshadowing.

Let V ⊆ An, and for brevity let R = C[V ] be its coordinate ring. We saw in previous
sections how to compute OV (D(g)) and OV,p for p ∈ V a point. For example, if we take A1
and consider a point p, then OA1(D(x − p)) =(cid:110) f (x)
g(x) | g(p) (cid:54)= 0(cid:111).

(x−p)n(cid:111) and OA1,p =(cid:110) f (x)

More generally, we had

OV (D(g)) =(cid:26) f
OV,p =(cid:26) f

gn | f ∈ R(cid:27) by Theorem 71.6.1
g | f, g ∈ R, g(p) (cid:54)= 0(cid:27) by Theorem 75.3.7.

We will soon deﬁne something called a localization, which will give us a nice way of
expressing the above: if R = C[V ] is the coordinate ring, then the above will become
abbreviated to just

OV (D(g)) = Rg

OV,p = Rm where {p} = V(m).

The former will be pronounced “R localized away from g” while the latter will be
pronounced “R localized at m”.

Even more generally, next chapter we will throw out the coordinate ring R altogether
and replace it with a general commutative ring A (which are still viewed as functions).
We will construct a ringed space called X = Spec A, whose elements are prime ideals of
A and is equipped with the Zariski topology and a sheaf OX . It will turn out that, in
analogy to what we had before,

OX (D(f )) = A[f−1]

OX,p = Ap

for any element f ∈ A and prime ideal p ∈ Spec A. Thus just as with complex aﬃne
varieties, localizations will give us a way to more or less describe the sheaf OX completely.

717

718

Napkin, by Evan Chen (v1.5.20190718)

§76.2 The deﬁnition

Deﬁnition 76.2.1. A subset S ⊆ A is a multiplicative set if 1 ∈ S and S is closed
under multiplication.

Deﬁnition 76.2.2. Let A be a ring and S ⊂ A a multiplicative set. Then the localiza-
tion of A at S, denoted S−1A, is deﬁned as the set of fractions

{a/s | a ∈ A, s ∈ S}

where we declare two fractions a1/s1 = a2/s2 to be equal if s(a1s2 − a2s1) = 0 for some
s ∈ S. Addition and multiplication in this ring are deﬁned in the obvious way.

In particular, if 0 ∈ S then S−1A is the zero ring. So we usually only take situations
We give in brief now two examples which will be motivating forces for the construction

where 0 /∈ S.
of the aﬃne scheme.

Example 76.2.3 (Localizations of C[x])
Let A = C[x].

(a) Suppose we let S =(cid:8)1, x, x2, x3, . . .(cid:9) be the powers of x. Then

S−1A =(cid:26) f (x)

xn | f ∈ C[x], n ∈ Z≥0(cid:27) .

In other words, we get the Laurent polynomials in x.

You might recognize this as

OV (U ) where V = A1, U = V \ {0}.

i.e. the sections of the punctured line. In line with the “hyperbola eﬀect”, this is
also expressible as C[x, y]/(xy − 1).

(b) Let p ∈ C. Suppose we let S = {g(x) | g(p) (cid:54)= 0}, i.e. we allow any denominators

where g(p) (cid:54)= 0. Then

S−1A =(cid:26) f (x)

g(x) | f, g ∈ C[x], g(p) (cid:54)= 0(cid:27) .

You might recognize this is as the stalk OA1,p. This will be important later on.

Remark 76.2.4 (Why the extra s?) — We cannot use the simpler a1s2 − a2s1 =
0 since otherwise the equivalence relation may fail to be transitive. Here is a
counterexample: take

A = Z/12Z
2 = 2
4 = 6

S = {2, 4, 8}.

Then we have for example 1
2 which is only
true with the ﬁrst deﬁnition. Of course, if A is an integral domain (and 0 /∈ S) then
this is a moot point.

2 . So we need to have 1

4 = 3

2 = 3

76 Localization

719

Example 76.2.5 (Field of fractions)
Let A be an integral domain and S = A \ {0}. Then S−1A = Frac(A).

§76.3 Localization away from an element

Prototypical example for this section: Z localized away from 6 has fractions m

2x3y .

We now focus on the two special cases of localization we will need the most; one in

this section, the other in the next section.
Deﬁnition 76.3.1. For f ∈ A, we deﬁne the localization of A away from f , denoted

A[1/f ] or A[f−1], to be {1, f, f 2, f 3, . . .}−1A. (Note that(cid:8)1, f, f 2, . . .(cid:9) is multiplicative.)

Remark 76.3.2 — In the literature it is more common to see the notation Af
instead of A[1/f ]. This is confusing, because in the next section we deﬁne Ap which
is almost the opposite. So I prefer this more suggestive (but longer) notation.

Example 76.3.3 (Some arithmetic examples of localizations)
(a) We localize Z away from 6:

Z[1/6] =(cid:110) m

6n | m ∈ Z, n ∈ Z≥0(cid:111) .

So A[1/6] consist of those rational numbers whose denominators have only powers
of 2 and 3. For example, it contains 5

12 = 15
36 .

(b) Here is a more confusing example: if we localize Z/60Z away from the element
5, we get (Z/60Z)[1/5] ∼= Z/12Z. You should try to think about why this is the
case. We will see a “geometric” reason later.

Example 76.3.4 (Localization at an element, algebraic geometry ﬂavored)
We saw that if A is the coordinate ring of a variety, then A[1/g] is interpreted
geometrically as OV (D(g)). Here are some special cases:

(a) As we saw, if A = C[x], then A[1/x] =(cid:110) f (x)

(b) Let A = C[x, y, z]. Then

xn (cid:111) consists of Laurent polynomials.
| f ∈ C[x, y, z], n ≥ 0(cid:27)

A[1/x] =(cid:26) f (x, y, z)

xn

is rational functions whose denominators are powers of x.

(c) Let A = C[x, y]. If we localize away from y − x2 we get

A[(y − x2)−1] =(cid:26) f (x, y)

(y − x2)n | f ∈ C[x, y], n ≥ 0(cid:27)

By now you should recognize this as OA2(D(y − x2)).

720

Napkin, by Evan Chen (v1.5.20190718)

Example 76.3.5 (An example with zero-divisors)
Let A = C[x, y]/(xy) (which intuitively is the coordinate ring of two axes). Suppose
we localize at x: equivalently, allowing denominators of x. Since xy = 0 in A, we
now have 0 = x−1(xy) = y, so y = 0 in A, and thus y just goes away completely.
From this we get a ring isomorphism

A[1/x] ∼= C[x, 1/x].

§76.4 Localization at a prime ideal

Prototypical example for this section: Z localized at (5) has fractions m

n with 5 (cid:45) n.

Deﬁnition 76.4.1. If A is a ring and p is a prime ideal, then we deﬁne

Ap := (A \ p)−1 A.

This is called the localization at p.

Question 76.4.2. Why is S = A \ p multiplicative in the above deﬁnition?

This special case is important because we will see that stalks of schemes will all be of

this shape. In fact, the same was true for aﬃne varieties too.

Example 76.4.3 (Relation to aﬃne varieties)
Let V ⊆ An, let A = C[V ] and let p = (a1, . . . , an) be a point. Consider the maximal
(hence prime) ideal

m = (x1 − a1, . . . , xn − an).

Observe that a function f ∈ A vanishes at p if and only if f (mod m) = 0, equivalently
f ∈ m. Thus, by Theorem 75.3.7 we can write

g | f, g ∈ A, g(p) (cid:54)= 0(cid:27)
g | f ∈ A, g ∈ A \ m(cid:27)

OV,p =(cid:26) f
=(cid:26) f
= (A \ m)−1 A = Am.

So, we can also express OV,p concisely as a localization.

Consequently, we give several examples in this vein.

76 Localization

721

Example 76.4.4 (Geometric examples of localizing at a prime)
(a) We let m be the maximal ideal (x) of A = C[x]. Then

Am =(cid:26) f (x)

g(x) | g(0) (cid:54)= 0(cid:27)

consists of the Laurent polynomials.

(b) We let m be the maximal ideal (x, y) of A = C[x, y]. Then

Am =(cid:26) f (x, y)

g(x, y) | g(0, 0) (cid:54)= 0(cid:27) .
(c) Let p be the prime ideal (y − x2) of A = C[x, y]. Then
g(x, y) | g /∈ (y − x2)(cid:27) .

Ap =(cid:26) f (x, y)

This is a bit diﬀerent from what we’ve seen before: the polynomials in the
denominator are allowed to vanish at a point like (1, 1), as long as they don’t
vanish on every point on the parabola. This doesn’t correspond to any stalk
we’re familiar with right now, but it will later (it will be the “stalk at the generic
point of the parabola”).

(d) Let A = C[x] and localize at the prime ideal (0). This gives

A(0) =(cid:26) f (x)

g(x) | g(x) (cid:54)= 0(cid:27) .

This is all rational functions, period.

Example 76.4.5 (Arithmetic examples)
We localize Z at a few diﬀerent primes.

(a) If we localize Z at (0):

(b) If we localize Z at (3), we get

Z(0) =(cid:110) m
Z(3) =(cid:110) m

n | n (cid:54)= 0(cid:111) ∼= Q.
n | gcd(n, 3) = 1(cid:111)

which is the ring of rational numbers whose denominators are relatively prime
to 3.

Example 76.4.6 (Field of fractions)
If A is an integral domain, the localization A(0) is the ﬁeld of fractions of A.

722

Napkin, by Evan Chen (v1.5.20190718)

§76.5 Prime ideals of localizations

Prototypical example for this section: The examples with A = Z.

We take the time now to mention how you can think about prime ideals of localized

rings.

Proposition 76.5.1 (The prime ideals of S−1A)
Let A be a ring and S ⊆ A a multiplicative set. Then there is a natural inclusion-
preserving bijection between:

 The set of prime ideals of S−1A, and

 The set of prime ideals of A not intersecting S.

Proof. Consider the homomorphism ι : A → S−1A. For any prime ideal q ⊆ S−1A, its
pre-image ψpre(q) is a prime ideal of A (by Problem 5C(cid:63)). Conversely, for any prime ideal
s | a ∈ p, s ∈ S(cid:9) is a prime ideal of S−1A. An annoying
p ⊆ A not meeting S, S−1p =(cid:8) a

check shows that this produces the required bijection.

In practice, we will almost always use the corollary where S is one of the two special
cases we discussed at length:

Corollary 76.5.2 (Spectrums of localizations)

Let A be a ring.

(a) If p is a prime ideal of A, then the prime ideals of A[1/f ] are naturally in

bijection with prime ideals of A do not contain the element f .

(b) If p is a prime ideal of A, then the prime ideals of Ap are naturally in bijection

with prime ideals of A which are subsets of p.

Proof. Part (b) is immediate; a prime ideal doesn’t meet A \ p exactly if it is contained
in p. For part (a), we want prime ideals of A not containing any power of f . But if the
ideal is prime and contains f n, then it should contain either f or f n−1, and so at least
for prime ideals these are equivalent.

Notice again how the notation is a bit of a nuisance. Anyways, here are some examples,
to help cement the picture.

Example 76.5.3 (Prime ideals of Z[1/6])
Suppose we localize Z away from the element 6, i.e. consider Z[1/6]. As we saw,

Z[1/6] =(cid:110) n

2x3y | n ∈ Z, x, y ∈ Z≥0(cid:111) .

consist of those rational numbers whose denominators have only powers of 2 and 3.
Note that (5) ⊂ Z[1/6] is a prime ideal: those elements of Z[1/6] with 5 dividing the
numerator. Similarly, (7), (11), (13), . . . and even (0) give prime ideals of Z[1/6].
But (2) and (3) no longer correspond to prime ideals; in fact in A6 we have

(2) = (3) = (1), the whole ring.

76 Localization

723

Example 76.5.4 (Prime ideals of A(5))
Suppose we localize Z at the prime (5). As we saw,

Z(5) =(cid:110) m

n | m, n ∈ Z, 5 (cid:45) n(cid:111) .

consist of those rational numbers whose denominators are not divisible by 5. This is
an integral domain, so (0) is still a prime ideal. There is one other prime ideal: (5),
i.e. those elements whose numerators are divisible by 5.

There are no other prime ideals: if p (cid:54)= 5 is a rational prime, then (p) = (1), the

whole ring, again.

§76.6 Prime ideals of quotients

While we are here, we mention that the prime ideals of quotients A/I can be interpreted
in terms of those of A (as in the previous section for localization). You may remember
this from Problem 4D(cid:63) a long time ago, if you did that problem; but for our purposes we
actually only care about the prime ideals.

Proposition 76.6.1 (The prime ideals of A/I)
If A is a ring and I is any ideal (not necessarily prime) then the prime (resp. maximal)
ideals of A/I are in bijection with prime (resp. maximal) ideals of A which are
supersets of I. This bijection is inclusion-preserving.

Proof. Consider the quotient homomorphism ψ : A (cid:16) A/I. For any prime ideal q ⊆ A/I,
its pre-image ιpre(q) is a prime ideal (by Problem 5C(cid:63)). Conversely, for any prime ideal
p with I ⊆ p ⊆ A, we get a prime ideal of A/I by looking at p (mod I). An annoying
check shows that this produces the required bijection. It is also inclusion-preserving —
from which the same statement holds for maximal ideals.

Example 76.6.2 (Prime ideals of Z/60Z)
The ring Z/60Z has three prime ideals:

(2) = {0, 2, 4, . . . , 58}
(3) = {0, 3, 6, . . . , 57}
(5) = {0, 5, 10, . . . , 55} .

Back in Z, these correspond to the three prime ideals which are supersets of 60Z =
{. . . ,−60, 0, 60, 120, . . .}.

§76.7 Localization commute with quotients

Prototypical example for this section: (C[xy]/(xy))[1/x] ∼= C[x, x−1].

While we are here, we mention a useful result from commutative algebra which lets us
compute localizations in quotient rings, which are surprisingly unintuitive. You will not
have a reason to care about this until we reach Section 76.4, and so this is only placed

724

Napkin, by Evan Chen (v1.5.20190718)

earlier to emphasize that it’s a purely algebraic fact that we can (and do) state this early,
even though we will not need it anytime soon.

Let’s say we have a quotient ring like

A/I = C[x, y]/(xy)

and want to compute the localization of this ring away from the element x. (To be
pedantic, we are actually localizing away from x (mod xy), the element of the quotient
ring, but we will just call it x.) You will quickly ﬁnd that even the notation becomes
clumsy: it is

(C[x, y]/(xy)) [1/x]

(76.1)

which is hard to think about, because the elements in play are part of the quotient: how
are we supposed to think about

1
(mod xy)
x (mod xy)

for example? The zero-divisors in play may already make you feel uneasy.

However, it turns out that we can actually do the localization ﬁrst, meaning the answer

is just

C[x, y, 1/x]/(xy)

(76.2)

which then becomes C[x, x−1, y]/(y) ∼= C[x, x−1].

This might look like it should be trivial, but it’s not as obvious as you might expect.

There is a sleight of hand present here with the notation:

 In (76.1), the notation (xy) stands for an ideal of C[x, y] — that is, the set xyC[x, y].

 In (76.2) the notation (xy) now stands for an ideal of C[x, x−1, y] — that is, the

set xyC[x, x−1, y].

So even writing down the statement of the theorem is actually going to look terrible.

In general, what we want to say is that if we have our ring A with ideal I and S is

some multiplicative subset of A, then

Colloquially: “S−1(A/I) = (S−1A)/I”.

But there are two things wrong with this:

 The main one is that I is not an ideal of S−1A, as we saw above. This is remedied
by instead using S−1I, which consists of those elements of those elements x
s for
x ∈ I and s ∈ S. As we saw this distinction is usually masked in practice, because
we will usually write I = (a1, . . . , an) ⊆ A in which case the new ideal S−1I ⊆ A
can be denoted in exactly the same way: (a1, . . . , an), just regarded as a subset of
S−1A now.

 The second is that S is not, strictly speaking, a subset of A/I, either. But this is
easily remedied by instead using the image of S under the quotient map A (cid:16) A/I.
We actually already saw this in the previous example: when trying to localize
C[x, y]/(xy), we were really localizing at the element x (mod xy), but (as always)
we just denoted it by x anyways.

And so after all those words, words, words, we have the hideous:

76 Localization

725

Theorem 76.7.1 (Localization commutes with quotients)

Let S be a multiplicative set of a ring A, and I an ideal of A. Let S be the image of
S under the projection map A (cid:16) A/I. Then

S−1

(A/I) ∼= S−1A/S−1I

where S−1I =(cid:8) x

s | x ∈ I, s ∈ S(cid:9).

Proof. Do you actually care? No? Cool, I didn’t either. (Atiyah-Macdonald is the right
reference for these type of things in the event that you do care.)

The notation is a hot mess. But when we do calculations in practice, we instead write

(cid:0)C[x, y, z]/(x2 + y2 − z2)(cid:1) [1/x] ∼= C[x, y, z, 1/x]/(x2 + y2 − z2)

or (for an example where we localize at a prime ideal)

(cid:0)Z[x, y, z]/(x2 + yz)(cid:1)(x,y) ∼= Z[x, y, z](x,y)/(x2 + yz)

and so on — the pragmatism of our “real-life” notation which hides some details actually
guides our intuition (rather than misleading us). So maybe the moral of this section is
that whenever you compute the localization of the quotient ring, if you just suspend
belief for a bit, then you will probably get the right answer.

We will later see geometric interpretations of these facts when we work with Spec A/I,

at which point they will become more natural.

§76.8 A few harder problems to think about

Problem 76A. Let A = Z/2016Z, and consider the element 60 ∈ A. Compute A[1/60],
the localization of A away from 60.

Problem 76B (Injectivity of localizations). Let A be a ring and S ⊆ A a multiplicative
set. Find necessary and suﬃcient conditions for the map A → S−1A to be injective.
Problem 76C(cid:63) (Alluding to local rings). Let A be a ring, and p a prime ideal. How
many maximal ideals does Ap have?

Problem 76D. Let A be a ring such that Ap is an integral domain for every prime ideal
p of A. Must A be an integral domain?

77 Aﬃne schemes: the Zariski

topology

Now that we understand sheaves well, we can deﬁne an aﬃne scheme. It will be a

ringed space, so we need to deﬁne

 The set of points,

 The topology on it, and

 The structure sheaf on it.

In this chapter, we handle the ﬁrst two parts; Chapter 78 does the last one.

Quick note: Chapter 79 contains a long list of examples of aﬃne schemes. So if
something written in this chapter is not making sense, one thing worth trying is skimming
through the next chapter to see if any of the examples there are more helpful.

§77.1 Some more advertising

Let me describe what the construction of Spec A is going to do.

In the case of An, we used Cn as the set of points and C[x1, . . . , xn] as the ring of
functions but then remarked that the set of points of Cn corresponded to the maximal
ideals of C[x1, . . . , xn]. In an aﬃne scheme, we will take an arbitrary ring A, and generate
the entire structure from just A itself. The ﬁnal result is called Spec A, the spectrum of
A. The aﬃne varieties V(I) we met earlier will just be C[x1, . . . , xn]/I, but now we will
be able to take any ideal I, thus ﬁnally completing the table at the end of the “aﬃne
variety” chapter.

The construction of the aﬃne scheme in this way will have three big generalizations:

1. We no longer have to work over an algebraically closed ﬁeld C, or even a ﬁeld at
all. This will be the most painless generalization: you won’t have to adjust your
current picture much for this to work.

2. We allow non-radical ideals: Spec C[x]/(x2) will be the double point we sought for

so long. This will let us formalize the notion of a “fat” or “fuzzy” point.

3. Our aﬃne schemes will have so-called non-closed points: points which you can
visualize as ﬂoating around, somewhere in the space but nowhere in particular.
(They’ll correspond to prime non-maximal ideals.) These will take the longest to
get used to, but as we progress we will begin to see that these non-closed points
actually make life easier, once you get a sense of what they look like.

§77.2 The set of points

Prototypical example for this section: Spec C[x1, . . . , xn]/I.

First surprise, for a ring A:

Deﬁnition 77.2.1. The set Spec A is deﬁned as the set of prime ideals of A.

727

728

Napkin, by Evan Chen (v1.5.20190718)

This might be a little surprising, since we might have guessed that Spec A should just
have the maximal ideals. What do the remaining ideals correspond to? The answer is
that they will be so-called non-closed points or generic points which are “somewhere” in
the space, but nowhere in particular. (The name “non-closed” is explained next chapter.)

Remark 77.2.2 — As usual A itself is not a prime ideal, but (0) is prime if and
only if A is an integral domain.

Example 77.2.3 (Examples of spectrums)
(a) Spec C[x] consists of a point (x − a) for every a ∈ C, which correspond to what
we geometrically think of as A1. In additionally consists of a point (0), which
we think of as a “non-closed point”, nowhere in particular.

(b) Spec C[x, y] consists of points (x − a, y − b) (which are the maximal ideals) as
well as (0) again, a non-closed point that is thought of as “somewhere in C2, but
nowhere in particular”. It also consists of non-closed points corresponding to
irreducible polynomials f (x, y), for example (y − x2), which is a “generic point
on the parabola”.

(c) If k is a ﬁeld, Spec k is a single point, since the only maximal ideal of k is (0).

Example 77.2.4 (Complex aﬃne varieties)
Let I ⊆ C[x1, . . . , xn] be an ideal. By Proposition 76.6.1, the set

Spec C[x1, . . . , xn]/I

consists of those prime ideals of C[x1, . . . , xn] which contain I: in other words, it
has a point for every closed irreducible subvariety of V(I). So in addition to the
“geometric points” (corresponding to the maximal ideals (x1 − a1, . . . , xn − an) we
have non-closed points along each of the varieties).

The non-closed points are the ones you are not used to: there is one for each non-
maximal prime ideal (visualized as “irreducible subvariety”). I like to visualize them in
my head like a ﬂy: you can hear it, so you know it is ﬂoating somewhere in the room, but
as it always moving, you never know exactly where. So the generic point of Spec C[x, y]
corresponding to the prime ideal (0) is ﬂoating everywhere in the plane, the one for the
ideal (y − x2) ﬂoats along the parabola, etc.

Image from [Wa].

77 Aﬃne schemes: the Zariski topology

729

Example 77.2.5 (More examples of spectrums)
(a) Spec Z consists of a point for every prime p, plus a generic point that is somewhere,

but no where in particular.

(b) Spec C[x]/(x2) has only (x) as a prime ideal. The ideal (0) is not prime since

0 = x · x. Thus as a topological space, Spec C[x]/(x2) is a single point.

(c) Spec Z/60Z consists of three points. What are they?

§77.3 The Zariski topology on the spectrum

Prototypical example for this section: Still Spec C[x1, . . . , xn]/I.

Now, we endow a topology on Spec A. Since the points on Spec A are the prime ideals,

we continue the analogy by thinking of the points f as functions on Spec A. That is:
Deﬁnition 77.3.1. Let f ∈ A and p ∈ Spec A. Then the value of f at p is deﬁned to
be f (mod p), an element of A/p. We denote it f (p).

Example 77.3.2 (Vanishing locii in An)
Suppose A = C[x1, . . . , xn], and m = (x1 − a1, x2 − a2, . . . , xn − an) is a maximal
ideal of A. Then for a polynomial f ∈ C,

f

(mod m) = f (a1, . . . , an)

with the identiﬁcation that C/m ∼= C.

Example 77.3.3 (Functions on Spec Z)
Consider A = Spec Z. Then 2019 is a function on A. Its value at the point (5) is 4
(mod 5); its value at the point (7) is 3 (mod 7).

Indeed if you replace A with C[x1, . . . , xn] and Spec A with An in everything that

follows, then everything will become quite familiar.
Deﬁnition 77.3.4. Let f ∈ A. We deﬁne the vanishing locus of f to be

V(f ) = {p ∈ Spec A | f (p) = 0} = {p ∈ Spec A | f ∈ p} .

More generally, just as in the aﬃne case, we deﬁne the vanishing locus for an ideal I as

V(I) = {p ∈ Spec A | f (p) = 0 ∀f ∈ I}

= {p ∈ Spec A | f ∈ p ∀f ∈ I}
= {p ∈ Spec A | I ⊆ p} .

Finally, we deﬁne the Zariski topology on Spec A by declaring that the sets of the form
V(I) are closed.

We now deﬁne a few useful topological notions:

Deﬁnition 77.3.5. Let X be a topological space. A point p ∈ X is a closed point if
the set {p} is closed.

730

Napkin, by Evan Chen (v1.5.20190718)

Question 77.3.6 (Mandatory). Show that a point (i.e. prime ideal) m ∈ Spec A is a closed
point if and only if m is a maximal ideal.

Recall also in Deﬁnition 7.2.4 we denote by S the closure of a set S (i.e. the smallest
closed set containing S); so you can think of a closed point p also as one whose closure is
just {p}, while with a generic point Therefore the Zariski topology lets us refer back to
the old “geometric” as just the closed points.

Example 77.3.7 (Non-closed points, continued)
Let A = C[x, y] and let p = (y − x2) ∈ Spec A; this is the “generic point” on a
parabola. It is not closed, but we can compute its closure:

{p} = V(p) = {q ∈ Spec A | q ⊇ p} .

This closure contains the point p as well as several maximal ideals q, such as
(x − 2, y − 4) and (x − 3, y − 9). In other words, the closure of the “generic point”
of the parabola is literally the set of all points that are actually on the parabola
(including generic points).

That means the way to picture p is a point that is ﬂoating “somewhere on the
parabola”, but nowhere in particular. It makes sense then that if we take the closure,
we get the entire parabola, since p “could have been” any of those points.

Example 77.3.8 (The generic point of the y-axis isn’t on the x-axis)
Let A = C[x, y] again. Consider V(y), which is the x-axis of Spec A. Then consider
p = (x), which is the generic point on the y-axis. Observe that

p /∈ V(y).

The geometric way of saying this is that a generic point on the y-axis does not lie
on the x-axis.

We now also introduce one more word:

Deﬁnition 77.3.9. A topological space X is irreducible if either of the following two
conditions hold:

 The space X cannot be written as the union of two proper closed subsets.

 Any two nonempty open sets of X intersect.

A subset Z of X (usually closed) is irreducible if it is irreducible as a subspace.

yx(y−x2)(x+1,y−1)77 Aﬃne schemes: the Zariski topology

731

Exercise 77.3.10. Show that the two conditions above are indeed equivalent. Also, show
that the closure of a set is always irreducible.

This is the analog of the “irreducible” we deﬁned for aﬃne varieties, but it is now a
topological deﬁnition, although in practice this deﬁnition is only useful for spaces with
the Zariski topology. Indeed, if any two nonempty open sets intersect (and there is more
than one point), the space is certainly not Hausdorﬀ! As with our old aﬃne varieties,
the intuition is that V(xy) (the union of two lines) should not be irreducible.

Example 77.3.11 (Reducible and irreducible spaces)
(a) The closed set V(xy) = V(x) ∪ V(y) is reducible.
(b) The entire plane Spec C[x, y] is irreducible. There is actually a simple (but
counter-intuitive, since you are just getting used to generic points) reason why
this is true: the generic point (0) is in every open set, ergo, any two open sets
intersect.

So actually, the generic points kind of let us cheat our way through the following

bit:

Proposition 77.3.12 (Spectrums of integral domains are irreducible)

If A is an integral domain, then Spec A is irreducible.

Proof. Just note (0) is a prime ideal, and in every open set.

You should compare this with our old classical result that C[x1, . . . , xn]/I was irreducible
as an aﬃne variety exactly when I was prime. This time, the generic point actually takes
care of the work for us: the fact that it is allowed to ﬂoat anywhere in the plane lets us
capture the idea that A2 should be irreducible without having to expend any additional
eﬀort.

Remark 77.3.13 — Surprisingly, the converse of this proposition is false: we have
seen Spec C[x]/(x2) has only one point, so is certainly irreducible. But A = C[x]/(x2)
is not an integral domain. So this is one weird-ness introduced by allowing “non-
radical” behavior.

At this point you might notice something:

Theorem 77.3.14 (Points are in bijection with irreducible closed sets)

Consider X = Spec A. For every irreducible closed set Z, there is exactly one point
p such that Z = {p}. (In particular points of X are in bijection with closed subsets
of X.)

Idea of proof. The point p corresponds to the closed set V(p), which one can show is
irreducible.

This gives you a better way to draw non-closed points: they are the generic points lying
along any irreducible closed set (consisting of more than just one point).

732

Napkin, by Evan Chen (v1.5.20190718)

At this point1, I may as well give you the real deﬁnition of generic point.

Deﬁnition 77.3.15. Given a topological space X, a generic point η is a point whose
closure is the entire space X.

So for us, when A is an integral domain, Spec A has generic point (0).

Abuse of Notation 77.3.16. Very careful readers might note I am being a little careless
with referring to (y−x2) as “the generic point along the parabola” in Spec C[x, y]. What’s
happening is that V(y − x2) is a closed set, and as a topological subspace, it has generic
point (y − x2).

§77.4 On radicals

Back when we studied classical algebraic geometry in Cn, we saw Hilbert’s Nullstellensatz
(Theorem 70.3.4) show up to give bijections between radical ideals and aﬃne varieties;
we omitted the proof, because it was nontrivial.

However, for a scheme, where the points are prime ideals (rather than tuples in Cn),
the corresponding results will actually be easy: even in the case where A = C[x1, . . . , xn],
the addition of prime ideals (instead of just maximal ideals) will actually simplify the
proof, because radicals play well with prime ideals.

We still have the following result.

Proposition 77.4.1 (V(√I) = V(I))
For any ideal I of a ring A we have V(√I) = V(I).
Proof. We have √I ⊇ I. Hence automatically V(√I) ⊆ V(I).
Conversely, if p ∈ V(I), then I ⊆ p, so √I ⊆ √p = p (by Proposition 70.3.3).

We hinted the key result in an earlier remark, and we now prove it.

Theorem 77.4.2 (Radical is intersection of primes)

Let I be an ideal of a ring A. Then

√I = (cid:92)p⊇I

p.

Proof. This is a famous statement from commutative algebra, and we prove it here only
for completeness. It is “doing most of the work”.

Note that if I ⊆ p, then √I ⊆ √p = p; thus √I ⊆(cid:84)p⊇I p.
Conversely, suppose x /∈ √I, meaning 1, x, x2, x3, . . . /∈ I. Then, consider the localiza-
I but not containing x. Thus x /∈(cid:84)p⊇I p, as desired.

tion A[1/x]. Like any ring, it has some maximal ideal (Krull’s theorem). This means our
usual bijection between prime ideals of A[1/x] gives some prime ideal p of A containing

1Pun not intended

77 Aﬃne schemes: the Zariski topology

733

Remark 77.4.3 (A variant of Krull’s theorem) — The longer direction of this proof
is essentially saying that for any x ∈ A, there is a maximal ideal of A not containing
x. The “short” proof is to use Krull’s theorem on A[1/x] as above, but one can
also still prove it directly using Zorn’s lemma (by copying the proof of the original
Krull’s theorem).

Example 77.4.4 ((cid:112)(2016) = (42) in Z)
In the ring Z, we see that (cid:112)(2016) = (42), since the distinct primes containing
Geometrically, this gives us a good way to describe √I: it is the set of all functions

(2016) are (2), (3), (7).

vanishing on all of V(I). Indeed, we may write

{f ∈ A | f (p) = 0} .

√I = (cid:92)p⊃I

p = (cid:92)p∈V(I)

p = (cid:92)p∈V(I)

We can now state:

Theorem 77.4.5 (Radical ideals correspond to closed sets)

Let I and J be ideals of A, and considering the space Spec A. Then

V(I) = V(J) ⇐⇒

√I = √J.

In particular, radical ideals exactly correspond to closed subsets of Spec A.

Proof. If V(I) = V(J), then √I =(cid:84)p∈V(I) p =(cid:84)p∈V(J) p = √J as needed.
Conversely, suppose √I = √J. Then V(I) = V(√I) = V(√J) = V(J).

Compare this to the theorem we had earlier that the irreducible closed subsets corre-

spond to prime ideals!

§77.5 A few harder problems to think about

As Chapter 79 contains many examples of aﬃne schemes to train your intuition, it’s
possibly worth reading even before attempting these problems, even though there will be
some parts that won’t make sense yet.

Problem 77A (Spec Q[x]). Describe the points and topology of Spec Q[x].
Problem 77B (Product rings). Describe the points and topology of Spec A× B in terms
of Spec A and Spec B.

Problem 77C. Show that if A is not an integral domain, then Spec A is not irreducible.

78 Aﬃne schemes: the sheaf

it into a ringed space. This is done quickly in the ﬁrst section.

We now complete our deﬁnition of X = Spec A by deﬁning the sheaf OX on it, making
However, we will then spend the next several chapters trying to convince the reader to
forget the deﬁnition we gave, in practice. This is because practically, the sections of the
sheaves are best computed by not using the deﬁnition directly, but by using some other
results.

Along the way we’ll develop some related theory: in computing the stalks we’ll ﬁnd
out the deﬁnition of a local ring, and in computing the sections we’ll ﬁnd out about
distinguished open sets.

A reminder once again: Chapter 79 has many more concrete examples. It’s not a bad

idea to look through there for more examples if anything in this chapter trips you up.

§78.1 A useless deﬁnition of the structure sheaf

Prototypical example for this section: Still C[x1, . . . , xn]/I.

We have now endowed Spec A with the Zariski topology, and so all that remains is to
put a sheaf OSpec A on it. To do this we want a notion of “regular functions” as before.

This is easy to do since we have localizations on hand.

Deﬁnition 78.1.1. First, let F be the pre-sheaf of “globally rational” functions: i.e.
we deﬁne F (U ) to be the localization

F (U ) =(cid:26) f

g | f, g ∈ A and g(p) (cid:54)= 0 ∀p ∈ U(cid:27) =A \ (cid:91)p∈U

−1

p

A.

We now deﬁne the structure sheaf on Spec A. It is

i.e. the sheaﬁﬁcation of the F we just deﬁned.

OSpec A = F sh

Exercise 78.1.2. Compare this with the deﬁnition for OV with V a complex variety, and
check that they essentially match.

And thus, we have completed the transition to adulthood, with a complete deﬁnition of
the aﬃne scheme.

If you really like compatible germs, you can write out the deﬁnition:

Deﬁnition 78.1.3. Let A be a ring. Then Spec A is made into a ringed space by setting

OSpec A(U ) = {(fp ∈ Ap)p∈U which are locally quotients} .

That is, it consists of sequence (fp)p∈U , with each fp ∈ Ap, such that for every point p
there is an open neighborhood Up and an f, g ∈ A such that fq = f
g ∈ Aq for all q ∈ Up.

735

736

Napkin, by Evan Chen (v1.5.20190718)

We will now basically forget about this deﬁnition, because we will never use it

in practice. In the next two sections, we will show you:

 that the stalks OSpec A,p are just Ap, and
 that the sections OSpec A(U ) can be computed, for any open set U , by focusing only

on the special case where U = D(f ) is a distinguished open set.

These two results will be good enough for all of our purposes, so we will be able to not
use this deﬁnition. (Hence the lack of examples in this section.)

§78.2 The value of distinguished open sets (or: how to

actually compute sections)

Prototypical example for this section: D(x) in Spec C[x] is the punctured line.

We will now really hammer in the importance of the distinguished open sets. The

deﬁnition is analogous to before:

Deﬁnition 78.2.1. Let f ∈ Spec A. Then D(f ) is the set of p such that f (p) (cid:54)= 0, a
distinguished open set.

Distinguished open sets will have three absolutely crucial properties, which build on

each other.

§78.2.i A basis of the Zariski topology

The ﬁrst is a topological observation:

Theorem 78.2.2 (Distinguished open sets form a base)

The distinguished open sets D(f ) form a basis for the Zariski topology: any open
set U is a union of distinguished open sets.

Proof. Let U be an open set; suppose it is the complement of closed set V (I). Then
verify that

U = (cid:91)f∈I

D(f ).

§78.2.ii Sections are computable

The second critical fact is that the sections on distinguished open sets can be computed
explicitly.

Theorem 78.2.3 (Sections of D(f ) are localizations away from f )
Let A be a ring and f ∈ A. Then

OSpec A(D(f )) ∼= A[1/f ].

Proof. Omitted, but similar to Theorem 71.6.1.

78 Aﬃne schemes: the sheaf

737

Example 78.2.4 (The punctured line is isomorphic to a hyperbola)
The “hyperbola eﬀect” appears again:

OSpec C[x](D(x)) = C[x, x−1] ∼= C[x, y]/(xy − 1).

On a tangential note, we had better also note somewhere that Spec A = D(1) is itself

distinguished open, so the global sections can be recovered.

Corollary 78.2.5 (A is the ring of global sections)
The ring of global sections of Spec A is A.

Proof. By previous theorem, OSpec A(Spec A) = OSpec A(D(1)) = A[1/1] = A.

§78.2.iii They are aﬃne

We know OX (D(f )) = A[1/f ]. In fact, if you draw Spec A[1/f ], you will ﬁnd that it
looks exactly like D(f ). So the third ﬁnal important fact is that D(f ) will actually
be isomorphic to Spec A[1/f ] (just like the line minus the origin is isomorphic to the
hyperbola). We can’t make this precise yet, because we have not yet discussed morphisms
of schemes, but it will be handy later (though not right away).

§78.2.iv Classic example: the punctured plane

We now give the classical example of a computation which shows how you can forget
about sheaﬁﬁcation, if you never liked it.1 The idea is that:

We can compute any section OX (U ) in practice by using distinguished
open sets and sheaf axioms.

Let X = Spec C[x, y], and consider the origin, i.e. the point m = (x, y). This ideal
is maximal, so it corresponds to a closed point, and we can consider the open set U
consisting of all the points other than m. We wish to compute OX (U ).

1This perspective is so useful that some sources, like Vakil [Va17, §4.1] will deﬁne OSpec A by requiring

OSpec A(D(f )) = A[1/f ], rather than use sheaﬁﬁcation as we did.

V(x)V(y)m=(x,y)738

Napkin, by Evan Chen (v1.5.20190718)

Unfortunately, U is not distinguished open. But, we can compute it anyways by writing
U = D(x) ∪ D(y): conveniently, D(x) ∩ D(y) = D(xy). By the sheaf axioms, we have a
pullback square

OX (U )

OX (D(x)) = C[x, y, x−1]

OX (D(x)) = C[x, y, y−1]y

OX (D(xy)) = C[x, y, x−1, y−1].

In other words, OX (U ) consists of pairs

f ∈ C[x, y, x−1]
g ∈ C[x, y, y−1]

which agree on the overlap: f = g on D(x) ∩ D(y). Well, we can describe f as a
polynomial with some x’s in the denominator, and g as a polynomial with some y’s in
the denominator. If they match, the denominator is actually constant. Put crudely,

C[x, y, x−1] ∩ C[x, y, y−1] = C[x, y].

In conclusion,

That is, we get no additional functions.

OX (U ) = C[x, y].

§78.3 The stalks of the structure sheaf

Prototypical example for this section: The stalk of Spec C[x, y] at m = (x, y) are rational
functions deﬁned at the origin.

Don’t worry, this one is easier than last section.

§78.3.i They are localizations

Theorem 78.3.1 (Stalks of Spec A are Ap)
Let A be a ring and let p ∈ Spec A. Then

In particular X is a locally ringed space.

OSpec A,p ∼= Ap.

Proof. Since sheaﬁﬁcation preserved stalks, it’s enough to check it for F the pre-sheaf
of globally rational functions in our deﬁnition. The proof is basically the same as
Theorem 75.3.7: there is an obvious map Fp → Ap on germs by

(U, f /g ∈ F (U )) (cid:55)→ f /g ∈ Ap.

(Note the f /g on the left lives in F (U ) but the one on the right lives in Ap). We show
injectivity and surjectivity:

 Injective: suppose (U1, f1/g1) and (U2, f2/g2) are two germs with f1/g1 = f2/g2 ∈
Ap. This means h(g1f2 − f2g1) = 0 in A, for some nonzero h. Then both germs
identify with the germ (U1 ∩ U2 ∩ D(h), f1/g1).

78 Aﬃne schemes: the sheaf

 Surjective: let U = D(g).

739

Example 78.3.2 (Denominators not divisible by x)
We have seen this example so many times that I will only write it in the new notation,
and make no further comment: if X = Spec C[x] then

OSpec X,(x) = C[x](x) =(cid:26) f

g | g(0) (cid:54)= 0(cid:27) .

Example 78.3.3 (Denominators not divisible by x)
Let X = Spec C[x, y] and let m = (x, y) be the origin. Then

C[x, y](x,y) =(cid:26) f (x, y)

g(x, y) | g(0, 0) (cid:54)= 0(cid:27) .

If you want more examples, take any of the ones from Section 76.4, and try to think

about what they mean geometrically.

§78.3.ii Motivating local rings: germs should package values

Let’s return to our well-worn example X = Spec C[x, y] and consider m = (x, y) the
origin. The stalk was

OX,m = C[x, y](x,y) =(cid:26) f (x, y)

g(x, y) | g(0, 0) (cid:54)= 0(cid:27) .

So let’s take some section like f = 1
xy+4 , which is a section of U = D(xy + 4) (or some
smaller open set, but we’ll just use this one for simplicity). We also have U (cid:51) m, and so
f gives a germ at m.
4 . And in general, the

On the other hand, f also has a value at m: it is f (mod m) = 1
ring of possible values of a section at the origin m is C[x, y]/m ∼= C.

Now, you might recall that I pressed the point of view that a germ might be thought
of as an “enriched value”. Then it makes sense that if you know the germ of a section f
at a point m — i.e., you know the “enriched value” — then you should be able to value
as well. What this means is that we ought to have some map

sending germs to their associated values.
Indeed you can, and this leads us to. . .

Am → A/m

§78.4 Local rings and residue ﬁelds: linking germs to values

Prototypical example for this section: The residue ﬁeld of Spec C[x, y] at m = (x, y) is C.

§78.4.i Localizations give local rings

This notation is about to get really terrible, but bear with me.

740

Napkin, by Evan Chen (v1.5.20190718)

Theorem 78.4.1 (Stalks are local rings)

Let A be a ring and p any prime ideal. Then the localization Ap has exactly one
maximal ideal, given explicitly by

pAp =(cid:26) f

g | f ∈ p g /∈ p(cid:27) .

The ideal pAp thus captures the idea of “germs vanishing at p”.2

Proof in a moment; for now let’s introduce some words so we can give our examples in

the proper language.

Deﬁnition 78.4.2. A ring R with exactly one maximal ideal m will be called a local
ring. The residue ﬁeld is the quotient A/m.

Question 78.4.3. Are ﬁelds local rings?

Thus what we ﬁnd is that:

The stalks consist of the possible enriched values (germs); the residue
ﬁeld is the set of (un-enriched) values.

Example 78.4.4 (The stalk at the origin of Spec C[x, y])
Again set A = C[x, y], X = Spec A and p = (x, y) so that OX,p = Ap. (I switched to
p for the origin, to avoid confusion with the maximal ideal pAp of the local ring Ap.)
As we said many times already, Ap consists of rational functions not vanishing at
the origin, such as f = 1

xy+4 .

What is the unique maximal ideal pAp? Answer:

it consists of the rational
functions which vanish at the origin: for example,
4(xy+4) . If we
allow ourselves to mod out by such functions, we get the residue ﬁeld C, and f will
have the value 1

x2+3y , or 3x+5y

, or −xy

x

2

4 , since

1

xy + 4 −

−xy

4(xy + 4)

=

1
4

.

vanishes at origin

(cid:124)

(cid:123)(cid:122)

(cid:125)

More generally, suppose f is any section of some open set containing p. Let c ∈ C
be the value f (p), that is, f (mod p). Then f − c is going to be another section
which vanishes at the origin p, so as promised, f ≡ c (mod pAp).

Okay, we can write down a proof of the theorem now.

Proof of Theorem 78.4.1. One may check set I = pAp is an ideal of Ap. Moreover, 1 /∈ I,
so I is proper.
To prove it is maximal and unique, it suﬃces to prove that any f ∈ Ap with f /∈ I is a
unit of Ap. This will imply I is maximal: there are no more non-units to add. It will
2The notation pAp really means the set of f · h where f ∈ p (viewed as a subset of Ap by f (cid:55)→ f

h ∈ Ap. I personally ﬁnd this is more confusing than helpful, so I’m footnoting it.

1 ) and

78 Aﬃne schemes: the sheaf

741

also imply I is the only maximal ideal: because any proper ideal can’t contain units, so
is contained in I.

This is actually easy. An element of Ap not in I must be x = f

f, g /∈ p. For such an element, x−1 = g

f /∈ p too. So x is a unit. End proof.

g for f, g ∈ A and

Even more generally:

If a sheaf F consists of “ﬁeld-valued functions”, the stalk Fp probably
has a maximal ideal consisting of the germs vanishing at p.

Example 78.4.5 (Local rings in non-algebaric geometry sheaves)
Let’s go back to the example of X = R and F (U ) the smooth functions, and consider
the stalk Fp, where p ∈ X. Deﬁne the ideal mp to be the set of germs (s, U ) for
which s(p) = 0.

Then mp is maximal: we have an exact sequence

0 → mp → Fp

(s,U )(cid:55)→s(p)
−−−−−−−→ R → 0

and so Fp/mp ∼= R, which is a ﬁeld.

It remains to check there are no nonzero maximal ideals. Now note that if s /∈ mp,
then s is nonzero in some open neighborhood of p, and one can construct the function
1/s on it. So every element of Fp \ mp is a unit; and again mp is in fact the only
maximal ideal!

Thus the stalks of each of the following types of sheaves are local rings, too.

 Sheaves of continuous real/complex functions on a topological space

 Sheaves of smooth functions on any manifold

 etc.

§78.4.ii Computing values: a convenient square

Very careful readers might have noticed something a little uncomfortable in our extended
example with Spec A with A = C[x, y] and p = (x, y) the origin. Let’s consider f = 1
xy+4 .
We took f (mod x, y) in the original ring A in order to decide the value “should” be 1
4 .
However, all our calculations actually took place not in the ring A, but instead in the
ring Ap. Does this cause issues?

Thankfully, no, nothing goes wrong, even in a general ring A.

Deﬁnition 78.4.6. We let the quotient Ap/pAp, i.e. the residue ﬁeld of the stalk of
Spec A at p, be denoted by κ(p).

The following result from commutative algebra shows that the order doesn’t mat-

ter.

742

Napkin, by Evan Chen (v1.5.20190718)

Theorem 78.4.7 (The germ-to-value square)

Let A be a ring and p a prime ideal. The following diagram commutes:

localize

A

Ap

mod p

mod p

A/p

Frac(−)

κ(p)

In particular, κ(p) can also be described as Frac(A/p).

So for example, if A = C[x, y] and p = (x, y), then A/p = C and Frac(Ap) = Frac(C) = C,
as we expected. In practice, Frac(A/p) is probably the easier way to compute κ(p) for
any prime ideal p.

§78.5 Recap

To recap the last two chapters, let A be a ring.

 We deﬁne X = Spec A to be the set of prime ideals of A.

– The maximal ideals are the “closed points” we are used to, but the prime

ideals are “generic points”.

 We equip Spec A with the Zariski topology by declaring V(I) to be the closed sets,

for ideals I ⊆ A.

– The distinguished open sets D(f ), form a topological basis.
– The irreducible closed sets are exactly the closures of points.
 Finally, we deﬁned a sheaf OX . We set up the deﬁnition such that

– OX (D(f )) = A[1/f ]: at distinguished open sets D(f ), we get localizations

too.

– OX,p = Ap: the stalks are localizations at a prime.

Since D(f ) is a basis, these two properties lets us explicitly compute OX (U ) for
any open set U , so we don’t have to resort to the deﬁnition using sheaﬁﬁcation.

§78.6 Functions are determined by germs, not values

Prototypical example for this section: The functions 0 and x on Spec C[x]/(x2).

We close the chapter with a word of warning.

In any ringed space, a section is
determined by its germs; so that on Spec A a function f ∈ A is determined by its germ
in each stalk Ap. However, we now will mention that an f ∈ A is not determined by its
value f (p) = f (mod p) at each point.

The famous example is:

Example 78.6.1 (On the double point, all multiples of x are zero at all points)
The space Spec C[x]/(x2) has only one point, (x). The functions 0 and x (and for that
matter 2x, 3x, . . . ) all vanish on it. This shows that functions are not determined
uniquely by values in general.

78 Aﬃne schemes: the sheaf

743

Fortunately, we can explicitly characterize when this sort of “bad” behavior happens.
Indeed, we want to see when f (p) = g(p) for every p, or equivalently, h = f − g vanishes
on every prime ideal p. This is equivalent to having

h ∈(cid:92)p

p =(cid:112)(0)

the radical of the zero ideal. Thus in the prototype, the failure was caused by the fact
that xn = 0 for some large n.

nilradical of A. Elements of the nilradical are called nilpotents. We say A is reduced

Deﬁnition 78.6.2. For a ring A, the radical of the zero ideal, (cid:112)(0), is called the
if 0 is the only nilpotent, i.e.(cid:112)(0) = (0).

Question 78.6.3. Are integral domains reduced?

Then our above discussion gives:

Theorem 78.6.4 (Nilpotents are the only issue)

Two functions f and g have the same value on all points of Spec A if and only if
f − g is nilpotent.

In particular, when A is a reduced ring, even the values f (p) as p ∈ Spec A are enough
to determine f ∈ A.

§78.7 A few harder problems to think about

As Chapter 79 contains many examples of aﬃne schemes to train your intuition; it’s
likely to be worth reading even before attempting these problems.

Problem 78A† (Spectrums are quasicompact). Show that Spec A is quasicompact for
any ring A.

Problem 78B (Punctured gyrotop, communicated by Aaron Pixton). The gyrotop is
the scheme X = Spec C[x, y, z]/(xy, z). We let U denote the open subset obtained by
deleting the closed point m = (x, y, z). Compute OX (U ).
Problem 78C. Show that a ring R is a local ring if and only of the following property
is true: for any x ∈ R, either x or 1 − x is a unit.
Problem 78D. Let R be a local ring, and m be its maximal ideal. Describe Rm.

Problem 78E. Let A be a ring, and m a maximal ideal. Consider m as point of Spec A
Show that κ(m) ∼= A/m.

79 Interlude: eighteen examples of

aﬃne schemes

To cement in the previous two chapters, we now give an enormous list of examples.

Each example gets its own section, rather than having page-long orange boxes.

One common theme you will ﬁnd as you wade through the examples is that your
geometric intuition may be better than your algebraic one. For example, while studying
k[x, y]/(xy) you will say “geometrically, I expect so-and-so to look like other thing”, but
when you write down the algebraic statements you ﬁnd two expressions that are don’t
look equal to you. However, if you then do some calculation you will ﬁnd that they were
isomorphic after all. So in that sense, in this chapter you will learn to begin drawing
pictures of algebraic statements — which is great!

As another example, all the lemmas about prime ideals from our study of localizations

will begin to now take concrete forms: you will see many examples that

 Spec A/I looks like V(I) of Spec A,
 Spec A[1/f ] looks like D(f ) of Spec A,

 Spec Ap looks like OSpec A,p of Spec A.
In everything that follows, k is any ﬁeld. We will also use the following color connota-

tions:

 The closed points of the scheme are drawn in blue.

 Non-closed points are drawn in red, with their “trails” dotted red.

 Stalks are drawn in green, when they appear.

§79.1 Example: Spec k, a single point

This one is easy: for any ﬁeld k, X = Spec k has a single point, corresponding to the
only proper ideal (0). There is only way to put a topology on it.

As for the sheaf,

OX (X) = OX,(0) = k.

So the space is remembering what ﬁeld it wants to be over. If we are complex analysists,
the set of functions on a single point is C; if we are number theoerists, maybe the set of
functions on a single point is Q.

§79.2 Spec C[x], a one-dimensional line
The scheme X = Spec C[x] is our beloved one-dimensional line. It consists of two types
of points:

 The closed points (x − a), corresponding to each complex number a ∈ C, and
 The generic point (0).

745

746

Napkin, by Evan Chen (v1.5.20190718)

As for the Zariski topology, every open set contains (0), which captures the idea it is
close to everywhere: no matter where you stand, you can still here the buzzing of the ﬂy!
True to the irreducibility of this space, the open sets are huge: the proper closed sets
consist of ﬁnitely many closed points.

Here is a picture: for lack of better place to put it, the point (0) is ﬂoating around just

above the line in red.

The notion of “value at p” works as expected. For example, f = x2 + 5 is a global
section of C[x]. If we evaluate it at p = x − 3, we ﬁnd f (p) = f (mod p) = x2 + 5
(mod x − 3) = 14 (mod x − 3). Indeed,

κ(p) ∼= C
meaning the stalks all have residue ﬁeld C. As

C[x]/p ∼= C

by x (cid:55)→ 3

we see we are just plugging x = 3.

Of course, the stalk at (x − 3) carries more information. In this case it is C[x](x−3).
Which means that if we stand near the point (x − 3), rational functions are all ﬁne as
long as no x − 3 appears in the denominator. So,
(x−1)(x−5) is a ﬁne example of a germ
near x = 3.

x2+8

of rational functions. And that’s what you expect. For example,
describes a rational function on “most” complex numbers.

Things get more interesting if we consider the generic point η = (0).
What is the stalk OX,η? Well, it should be C[x](0) = C(x), which is the again the set
(x−1)(x−5) certainly
What happens if we evaluate the global section f = x2 + 5 at η? Well, we just get
f (η) = x2 + 5 — taking modulo 0 doesn’t do much. Fitting, it means that if you want to
be able to evaluate a polynomial f at a general complex number, you actually just need
the whole polynomial (or rational function). We can think of this in terms of the residue
ﬁeld being C(x):

x2+8

κ((0)) = Frac (C[x]/(0)) ∼= Frac C[x] = C(x).

§79.3 Spec R[x], a one-dimensional line with complex

conjugates glued (no fear nullstellensatz)

Despite appearances, this actually looks almost exactly like Spec C[x], even more than
you expect. The main thing to keep in mind is that now (x2 + 1) is a point, which you can
loosely think of as ±i. So it almost didn’t matter that R is not algebraically closed; the
C is showing through anyways. But this time, because we only consider real coeﬃcient
polynomials, we do not distinguish between “conjugate” +i and −i. Put another way, we
have folded a + bi and a − bi into a single point: (x + i) and (x − i) merge to form x2 + 1.

To be explicit, there are three types of points:

 (x − a) for each real number a
 (x2 − ax + b) if a2 < 4b, and

(x)(x−3)(x+i)(0)SpecC[x]79 Interlude: eighteen examples of aﬃne schemes

747

 the generic point (0), again.

The ideals (x − a) and (x2 − ax + b) are each closed points: the quotients with R[x] are
both ﬁelds (R and C, respectively).
We have been drawing Spec C[x] as a one-dimensional line, so Spec R[x] will be drawn

the same way.

One nice thing about this is that the nullstellensatz is less scary than it was with
classical varieties. The short version is that the function x2 + 1 vanishes at a point of
Spec R[x], namely (x2 + 1) itself! (So in some ways we’re sort of automatically working
with the algebraic closure.)

You might remember a long time ago we made a big fuss about the weak nullstellensatz,
for example in Problem 70B: if I was a proper ideal in C[x1, . . . , xn] there was some point
(a1, . . . , an) ∈ Cn such that f (a1, . . . , an) = 0 for all f ∈ I. With schemes, it doesn’t
matter anymore: if I is a proper ideal of a ring A, then some maximal ideal contains it,
and so V(I) is nonempty in Spec A.
some examples:

We better mention that the stalks this time look diﬀerent than expected. Here are

κ(cid:0)(x2 + 1)(cid:1) ∼= R[x]/(x2 + 1) ∼= C
κ ((x − 3)) ∼= R[x]/(x − 3) ∼= R

κ ((0)) ∼= Frac(R[x]/(0)) ∼= R(x)

Notice the residue ﬁelds above the “complex” points are bigger: functions on them take
values in C.

§79.4 Spec k[x], over any ground ﬁeld

In general, if k is the algebraic closure of k, then Spec k[x] looks like Spec k[x] with
all the Galois conjugates glued together. So we will almost never need “algebraically
closed” hypotheses anymore: we’re working with polynomial ideals, so all the elements
are implicitly there, anyways.

§79.5 Spec Z, a one-dimensional scheme
The great thing about Spec Z is that it basically looks like Spec k[x], too, being a
one-dimensional scheme. It has two types of prime ideals:

 (p), for every rational prime p,

 and the generic point (0).

So the picture almost does not change.

(x)(x−3)(x2+1)(0)SpecR[x](7)(3)(19)(0)SpecZ748

Napkin, by Evan Chen (v1.5.20190718)

This time η = (0) has stalk Z(0) = Q, so a “rational function” is literally a rational
number! Thus, 20
19 is a function with a double root at (2), a root at (5), and a simple
pole at (19). If we evaluate it at p = (7), we get 3 (mod 7). In general, the residue ﬁelds
are what you’d guess:

κ ((p)) = Z/(p) ∼= Fp

for each prime p, and κ ((0)) ∼= Q.

The stalk is bigger than the residue ﬁeld at the closed points: for example

OSpec Z,(3) ∼=(cid:110) m

n | 3 (cid:45) n(cid:111)

consists of rational numbers with no pole at 3. The stalk at the generic point is
Z(0) ∼= Frac Z = Q.

§79.6 Spec k[x]/(x2 − x), two points

If we were working with aﬃne varieties, you would already know what the answer is:
x2 − x = 0 has solutions x = 0 and x = 1, so this should be a scheme with two points.
To see this come true, we use Proposition 76.6.1: the points of Spec k[x]/(x2 − x)
should correspond to prime ideals of k[x] contained in (x2 − x). As k[x] is a PID, there
are only two, (x − 1) and (x). They are each maximal, since their quotient with k[x] is a
ﬁeld (namely k), so as promised Spec k[x]/(x2 − x) has just two closed points.
Each point has a stalk above it isomorphic to k. A section on the whole space X is
just a choice of two values, one at (x) and one at (x − 1).

So actually, this is a geometric way of thinking about the ring-theoretic fact that

k[x]/(cid:0)x2 − x(cid:1) ∼= k × k by f (cid:55)→ (f (0), f (1)) .

Also, this is the ﬁrst example of a reducible space in this chapter: in fact X is even
disconnected. Accordingly there is no generic point ﬂoating around: as the space is
discrete, all points are closed.

§79.7 Spec k[x]/(x2), the double point

We can now elaborate on the “double point” scheme

X2 = Spec k[x]/(x2)

since it is such an important motivating example. How it does diﬀer from the “one-point”
scheme X1 = Spec k[x]/(x) = Spec k? Both X2 and X1 have exactly one point, and so
obviously the topologies are the same too.

The diﬀerence is that the stalk (equivalently, the section, since we have only one point)

is larger:

OX2,(x) = OX2(X2) = k[x]/(x2).

kk(x)(x−1)Speck[x]/(x2−x)79 Interlude: eighteen examples of aﬃne schemes

749

So to specify a function on a double point, you need to specify two parameters, not just
one: if we take a polynomial

f = a0 + a1x + ··· ∈ k[x]

then evaluating it at the double point will remember both a0 and the “ﬁrst derivative”
say.

I should mention that if you drop all the way to the residue ﬁelds, you can’t tell the
diﬀerence between the double point and the single point anymore. For the residue ﬁeld
of Spec k[x]/(x2) at (x) is

Frac (A/(x)) = Frac k = k.

Thus the set of values is still just k (leading to the “nilpotent” discussion at the end of
last chapter); but the stalk, having “enriched” values, can tell the diﬀerence.

§79.8 Spec k[x]/(x3 − x), a double point and a single point

There is no problem putting the previous two examples side by side: the scheme X =
Spec k[x]/(x3 − x) consists of a double point next to a single point. Note that the stalks
are diﬀerent: the one above the double point is larger.

This time, we implicitly have the ring isomorphism

k[x]/(x3 − x) ∼= k[x]/(x2) × k

by f (cid:55)→ (f (0) + f(cid:48)(0)x, f (1)). The derivative is meant formally here!

§79.9 Spec Z/60Z, a scheme with three points

We’ve being seeing geometric examples of ring products coming up, but actually the
Chinese remainder theorem you are used to with integers is no diﬀerent. (This example
X = Spec Z/60Z is taken from [Va17, §4.4.11].)

By Proposition 76.6.1, the prime ideals of Z/60Z are (2), (3), (5). But you can think
of this also as coming out of Spec Z: as 60 was a function with a double root at (2), and
single roots at (3) and (5).

k[x]/(x2)k(x)(x−1)Speck[x]/(x3−x)750

Napkin, by Evan Chen (v1.5.20190718)

Actually, although I have been claiming the ring isomorphisms, the sheaves really actually
give us a full proof. Let me phrase it in terms of global sections:

Z/60Z = OX (X)

= OX ({(2)}) × OX ({(3)}) × OX ({(5)})
= OX,(2) × OX,(3) × OX,(5)
= Z/4Z × Z/3Z × Z/5Z.

So the theorem that OX (X) = A for X = Spec A is doing the “work” here; the sheaf
axioms then give us the Chinese remainder theorem from here.

On that note, this gives us a way of thinking about the earlier example that

(Z/60Z)[1/5] ∼= Z/12Z.

Indeed, Spec Z/60Z[1/5] is supposed to look like the distinguished open set D(5): which
means we delete the point (5) from the picture above. That leaves us with Z/12Z.

§79.10 Spec k[x, y], the two-dimensional plane

We have seen this scheme already: it is visualized as a plane. There are three types of
points:

 The closed points (x − a, y − b), which consists of single points of the plane.

 A non-closed point (f (x, y)) for any irreducible polynomial f , which ﬂoats along
some irreducible curve. We illustrate this by drawing the dotted curve along which
the point is ﬂoating.

 The generic point (0), ﬂoating along the entire plane. I don’t know a good place to
put it in the picture, so I’ll just put it somewhere and draw a dotted circle around
it.

Here is an illustration of all three types of points.

Z/4ZZ/3ZZ/5Z(2)(3)(5)SpecZ/60Z79 Interlude: eighteen examples of aﬃne schemes

751

We also go ahead and compute the stalks above each point.

 The stalk above (x − 1, y + 2) is the set of rational functions f (x,y)
g(1,−2) (cid:54)= 0.
 The stalk above the non-closed point (y − x2) the set of rational functions f (x,y)
such that g(t, t2) (cid:54)= 0. For example the function
x+y−2 is still ﬁne; despite the fact
it vanishes at the point (1, 1) and (−2, 4) on the parabola it is a function on a
“generic point” (crudely, “most points”) of the parabola.

g(x,y) such that

xy

g(x,y)

 The stalk above (0) is the entire fraction ﬁeld k(x, y) of rational functions.

Let’s consider the global section f = x2 + y2 and also take the value at each of the

above points.

 f (mod x − 1, y − 2) = 5, so f has value 5 at (x − 1, y + 2).

 The new bit is that we can think of evaluating f along the parabola too — it is
given a particular value in the quotient k[x, y]/(y − x2). We can think of it as
f = x2 + y2 ≡ x2 + x4 (mod y − x2) for example. Note that if we know the value
of f at the generic point of the parabola, we can therefore also evaluate it at any
closed point on the parabola.

 At the generic point (0), f (mod 0) = f . So “evaluating at the generic point” does

nothing, as in any other scheme.

§79.11 Spec Z[x], a two-dimensional scheme, and Mumford’s

picture

We saw Spec Z looked a lot like Spec k[x], and we will now see that Spec Z[x] looks a lot
like Spec k[x, y].

There is a famous picture of this scheme in Mumford’s “red book”, which I will produce
here for culture-preservation reasons, even though there are some discrepancies between
the pictures that we previously drew.

yx(y−x2)(x−1,y+2)(0)752

Napkin, by Evan Chen (v1.5.20190718)

Mumford uses [p] to denote the point p, which we don’t, so you can ignore the square
brackets that appear everywhere. The non-closed points are illustrated as balls of fuzz.
As before, there are there types of prime ideals, but they will look somewhat more

diﬀerent:

 The closed points are now pairs (p, f (x)) where p is a prime and f is an irreducible
polynomial modulo p. Indeed, these are the maximal ideals: the quotient Z[x]/(p, f )
becomes some ﬁnite extension of Fp.

 There are now two diﬀerent “one-dimensional” non-closed points:

– Each rational prime gives a point (p) and

– Each irreducible polynomial f gives a point (f ).

Indeed, note that the quotients of Z[x] by each are integral domains.

 Z[x] is an integral domain, so as always (0) is our generic point for the entire space.

There is one bit that I would do diﬀerently, in V(3) and V(7), there ought to be a point
(3, x2 + 1), which is not drawn as a closed point in the picture, but rather as dashed
oval. This is not right in the topological sense: as m = (3, x2 + 1) is a maximal ideal,
so it really is one closed point in the scheme. But the reason it might be thought of as
“doubled”, is that Z[x]/(3, x2 + 1), the residue ﬁeld at m, is a two-dimensional F3 vector
space.

§79.12 Spec k[x, y]/(y − x2), the parabola
By Proposition 76.6.1, the prime ideals of k[x, y]/(y−x2) correspond to the prime ideals of
k[x, y] which are supersets of (y − x2), or equivalently the points of Spec k[x, y] contained
inside the closed set V(y − x2). Moreover, the subspace topology on V(y − x2) coincides
with the topology on Spec k[x, y]/(y − x2).

79 Interlude: eighteen examples of aﬃne schemes

753

This holds much more generally:

Exercise 79.12.1 (Boring check). Show that if I is an ideal of a ring A, then Spec A/I is
homeomorphic as a topological space to the closed subset V(I) of Spec A.

So this is the notion of “closed embedding”: the parabola, which was a closed subset
of Spec k[x, y], is itself a scheme. It will be possible to say more about this, once we
actually deﬁne the notion of a morphism.

The sheaf on this scheme only remembers the functions on the parabola, though: the
stalks are not “inherited”, so to speak. To see this, let’s compute the stalk at the origin:
Theorem 76.7.1 tells us it is

k[x, y](x,y)/(y − x2) ∼= k[x, x2](x,x2) ∼= k[x](x)

which is the same as the stalk of the aﬃne line Spec k[x] at the origin. Intuitively, not
surprising; if one looks at any point of the parabola near the origin, it looks essentially
like a line, as do the functions on it.

the identiﬁcation that y = x2. Also unsurprising.

The stalk above the generic point is Frac(k[x, y]/(y − x2)): so rational functions, with
Finally, we note expect the parabola is actually isomorphic to Spec k[x], since there

is an isomorphism k[x, y]/(y − x2) ∼= k[x] by sending y (cid:55)→ x2. Pictorially, this looks like

“un-bending” the hyperbola. In general, we would hope that when two rings A and B
are isomorphic, then Spec A and Spec B should be “the same” (otherwise we would be
horriﬁed), and we’ll see later this is indeed the case.

§79.13 Spec Z[i], the Gaussian integers (one-dimensional)

You can play on this idea some more in the integer case. Note that

Z[i] ∼= Z[x]/(x2 + 1)

which means this is a “dimension-one” closed set within Spec Z[x]. In this way, we get a
scheme whose elements are Gaussian primes.

You can tell which closed points are “bigger” than others by looking at the residue

ﬁelds. For example the residue ﬁeld of the point (2 + i) is

κ ((2 + i)) = Z[i]/(2 + i) ∼= F5

Speck[x,y]/(y−x2)754

Napkin, by Evan Chen (v1.5.20190718)

but the residue ﬁeld of the point (3) is

κ ((3)) ∼= Z[i]/(3) ∼= F9

which is a degree two F3-extension.

§79.14 Long example: Spec k[x, y]/(xy), two axes

This is going to be our ﬁrst example of a non-irreducible scheme.

§79.14.i Picture

Like before, topologically it looks like the closed set V(xy) of Spec k[x, y]. Here is a
picture:

To make sure things are making sense:

Question 79.14.1 (Sanity check). Verify that (y+3) is really a maximal ideal of Spec k[x, y]/(xy)
lying in V(x).

The ideal (0) is longer prime, so it is not a point of this space. Rather, there are two
non-closed points this time: the ideals (x) and (y), which can be visualized as ﬂoating
around each of the two axes. This space is reducible, since it can be written as the union
of two proper closed sets, V(x) ∪ V(y). (It is still connected, as a topological space.)

§79.14.ii Throwing out the y-axis

Consider the distinguished open set U = D(x). This corresponds to deleting V(x), the
y-axis. Therefore we expect that D(x) “is” just Spec k[x] with the origin deleted, and in
particular that we should get k[x, x−1] for the sections. Indeed,

OSpec k[x,y]/(xy)(D(x)) ∼= (k[x, y]/(xy))[1/x]

∼= k[x, x−1, y]/(xy) ∼= k[x, x−1, y]/(y) ∼= k[x, x−1].

where (xy) = (y) follows from x being a unit. Everything as planned.

(x)(y)(x+2)(y+3)79 Interlude: eighteen examples of aﬃne schemes

755

§79.14.iii Stalks above some points

Let’s compute the stalk above the point m = (x + 2), which we think of as the point
(−2, 0) on the x-axis. (If it makes you more comfortable, note that m (cid:51) y(x + 2) = 2y
and hence y ∈ m, so we could also write m = (x + 2, y).) The stalk is

OSpec k[x,y]/(xy),m = k[x, y](x+2)/(xy).

This might look confusing until we realize that 1
unit; thus (xy) and (y) specify the same ideal and

x is an element of k[x, y](x+2), so x is a

OSpec k[x,y]/(xy),(x+2) ∼= k[x, y](x+2)/(y) ∼= k[x](x+2)

which anyways looks just like the stalk of (x + 2) in Spec k[x]. That’s expected. If we
have a space with two lines but we’re standing away from the origin, then the stalk is
not going to pick up the weird behavior at that far-away point; it only cares about what
happens near m, and so it looks just like an aﬃne line there.

The generic point (y) (which ﬂoats around the x-axis) will tell a similar tale: if we look
at the stalk above it, we ought to ﬁnd that it doesn’t recognize the presence of the y-axis,
because “nearly all” points don’t recognize it either. To actually compute the stalk:

OSpec k[x,y]/(xy),(y) = k[x, y](y)/(xy).

Again as 1

x is an element of k[x, y](y), we have (xy) = (y) and thus
OSpec k[x,y]/(xy),(y) = k[x, y](y)/(y) ∼= k[x](0) ∼= k(x).

which is what we expected (it is the same as the stalk above (0) in Spec k[x]).

§79.14.iv Stalk above the origin (tricky)

The stalk above the origin (x, y) is interesting, and has some ideas in it we won’t be able
to explore fully without talking about localizations of modules. We could of course write
it as

(k[x, y]/(xy))(x,y) = k[x, y](x,y)/(xy).

and hence the elements should be

c + (a1x + a2x2 + . . . ) + (b1y + b2y2 + . . . )
c(cid:48) + (a(cid:48)1x + a(cid:48)2x2 + . . . ) + (b(cid:48)1y + b(cid:48)2y2 + . . . )

where c(cid:48) (cid:54)= 0.
You can write the global section ring as

You might feel unsatisﬁed with this characterization. Here is some geometric intuition.

k[x, y]/(xy) = c + (a1x + a2x2 + . . . ) + (b1y + b2y2 + . . . )

meaning that any global section is the sum of an x-polynomial and a y-polynomial. This
is not just the ring product k[x] × k[y], though: because the constant term is shared.
So it is better thought of as pairs of polynomials in k[x] and k[y] which agree on the
constant term.

If you like category theory, it is thus a ﬁbered product

k[x, y]/(xy) ∼= k[x] ×k k[y]

756

Napkin, by Evan Chen (v1.5.20190718)

with morphism k[x] → k and k[y] → k by sending x and y to zero. In that way, we can
mostly decompose k[x, y]/(xy) into its two components.

We really ought to be able to do the same as the stalk: we wish to say that

OSpec k[x,y]/(xy),(x,y) ∼= k[x](x) ×k k[y](y).

English translation: a “typical” germ ought to look like 3+x
y2+y+7 , with the x and y
parts decoupled. Equivalently, the stalk should consist of pairs of x-germs and y-germs
that agree at the origin.

x2+7 + 4+y3

In fact, this is true! This might come as a surprise, but let’s see why we expect this.

Suppose we take the germ

1

.

1 − (x + y)

If we hold our breath, we could imagine expanding it as a geometric series: 1 + (x + y) +
(x + y)2 + . . . . As xy = 0, this just becomes 1 + x + x2 + x3 + ··· + y + y2 + y3 + . . . .
This is nonsense (as written), but nonetheless it suggests the conjecture

1 − (x + y)
which you can actually verify is true.

1

=

1

1 − x

+

1

1 − y − 1

Question 79.14.2. Check this identity holds.

Of course, this is a lot of computation just for one simple example. Is there a way to
make it general? Yes: the key claim is that “localization commutes with limits”. You
can try to work out the statement now if you want, but we won’t do so.

§79.15 Spec k[x, x−1], the punctured line (or hyperbola)

This is supposed to look like D(x) of Spec k[x], or the line with the origin deleted it.
Alternatively, we could also write

k[x, x−1] ∼= k[x, y]/(xy − 1)

so that the scheme could also be drawn as a hyperbola.

First, here’s the 1D illustration.

We actually saw this scheme already when we took Spec k[x, y]/(xy) and looked at D(y),
too. Anyways, let us compute the stalk at (x − 3) now; it is

OSpec k[x,x−1],(x−3) ∼= k[x, x−1](x−3) ∼= k[x](x−3)

since x−1 is in k[x](x−3) anyways. So again, we see that the deletion of the origin doesn’t
aﬀect the stalk at the farther away point (x − 3).
As mentioned, since k[x, x−1] is isomorphic to k[x, y]/(xy − 1), another way to draw
the visualize the same curve would be to draw the hyperbola (which you can imagine as
ﬂattening to give the punctured line.) There is one generic point (0) since k[x, y]/(xy − 1)
really is an integral domain, as well as points like (x + 2, y + 1/2) = (x + 2) = (y + 1/2).

(x−3)(x+2)(0)Speck[x,x−1]79 Interlude: eighteen examples of aﬃne schemes

757

§79.16 Spec k[x](x), zooming in to the origin of the line
We know already that OSpec A,p ∼= Ap: so Ap should be the stalk at p. In this example
we will see that Spec Ap should be drawn sort of as this stalk, too.

We saw earlier how to draw a picture of Spec k[x]. You can also draw a picture of the
stalk above the origin (x), which you might visualize as a grass or other plant growing
above (x) if you like agriculture. In that case, Spec k[x](x) might look like what happens
if you pluck out that stalk from the aﬃne line.

Since k[x](x) is a local ring (it is the localization of a prime ideal), this point has only
one closed point: the maximal ideal (x). However, surprisingly, it has one more point: a
“generic” point (0). So Spec k[x](x) is a two-point space, but it does not have the discrete
topology: (x) is a closed point, but (0) is not. (This makes it a nice counter-example for
exercises of various sorts.)

So, topologically what’s happening is that when we zoom in to (x), the generic point

(0) (which was “close to every point”) remains, ﬂoating above the point (x).

Note that the stalk above our single closed point (x) is the same as it was before:

(cid:0)k[x](x)(cid:1)(x) ∼= k[x](x).

Indeed, in general if R is a local ring with maximal ideal m, then Rm ∼= R: since every
element x /∈ m was invertible anyways. Thus in the picture, the stalk is drawn the same.

Similarly, the stalk above (0) is the same as it was before we plucked it out:

More generally:

(cid:0)k[x](x)(cid:1)(0) = Frac k[x](x) = k(x).

Exercise 79.16.1. Let A be a ring, and q ⊆ p prime ideals. Check that Aq ∼= (Ap)q, where
we view q as a prime ideal of Ap.

So when we zoom in like this, all the stalks stay the same, even above the non-closed
points.

yxSpeck[x,y]/(xy−1)(0)(x+2)Speck[x]OSpeck[x],(x)(x)(0)pluckSpeck[x](x)(0)(x)758

Napkin, by Evan Chen (v1.5.20190718)

§79.17 Spec k[x, y](x,y), zooming in to the origin of the plane

The situation is more surprising if we pluck the stalk above the origin of Spec k[x, y], the
two-dimensional plane. The points of Spec k[x, y](x,y) are supposed to be the prime ideals
of k[x, y] which are contained in (x, y); geometrically these are (x, y) and the generic
points passing through the origin. For example, there will be a generic point for the
parabola (y − x2) contained in k[x, y](x,y), and another one (y − x) corresponding to a
straight line, etc.
So we have the single closed point (x, y) sitting at the bottom, and all sorts of “one-
dimensional” generic points ﬂoating above it: lines, parabolas, you name it. Finally,
we have (0), a generic point ﬂoating in two dimensions, whose closure equals the entire
space.

§79.18 Spec k[x, y](0) = Spec k(x, y), the stalk above the generic

point

The generic point of the plane just has stalk Spec k(x, y): which is the spectrum of a
ﬁeld, hence a single point. The stalk remains intact as compared to when planted in
Spec k[x, y]; the functions are exactly rational functions in x and y.

§79.19 A few harder problems to think about

Problem 79A. Draw a picture of Spec Z[1/55], describe the topology, and compute the
stalk at each point.

Problem 79B. Draw a picture of Spec Z(5), describe the topology, and compute the
stalk at each point.

Problem 79C. Let A = (k[x, y]/(xy))[(x + y)−1]. Draw a picture of Spec A. Show that
it is not connected as a topological space.

Problem 79D. Let A = k[x, y](y−x2). Draw a picture of Spec A.

Speck[x,y](x,y)(3x−2y)(y−x2)(2y−x3)(x,y)(0)80 Morphisms of locally ringed spaces

Having set up the deﬁnition of a locally ringed space, we are almost ready to deﬁne
morphisms between them. Throughout this chapter, you should imagine your ringed
spaces are the aﬃne schemes we have so painstakingly deﬁned; but it will not change
anything to work in the generality of arbitrary locally ringed spaces.

§80.1 Morphisms of ringed spaces via sections

Let (X,OX ) and (Y,OY ) be ringed spaces. We want to give a deﬁne what it means
to have a function π : X → Y between them.1 We start by requiring the map to be
continuous, but this is not enough: there is a sheaf on it!
Well, you might remember what we did for baby ringed spaces: any time we had a
function on an open set of U ⊆ Y , we wanted there to be an analogous function on
πpre(U ) ⊆ X. For baby ringed spaces, this was done by composition, since the elements
of the sheaf were really complex valued functions:

π(cid:93)φ was deﬁned as φ ◦ π.

The upshot was that we got a map OY (U ) → OX (πpre(U )) for every open set U .

Now, for general locally ringed spaces, the sections are just random rings, which may
not be so well-behaved. So the solution is that we include the data of f (cid:93) as part of the
deﬁnition of a morphism.

Deﬁnition 80.1.1. A morphism of ringed spaces (X,OX ) → (Y,OY ) consists of a
pair (π, π(cid:93)) where π : X → Y is a continuous map (of topological spaces), and π(cid:93) consists
of a choice of ring homomorphism

π(cid:93)
U : OY (U ) → OX (πpre(U ))

for every open set U ⊆ Y , such that the restriction diagram
1Notational connotations: for ringed spaces, π will be used for maps, since f is often used for sections.

759

Xπpre(U)YUπf∈OY(U)π♯f∈OX(fpre(U))760

Napkin, by Evan Chen (v1.5.20190718)

OY (U )

OY (πpre(U ))

OY (V )

OX (πpre(V ))

commutes for V ⊆ U .
Abuse of Notation 80.1.2. We will abbreviate (π, π(cid:93)) : (X,OX ) → (Y,OY ) to just
π : X → Y , despite the fact this notation is exactly the same as that for topological
spaces.

There is an obvious identity map, and so we can also deﬁne isomorphism etc. in the

categorical way.

§80.2 Morphisms of ringed spaces via stalks

Unsurprisingly, the sections are clumsier to work with than the stalks, now that we have
grown to love localization. So rather than specifying π(cid:93)
U on every open set U , it seems
better if we could do it by stalks (there are fewer stalks than open sets, so this saves us a
lot of work!).

We start out by observing that we do get a morphism of stalks.

Proposition 80.2.1 (Induced stalk morphisms)
If π : X → Y is a map of ringed spaces sending π(p) = q, then we get a map

whenever π(p) = q.

π(cid:93)
p : OY,q → OX,p

This means you can draw a morphism of locally ringed spaces as a continuous map on
the topological space, plus for each π(p) = q, an assignment of each germ at q to a germ
at p.

Again, compare this to the pullback picture: this is roughly saying that if a function f
has some enriched value at q, then π(cid:93)(f ) should be assigned a corresponding enriched
value at p. The analogy is not perfect since the stalks at q and p may be diﬀerent rings
in general, but there should at least be a ring homomorphism (the assignment).

XYOX,pOY,qpq[s]q[π♯(s)]pπ♯pπ80 Morphisms of locally ringed spaces

761

Proof. If (s, U ) is a germ at q, then (π(cid:93)(s), πpre(U )) is a germ at p, and this is a well-
deﬁned morphism because of compatibility with restrictions.

We already obviously have uniqueness in the following senes.

Proposition 80.2.2 (Uniqueness of morphisms via stalks)
Consider a map of ringed spaces (π, π(cid:93)) : (X,OX ) → (Y,OY ) and the corresponding
map π(cid:93)

p of stalks. Then π(cid:93) is uniquely determined by π(cid:93)
p.

Proof. Given a section s ∈ OY (U ), let
t = π(cid:93)

U (s) ∈ OX (πpre(U ))

denote the image under π(cid:93).

We know tp for each p ∈ πpre(U ), since it equals π(cid:93)

all the germs of t. So we know t.

p(t) by deﬁnition. That is, we know

However, it seems clear that not every choice of stalk morphisms will lead to π(cid:93)
U :
some sort of “continuity” or “compatibility” is needed. You can actually write down the
explicit statement: each sequence of compatible germs over U should gets mapped to
a sequence of compatible germs over πpre(U ). We avoid putting up a formal statement
of this for now, because the statement is clumsy, and you’re about to see it in practice
(where it will make more sense).

Remark 80.2.3 (Isomorphisms are determined by stalks) — One fact worth mention-
ing, that we won’t prove, but good to know: a map of ringed spaces (π, π(cid:93)) : (X,OX ) →
(Y,OY ) is an isomorphism if and only if π is a homeomorphism, and moreover π(cid:93)
p is
an isomorphism for each p ∈ X.

§80.3 Morphisms of locally ringed spaces

On the other hand, we’ve seen that our stalks are local rings, which enable us to actually
talk about values. And so we want to add one more compatibility condition to ensure
that our notion of value is preserved. Now the stalks at p and q in the previous picture
might be diﬀerent, so κ(p) and κ(q) might even be diﬀerent ﬁelds.

Deﬁnition 80.3.1. A morphism of locally ringed spaces is a morphism of ringed
spaces π : X → Y with the following additional property: whenever π(p) = q, the map at
the stalks also induces a well-deﬁned ring homomorphism

π(cid:93)
p : κ(q) → κ(p).

So we require π(cid:93)

p induces a ﬁeld homomorphisms2 on the residue ﬁelds. In particular,

since π(cid:93)(0) = 0, this means something very important:

In a morphism of locally ringed spaces, a germ vanishes at q if and only
if the corresponding germ vanishes at p.

2Which means it is automatically injective, by Problem 5B.

762

Napkin, by Evan Chen (v1.5.20190718)

Exercise 80.3.2 (So-called “local ring homomorphism”). Show that this is equivalent to
requiring

(π(cid:93)
p)img(mY,q) ⊆ mX,p

or in English, a germ at q has value zero iﬀ the corresponding germ at p has value zero.

I don’t like this formulation (π(cid:93))img(mY,q) ⊆ mX,p as much since it hides the geometric
intuition behind a lot of symbols: that we want the notion of “value at a point” to be
preserved in some way.

At this point, we can state the deﬁnition of a scheme, and we do so, although we won’t

really use it for a few more sections.

Deﬁnition 80.3.3. A scheme is a locally ringed space for which every point has an
open neighborhood isomorphic to an aﬃne scheme. A morphism of schemes is just a
morphism of locally ringed spaces.

In particular, Spec A is a scheme (the open neighborhood being the entire space!). And

so let’s start by looking at those.

§80.4 A few examples of morphisms between aﬃne schemes

Okay, sorry for lack of examples in previous few sections. Let’s make amends now, where
you can see all the moving parts in action.

§80.4.i One-point schemes

Example 80.4.1 (Spec R is well-behaved)
There is only one map X = Spec R → Spec R = Y . Indeed, these are spaces with
one point, and specifying the map R = OY (Y ) → OX (X) = R can only be done in
one way, since there is only one ﬁeld automorphism of R (the identity).

Example 80.4.2 (Spec C horror story)
There are multiple maps X = Spec C → Spec C = Y , horribly enough! Indeed,
these are spaces with one point, so again we’re just reduced to specifying a map
C = OY (Y ) → OX (X) = C. However, in addition to the identity map, complex
conjugation also works, as well as some so-called “wild automorphisms” of C.

This behavior is obviously terrible, so for illustration reasons, some of the examples
use R instead of C to avoid the horror story we just saw. However, there is an easy ﬁx
using “scheme over C” which will force the ring homomorphisms to ﬁx C, later.

Example 80.4.3 (Spec k and Spec k(cid:48))
In general, if k and k(cid:48) are ﬁelds, we see that maps Spec k → Spec k(cid:48) are in bijection
with ﬁeld homomorphism k(cid:48) → k, since that’s all there is left to specify.

§80.4.ii Examples of constant maps

80 Morphisms of locally ringed spaces

763

Example 80.4.4 (Constant map to (y − 3))
We analyze scheme morphisms

X = Spec R[x] π−→ Spec R[y] = Y

which send all points of X to m = (y − 3) ∈ Y . Constant maps are continuous no
matter how bizarre your topology is, so this lets us just focus our attention on the
sections.

This example is simple enough that we can even do it by sections, as much as I
think stalks are simpler. Let U be any open subset of Y , then we need to specify a
map

π(cid:93)
U : OY (U ) → OX (πpre(U )).

If U does not contain (y − 3), then πpre(U ) = ∅, so OX (∅) = 0 is the zero ring and
there is nothing to do.
Conversely, if U does contain (y − 3) then πpre(U ) = X, so this time we want to

specify a map

π(cid:93)
U : OY (U ) → OX (X) = R[x]

which satisﬁes restriction maps. Note that for any U , the element y must map to a
unit in R[x]; since 1/y is a section too for a subset of U not containing (x). Actually
for any real number c (cid:54)= 3, y − c must be map to a unit in R[x]. This can only
happen if y (cid:55)→ 3 ∈ R[x].
As we have speciﬁed R[y] (cid:55)→ R[x] with y (cid:55)→ 3, that determines all the ring
homomorphisms we needed.

But we could have used stalks, too. We wanted to specify a morphism

R[y](y−3) = OSpec Y,(y−3) → OSpec X,p

for every prime ideal p, sending compatible germs to compatible germs. . . but wait, (y− 3)
is spitting out all the germs. So every individual germ in OSpec Y,(y−3) needs to yield a
(compatible) germ above every point of Spec X, which is the data of an entire global
section. So we’re actually trying to specify

R[y](y−3) = OSpec Y,(y−3) → OSpec X (Spec X) = R[x].

This requires y (cid:55)→ 3, as we saw, since y − c is a unit of R[x] for any c (cid:54)= 3.

Example 80.4.5 (Constant map to (y2 + 1) does not exist)
Let’s see if there are constant maps X = Spec R[x] → Spec R[y] = Y which send
everything to (y2 + 1). Copying the previous example, we see that we want

OY (U ) → OX (X) = R[x].

We ﬁnd that y and 1/y has nowhere to go: the same argument as last time shows
that y − c should be a unit of R[x]; this time for any real number c.
example need a ﬁeld homomorphism

Like this time, stalks show this too, even with just residue ﬁelds. We would for

C = κ((y2 + 1)) → κ((x)) = R

which does not exist.

764

Napkin, by Evan Chen (v1.5.20190718)

You might already notice the following:

Example 80.4.6 (The generic point repels smaller points)
Changing the tune, consider maps Spec C[x] → Spec C[y]. We claim that if m is a
maximal ideal (closed point) of C[x], then it can never be mapped to the generic
point (0) of C[y].

For otherwise, we would get a local ring homomorphism

C(y) ∼= OSpec k[y],(0) → OSpec C[x],m ∼= C[x]m

which in particular means we have a map on the residue ﬁelds

which is impossible, there is no such ﬁeld homomorphism at all (why?).

C(y) → C[x]/m ∼= C

The last example gives some nice intuition in general: “more generic” points tend to
have bigger stalks than “less generic” points, hence repel them.

§80.4.iii The map t (cid:55)→ t2
We now consider what we would think of as the map t (cid:55)→ t2.

Example 80.4.7 (The map t (cid:55)→ t2)
We consider a map

deﬁned on points as follows:

π : X = Spec C[x] → Spec C[y] = Y

π ((0)) = (0)

π ((x − a)) = (y − a2).

You may check if you wish this map is continuous. I claim that, surprisingly, you
can actually read oﬀ π(cid:93) from just this behavior at points. The reason is that we
imposed the requirement that a section s can vanish at q ∈ Y if and only if π(cid:93)
X (s)
vanishes at p ∈ X, where π(p) = q. So, now:

 Consider the section y ∈ OY (Y ), which vanishes only at (y) ∈ Spec C[y];
Y (y) ∈ OX (X) must vanish at exactly (x) ∈ Spec C[x], so

then its image π(cid:93)
π(cid:93)
Y (y) = xn for some integer n ≥ 1.

 Consider the section y− 4 ∈ OY (Y ), which vanishes only at (y− 4) ∈ Spec C[y];
then its image π(cid:93)
Y (y − 4) ∈ OX (X) must vanish at exactly (x − 2) ∈ Spec C[x]
and (x + 2) ∈ Spec C[x]. So π(cid:93)
Y (y) − 4 is divisible by (x − 2)a(x + 2)b for some
a ≥ 1 and b ≥ 1.

Thus y (cid:55)→ x2 in the top level map of sections π(cid:93)
sections (as well as at all the stalks).

Y : and hence also in all the maps of

The above example works equally well if t2 is replaced by some polynomial f (t), so that
(x − a) maps to (y − f (y)). The image of y must be a polynomial g(x) with the property

80 Morphisms of locally ringed spaces

765

that g(x) − c has the same roots as f (x) − c for any c ∈ C. Put another way, f and g
have the same values, so f = g.

Remark 80.4.8 (Generic point stalk overpowered) — I want to also point out that
you can read oﬀ the polynomial just from the stalk at the generic point: for example,
the previous example has

C(y) ∼= OSpec C[y],(0) → OSpec C[x],(0) ∼= C(x)

y (cid:55)→ x2.

This is part of the reason why generic points are so powerful. We expect that with
polynomials, if you know what happens to a “generic” point, you can ﬁgure out the
entire map. This intuition is true: knowing where each germ at the generic point
goes is enough to tell us the whole map.

§80.4.iv An arithmetic example

Example 80.4.9 (Spec Z[i] → Spec Z)
We now construct a morphism of schemes π : Spec Z[i] → Spec Z. On points it
behaves by

π ((0)) = (0)

π ((p)) = (p)

π ((a + bi)) = (a2 + b2)

where a + bi is a Gaussian prime: so for example π((2 + i)) = (5) and π((1 + i)) = (2).
We could ﬁgure out the induced map on stalks now, much like before, but in a
moment we’ll have a big theorem that spares us the trouble.

§80.5 The big theorem

We did a few examples of Spec A → Spec B by hand, specifying the full data of a map of
locally ringed spaces. It turns out that in fact, we didn’t to specify that much data, and
much of the process can be automated:

Proposition 80.5.1 (Aﬃne reconstruction)
Let π : Spec A → Spec B be a map of schemes. Let ψ : B → A be the ring homo-
morphism obtained by taking global sections, i.e.

ψ = π(cid:93)

B : OSpec B(Spec B) → OSpec A(Spec A).

Then we can recover π given only ψ; in fact, π is given explicitly by

π(p) = ψpre(p)

and

π(cid:93)
p : OY,π(p) → OX,p

by

f /g (cid:55)→ ψ(f )/ψ(g).

This is the big miracle of aﬃne schemes. Despite the enormous amount of data packaged

766

Napkin, by Evan Chen (v1.5.20190718)

into the deﬁnition, we can compress maps between aﬃne schemes into just the single
ring homomorphism on the top level.

Proof. This requires two parts.

 We need to check that the maps agree on points; surprisingly this is the harder
half. To see how this works, let q = π(p). The key fact is that a function f ∈ B
vanishes on q if and only if π(cid:93)
B is supposed to be a
homomorphism of local rings). Therefore,

B(f ) vanishes on p (because π(cid:93)

π(p) = q = {f ∈ B | f ∈ q}

= {f ∈ B | f vanishes on q}

=(cid:110)f ∈ B | π(cid:93)
=(cid:110)f ∈ B | π(cid:93)

= ψpre(p).

B(f ) vanishes on p(cid:111)
B(f ) ∈ p(cid:111) = {f ∈ B | ψ(f ) ∈ p}

 We also want to check the maps on the stalks is the same. Suppose p ∈ Spec A,

q ∈ Spec B, and p (cid:55)→ q (under both of the above).
In our original π, consider the map π(cid:93)
p : Bq → Ap. We know that it sends each
f ∈ B to ψ(f ) ∈ A, by taking the germ of each global section f ∈ B at q. Thus it
must send f /g to ψ(f )/ψ(g), being a ring homomorphism, as needed.

All of this suggests a great idea: if ψ : B → A is any ring homomorphism, we ought to
be able to construct a map of schemes by using fragments of the proof we just found.
The only extra work we have to do is verify that we get a continuous map in the Zariski
topology, and that we can get a suitable π(cid:93).

We thus get the huge important theorem about aﬃne schemes.

Theorem 80.5.2 (Spec A → Spec B is just B → A)
These two construction gives a bijection between ring homomorphisms B → A and
Spec A → Spec B.

Proof. We have seen how to take each π : Spec A → Spec B and get a ring homomorphism
ψ. Proposition 80.5.1 shows this map is injective. So we just need to check it is surjective
— that every ψ arises from some π.

Given ψ : B → A, we deﬁne (π, π(cid:93)) : Spec A → Spec B by copying Proposition 80.5.1

and checking that everything is well-deﬁnced. The details are:

 For each prime ideal p ∈ Spec A, we let π(p) = ψpre(p) ∈ Spec B (which by

Problem 5C(cid:63) is also prime).

Exercise 80.5.3. Show that the resulting map π is continuous in the Zariski topology.

 Now we want to also deﬁne maps on the stalks, and so for ecah π(p) = q we set

Bq (cid:51)

f
g (cid:55)→

ψ(f )
ψ(g) ∈ Ap.

This makes sense since g /∈ q =⇒ ψ(g) /∈ p. Also f ∈ q =⇒ ψ(f ) ∈ p, we ﬁnd
this really is a local ring homomorphism (sending the maximal ideal of Bq into the
one of Ap).

80 Morphisms of locally ringed spaces

767

Observe that if f /g is a section over an open set U ⊆ B (meaning g does not vanish
at the primes in U ), then ψ(f )/ψ(g) is a section over πpre(U ) (meaning ψ(g) does
not vanish at the primes in πpre(U )). Therefore, compatible germs over B get sent
to compatible germs over A, as needed.

Finally, the resulting π has π(cid:93)

B = ψ on global sections, completing the proof.

This can be summarized succinctly using category theory:

Corollary 80.5.4 (Categorical interpretation)
The opposite category of rings CRingop, is “equivalent” to the category of aﬃne
schemes, AﬀSch, with Spec as a functor.

This means for example that Spec A ∼= Spec B, naturally, whenever A ∼= B.

To make sure you realize that this theorem is important, here is an amusing comment

I found on MathOverﬂow while reading about algebraic geometry references3:

He [Hartshorne] never mentions that the category of aﬃne schemes is dual to the category
of rings, as far as I can see. I’d expect to see that in huge letters near the deﬁnition of
scheme. How could you miss that out?

§80.6 More examples of scheme morphisms

Now that we have the big hammer, we can talk about examples much more brieﬂy
than we did a few sections ago. Before throwing things around, I want to give another
deﬁnition that will eliminate the weird behavior we saw with C → C having nontrivial
ﬁeld automorphisms:

Deﬁnition 80.6.1. Let S be a scheme. A scheme over S or S-scheme is a scheme X
together with a map X → S. A morphism of S-schemes is a scheme morphism X → Y

X

Y

such that the diagram

commutes. Often, if S = Spec k, we will use refer

to schemes over k or k-schemes for short.

S

Example 80.6.2 (Spec k[. . . ])
If X = Spec k[x1, . . . , xn]/I for some ideal I, then X being a k-scheme in a natural
way; since we have an obvious homomorphism k (cid:44)→ k[x1, . . . , xn]/I which gives a
map X → Spec k.

3From https://mathoverflow.net/q/2446/70654.

768

Napkin, by Evan Chen (v1.5.20190718)

Example 80.6.3 (Spec C[x] → Spec C[y])
As C-schemes, maps Spec C[x] → Spec C[y] coincide with ring homomorphisms from
ψ : C[y] → C[x] such that the diagram
C[x]

C[y]

ψ

C

We see that the “over C” condition is eliminating the pathology from before: the
ψ is required to preserve C. So the morphism is determined by the image of y, i.e.
the choice of a polynomial in C[x]. For example, if ψ(y) = x2 we recover the ﬁrst
example we saw. This matches our intuition that these maps should correspond to
polynomials.

Example 80.6.4 (SpecOK)
This generalize Spec Z[i] from before. If K is a number ﬁeld and OK, then there
is a natural morphism SpecOK → Spec Z from the (unique) ring homomorphism
Z (cid:44)→ OK. Above each rational prime (p) ∈ Z, one obtains the prime ideals that p
splits as. (We don’t have a way of capturing ramiﬁcation yet, alas.)

§80.7 A little bit on non-aﬃne schemes

We can ﬁnally state the isomorphism that we wanted for a long time (ﬁrst mentioned in
Section 78.2.iii):

Theorem 80.7.1 (Distinguished open sets are isomorphic to aﬃne schemes)

Let A be a ring and f an element. Then

Spec A[1/f ] ∼= D(f ) ⊆ Spec A.

Proof. Annoying check, not included yet. (We have already seen the bijection of prime
ideals, at the level of points.)

Corollary 80.7.2 (Open subsets are schemes)

(a) Any nonempty open subset of an aﬃne scheme is itself a scheme.

(b) Any nonempty open subset of any scheme (aﬃne or not) is itself a scheme.

Proof. Part (a) has essentially been done already:

Question 80.7.3. Combine Theorem 78.2.2 with the previous proposition to deduce (a).

80 Morphisms of locally ringed spaces

769

Part (b) then follows by noting that if U is an open set, and p is a point in U , then
we can take an aﬃne open neighborhood Spec A at p, and then cover U ∩ Spec A with
distinguished open subsets of Spec A as in (a).

We now reprise Section 78.2.iv (except C will be replaced by k). We have seen it is an

open subset U of Spec k[x, y], so it is aﬃne.

Question 80.7.4. Show that in fact U can be covered by two open sets which are both
aﬃne.

However, we show now that you really do need two open sets.

Proposition 80.7.5 (Famous example: punctured plane isn’t aﬃne)
The punctured plane U = (U,OU ), obtained by deleting (x, y) from Spec k[x, y], is
not isomorphic to any aﬃne scheme Spec B.

The intuition is that OU (U ) = k[x, y], but U is not the plane.

Proof. We already know OU (U ) = k[x, y] and we have a good handle on it. For example,
y ∈ OU (U ) is a global section which vanishes on what looks like the y-axis. Similarly,
x ∈ OX (X) is a global section which vanishes on what looks like the y-axis. In particular,
no point of U vanishes at both.

Now assume for contradiction that we have an isomorphism

By taking the map on global sections (part of the deﬁnition),

ψ : Spec B → U.

k[x, y] = OU (U )

ψ(cid:93)

−→ OSpec B(Spec B) ∼= B.

The global sections x and y in OU (U ) should then have images a and b in B; and it
follows we have a ring isomorphism B ∼= k[a, b].

Now in Spec B, V(a) ∩ V(b) is a closed set containing a single point, the maximal ideal
m = (a, b). Thus in Spec B there is exactly one point vanishing at both a and b. Because
we required morphisms of schemes to preserve values (hence the big fuss about locally
ringed spaces), that means there should be a single point of U vanishing at both x and y.
But there isn’t — it was the origin we deleted.

V(x)V(y)UOU(U)=k[x,y]V(a)V(b)(a,b)SpecBOSpecB(SpecB)∼=k[a,b]770

Napkin, by Evan Chen (v1.5.20190718)

§80.8 Where to go from here

This chapter concludes the long setup for the deﬁnition of a scheme. For now, this
unfortunately is as far as I have time to go. So, if you want to actually see how schemes
are used in “real life”, you’ll have to turn elsewhere.

A good reference I happily recommend is [Ga03]; a more diﬃcult (and famous) one is

[Va17]. See Appendix A for further remarks.

§80.9 A few harder problems to think about

Problem 80A. Given an aﬃne scheme X = Spec R, show that there is a unique
morphism of schemes X → Spec Z, and describe where it sends points of X.
Problem 80B. Is the open subset of Spec Z[x] obtained by deleting the point m = (2, x)
isomorphic to some aﬃne scheme?

XX

Algebraic Geometry III: Schemes

(TO DO)

XXI

Set Theory I: ZFC, Ordinals, and

Cardinals

Part XXI: Contents

81 Interlude: Cauchy’s functional equation and Zorn’s lemma

775
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 775
81.1 Let’s construct a monster
81.2 Review of ﬁnite induction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 776
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 776
81.3 Transﬁnite induction
. . . . . . . . . . . . . . . . . . . . . . . . . . 778
81.4 Wrapping up functional equations
81.5 Zorn’s lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 779
. . . . . . . . . . . . . . . . . . . . . . . . 781
81.6 A few harder problems to think about

82 Zermelo-Fraenkel with choice

783
82.1 The ultimate functional equation . . . . . . . . . . . . . . . . . . . . . . . . . . . 783
82.2 Cantor’s paradox . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 783
82.3 The language of set theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 784
82.4 The axioms of ZFC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 785
82.5 Encoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 787
82.6 Choice and well-ordering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 787
82.7 Sets vs classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 788
. . . . . . . . . . . . . . . . . . . . . . . . 789
82.8 A few harder problems to think about

83 Ordinals

791
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 791
83.1 Counting for preschoolers
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 792
83.2 Counting for set theorists
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 794
83.3 Deﬁnition of an ordinal
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 795
83.4 Ordinals are “tall”
83.5 Transﬁnite induction and recursion . . . . . . . . . . . . . . . . . . . . . . . . . . 796
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 797
83.6 Ordinal arithmetic
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 798
83.7 The hierarchy of sets
. . . . . . . . . . . . . . . . . . . . . . . . 800
83.8 A few harder problems to think about

84 Cardinals

801
84.1 Equinumerous sets and cardinals . . . . . . . . . . . . . . . . . . . . . . . . . . . 801
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 802
84.2 Cardinalities
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 802
84.3 Aleph numbers
84.4 Cardinal arithmetic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 803
84.5 Cardinal exponentiation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 805
84.6 Coﬁnality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 805
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 807
84.7 Inaccessible cardinals
. . . . . . . . . . . . . . . . . . . . . . . . 807
84.8 A few harder problems to think about

81 Interlude: Cauchy’s functional

equation and Zorn’s lemma

This is an informal chapter on Zorn’s lemma, which will give an overview of what’s
going to come in the last parts of the Napkin. It can be omitted without loss of continuity.

In the world of olympiad math, there’s a famous functional equation that goes as

follows:

f : R → R

f (x + y) = f (x) + f (y).

Everyone knows what its solutions are! There’s an obvious family of solutions f (x) = cx.
Then there’s also this family of. . . uh. . . noncontinuous solutions (mumble grumble)
pathological (mumble mumble) Axiom of Choice (grumble).

There’s also this thing called Zorn’s lemma. It sounds terrifying, because it’s equivalent

to the Axiom of Choice, which is also terrifying because why not.

In this post I will try to de-terrify these things, because they’re really as terrifying as

they sound.

§81.1 Let’s construct a monster

Let us just see if we can try and construct a “bad” f and see what happens.

By scaling, let’s assume WLOG that f (1) = 1. Thus f (n) = n for every integer n, and

you can easily show from here that

None of this is useful for determining, say, f (√2). You could add and subtract rational

So f is determined for all rationals. And then you get stuck.
numbers all day and, say, √2 isn’t going to show up at all.

Well, we’re trying to set things on ﬁre anyways, so let’s set

because why not? By the same induction, we get f (n√2) = 2015n, and then that

f(cid:16) m
n(cid:17) =

m
n

.

f (√2) = 2015

f(cid:16)a + b√2(cid:17) = a + 2015b.

Here a and b are rationals. Well, so far so good – as written, this is a perfectly good
solution, other than the fact that we’ve only deﬁned f on a tiny portion of the real
numbers.

Well, we can do this all day:

f(cid:16)a + b√2 + c√3 + dπ(cid:17) = a + 2015b + 1337c − 999d.

Perfectly consistent.

You can kind of see how we should keep going now. Just keep throwing in new real
numbers which are “independent” to the previous few, assigning them to whatever junk
we want. It feels like it should be workable. . .

In a moment I’ll explain what “independent” means (though you might be able to
guess already), but at the moment there’s a bigger issue: no matter how many numbers
we throw, it seems like we’ll never ﬁnish. Let’s address the second issue ﬁrst.

775

776

Napkin, by Evan Chen (v1.5.20190718)

§81.2 Review of ﬁnite induction

When you do induction, you get to count oﬀ 1, 2, 3, . . . and so on. So for example,
suppose we had a “problem” such as:

Prove that the intersection of n open intervals is either ∅ or an open interval.

You can do this by induction easily: it’s true for n = 2, and for the larger cases it’s
similarly easy.

But you can’t conclude from this that inﬁnitely many open intervals intersect at some

open interval. Indeed, this is false: consider the intervals

(−1, 1) , (cid:18)−

1
2

,

1

2(cid:19) , (cid:18)−

1
3

,

1

3(cid:19) , (cid:18)−

1
4

,

1

4(cid:19) ,

. . .

This inﬁnite set of intervals intersects at a single point {0}!

The moral of the story is that induction doesn’t let us reach inﬁnity. Too bad, because
we’d have loved to use induction to help us construct a monster. That’s what we’re
doing, after all – adding things in one by one.

§81.3 Transﬁnite induction

Well, it turns out we can, but we need a new notion of number, the so-called ordinal
number. I deﬁne these in their full glory in the ﬁrst two sections of Chapter 83 (and
curious readers are even invited to jump ahead to those two sections), but for this chapter
I won’t need that full deﬁnition yet.

Here’s what I want to say: after all the natural numbers

0, 1, . . . ,

I’ll put a new number called ω, the ﬁrst ordinal greater than all the natural numbers.
After that there’s more numbers called

ω + 1, ω + 2, . . .

and eventually a number called 2ω.

The list goes on:

0,1, 2, 3, . . . , ω

ω + 1, ω + 2, . . . , ω + ω

2ω + 1, 2ω + 2, . . . , 3ω
...
ω2 + 1, ω2 + 2, . . .
...
ω3, . . . , ω4, . . . , ωω . . . , ωωω...

Pictorially, it kind of looks like this:

81 Interlude: Cauchy’s functional equation and Zorn’s lemma

777

Image from [ca]

(Note that the diagram only shows an initial segment; there are still larger ordinals like
ωωω

+ 1000 and so on).

Anyways, in the same way that natural numbers “dominate” all ﬁnite sets, the ordinals
dominate all the sets, in the following sense. Essentially, assuming the Axiom of Choice,
it follows that for every set S there’s some ordinal α which is larger than S (in a sense I
won’t make precise until later chapters).

But it turns out (and you can intuitively see) that as large as the ordinals grow, there
is no inﬁnite descending chain. Meaning:
if I start at an ordinal (like 2ω + 4) and
jump down, I can only take ﬁnitely many jumps before I hit 0. (To see this, try writing
down a chain starting at 2ω + 4 yourself.) Hence, induction and recursion still work
verbatim:

778

Napkin, by Evan Chen (v1.5.20190718)

Theorem 81.3.1 (Transﬁnite induction)
Given a statement P (−), suppose that

 P (0) is true, and

 If P (α) is true for all α < β, then P (β) is true.

Then P (β) is true.

Similarly, you’re allowed to do recursion to deﬁne xβ if you know the value of xα for all
α < β.

The diﬀerence from normal induction or recursion is that we’ll often only do things
like “deﬁne xn+1 = . . . ”. But this is not enough to deﬁne xα for all α. To see this, try
using our normal induction and see how far we can climb up the ladder.

Answer: you can’t get ω! It’s not of the form n + 1 for any of our natural numbers
n – our ﬁnite induction only lets us get up to the ordinals less than ω. Similarly, the
simple +1 doesn’t let us hit the ordinal 2ω, even if we already have ω + n for all n. Such
ordinals are called limit ordinals. The ordinal that are of the form α + 1 are called
successor ordinals.

So a transﬁnite induction or recursion is very often broken up into three cases. In the

induction phrasing, it looks like

 (Zero Case) First, resolve P (0).

 (Successor Case) Show that from P (α) we can get P (α + 1).

 (Limit Case) Show that P (λ) holds given P (α) for all α < λ, where λ is a limit

ordinal.

Similarly, transﬁnite recursion often is split into cases too.

 (Zero Case) First, deﬁne x0.

 (Successor Case) Deﬁne xα+1 from xα.

 (Limit Case) Deﬁne xλ from xα for all α < λ, where λ is a limit ordinal.

In both situations, ﬁnite induction only does the ﬁrst two cases, but if we’re able to do
the third case we can climb far above the barrier ω.

§81.4 Wrapping up functional equations

Let’s return to solving our problem.

Let Sn denote the set of “base” numbers we have at the nth step. In our example, we

might have

S1 = {1} , S2 =(cid:110)1,√2(cid:111) , S3 =(cid:110)1,√2,√3(cid:111) , S4 =(cid:110)1,√2,√3, π(cid:111) ,

. . .

and we’d like to keep building up Si until we can express all real numbers. For complete-
ness, let me declare S0 = ∅.

First, I need to be more precise about “independent”. Intuitively, this construction is

working because

a + b√2 + c√3 + dπ

81 Interlude: Cauchy’s functional equation and Zorn’s lemma

779

is never going to equal zero for rational numbers a, b, c, d (other than all zeros). In
general, a set X of numbers is “independent” if the combination

c1x1 + c2x2 + ··· + cmxm = 0

never occurs for rational numbers Q unless c1 = c2 = ··· = cm = 0. Here xi ∈ X are
distinct. Note that even if X is inﬁnite, I can only take ﬁnite sums! (This notion has a
name: we want X to be linearly independent over Q; see the chapter on vector spaces
for more on this!)

When do we stop? We’d like to stop when we have a set Ssomething that’s so big, every
real number can be written in terms of the independent numbers. (This notion also has
a name: it’s called a Q-basis.) Let’s call such a set spanning; we stop once we hit a
spanning set.
The idea that we can induct still seems okay: suppose Sα isn’t spanning. Then there’s
some number that is independent of Sα, say √2015π or something. Then we just add it
to get Sα+1. And we keep going.

Unfortunately, as I said before it’s not enough to be able to go from Sα to Sα+1
(successor case); we need to handle the limit case as well. But it turns out there’s a trick
we can do. Suppose we’ve constructed all the sets S0, S1, S2, . . . , one for each positive
integer n, and none of them are spanning. The next thing I want to construct is Sω;
somehow I have to “jump”. To do this, I now take the inﬁnite union

Sω

def= S0 ∪ S1 ∪ S2 ∪ . . . .

The elements of this set are also independent (why?).

Ta-da! With the simple trick of “union all the existing sets”, we’ve just jumped the
hurdle to the ﬁrst limit ordinal ω. Then we can construct Sω+1, Sω+2, . . . , once again –
just keep throwing in elements. Then when we need to jump the next hurdle to S2ω, we
just do the same trick of “union-ing” all the previous sets.

So we can formalize the process as follows:

1. Let S0 = ∅.

2. For a successor stage Sα+1, add any element to Sα to obtain Sα+1.

3. For a limit stage Sλ, take the union(cid:83)γ<λ Sγ.

How do we know that we’ll stop eventually? Well, the thing is that this process consumes
a lot of real numbers. In particular, the ordinals get larger than the size of R (assuming
Choice). Hence if we don’t stop we will quite literally reach a point where we have used
up every single real number. Clearly that’s impossible, because by then the elements
can’t possibly be independent!

So by transﬁnite recursion, we eventually hit some Sγ which is spanning: the elements

are all independent, but every real number can be expressed using it. Done!

§81.5 Zorn’s lemma

Now I can tell you what Zorn’s lemma is: it lets us do the same thing in any poset.

We can think of the above example as follows: consider all sets of independent elements.
These form a partially ordered set by inclusion, and what we did was quite literally climb
up a chain

S0 (cid:40) S1 (cid:40) S2 (cid:40) . . . .

780

Napkin, by Evan Chen (v1.5.20190718)

It’s not quite climbing since we weren’t just going one step at a time: we had to do
“jumps” to get up to Sω and resume climbing. But the main idea is to climb up a poset
until we’re at the very top; in the previous case, when we reached the spanning set.

The same thing works verbatim with any partially ordered set P. Let’s deﬁne some
terminology. A local maximum of the entire poset P is an element which has no other
elements strictly greater than it. (Most authors refer to this as “maximal element”, but I
think “local maximum” is a more accurate term.)

Now a chain of length γ is a set of elements pα for every α < γ such that p0 < p1 <
p2 < . . . . (Observe that a chain has a last element if and only if γ is a successor ordinal,
like ω + 3.) An upper bound to a chain is an element ˜p which is greater than or equal
to all elements of the chain; In particular, if γ is a successor ordinal, then just taking the
last element of the chain works.

In this language, Zorn’s lemma states that

Theorem 81.5.1 (Zorn’s lemma)
Let P be a nonempty partially ordered set. If every chain has an upper bound, then
P has a local maximum.

Chains with length equal to a successor ordinal always have upper bounds, but this
is not true in the limit case. So the hypothesis of Zorn’s lemma is exactly what lets
us “jump” up to deﬁne pω and other limit ordinals. And the proof of Zorn’s lemma is
straightforward: keep climbing up the poset at successor stages, using Zorn’s condition to
jump up at limit stages, and thus building a really long chain. But we have to eventually
stop, or we literally run out of elements of P. And the only possible stopping point is a
local maximum.

If we want to phrase our previous solution in terms of Zorn’s lemma, we’d say:

Proof. Look at the poset whose elements are sets of independent real numbers. Every

chain S0 (cid:40) S1 (cid:40) . . . has an upper bound(cid:83) Sα (which you have to check is actually an

element of the poset). Thus by Zorn, there is a local maximum S. Then S must be
spanning, because otherwise we could add an element to it.

So really, Zorn’s lemma is encoding all of the work of climbing that I argued earlier.
It’s a neat little package that captures all the boilerplate, and tells you exactly what you
need to check.

Image from [Go09]

81 Interlude: Cauchy’s functional equation and Zorn’s lemma

781

One last thing you might ask: where is the Axiom of Choice used? Well, the idea is
that for any chain there could be lots of ˜p’s, and you need to pick one of them. Since
you are making arbitrary choices inﬁnitely many times, you need the Axiom of Choice.
(Actually, you also need choice to talk about cardinalities as in theorem 1.) But really,
it’s nothing special.

§81.6 A few harder problems to think about

Problem 81A (Tukey’s lemma). Let F be a nonempty family of sets. Assume that for
any set A, the set A is in F if and only if all its ﬁnite subsets are in F. Prove that there
exists Y ∈ F such that X ⊆ Y for every X ∈ F.

82 Zermelo-Fraenkel with choice

Chapter 2 1

2 of [Le14] has a nice description of this.

§82.1 The ultimate functional equation

In abstract mathematics, we often deﬁne structures by what properties they should have;
for example, a group is a set and a binary operation satisfying so-and-so axioms, while a
metric space is a set and a distance function satisfying so-and-so axioms.

Nevertheless, these deﬁnitions rely on previous deﬁnitions. The colorful illustration of

[Le14] on this:

 A vector space is an abelian group with. . .

 An abelian group has a binary operation such that. . .

 A binary operation on a set is. . .

 A set is . . .

and so on.

We have to stop at some point, because inﬁnite lists of deﬁnitions are bad. The stopping
turns out to be a set, “deﬁned” by properties. The trick is that we never actually deﬁne
what a set is, but nonetheless postulate that these sets satisfy certain properties: these
are the ZFC axioms. Loosely, ZFC can be thought of as the ultimate functional equation.

Before talking about what these axioms are, I should talk about the caveats.

§82.2 Cantor’s paradox

Intuitively, a set is an unordered collection of elements. Two sets are equal if they share
the same elements:

{x | x is a featherless biped} = {x | x is human}

(let’s put aside the issue of dinosaurs).

As another example, we have our empty set ∅ that contains no objects. We can have
a set {1, 2, 3}, or maybe the set of natural numbers N = {0, 1, 2, . . .}. (For the purposes
of set theory, 0 is usually considered a natural number.) Sets can even contain other sets,
like {Z, Q, N}. Fine and dandy, right?
The trouble is that this deﬁnition actually isn’t good enough, and here’s why. If we
just say “a set is any collection of objects”, then we can consider a really big set V , the
set of all sets. So far no problem, right? We would have the oddity that V ∈ V , but oh
well, no big deal.
Unfortunately, this existence of this V leads immediately to a paradox. The classical
one is Bertrand’s Paradox. I will instead present a somewhat simpler one: not only does
V contain itself, every subset S ⊆ V is itself an element of V (i.e. S ∈ V ). If we let P(V )
denote the power set of V (i.e. all the subsets of V ), then we have an inclusion

This is bad, since:

P(V ) (cid:44)→ V.

783

784

Napkin, by Evan Chen (v1.5.20190718)

Lemma 82.2.1 (Cantor’s diagonal argument)
For any set X, it’s impossible to construct an injective map ι : P(X) (cid:44)→ X.

Proof. Assume for contradiction ι exists.

Exercise 82.2.2. Show that if, ι exists, then there exists a surjective map j : X (cid:16) P(X).

(This is easier than it appears, just “invert ι”).

We now claim that j can’t exist.

Let me draw a picture for j to give the idea ﬁrst:

x1 x2 x3 x4 x5
1

1

1

0

x1
x2
x3
x4
x5
...

j

j

j

j

(cid:55)−→ 0
(cid:55)−→ 1
(cid:55)−→ 0
(cid:55)−→ 1
(cid:55)−→ 0
...

j

. . .

. . .

. . .

. . .

1

1

0
. . .
1 . . .
...
. . .

1

1

0

1
...

0

0

0

1
...

1

0

1

1
...

Here, for each j(x) ⊆ X, I’m writing “1” to mean that the element is inside j(x), and “0”
otherwise. So j(x1) = {x2, x3, x5 . . .}. (Here the indices are ordinals rather than integers
as X may be uncountable. Experts may notice I’ve tacitly assumed a well-ordering
of X; but this picture is for motivation only so I won’t dwell on the point.) Then we
can read oﬀ the diagonal to get a new set. In our example, the diagonal speciﬁes a set
A = {x2, x4, x5 . . .}. Then we “invert” it to get a set B = {x1, x3, . . .}.

Back to the formal proof. As motivated above, we deﬁne

B = {x | x /∈ j(x)} .

By construction, B ⊆ X is not in the image of j, which is a contradiction since j was
supposed to be surjective.

Now if you’re not a set theorist, you could probably just brush this oﬀ, saying “oh
well, I guess you can’t look at certain sets”. But if you’re a set theorist, this worries you,
because you realize it means that you can’t just deﬁne a set as “a collection of objects”,
because then everything would blow up. Something more is necessary.

§82.3 The language of set theory

We need a way to refer to sets other than the informal description of “collection of
objects”.

So here’s what we’re going to do. We’ll start by deﬁning a formal language of set
theory, a way of writing logical statements. First of all we can throw in our usual logical
operators:

 ∀ means “for all”
 ∃ means “exists”
 = means “equal”

82 Zermelo-Fraenkel with choice

785

 X =⇒ Y means “if X then Y ”
 A ∧ B means “A and B”
 A ∨ B means “A or B”
 ¬A means “not A”.
Since we’re doing set theory, there’s only one more operator we add in: the inclusion

∈. And that’s all we’re going to use (for now).
to actually “construct” any sets, but rather refer to them indirectly, like so:

So how do we express something like “the set {1, 2}”? The trick is that we’re not going

∃S : x ∈ S ⇐⇒ ((x = 1) ∨ (x = 2)) .

This reads: “there exists an S such that x is in S if and only if either x = 1 or x = 2”.
We don’t have to refer to sets as objects in and of themselves anymore — we now have
a way to “create” our sets, by writing formulas for exactly what they contain. This is
something a machine can parse.

Well, what are we going to do with things like 1 and 2, which are not sets? Answer:

Elements of sets are themselves sets.

We’re going to make everything into a set. Natural numbers will be sets. Ordered pairs
will be sets. Functions will be sets. Later, I’ll tell you exactly how we manage to do
something like encode 1 as a set. For now, all you need to know is that that sets don’t
just hold objects; they hold other sets.

So now it makes sense to talk about whether something is a set or not: ∃x means “x
is a set”, while (cid:64)x means “x is not a set”. In other words, we’ve rephrased the problem
of deciding whether something is a set to whether it exists, which makes it easier to deal
with in our formal language. That means that our axiom system had better ﬁnd some
way to let us show a lot of things exist, without letting us prove

For if we prove this formula, then we have our “bad” set that caused us to go down the
rabbit hole in the ﬁrst place.

∃S∀x : x ∈ S.

§82.4 The axioms of ZFC

I don’t especially want to get into details about these axioms; if you’re interested, read:

 https://usamo.wordpress.com/2014/11/13/set-theory-an-intro-to-zfc-part-1/

 https://usamo.wordpress.com/2014/11/18/set-theory-part-2-constructing-the-ordinals/

Here is a much terser description of the axioms, which also includes the corresponding
sentence in the language of set theory. It is worth the time to get some practice parsing ∀,
∃, etc. and you can do so by comparing the formal sentences with the natural statement
of the axiom.

First, the two easiest axioms:

 Extensionality is the sentence ∀x∀y ((∀a (a ∈ x ⇐⇒ a ∈ y)) =⇒ x = y), which

says that if two sets x and y have the same elements, then x = y.

786

Napkin, by Evan Chen (v1.5.20190718)

 EmptySet is the sentence ∃a : ∀x ¬(x ∈ a); it says there exists a set with no

elements. By Extensionality this set is unique, so we denote it ∅.

The next two axioms give us basic ways of building new sets.

 Given two elements x and y, there exists a set a containing only those two elements.

In machine code, this is the sentence Pairing, written

∀x∀y∃a ∀z, z ∈ a ⇐⇒ ((z = x) ∨ (z = y)) .
By Extensionality this set a is unique, so we write a = {x, y}.

 Given a set a, we can create the union of the elements of a. For example, if
a = {{1, 2},{3, 4}}, then U = {1, 2, 3, 4} is a set. Formally, this is the sentence
Union:

∀a∃U ∀x [(x ∈ U ) ⇐⇒ (∃y : x ∈ y ∈ a)] .

Since U is unique by Extensionality, we denote it ∪a.

 We can construct the power set P(x). Formally, the sentence PowerSet says that

∀x∃P∀y(y ∈ P ⇐⇒ y ⊆ x)

where y ⊆ x is short for ∀z(z ∈ y =⇒ z ∈ x). As Extensionality gives us
uniqueness of P , we denote it P(x).

 Foundation says there are no inﬁnite descending chains

x0 (cid:51) x1 (cid:51) x2 (cid:51) . . . .

This is important, because it lets us induct. In particular, no set contains itself.

 Inﬁnity implies that ω = {0, 1, . . .} is a set.

These are all things you are already used to, so keep your intuition there. The next one
is less intuitive:

 The schema of restricted comprehension says: if we are given a set X, and

some formula φ(x) then we can ﬁlter through the elements of X to get a subset

Y = {x ∈ X | φ(x)} .

Formally, given a formula φ:
∀X ∃Y

∀y(y ∈ Y ⇐⇒ y ∈ X ∧ φ(y)).

Notice that we may only do this ﬁltering over an already given set. So it is not valid
to create {x | x is a set}. We are thankful for this, because this lets us evade Cantor’s
paradox.

Abuse of Notation 82.4.1. Note that technically, there are inﬁnitely many sen-
tences, a Comprehensionφ for every possible formula φ. By abuse of notation, we
let Comprehension abbreviate the inﬁnitely many axioms Comprehensionφ for every φ.
There is one last schema called Replacementφ. Suppose X is a set and φ(x, y) is some
formula such that for every x ∈ X, there is a unique y in the universe such that φ(x, y)
is true: for example “y = x ∪ {x}” works. (In eﬀect, φ is deﬁning a function f on X.)
Then there exists a set Y consisting exactly of these images: (i.e. f “X is a set).

Abuse of Notation 82.4.2. By abuse of notation, we let Replacement abbreviate the
inﬁnitely many axioms Replacementφ for every φ.

We postpone discussion of the Axiom of Choice momentarily.

82 Zermelo-Fraenkel with choice

§82.5 Encoding

787

Now that we have this rickety universe of sets, we can start re-building math. You’ll get
to see this more in the next chapter on ordinal numbers.

Deﬁnition 82.5.1. An ordered pair (x, y) is a set of the form

(x, y) := {{x} ,{x, y}} .

Note that (x, y) = (a, b) if and only if x = a and y = b. Ordered k-tuples can be

deﬁned recursively: a three-tuple (a, b, c) means (a, (b, c)).
Deﬁnition 82.5.2. A function f : X → Y is deﬁned as a collection of ordered pairs
such that

 If (x, y) ∈ f , then x ∈ X and y ∈ Y .
 For every x ∈ X, there is a unique y ∈ Y such that (x, y) ∈ f . We denote this y by

f (x).

Deﬁnition 82.5.3. The natural numbers are deﬁned inductively as

0 = ∅
1 = {0}
2 = {0, 1}
3 = {0, 1, 2}

...

The set of all natural numbers is denoted ω.

Abuse of Notation 82.5.4. Yes, I’m sorry, in set theory 0 is considered a natural
number. For this reason I’m using ω and not N since I explicitly have 0 /∈ N in all other
parts of this book.

Et cetera, et cetera.

§82.6 Choice and well-ordering

The Axiom of Choice states that given a collection Y of nonempty sets, there is a function
g : Y → ∪Y which “picks” an element of each member of Y . That means g(y) ∈ y for
every y ∈ Y . (The typical illustration is that Y contains inﬁnitely many drawers, and
each drawer (a y) has some sock in it.)

Formally, it is the sentence

∀Y (∅ /∈ Y =⇒ ∃g : Y → ∪Y such that ∀y ∈ Y (g(y) ∈ y) .)

The tricky part is not that we can conceive of such a function g, but that in fact this
function g is actually a set.

There is an equivalent formulation which is often useful.

Deﬁnition 82.6.1. A well-ordering < of X is a strict, total order on X which has no
inﬁnite descending chains.

Well-orderings on a set are very nice, because we can pick minimal elements: this lets
us do induction, for example. (And the Foundation axiom tells us ∈ is a well-ordering
itself.)

788

Napkin, by Evan Chen (v1.5.20190718)

Example 82.6.2 (Examples and non-examples of well-orderings)
(a) The natural numbers ω = {0, 1, 2, . . .} are well-ordered by <.
(b) The integers Z = {. . . ,−2,−1, 0, 1, 2, . . .} are not well-ordered by <, because

there are inﬁnite descending chains (take −1 > −2 > −3 > . . . ).

(c) The positive real numbers are not well-ordered by <, again because of the

descending chain 1

3 > . . . .

1 > 1

2 > 1

(d) The positive integers are not well-ordered by the divisibility operation |. While
there are no descending chains, there are elements which cannot be compared
(for example 3 (cid:45) 5, 5 (cid:45) 3 and 3 (cid:54)= 5).

Theorem 82.6.3 (Well-ordering theorem)

Assuming Choice, for every set we can place some well-ordering on it.

In fact, the well-ordering theorem is actually equivalent to the axiom of choice.

§82.7 Sets vs classes

Prototypical example for this section: The set of all sets is the standard example of a
proper class.

We close the discussion of ZFC by mentioning “classes”.
Roughly, the “bad thing” that happened was that we considered a set S, the “set of

all sets”, and it was too big. That is,

is not good. Similarly, we cannot construct a set

{x | x is a set}

{x | x is an ordered pair} .

The lesson of Cantor’s Paradox is that we cannot create any sets we want; we have to be
more careful than that.

Nonetheless, if we are given a set we can still tell whether or not it is an ordered pair.
So for convenience, we will deﬁne a class to be a “concept” like the “class of all ordered
pairs”. Formally, a class is deﬁned by some formula φ: it consists of the sets which satisfy
the formula.

In particular:

Deﬁnition 82.7.1. The class of all sets is denoted V , deﬁned by V = {x | x = x}. It is
called the von Neumann universe.

A class is a proper class if it is not a set, so for example we have:

Theorem 82.7.2 (There is no set of all sets)

V is a proper class.

82 Zermelo-Fraenkel with choice

789

Proof. Assume not, and V is a set. Then V ∈ V , which violates Foundation. (In fact, V
cannot be a set even without Foundation, as we saw earlier).

Abuse of Notation 82.7.3. Given a class C, we will write x ∈ C to mean that x has
the deﬁning property of C. For example, x ∈ V means “x is a set”.

It does not mean x is an element of V – this doesn’t make sense as V is not a set.

§82.8 A few harder problems to think about

Problem 82A. Let A and B be sets. Show that A ∩ B and A × B are sets.
Problem 82B. Show that the class of all groups is a proper class. (You can take the
deﬁnition of a group as a pair (G,·) where · is a function G × G → G.)
Problem 82C. Show that the axiom of choice follows from the well-ordering theorem.
Problem 82D†. Prove that actually, Replacement =⇒ Comprehension.
Problem 82E (From Taiwan IMO training camp). Consider inﬁnitely many people
each wearing a hat, which is either red, green, or blue. Each person can see the hat color
of everyone except themselves. Simultaneously each person guesses the color of their hat.
Show that they can form a strategy such that at most ﬁnitely many people guess their
color incorrectly.

83 Ordinals

§83.1 Counting for preschoolers

In preschool, we were told to count as follows. We deﬁned a set of symbols 1, 2, 3, 4, . . . .
Then the teacher would hold up three apples and say:

“One . . . two . . . three! There are three apples.”

Image from [Ho]

The implicit deﬁnition is that the last number said is the ﬁnal answer. This raises
some obvious problems if we try to count inﬁnite sets, but even in the ﬁnite world, this
method of counting fails for the simplest set of all: how many apples are in the following
picture?

Image from [Kr]

Answer: 0. There is nothing to say, and our method of counting has failed for the

simplest set of all: the empty set.

791

792

Napkin, by Evan Chen (v1.5.20190718)

§83.2 Counting for set theorists

Prototypical example for this section: ω + 1 = {0, 1, 2, . . . , ω} might work.

Rather than using the last number listed, I propose instead starting with a list of
symbols 0, 1, 2, . . . and making the ﬁnal answer the ﬁrst number which was not said.
Thus to count three apples, we would say

“Zero . . . one . . . two! There are three apples.”

We will call these numbers ordinal numbers (rigorous deﬁnition later). In particular,
we’ll deﬁne each ordinal to be the set of things we say:

0 = ∅
1 = {0}
2 = {0, 1}
3 = {0, 1, 2}

...

In this way we can write out the natural numbers. You can have some fun with this, by
saying things like

4 := {{} ,{{}} ,{{} ,{{}}} ,{{} ,{{}} ,{{} ,{{}}}}} .

In this way, we soon write down all the natural numbers. The next ordinal, ω,1 is deﬁned
as

ω = {0, 1, 2, . . .}

Then comes

ω + 1 = {0, 1, 2, . . . , ω}
ω + 2 = {0, 1, 2, . . . , ω, ω + 1}
ω + 3 = {0, 1, 2, . . . , ω, ω + 1, ω + 2}

...

And in this way we deﬁne ω + n, and eventually reach

ω · 2 = ω + ω = {0, 1, 2 . . . , ω, ω + 1, ω + 2, . . .}

ω · 2 + 1 = {0, 1, 2 . . . , ω, ω + 1, ω + 2, . . . , ω · 2} .

1As mentioned in the last chapter, it’s not immediate that ω is a set; its existence is generally postulated

by the Inﬁnity axiom.

83 Ordinals

In this way we obtain

793

0, 1, 2, 3, . . . , ω

ω + 1, ω + 2, . . . , ω + ω
ω · 2 + 1, ω · 2 + 2, . . . , ω · 3,
...
ω2 + 1, ω2 + 2, . . .
...
ω3, . . . , ω4, . . . , ωω
...
ωωω...

The ﬁrst several ordinals can be illustrated in a nice spiral.

Remark 83.2.1 — (Digression) The number ωωω...
has a name, ε0; it has the
property that ωε0 = ε0. The reason for using “ε” (which is usually used to denote

794

Napkin, by Evan Chen (v1.5.20190718)

small quantities) is that, despite how huge it may appear, it is actually a countable
set. More on that later.

§83.3 Deﬁnition of an ordinal

Our informal description of ordinals gives us a chain

0 ∈ 1 ∈ 2 ∈ ··· ∈ ω ∈ ω + 1 ∈ . . . .

To give the actual deﬁnition of an ordinal, I need to deﬁne two auxiliary terms ﬁrst.

Deﬁnition 83.3.1. A set x is transitive if whenever z ∈ y ∈ x, we have z ∈ x also.

Example 83.3.2 (7 is transitive)
The set 7 is transitive: for example, 2 ∈ 5 ∈ 7 =⇒ 2 ∈ 7.

Question 83.3.3. Show that this is equivalent to: whenever y ∈ x, y ⊆ x.

Moreover, recall the deﬁnition of “well-ordering”: a strict linear order with no inﬁnite
descending chains.

Example 83.3.4 (∈ is a well-ordering on ω · 3)
In ω · 3, we have an ordering

0 ∈ 1 ∈ 2 ∈ ··· ∈ ω ∈ ω + 1 ∈ ··· ∈ ω · 2 ∈ ω · 2 + 1 ∈ . . . .

which has no inﬁnite descending chains. Indeed, a typical descending chain might
look like

ω · 2 + 6 (cid:51) ω · 2 (cid:51) ω + 2015 (cid:51) ω + 3 (cid:51) ω (cid:51) 1000 (cid:51) 256 (cid:51) 42 (cid:51) 7 (cid:51) 0.

Even though there are inﬁnitely many elements, there is no way to make an inﬁnite
descending chain.

Exercise 83.3.5. (Important) Convince yourself there are no inﬁnite descending chains of
ordinals at all, without using the Foundation axiom.

Deﬁnition 83.3.6. An ordinal is a transitive set which is well-ordered by ∈. The class
of all ordinals is denoted On.

Question 83.3.7. Satisfy yourself that this deﬁnition works.

We typically use Greek letters α, β, etc. for ordinal numbers.

Deﬁnition 83.3.8. We write

 α < β to mean α ∈ β, and α > β to mean α (cid:51) β.
 α ≤ β to mean α ∈ β or α = β, and α ≥ β to mean α (cid:51) β or α = β,

83 Ordinals

795

Theorem 83.3.9 (Ordinals are strictly ordered)

Given any two ordinal numbers α and β, either α < β, α = β or α > β.

Proof. Surprisingly annoying, thus omitted.

Theorem 83.3.10 (Ordinals represent all order types)

Suppose < is a well-ordering on a set X. Then there exists a unique ordinal α such
that there is a bijection α → X which is order preserving.

Thus ordinals represent the possible equivalence classes of order types. Any time you
have a well-ordered set, it is isomorphic to a unique ordinal.

We now formalize the “+1” operation we were doing:

Deﬁnition 83.3.11. Given an ordinal α, we let α + 1 = α∪{α}. An ordinal of the form
α + 1 is called a successor ordinal.

Deﬁnition 83.3.12. If λ is an ordinal which is neither zero nor a successor ordinal, then
we say λ is a limit ordinal.

Example 83.3.13 (Sucessor and limit ordinals)
7, ω + 3, ω · 2 + 2015 are successor ordinals, but ω and ω · 2 are limit ordinals.

§83.4 Ordinals are “tall”

First, we note that:

Theorem 83.4.1 (There is no set of all ordinals)

On is a proper class.

Proof. Assume for contradiction not. Then On is well-ordered by ∈ and transitive, so
On is an ordinal, i.e. On ∈ On, which violates Foundation.

Exercise 83.4.2 (Unimportant). Give a proof without Foundation by considering On + 1.

From this we deduce:

Theorem 83.4.3 (Sets of ordinals are bounded)
Let A ⊆ On. Then there is some ordinal α such that A ⊆ α (i.e. A must be bounded).

Proof. Otherwise, look at (cid:83) A. It is a set. But if A is unbounded it must equal On,

which is a contradiction.

In light of this, every set of ordinals has a supremum, which is the least upper bound.
We denote this by sup A.

796

Napkin, by Evan Chen (v1.5.20190718)

Question 83.4.4. Show that

(a) sup(α + 1) = α for any ordinal α.

(b) sup λ = λ for any limit ordinal λ.

The pictorial “tall” will be explained in a few sections.

§83.5 Transﬁnite induction and recursion

The fact that ∈ has no inﬁnite descending chains means that induction and recursion
still work verbatim.

Theorem 83.5.1 (Transﬁnite induction)
Given a statement P (−), suppose that

 P (0) is true, and

 If P (α) is true for all α < β, then P (β) is true.

Then P (α) is true for every ordinal α.

Theorem 83.5.2 (Transﬁnite recursion)

To deﬁne a sequence xα for every ordinal α, it suﬃces to

 deﬁne x0, then

 for any β, deﬁne xβ for any α < β.

The diﬀerence between this and normal induction lies in the limit ordinals. In real life,
we might only do things like “deﬁne xn+1 = . . . ”. But this is not enough to deﬁne xα for
all α, because we can’t hit ω this way. Similarly, the simple +1 doesn’t let us hit the
ordinal 2ω, even if we already have ω + n for all n. In other words, simply incrementing
by 1 cannot get us past limit stages, but using transﬁnite induction to jump upwards
lets us sidestep this issue.

So a transﬁnite induction or recursion is very often broken up into three cases. In the

induction phrasing, it looks like

 (Zero Case) First, resolve P (0).

 (Successor Case) Show that from P (α) we can get P (α + 1).

 (Limit Case) For λ a limit ordinal, show that P (λ) holds given P (α) for all α < λ.

Similarly, transﬁnite recursion often is split into cases too.

 (Zero Case) First, deﬁne x0.

 (Successor Case) Deﬁne xα+1 from xα.

 (Limit Case) Deﬁne xλ from xα for all α < λ, where λ is a limit ordinal.

In both situations, ﬁnite induction only does the ﬁrst two cases, but if we’re able to do
the third case we can climb above the barrier ω.

83 Ordinals

§83.6 Ordinal arithmetic

797

Prototypical example for this section: 1 + ω = ω (cid:54)= ω + 1.

To give an example of transﬁnite recursion, let’s deﬁne addition of ordinals. Recall

that we deﬁned α + 1 = α ∪ {α}. By transﬁnite recursion, let

α + 0 = α

α + (β + 1) = (α + β) + 1

α + λ = (cid:91)β<λ

(α + β).

Here λ (cid:54)= 0.
consider the set

We can also do this explicitly: The picture is to just line up α after β. That is, we can

X = ({0} × α) ∪ ({1} × β)

(i.e. we tag each element of α with a 0, and each element of β with a 1). We then impose
a well-ordering on X by a lexicographic ordering <lex (sort by ﬁrst component, then by
second). This well-ordering is isomorphic to a unique ordinal,

Example 83.6.1 (2 + 3 = 5)
Under the explicit construction for α = 2 and β = 3, we get the set

X = {(0, 0) < (0, 1) < (1, 0) < (1, 1) < (1, 2)}

which is isomorphic to 5.

Example 83.6.2 (Ordinal arithmetic is not commutative)
Note that 1 + ω = ω! Indeed, under the transﬁnite deﬁnition, we have

1 + ω = ∪n(1 + n) = 2 ∪ 3 ∪ 4 ∪ ··· = ω.

With the explicit construction, we have

X = {(0, 0) < (1, 0) < (1, 1) < (1, 2) < . . .}

which is isomorphic to ω.

Exercise 83.6.3. Show that n + ω = ω for any n ∈ ω.

Remark 83.6.4 — Ordinal addition is not commutative. However, from the explicit
construction we can see that it is at least associative.

Similarly, we can deﬁne multiplication in two ways. By transﬁnite induction:

α · 0 = 0

α · (β + 1) = (α · β) + α
α · β.

α · λ = (cid:91)β<λ

798

Napkin, by Evan Chen (v1.5.20190718)

We can also do an explicit construction: α · β is the order type of

<lex applied to β × α.

Example 83.6.5 (Ordinal multiplication is not commutative)
We have ω · 2 = ω + ω, but 2 · ω = ω.

Exercise 83.6.6. Prove this.

Exercise 83.6.7. Verify that ordinal multiplication (like addition) is associative but not
commutative. (Look at γ × β × α.)
Exponentiation can also be so deﬁned, though the explicit construction is less natural.

α0 = 1

αβ+1 = αβ · α
αλ = (cid:91)β<λ
αβ.

Exercise 83.6.8. Verify that 2ω = ω.

§83.7 The hierarchy of sets

We now deﬁne the von Neumann Hierarchy by transﬁnite recursion.

Deﬁnition 83.7.1. By transﬁnite recursion, we set

V0 = ∅

Vα+1 = P(Vα)
Vα

Vλ = (cid:91)α<λ

By transﬁnite induction, we see Vα is transitive and that Vα ⊆ Vβ for all α < β.

Example 83.7.2 (Vα for α ≤ 3)
The ﬁrst few levels of the hierarchy are:

V0 = ∅
V1 = {0}
V2 = {0, 1}
V3 = {0, 1, 2,{1}} .

Notice that for each n, Vn consists of only ﬁnite sets, and each n appears in Vn+1 for
the ﬁrst time. Observe that

consists only of ﬁnite sets; thus ω appears for the ﬁrst time in Vω+1.

Vω = (cid:91)n∈ω

Vn

83 Ordinals

799

Question 83.7.3. How many sets are in V5?

Deﬁnition 83.7.4. The rank of a set y, denoted rank(y), is the smallest ordinal α such
that y ∈ Vα+1.

Example 83.7.5

rank(2) = 2, and actually rank(α) = α for any ordinal α (problem later). This is
the reason for the extra “+1”.

Question 83.7.6. Show that rank(y) is the smallest ordinal α such that y ⊆ Vα.

It’s not yet clear that the rank of a set actually exists, so we prove:

Theorem 83.7.7 (The von Neumann hierachy is complete)

The class V is equal to(cid:83)α∈On Vα. In other words, every set appears in some Vα.

Proof. Assume for contradiction this is false. The key is that because ∈ satisﬁes
Foundation, we can take a ∈-minimal counterexample x. Thus rank(y) is deﬁned for
every y ∈ x, and we can consider (by Replacement) the set

{rank(y) | y ∈ x} .

Since it is a set of ordinals, it is bounded. So there is some large ordinal α such that
y ∈ Vα for all y ∈ x, i.e. x ⊆ Vα, so x ∈ Vα+1.
This leads us to a picture of the universe V :

VV0=∅V1={∅}V2={∅,{∅}}VnVn+1=P(Vn)nVω=SVnVω+1=P(Vω)ωVω+2=P(Vω+1)ω+1Vω+ωOn800

Napkin, by Evan Chen (v1.5.20190718)

We can imagine the universe V as a triangle, built in several stages or layers, V0 (cid:40)
V1 (cid:40) V2 (cid:40) . . . . This universe doesn’t have a top: but each of the Vi do. However, the
universe has a very clear bottom. Each stage is substantially wider than the previous
one.

In the center of this universe are the ordinals: for every successor Vα, exactly one new
ordinal appears, namely α. Thus we can picture the class of ordinals as a thin line that
stretches the entire height of the universe. A set has rank α if it appears at the same
stage that α does.

All of number theory, the study of the integers, lives inside Vω. Real analysis, the study
of real numbers, lives inside Vω+1, since a real number can be encoded as a subset of N
(by binary expansion). Functional analysis lives one step past that, Vω+2. For all intents
and purposes, most mathematics does not go beyond Vω+ω. This pales in comparison to
the true magnitude of the whole universe.

§83.8 A few harder problems to think about

Problem 83A. Prove that rank(α) = α for any α by transﬁnite induction.

Problem 83B (Online Math Open). Count the number of transitive sets in V5.

Problem 83C (Goodstein). Let a2 be any positive integer. We deﬁne the inﬁnite
sequence a2, a3, . . . recursively as follows. If an = 0, then an+1 = 0. Otherwise, we write
an in base n, then write all exponents in base n, and so on until all numbers in the
expression are at most n. Then we replace all instances of n by n + 1 (including the
exponents!), subtract 1, and set the result to an+1. For example, if a2 = 11 we have

a2 = 23 + 2 + 1 = 22+1 + 2 + 1
a3 = 33+1 + 3 + 1 − 1 = 33+1 + 3
a4 = 44+1 + 4 − 1 = 44+1 + 3
a5 = 55+1 + 3 − 1 = 55+1 + 2
and so on. Prove that aN = 0 for some integer N > 2.

84 Cardinals

An ordinal measures a total ordering. However, it does not do a fantastic job at
measuring size. For example, there is a bijection between the elements of ω and ω + 1:

ω + 1 = { ω 0 1 2 . . .
ω = { 0 1 2 3 . . .

}
}.

In fact, as you likely already know, there is even a bijection between ω and ω2:

0
0
2
5
9

+
1
0
1
ω
4
ω · 2
8
ω · 3
13
ω · 4 14 . . .

. . .
. . .

4
10
. . .

3
6
11
. . .

2
3
7
12
. . .

So ordinals do not do a good job of keeping track of size. For this, we turn to the notion
of a cardinal number.

§84.1 Equinumerous sets and cardinals

Deﬁnition 84.1.1. Two sets A and B are equinumerous, written A ≈ B, if there is a
bijection between them.
Deﬁnition 84.1.2. A cardinal is an ordinal κ such that for no α < κ do we have α ≈ κ.

Example 84.1.3 (Examples of cardinals)
Every ﬁnite number is a cardinal. Moreover, ω is a cardinal. However, ω + 1, ω2,
ω2015 are not, because they are countable.

Example 84.1.4 (ωω is countable)
Even ωω is not a cardinal, since it is a countable union

and each ωn is countable.

ωω =(cid:91)n

ωn

Question 84.1.5. Why must an inﬁnite cardinal be a limit ordinal?

Remark 84.1.6 — There is something ﬁshy about the deﬁnition of a cardinal: it
relies on an external function f . That is, to verify κ is a cardinal I can’t just look at
κ itself; I need to examine the entire universe V to make sure there does not exist a
bijection f : κ → α for α < κ. For now this is no issue, but later in model theory

801

802

Napkin, by Evan Chen (v1.5.20190718)

this will lead to some highly counterintuitive behavior.

§84.2 Cardinalities

Now that we have deﬁned a cardinal, we can discuss the size of a set by linking it to a
cardinal.

Deﬁnition 84.2.1. The cardinality of a set X is the least ordinal κ such that X ≈ κ.
We denote it by |X|.

Question 84.2.2. Why must |X| be a cardinal?

Remark 84.2.3 — One needs the well-ordering theorem (equivalently, choice) in
order to establish that such an ordinal κ actually exists.

Since cardinals are ordinals, it makes sense to ask whether κ1 ≤ κ2, and so on. Our usual
intuition works well here.

Proposition 84.2.4 (Restatement of cardinality properties)

Let X and Y be sets.

(i) X ≈ Y if and only |X| = |Y |, if and only if there’s a bijection from X to Y .
(ii) |X| ≤ |Y | if and only if there is an injective map X (cid:44)→ Y .

Diligent readers are invited to try and prove this.

§84.3 Aleph numbers

Prototypical example for this section: ℵ0 = ω, and ℵ1 is the ﬁrst uncountable ordinal.

First, let us check that cardinals can get arbitrarily large:

Proposition 84.3.1
We have |X| < |P(X)| for every set X.

Proof. There is an injective map X (cid:44)→ P(X) but there is no injective map P(X) (cid:44)→ X
by Lemma 82.2.1.

Thus we can deﬁne:

Deﬁnition 84.3.2. For a cardinal κ, we deﬁne κ+ to be the least cardinal above κ,
called the successor cardinal.

This κ+ exists and has κ+ ≤ |P(κ)|.
Next, we claim that:

84 Cardinals

803

Exercise 84.3.3. Show that if A is a set of cardinals, then ∪A is a cardinal.

Thus by transﬁnite induction we obtain that:

Deﬁnition 84.3.4. For any α ∈ On, we deﬁne the aleph numbers as

ℵ0 = ω

ℵα+1 = (ℵα)+
ℵλ = (cid:91)α<λ
ℵα.

Thus we have the sequence of cardinals

0 < 1 < 2 < ··· < ℵ0 < ℵ1 < ··· < ℵω < ℵω+1 < . . . .

By deﬁnition, ℵ0 is the cardinality of the natural numbers, ℵ1 is the ﬁrst uncountable
ordinal, . . . .

We claim the aleph numbers constitute all the cardinals:

Lemma 84.3.5 (Aleph numbers constitute all inﬁnite cardinals)
If κ is a cardinal then either κ is ﬁnite (i.e. κ ∈ ω) or κ = ℵα for some α ∈ On.

Proof. Assume κ is inﬁnite, and take α minimal with ℵα ≥ κ. Suppose for contradiction
that we have ℵα > κ. We may assume α > 0, since the case α = 0 is trivial.

If α = α + 1 is a successor, then

ℵα < κ < ℵα = (ℵα)+

which contradicts the deﬁnition of the successor cardinal.

If α = λ is a limit ordinal, then ℵλ is the supremum(cid:83)γ<λ ℵγ. So there must be some

γ < λ with ℵγ > κ, which contradicts the minimality of α.
Deﬁnition 84.3.6. An inﬁnite cardinal which is not a successor cardinal is called a
limit cardinal. It is exactly those cardinals of the form ℵλ, for λ a limit ordinal, plus
ℵ0.

§84.4 Cardinal arithmetic

Prototypical example for this section: ℵ0 · ℵ0 = ℵ0 + ℵ0 = ℵ0

Recall the way we set up ordinal arithmetic. Note that in particular, ω + ω > ω and

ω2 > ω. Since cardinals count size, this property is undesirable, and we want to have

ℵ0 + ℵ0 = ℵ0
ℵ0 · ℵ0 = ℵ0

because ω + ω and ω · ω are countable. In the case of cardinals, we simply “ignore order”.

The deﬁnition of cardinal arithmetic is as expected:

Deﬁnition 84.4.1 (Cardinal arithmetic). Given cardinals κ and µ, deﬁne

and

κ + µ := |({0} × κ) ∪ ({1} × µ)|

κ · µ := |µ × κ| .

804

Napkin, by Evan Chen (v1.5.20190718)

Question 84.4.2. Check this agrees with what you learned in pre-school for ﬁnite cardinals.

Abuse of Notation 84.4.3. This is a slight abuse of notation since we are using the
same symbols as for ordinal arithmetic, even though the results are diﬀerent (ω · ω = ω2
but ℵ0 · ℵ0 = ℵ0). In general, I’ll make it abundantly clear whether I am talking about
cardinal arithmetic or ordinal arithmetic.

To help combat this confusion, we use separate symbols for ordinals and cardinals.
Speciﬁcally, ω will always refer to {0, 1, . . .} viewed as an ordinal; ℵ0 will always refer to
the same set viewed as a cardinal. More generally,

Deﬁnition 84.4.4. Let ωα = ℵα viewed as an ordinal.

However, as we’ve seen already we have that ℵ0 ·ℵ0 = ℵ0. In fact, this holds even more

generally:

Theorem 84.4.5 (Inﬁnite cardinals squared)
Let κ be an inﬁnite cardinal. Then κ · κ = κ.

Proof. Obviously κ · κ ≥ κ, so we want to show κ · κ ≤ κ.
The idea is to try to repeat the same proof that we had for ℵ0·ℵ0 = ℵ0, so we re-iterate
it here. We took the “square” of elements of ℵ0, and then re-ordered it according to the
diagonal:

. . .
. . .

4
10
. . .

0
0
2
5
9

1
1
0
4
1
8
2
3
13
4 14 . . .

3
6
11
. . .

2
3
7
12
. . .

We’d like to copy this idea for a general κ; however, since addition is less well-behaved
for inﬁnite ordinals it will be more convenient to use max{α, β} rather than α + β.
Speciﬁcally, we put the ordering <max on κ × κ as follows: for (α1, β1) and (α2, β2) in
κ × κ we declare (α1, β1) <max (α2, β2) if

 max{α1, β1} < max{α2, β2} or
 max{α1, β1} = max{α2, β2} and (α1, β1) is lexicographically earlier than (α2, β2).
This alternate ordering (which deliberately avoids referring to the addition) looks like:

2
4
5
8

0
0
2
6

4
. . .
3
9
16 . . .
0
10 17 . . .
1
2
11 18 . . .
3 12 13 14 15 19 . . .
4 20 21 22 23 24 . . .
...
. . .

1
1
3
7

...

...

...

...

...

Now we proceed by transﬁnite induction on κ. The base case is κ = ℵ0, done above.
Now, <max is a well-ordering of κ × κ, so we know it is in order-preserving bijection with
some ordinal γ. Our goal is to show that |γ| ≤ κ. To do so, it suﬃces to prove that for
any γ ∈ γ, we have |γ| < κ.

84 Cardinals

805

Suppose γ corresponds to the point (α, β) ∈ κ × κ under this bijection. If α and β are
both ﬁnite then certainly γ is ﬁnite too. Otherwise, let κ = max{α, β} < κ; then the
number of points below γ is at most

|α| · |β| ≤ κ · κ = κ
by the inductive hypothesis. So |γ| ≤ κ < κ as desired.

From this it follows that cardinal addition and multiplication is really boring:

Theorem 84.4.6 (Inﬁnite cardinal arithmetic is trivial)

Given cardinals κ and µ, one of which is inﬁnite, we have

κ · µ = κ + µ = max{κ, µ} .

Proof. The point is that both of these are less than the square of the maximum. Writing
out the details:

max{κ, µ} ≤ κ + µ
≤ κ · µ
≤ max{κ, µ} · max{κ, µ}
= max{κ, µ} .

§84.5 Cardinal exponentiation

Prototypical example for this section: 2κ = |P(κ)|.
Deﬁnition 84.5.1. Suppose κ and λ are cardinals. Then

κλ := |F (λ, κ)| .
Here F (A, B) is the set of functions from A to B.

Abuse of Notation 84.5.2. As before, we are using the same notation for both cardinal
and ordinal arithmetic. Sorry!

In particular, 2κ = |P(κ)| > κ, and so from now on we can use the notation 2κ freely.
(Note that this is totally diﬀerent from ordinal arithmetic; there we had 2ω =(cid:83)n∈ω 2n = ω.
In cardinal arithmetic 2ℵ0 > ℵ0.)
I have unfortunately not told you what 2ℵ0 equals. A natural conjecture is that 2ℵ0 = ℵ1;
this is called the Continuum Hypothesis. It turns out that this is undecidable – it is
not possible to prove or disprove this from the ZFC axioms.

§84.6 Coﬁnality

Prototypical example for this section: ℵ0, ℵ1, . . . are all regular, but ℵω has coﬁnality ω.
Deﬁnition 84.6.1. Let λ be an ordinal (usually a limit ordinal), and α another ordinal.
A map f : α → λ of ordinals is called coﬁnal if for every λ < λ, there is some α ∈ α
such that f (α) ≥ λ. In other words, the map reaches arbitrarily high into λ.

806

Napkin, by Evan Chen (v1.5.20190718)

Example 84.6.2 (Example of a coﬁnal map)
(a) The map ω → ωω by n (cid:55)→ ωn is coﬁnal.
(b) For any ordinal α, the identity map α → α is coﬁnal.

Deﬁnition 84.6.3. Let λ be a limit ordinal. The coﬁnality of λ, denoted cof(λ), is the
smallest ordinal α such that there is a coﬁnal map α → λ.

Question 84.6.4. Why must α be an inﬁnite cardinal?

Usually, we are interested in taking the coﬁnality of a cardinal κ.
Pictorially, you can imagine standing at the bottom of the universe and looking up the
chain of ordinals to κ. You have a machine gun and are ﬁring bullets upwards, and you
want to get arbitrarily high but less than κ. The coﬁnality is then the number of bullets
you need to do this.

We now observe that “most” of the time, the coﬁnality of a cardinal is itself. Such a

cardinal is called regular.

Example 84.6.5 (ℵ0 is regular)
cof(ℵ0) = ℵ0, because no ﬁnite subset of ℵ0 = ω can reach arbitrarily high.

Example 84.6.6 (ℵ1 is regular)
cof(ℵ1) = ℵ1. Indeed, assume for contradiction that some countable set of ordinals
A = {α0, α1, . . .} ⊆ ℵ1 reaches arbitrarily high inside ℵ1. Then Λ = ∪A is a
countable ordinal, because it is a countable union of countable ordinals. In other
words Λ ∈ ℵ1. But Λ is an upper bound for A, contradiction.

On the other hand, there are cardinals which are not regular; since these are the “rare”
cases we call them singular.

Example 84.6.7 (ℵω is not regular)
Notice that ℵ0 < ℵ1 < ℵ2 < . . . reaches arbitrarily high in ℵω, despite only having
ℵ0 terms. It follows that cof(ℵω) = ℵ0.

We now conﬁrm a suspicion you may have:

Theorem 84.6.8 (Successor cardinals are regular)
If κ = κ+ is a successor cardinal, then it is regular.

Proof. We copy the proof that ℵ1 was regular.
Assume for contradiction that for some µ ≤ κ, there are µ sets reaching arbitrarily
high in κ as a cardinal. Observe that each of these sets must have cardinality at most κ.
We take the union of all µ sets, which gives an ordinal Λ serving as an upper bound.

The number of elements in the union is at most

and hence |Λ| ≤ κ < κ.

#sets · #elms ≤ µ · κ = κ

84 Cardinals

807

§84.7 Inaccessible cardinals

So, what about limit cardinals? It seems to be that most of them are singular: if ℵλ (cid:54)= ℵ0
is a limit ordinal, then the sequence {ℵα}α∈λ (of length λ) is certainly coﬁnal.

Example 84.7.1 (Beth ﬁxed point)
Consider the monstrous cardinal

κ = ℵℵℵ

.

...

This might look frighteningly huge, as κ = ℵκ, but its coﬁnality is ω as it is the limit
of the sequence

ℵ0,ℵℵ0,ℵℵℵ0

, . . .

More generally, one can in fact prove that

cof(ℵλ) = cof(λ).

But it is actually conceivable that λ is so large that |λ| = |ℵλ|.
A regular limit cardinal other than ℵ0 has a special name: it is weakly inaccessible.
Such cardinals are so large that it is impossible to prove or disprove their existence in
ZFC. It is the ﬁrst of many so-called “large cardinals”.

An inﬁnite cardinal κ is a strong limit cardinal if

∀κ < κ 2κ < κ

for any cardinal κ. For example, ℵ0 is a strong limit cardinal.

Question 84.7.2. Why must strong limit cardinals actually be limit cardinals? (This is
oﬀensively easy.)

A regular strong limit cardinal other than ℵ0 is called strongly inaccessible.

§84.8 A few harder problems to think about

Problem 84A. Compute |Vω|.
Problem 84B. Prove that for any limit ordinal α, cof(α) is a regular cardinal.

Problem 84C(cid:63) (Strongly inaccessible cardinals). Show that for any strongly inaccessible
κ, we have |Vκ| = κ.
Problem 84D (K¨onig’s theorem). Show that

for every inﬁnite cardinal κ.

κcof(κ) > κ

XXII

Set Theory II: Model Theory and

Forcing

Part XXII: Contents

85 Inner model theory

811
85.1 Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 811
85.2 Sentences and satisfaction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 812
85.3 The Levy hierarchy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 814
. . . . . . . . . . . . . . . . . . . . . . . . . . 815
85.4 Substructures, and Tarski-Vaught
85.5 Obtaining the axioms of ZFC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 816
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 817
85.6 Mostowski collapse
85.7 Adding an inaccessible . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 817
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 819
85.8 FAQ’s on countable models
85.9 Picturing inner models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 820
. . . . . . . . . . . . . . . . . . . . . . . . 821
85.10A few harder problems to think about

86 Forcing

823
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 824
86.1 Setting up posets
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 825
86.2 More properties of posets
. . . . . . . . . . . . . . . . . . . . . . . . . . 826
86.3 Names, and the generic extension
86.4 Fundamental theorem of forcing . . . . . . . . . . . . . . . . . . . . . . . . . . . 829
86.5 (Optional) Deﬁning the relation . . . . . . . . . . . . . . . . . . . . . . . . . . . 829
86.6 The remaining axioms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 831
. . . . . . . . . . . . . . . . . . . . . . . . 831
86.7 A few harder problems to think about

87 Breaking the continuum hypothesis

833
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 833
87.1 Adding in reals
87.2 The countable chain condition . . . . . . . . . . . . . . . . . . . . . . . . . . . . 834
87.3 Preserving cardinals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 835
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 836
87.4 Inﬁnite combinatorics
. . . . . . . . . . . . . . . . . . . . . . . . 837
87.5 A few harder problems to think about

85 Inner model theory

Model theory is really meta, so you will have to pay attention here.
Roughly, a “model of ZFC” is a set with a binary relation that satisﬁes the ZFC
axioms, just as a group is a set with a binary operation that satisﬁes the group axioms.
Unfortunately, unlike with groups, it is very hard for me to give interesting examples of
models, for the simple reason that we are literally trying to model the entire universe.

§85.1 Models

Prototypical example for this section: (ω,∈) obeys PowerSet, Vκ is a model for κ inacces-
sible (later).
Deﬁnition 85.1.1. A model M consists of a set M and a binary relation E ⊆ M × M .
(The E relation is the “∈” for the model.)

Remark 85.1.2 — I’m only considering set-sized models where M is a set. Experts
may be aware that I can actually play with M being a class, but that would require
too much care for now.

If you have a model, you can ask certain things about it, such as “does it satisfy
EmptySet?”. Let me give you an example of what I mean and then make it rigor-
ous.

Example 85.1.3 (A stupid model)
Let’s take M = (M, E) = (ω,∈). This is not a very good model of ZFC, but let’s
see if we can make sense of some of the ﬁrst few axioms.

(a) M satisﬁes Extensionality, which is the sentence

∀x∀y∀a : (a ∈ x ⇐⇒ a ∈ y) =⇒ x = y.

This just follows from the fact that E is actually ∈.

(b) M satisﬁes EmptySet, which is the sentence

Namely, take a = ∅ ∈ ω.

∃a : ∀x ¬(x ∈ a).

(c) M does not satisfy Pairing, since {1, 3} is not in ω, even though 1, 3 ∈ ω.
(d) Miraculously, M satisﬁes Union, since for any n ∈ ω, ∪n is n − 1 (unless n = 0).

The Union axiom statements that

∀a∃U ∀x [(x ∈ U ) ⇐⇒ (∃y : x ∈ y ∈ a)] .

An important thing to notice is that the “∀a” ranges only over the sets in the
model of the universe, M .

811

812

Napkin, by Evan Chen (v1.5.20190718)

Example 85.1.4 (Important: this stupid model satisﬁes PowerSet)
Most incredibly of all: M = (ω,∈) satisﬁes PowerSet. This is a really important
example.
You might think this is ridiculous. Look at 2 = {0, 1}. The power set of this is
Well, let’s look more closely at PowerSet. It states that:

{0, 1, 2,{1}} which is not in the model, right?

∀x∃a∀y(y ∈ a ⇐⇒ y ⊆ x).

What happens if we set x = 2 = {0, 1}? Well, actually, we claim that a = 3 = {0, 1, 2}
works. The key point is “for all y” – this only ranges over the objects in M . In M ,
the only subsets of 2 are 0 = ∅, 1 = {0} and 2 = {0, 1}. The “set” {1} in the “real
world” (in V ) is not a set in the model M .
In particular, you might say that in this strange new world, we have 2n = n + 1,

since n = {0, 1, . . . , n − 1} really does have only n + 1 subsets.

Example 85.1.5 (Sentences with parameters)
The sentences we ask of our model are allowed to have “parameters” as well. For
example, if M = (ω,∈) as before then M satisﬁes the sentence

∀x ∈ 3(x ∈ 5).

§85.2 Sentences and satisfaction

With this intuitive notion, we can deﬁne what it means for a model to satisfy a sentence.

Deﬁnition 85.2.1. Note that any sentence φ can be written in one of ﬁve forms:

 x ∈ y
 x = y

 ¬ψ (“not ψ”) for some shorter sentence ψ
 ψ1 ∨ ψ2 (“ψ1 or ψ2”) for some shorter sentences ψ1, ψ2
 ∃xψ (“exists x”) for some shorter sentence ψ.

Question 85.2.2. What happened to ∧ (and) and ∀ (for all)? (Hint: use ¬.)

Often (almost always, actually) we will proceed by so-called “induction on formula
complexity”, meaning that we deﬁne or prove something by induction using this. Note
that we require all formulas to be ﬁnite.

M = (M, E). We want to ask whether M satisﬁes φ.

Now suppose we have a sentence φ, like a = b or ∃a∀x¬(x ∈ a), plus a model
To give meaning to this, we have to designate certain variables as parameters. For

example, if I asked you

“Does a = b?”

85 Inner model theory

813

the ﬁrst question you would ask is what a and b are. So a, b would be parameters: I have
to give them values for this sentence to make sense.

On the other hand, if I asked you

“Does ∃a∀x¬(x ∈ a)?”

then you would just say “yes”. In this case, x and a are not parameters. In general,
parameters are those variables whose meaning is not given by some ∀ or ∃.
In what follows, we will let φ(x1, . . . , xn) denote a formula φ, whose parameters are
x1, . . . , xn. Note that possibly n = 0, for example all ZFC axioms have no parameters.

Question 85.2.3. Try to guess the deﬁnition of satisfaction before reading it below. (It’s
not very hard to guess!)

Deﬁnition 85.2.4. Let M = (M, E) be a model. Let φ(x1, . . . , xn) be a sentence, and
let b1, . . . , bn ∈ M . We will deﬁne a relation

M (cid:15) φ[b1, . . . , bn]

and say M satisﬁes the sentence φ with parameters b1, . . . , bn.

The relationship is deﬁned by induction on formula complexity as follows:

 If φ is “x1 = x2” then M (cid:15) φ[b1, b2] ⇐⇒ b1 = b2.
 If φ is “x1 ∈ x2” then M (cid:15) φ[b1, b2] ⇐⇒ b1 E b2.
(This is what we mean by “E interprets ∈”.)
 If φ is “¬ψ” then M (cid:15) φ[b1, . . . , bn] ⇐⇒ M (cid:54)(cid:15) ψ[b1, . . . , bn].
 If φ is “ψ1 ∨ ψ2” then M (cid:15) φ[b1, . . . , bn] means M (cid:15) ψi[b1, . . . , bn] for some i = 1, 2.
 Most important case: suppose φ is ∃xψ(x, x1, . . . , xn). Then M (cid:15) φ[b1, . . . , bn] if

and only if

∃b ∈ M such that M (cid:15) ψ[b, b1, . . . , bn].

Note that ψ has one extra parameter.

Notice where the information of the model actually gets used. We only ever use E in
interpreting x1 ∈ x2; unsurprising. But we only ever use the set M when we are running
over ∃ (and hence ∀). That’s well-worth keeping in mind:

The behavior of a model essentially comes from ∃ and ∀, which search
through the entire model M .

And ﬁnally,

Deﬁnition 85.2.5. A model of ZFC is a model M = (M, E) satisfying all ZFC axioms.

We are especially interested in models of the form (M,∈), where M is a transitive
set. (We want our universe to be transitive, otherwise we would have elements of sets
which are not themselves in the universe, which is very strange.) Such a model is called
a transitive model.

Abuse of Notation 85.2.6. If M is a transitive set, the model (M,∈) will be abbreviated
to just M .

Deﬁnition 85.2.7. An inner model of ZFC is a transitive model satisfying ZFC.

814

Napkin, by Evan Chen (v1.5.20190718)

§85.3 The Levy hierarchy

Prototypical example for this section: isSubset(x, y) is absolute. The axiom EmptySet
is Σ1, isPowerSetOf(X, x) is Π1.

A key point to remember is that the behavior of a model is largely determined by ∃
Consider a formula such as

and ∀. It turns out we can say even more than this.

isEmpty(x) : ¬∃a(a ∈ x)

which checks whether a given set x has an element in it. Technically, this has an “∃” in it.
But somehow this ∃ does not really search over the entire model, because it is bounded
to search in x. That is, we might informally rewrite this as

¬(∃a ∈ x)

which doesn’t ﬁt into the strict form, but points out that we are only looking over a ∈ x.
We call such a quantiﬁer a bounded quantiﬁer.
We like sentences with bounded quantiﬁers because they designate properties which
are absolute over transitive models. It doesn’t matter how strange your surrounding
model M is. As long as M is transitive,

M (cid:15) isEmpty(∅)

will always hold. Similarly, the sentence

isSubset(x, y) : x ⊆ y i.e. ∀a ∈ x(a ∈ y)

is absolute. Sentences with this property are called Σ0 or Π0.

The situation is diﬀerent with a sentence like

isPowerSetOf(y, x) : ∀z (z ⊆ x ⇐⇒ z ∈ y)

which in English means “y is the power set of x”, or just y = P(x). The ∀z is not
bounded here. This weirdness is what allows things like

ω (cid:15) “{0, 1, 2} is the power set of {0, 1}”

and hence

ω (cid:15) PowerSet

which was our stupid example earlier. The sentence isPowerSetOf consists of an un-
bounded ∀ followed by an absolute sentence, so we say it is Π1.
Speciﬁcally,

More generally, the Levy hierarchy keeps track of how bounded our quantiﬁers are.

 Formulas which have only bounded quantiﬁers are ∆0 = Σ0 = Π0.

 Formulas of the form ∃x1 . . .∃xkψ where ψ is Πn are considered Σn+1.
 Formulas of the form ∀x1 . . .∀xkψ where ψ is Σn are considered Πn+1.

(A formula which is both Σn and Πn is called ∆n, but we won’t use this except for n = 0.)

85 Inner model theory

815

Example 85.3.1 (Examples of ∆0 sentences)
(a) The sentences isEmpty(x), x ⊆ y, as discussed above.
(b) The formula “x is transitive” can be expanded as a ∆0 sentence.

(c) The formula “x is an ordinal” can be expanded as a ∆0 sentence.

Exercise 85.3.2. Write out the expansions for “x is transitive” and “x is ordinal” in a ∆0
form.

Example 85.3.3 (More complex formulas)
(a) The axiom EmptySet is Σ1; it is ∃a(isEmpty(a)), and isEmpty(a) is ∆0.
(b) The formula “y = P(x)” is Π1, as discussed above.
(c) The formula “x is countable” is Σ1. One way to phrase it is “∃f an injective

map x (cid:44)→ ω”, which necessarily has an unbounded “∃f ”.

(d) The axiom PowerSet is Π3:

∀y∃P∀x(x ⊆ y ⇐⇒ x ∈ P ).

§85.4 Substructures, and Tarski-Vaught

Let M1 = (M1, E1) and M2 = (M2, E2) be models.
Deﬁnition 85.4.1. We say that M1 ⊆ M2 if M1 ⊆ M2 and E1 agrees with E2; we say
M1 is a substructure of M2.

That’s boring. The good part is:

Deﬁnition 85.4.2. We say M1 ≺ M2, or M1 is an elementary substructure of M2,
if M1 ⊆ M2 and for every sentence φ(x1, . . . , xn) and parameters b1, . . . , bn ∈ M1, we
have

M1 (cid:15) φ[b1, . . . , bn] ⇐⇒ M2 (cid:15) φ[b1, . . . , bn].

In other words, M1 and M2 agree on every sentence possible. Note that the bi have
to come from M1; if the bi came from M2 then asking something of M1 wouldn’t make
sense.

Let’s ask now: how would M1 ≺ M2 fail to be true? If we look at the possible
sentences, none of the atomic formulas, nor the “∧” and “¬”, are going to cause issues.
The intuition you should be getting by now is that things go wrong once we hit ∀ and
∃. They won’t go wrong for bounded quantiﬁers. But unbounded quantiﬁers search the
entire model, and that’s where things go wrong.
To give a “concrete example”: imagine M1 is MIT, and M2 is the state of Massachusetts.
If M1 thinks there exist hackers at MIT, certainly there exist hackers in Massachusetts.
Where things go wrong is something like:

M2 (cid:15) “∃x : x is a course numbered > 50”.

This is true for M2 because we can take the witness x = Math 55, say. But it’s false for
M1, because at MIT all courses are numbered 18.701 or something similar.

816

Napkin, by Evan Chen (v1.5.20190718)

The issue is that the witnesses for statements in M2 do not necessarily
propagate down to witnesses for M1.

The Tarski-Vaught test says this is the only impediment: if every witness in M2 can

be replaced by one in M1 then M1 ≺ M2.

Lemma 85.4.3 (Tarski-Vaught)
Let M1 ⊆ M2. Then M1 ≺ M2 if and only if: For every sentence φ(x, x1, . . . , xn)
and parameters b1, . . . , bn ∈ M1: if there is a witness ˜b ∈ M2 to M2 (cid:15) φ(˜b, b1 . . . , bn)
then there is a witness b ∈ M1 to M1 (cid:15) φ(b, b1, . . . , bn).

Proof. Easy after the above discussion. To formalize it, use induction on formula
complexity.

§85.5 Obtaining the axioms of ZFC

We now want to write down conditions for M to satisfy ZFC axioms. The idea is that
almost all the ZFC axioms are just Σ1 claims about certain desired sets, and so verifying
an axiom reduces to checking some appropriate “closure” condition: that the witness to
the axiom is actually in the model.

For example, the EmptySet axiom is “∃a(isEmpty(a))”, and so we’re happy as long as

∅ ∈ M , which is of course true for any nonempty transitive set M .

Lemma 85.5.1 (Transitive sets inheriting ZFC)

Let M be a nonempty transitive set. Then

(i) M satisﬁes Extensionality, Foundation, EmptySet.

(ii) M (cid:15) Pairing if x, y ∈ M =⇒ {x, y} ∈ M .
(iii) M (cid:15) Union if x ∈ M =⇒ ∪x ∈ M .
(iv) M (cid:15) PowerSet if x ∈ M =⇒ P(x) ∩ M ∈ M .
(v) M (cid:15) Replacement if for every x ∈ M and every function F : x → M which is
(vi) M (cid:15) Inﬁnity as long as ω ∈ M .

M -deﬁnable with parameters, we have F “x ∈ M as well.

Here, a set X ⊆ M is M -deﬁnable with parameters if it can be realized as

X = {x ∈ M | φ[x, b1, . . . , bn]}

for some (ﬁxed) choice of parameters b1, . . . , bn ∈ M . We allow n = 0, in which case we
say X is M -deﬁnable without parameters. Note that X need not itself be in M ! As
a trivial example, X = M is M -deﬁnable without parameters (just take φ[x] to always
be true), and certainly we do not have X ∈ M .

Exercise 85.5.2. Verify (i)-(iv) above.

85 Inner model theory

817

Remark 85.5.3 — Converses to the statements of Lemma 85.5.1 are true for all
claims other than (vi).

§85.6 Mostowski collapse

Up until now I have been only talking about transitive models, because they were easier
to think about. Here’s a second, better reason we might only care about transitive
models.

Lemma 85.6.1 (Mostowski collapse lemma)
Let X = (X,∈) be a model, where X is a set (possibly not transitive). Then there
exists an isomorphism π : X → M for a transitive model M = (M,∈).

This is also called the transitive collapse. In fact, both π and M are unique.

Proof. The idea behind the proof is very simple. Since ∈ is well-founded and extensional
(satisﬁes Foundation and Extensionality, respectively), we can look at the ∈-minimal
element x∅ of X with respect to ∈. Clearly, we want to send that to 0 = ∅.
Then we take the next-smallest set under ∈, and send it to 1 = {∅}. We “keep doing
this”; it’s not hard to see this does exactly what we want.
To formalize, deﬁne π by transﬁnite recursion:

This π, by construction, does the trick.

π(x) := {π(y) | y ∈ x} .

Remark 85.6.2 (Digression for experts) — Earlier versions of Napkin claimed this
was true for general models X = (X, E) with X (cid:15) Foundation + Extensionality.
This is false; it does not even imply E is well-founded, because there may be inﬁnite
descending chains of subsets of X which do not live in X itself. Another issue is
that E may not be set-like.

The picture of this is “collapsing” the elements of M down to the bottom of V , hence

the name.

Missing

ﬁgure

Picture of Mostowski collapse

§85.7 Adding an inaccessible

Prototypical example for this section: Vκ

At this point you might be asking, well, where’s my model of ZFC?

818

Napkin, by Evan Chen (v1.5.20190718)

I unfortunately have to admit now: ZFC can never prove that there is a model of ZFC
(unless ZFC is inconsistent, but that would be even worse). This is a result called G¨odel’s
incompleteness theorem.

Nonetheless, with some very modest assumptions added, we can actually show that a
model does exist: for example, assuming that there exists a strongly inaccessible cardinal
κ would do the trick, Vκ will be such a model (Problem 85D(cid:63)). Intuitively you can see
why: κ is so big that any set of rank lower than it can’t escape it even if we take their
power sets, or any other method that ZFC lets us do.

More pessimistically, this shows that it’s impossible to prove in ZFC that such a κ exists.
Nonetheless, we now proceed under ZFC+ for convenience, which adds the existence of
such a κ as a ﬁnal axiom. So we now have a model Vκ to play with. Joy!

Great. Now we do something really crazy.

Theorem 85.7.1 (Countable transitive model)
Assume ZFC+. Then there exists a transitive model X of ZFC such that X is a
countable set.

Proof. Fasten your seat belts.

First, since we assumed ZFC+, we can take Vκ = (Vκ,∈) as our model of ZFC. Start

with the set X0 = ∅. Then for every integer n, we do the following to get Xn+1.

 Start with Xn+1 containing every element of Xn.

 Consider a formula φ(x, x1, . . . , xn) and b1, . . . , bn in Xn. Suppose that Vκ thinks

there is a b ∈ Vκ for which

Vκ (cid:15) φ[b, b1, . . . , bn].

We then add in the element b to Xn+1.

 We do this for every possible formula in the language of set theory. We also have

to put in every possible set of parameters from the previous set Xn.

At every step Xn is countable. Reason: there are countably many possible ﬁnite sets
of parameters in Xn, and countably many possible formulas, so in total we only ever
add in countably many things at each step. This exhibits an inﬁnite nested sequence of
countable sets

None of these is a substructure of Vκ, because each Xn relies on witnesses in Xn+1. So
we instead take the union:

X0 ⊆ X1 ⊆ X2 ⊆ . . .

X =(cid:91)n

Xn.

This satisﬁes the Tarski-Vaught test, and is countable.

There is one minor caveat: X might not be transitive. We don’t care, because we just

take its Mostowski collapse.

Please take a moment to admire how insane this is. It hinges irrevocably on the fact

that there are countably many sentences we can write down.

Remark 85.7.2 — This proof relies heavily on the Axiom of Choice when we add
in the element b to Xn+1. Without Choice, there is no way of making these decisions
all at once.

Usually, the right way to formalize the Axiom of Choice usage is, for every formula

85 Inner model theory

819

φ(x, x1, . . . , xn), to pre-commit (at the very beginning) to a function fφ(x1, . . . , xn),
such that given any b1, . . . , bn fφ(b1, . . . , bn) will spit out the suitable value of b (if
one exists). Personally, I think this is hiding the spirit of the proof, but it does make
it clear how exactly Choice is being used.

These fφ’s have a name: Skolem functions.

The trick we used in the proof works in more general settings:

Theorem 85.7.3 (Downward L¨owenheim-Skolem theorem)
Let M = (M, E) be a model, and A ⊆ M . Then there exists a set B (called the
Skolem hull of A) with A ⊆ B ⊆ M , such that (B, E) ≺ M , and

|B| = max{ω,|A|} .

In our case, what we did was simply take A to be the empty set.

Question 85.7.4. Prove this. (Exactly the same proof as before.)

§85.8 FAQ’s on countable models

The most common one is “how is this possible?”, with runner-up “what just happened”.
Let me do my best to answer the ﬁrst question. It seems like there are two things

running up against each other:

(1) M is a transitive model of ZFC, but its universe is uncountable.

(2) ZFC tells us there are uncountable sets!

(This has confused so many people it has a name, Skolem’s paradox.)

The reason this works I actually pointed out earlier: countability is not absolute, it is

a Σ1 notion.

Recall that a set x is countable if there exists an injective map x (cid:44)→ ω. The ﬁrst
statement just says that in the universe V , there is a injective map F : M (cid:44)→ ω. In
particular, for any x ∈ M (hence x ⊆ M , since M is transitive), x is countable in V .
This is the content of the ﬁrst statement.
But for M to be a model of ZFC, M only has to think statements in ZFC are true.

More to the point, the fact that ZFC tells us there are uncountable sets means

M (cid:15) ∃x uncountable.

In other words,

M (cid:15) ∃x∀f If f : x → ω then f isn’t injective.

The key point is the ∀f searches only functions in our tiny model M . It is true that in
the “real world” V , there are injective functions f : x → ω. But M has no idea they
exist! It is a brain in a vat: M is oblivious to any information outside it.
So in fact, every ordinal which appears in M is countable in the real world. It is
just not countable in M . Since M (cid:15) ZFC, M is going to think there is some smallest
uncountable cardinal, say ℵM
1 . It will be the smallest (inﬁnite) ordinal in M with the
property that there is no bijection in the model M between ℵM
1 and ω. However, we
necessarily know that such a bijection is going to exist in the real world V .

820

Napkin, by Evan Chen (v1.5.20190718)

Put another way, cardinalities in M can look vastly diﬀerent from those in the real
world, because cardinality is measured by bijections, which I guess is inevitable, but leads
to chaos.

§85.9 Picturing inner models

Here is a picture of a countable transitive model M .

Note that M and V must agree on ﬁnite sets, since every ﬁnite set has a formula that

can express it. However, past Vω the model and the true universe start to diverge.

The entire model M is countable, so it only occupies a small portion of the universe,
below the ﬁrst uncountable cardinal ℵV
1 (where the superscript means “of the true universe
V ”). The ordinals in M are precisely the ordinals of V which happen to live inside the
model, because the sentence “α is an ordinal” is absolute. On the other hand, M has
only a portion of these ordinals, since it is only a lowly set, and a countable set at that.
To denote the ordinals of M , we write OnM , where the superscript means “the ordinals
as computed in M ”. Similarly, OnV will now denote the “set of true ordinals”.

Nonetheless, the model M has its own version of the ﬁrst uncountable cardinal ℵM

1 . In
1 ), but the necessary bijection witnessing

is countable (below ℵV

the true universe, ℵM

1

VMV0=∅V1={∅}V2={∅,{∅}}VωVω+1=P(Vω)ωℵV1ℵV2ℵM1ℵM2fOnVOnM85 Inner model theory

821

this might not be inside M . That’s why M can think ℵM
countable cardinal in the original universe.

1

is uncountable, even if it is a

So our model M is a brain in a vat. It happens to believe all the axioms of ZFC, and
so every statement that is true in M could conceivably be true in V as well. But M
can’t see the universe around it; it has no idea that what it believes is the uncountable
ℵM

is really just an ordinary countable cardinal.

1

§85.10 A few harder problems to think about

Problem 85A(cid:63). Show that for any transitive model M , the set of ordinals in M is itself
some ordinal.
Problem 85B†. Assume M1 ⊆ M2. Show that
(a) If φ is ∆0, then M1 (cid:15) φ[b1, . . . , bn] ⇐⇒ M2 (cid:15) φ[b1, . . . , bn].
(b) If φ is Σ1, then M1 (cid:15) φ[b1, . . . , bn] =⇒ M2 (cid:15) φ[b1, . . . , bn].
(c) If φ is Π1, then M2 (cid:15) φ[b1, . . . , bn] =⇒ M1 (cid:15) φ[b1, . . . , bn].

(This should be easy if you’ve understood the chapter.)
Problem 85C† (Reﬂection). Let κ be an inaccessible cardinal such that |Vα| < κ for
all α < κ. Prove that for any δ < κ there exists δ < α < κ such that Vα ≺ Vκ; in other
words, the set of α such that Vα ≺ Vκ is unbounded in κ. This means that properties of
Vκ reﬂect down to properties of Vα.

Problem 85D(cid:63) (Inaccessible cardinals produce models). Let κ be an inaccessible cardinal.
Prove that Vκ is a model of ZFC.

86 Forcing

We are now going to introduce Paul Cohen’s technique of forcing, which we then use

to break the Continuum Hypothesis.

Here is how it works. Given a transitive model M and a poset P inside it, we can
consider a “generic” subset G ⊆ P, where G is not in M . Then, we are going to construct
a bigger universe M [G] which contains both M and G. (This notation is deliberately the
same as Z[√2], for example – in the algebra case, we are taking Z and adding in a new
element √2, plus everything that can be generated from it.) By choosing P well, we can

cause M [G] to have desirable properties.

Picture:

The model M is drawn in green, and its extension M [G] is drawn in red.
The models M and M [G] will share the same ordinals, which is represented here as M
being no taller than M [G]. But one issue with this is that forcing may introduce some
new bijections between cardinals of M that were not there originally; this leads to the
phenomenon called cardinal collapse: quite literally, cardinals in M will no longer be

823

VV0=∅V1={∅}V2={∅,{∅}}VωVω+1=P(Vω)ωℵV1fℵM1ℵM[G]1OnVOnM=OnM[G]M⊆M[G]PG824

Napkin, by Evan Chen (v1.5.20190718)

cardinals in M [G], and instead just an ordinal. This is because in the process of adjoining
G, we may accidentally pick up some bijections which were not in the earlier universe.
In the diagram drawn, this is the function f mapping ω to ℵM
1 . Essentially, the diﬃculty
is that “κ is a cardinal” is a Π1 statement.

In the case of the Continuum Hypothesis, we’ll introduce a P such that any generic
real numbers. We’ll then show cardinal collapse does not

subset G will “encode” ℵM
occur, meaning ℵM [G]
= ℵM

2

2

2 . Thus M [G] will have ℵM [G]

2

real numbers, as desired.

§86.1 Setting up posets

Prototypical example for this section: Inﬁnite Binary Tree

Let M be a transitive model of ZFC. Let P = (P,≤) ∈ M be a poset with a maximal
element 1P which lives inside a model M . The elements of P are called conditions;
because they will force things to be true in M [G].
Deﬁnition 86.1.1. A subset D ⊆ P is dense if for all p ∈ P, there exists a q ∈ D such
that q ≤ p.

Examples of dense subsets include the entire P as well as any downwards “slice”.

Deﬁnition 86.1.2. For p, q ∈ P we write p (cid:107) q, saying “p is compatible with q”, if
there exists r ∈ P with r ≤ p and r ≤ q. Otherwise, we say p and q are incompatible
and write p ⊥ q.

Example 86.1.3 (Inﬁnite binary tree)
Let P = 2<ω be the inﬁnite binary tree shown below, extended to inﬁnity in the
obvious way:

(a) The maximal element 1P is the empty string ∅.
(b) D = {all strings ending in 001} is an example of a dense set.
(c) No two elements of P are compatible unless they are comparable.

Now, I can specify what it means to be “generic”.

Deﬁnition 86.1.4. A nonempty set G ⊆ P is a ﬁlter if
(a) The set G is upwards-closed: ∀p ∈ G(∀q ≥ p)(q ∈ G).
(b) Any pair of elements in G is compatible.

∅0100011011000001010011100101110111............86 Forcing

825

We say G is M -generic if for all D which are in the model M , if D is dense then
G ∩ D (cid:54)= ∅.

Question 86.1.5. Show that if G is a ﬁlter then 1P ∈ G.

Example 86.1.6 (Generic ﬁlters on the inﬁnite binary tree)
Let P = 2<ω. The generic ﬁlters on P are sets of the form

{0, b1, b1b2, b1b2b3, . . .} .

So every generic ﬁlter on P correspond to a binary number b = 0.b1b2b3 . . . .

It is harder to describe which reals correspond to generic ﬁlters, but they should
really “look random”. For example, the set of strings ending in 011 is dense, so one
should expect “011” to appear inside b, and more generally that b should contain
every binary string. So one would expect the binary expansion of π − 3 might
correspond to a generic, but not something like 0.010101 . . . . That’s why we call
them “generic”.

Exercise 86.1.7. Verify that these are every generic ﬁlter 2<ω has the form above. Show
that conversely, a binary number gives a ﬁlter, but it need not be generic.

Notice that if p ≥ q, then the sentence q ∈ G tells us more information than the
sentence p ∈ G. In that sense q is a stronger condition. In another sense 1P is the weakest
possible condition, because it tells us nothing about G; we always have 1P ∈ G since G is
upwards closed.

§86.2 More properties of posets

We had better make sure that generic ﬁlters exist. In fact this is kind of tricky, but for
countable models it works:

Lemma 86.2.1 (Rasiowa-Sikorski lemma)
Suppose M is a countable transitive model of ZFC and P is a partial order. Then
there exists an M -generic ﬁlter G.

Proof. Essentially, hit them one by one. Problem 86B.

∅0100011011000001010011100101110111G826

Napkin, by Evan Chen (v1.5.20190718)

Fortunately, for breaking CH we would want M to be countable anyways.
The other thing we want to do to make sure we’re on the right track is guarantee that
a generic set G is not actually in M . (Analogy: Z[3] is a really stupid extension.) The
condition that guarantees this is:
Deﬁnition 86.2.2. A partial order P is splitting if for all p ∈ P, there exists q, r ≤ p
such that q ⊥ r.

Example 86.2.3 (Inﬁnite binary tree is (very) splitting)
The inﬁnite binary tree is about as splitting as you can get. Given p ∈ 2<ω, just
consider the two elements right under it.

Lemma 86.2.4 (Splitting posets omit generic sets)
Suppose P is splitting. Then if F ⊆ P is a ﬁlter such that F ∈ M , then P \ F is
dense. In particular, if G ⊆ P is generic, then G /∈ M .

Proof. Consider p /∈ P \ F ⇐⇒ p ∈ F . Then there exists q, r ≤ p which are not
compatible. Since F is a ﬁlter it cannot contain both; we must have one of them outside
F , say q. Hence every element of p ∈ P \ (P \ F ) has an element q ≤ p in P \ F . That’s
enough to prove P \ F is dense.

Question 86.2.5. Deduce the last assertion of the lemma about generic G.

§86.3 Names, and the generic extension

We now deﬁne the names associated to a poset P.
Deﬁnition 86.3.1. Suppose M is a transitive model of ZFC, P = (P,≤) ∈ M is a partial
order. We deﬁne the hierarchy of P-names recursively by

Name0 = ∅

Nameα+1 = P(Nameα × P)
Nameλ = (cid:91)α<λ

Nameα.

Finally, Name =(cid:83)α Nameα denote the class of all P-names.

(These Nameα’s are the analog of the Vα’s: each Nameα is just the set of all names

with rank ≤ α.)
Deﬁnition 86.3.2. For a ﬁlter G, we deﬁne the interpretation of τ by G, denoted τ G,
using the transﬁnite recursion

We then deﬁne the model

τ G =(cid:8)σG | (cid:104)σ, p(cid:105) ∈ τ and p ∈ G(cid:9) .
M [G] =(cid:8)τ G | τ ∈ NameM(cid:9) .

In words, M [G] is the interpretation of all the possible P-names (as computed by M ).

86 Forcing

827

You should think of a P-name as a “fuzzy set”. Here’s the idea. Ordinary sets
are collections of ordinary sets, so fuzzy sets should be collections of fuzzy sets. These
fuzzy sets can be thought of like the Ghosts of Christmases yet to come: they represent
things that might be, rather than things that are certain. In other words, they represent
the possible futures of M [G] for various choices of G.

Every fuzzy set has an element p ∈ P pinned to it. When it comes time to pass
judgment, we pick a generic G and ﬁlter through the universe of P-names. The fuzzy
sets with an element of G attached to it materialize into the real world, while the fuzzy
sets with elements outside of G fade from existence. The result is M [G].

Example 86.3.3 (First few levels of the name hierarchy)
Let us compute

Name0 = ∅
Name1 = P(∅ × P)

= {∅}

Name2 = P({∅} × P)

= P ({(cid:104)∅, p(cid:105) | p ∈ P}) .

Compare the corresponding von Neuman universe.

V0 = ∅, V1 = {∅}, V2 = {∅,{∅}} .

Example 86.3.4 (Example of an interpretation)
As we said earlier, Name1 = {∅}. Now suppose

τ = {(cid:104)∅, p1(cid:105) ,(cid:104)∅, p2(cid:105) , . . . ,(cid:104)∅, pn(cid:105)} ∈ Name2.

Then

τ G = {∅ | (cid:104)∅, p(cid:105) ∈ τ and p ∈ G} =(cid:40){∅} if some pi ∈ G

otherwise.

∅

In particular, remembering that G is nonempty we see that

In fact, this holds for any natural number n, not just 2.

(cid:8)τ G | τ ∈ Name2(cid:9) = V M

2 .

So, M [G] and M agree on ﬁnite sets.

Now, we want to make sure M [G] contains the elements of M . To do this, we take

advantage of the fact that 1P must be in G, and deﬁne for every x ∈ M the set

ˇx = {(cid:104)ˇy, 1P(cid:105) | y ∈ x}

by transﬁnite recursion. Basically, ˇx is just a copy of x where we add check marks and
tag every element with 1P.

828

Napkin, by Evan Chen (v1.5.20190718)

Example 86.3.5

Compute ˇ0 = 0 and ˇ1 =(cid:8)(cid:10)ˇ0, 1P(cid:11)(cid:9). Thus

(ˇ0)G = 0

and (ˇ1)G = 1.

Question 86.3.6. Show that in general, (ˇx)G = x. (Rank induction.)

However, we’d also like to cause G to be in M [G]. In fact, we can write down the

name exactly:

˙G = {(cid:104)ˇp, p(cid:105) | p ∈ P} .

Question 86.3.7. Show that ( ˙G)G = G.

Question 86.3.8. Verify that M [G] is transitive: that is, if σG ∈ τ G ∈ M [G], show that
σG ∈ M [G]. (This is oﬀensively easy.)

In summary,

M [G] is a transitive model extending M (it contains G).

Moreover, it is reasonably well-behaved even if G is just a ﬁlter. Let’s see what we can

get oﬀ the bat.

Lemma 86.3.9 (Properties obtained from ﬁlters)
Let M be a transitive model of ZFC. If G is a ﬁlter, then M [G] is transitive and
satisﬁes Extensionality, Foundation, EmptySet, Inﬁnity, Pairing, and Union.

This leaves PowerSet, Replacement, and Choice.

Proof. Hence, we get Extensionality and Foundation for free. Then Inﬁnity and EmptySet
follows from M ⊆ M [G].
For Pairing, suppose σG

1 , σG

2 ∈ M [G]. Then

σ = {(cid:104)σ1, 1P(cid:105) ,(cid:104)σ2, 1P(cid:105)}

satisﬁes σG = {σG
which you are encouraged to try now.

1 , σG

2 }. (Note that we used M (cid:15) Pairing.) Union is left as a problem,

Up to here, we don’t need to know anything about when a sentence is true in M [G];
all we had to do was contrive some names like ˇx or {(cid:104)σ1, 1P(cid:105) ,(cid:104)σ2, 1P(cid:105)} to get the facts
we wanted. But for the remaining axioms, we are going to need this extra power are true
in M [G]. For this, we have to introduce the fundamental theorem of forcing.

86 Forcing

829

§86.4 Fundamental theorem of forcing

The model M unfortunately has no idea what G might be, only that it is some generic
ﬁlter.1 Nonetheless, we are going to deﬁne a relation (cid:13), called the forcing relation.
Roughly, we are going to write

p (cid:13) ϕ(σ1, . . . , σn)

where p ∈ P, σ1, . . . , σn ∈ M [G], if and only if:

For any generic G, if p ∈ G, then M [G] (cid:15) ϕ(σG

1 , . . . , σG

n ).

Note that (cid:13) is deﬁned without reference to G: it is something that M can see. We say
p forces the sentences ϕ(σ1, . . . , σn). And miraculously, we can deﬁne this relation in
such a way that the converse is true: a sentence holds if and only if some p forces it.

Theorem 86.4.1 (Fundamental theorem of forcing)
Suppose M is a transitive model of ZF. Let P ∈ M be a poset, and G ⊆ P is an
M -generic ﬁlter. Then,
(1) Consider σ1, . . . , σn ∈ NameM , Then

M [G] (cid:15) ϕ[σG

1 , . . . , σG
n ]

if and only if there exists a condition p ∈ G such that p forces the sentence
ϕ(σ1, . . . , σn). We denote this by p (cid:13) ϕ(σ1, . . . , σn).

(2) This forcing relation is (uniformly) deﬁnable in M .

I’ll tell you how the deﬁnition works in the next section.

§86.5 (Optional) Deﬁning the relation

Here’s how we’re going to go. We’ll deﬁne the most generous condition possible such
that the forcing works in one direction (p (cid:13) ϕ(σ1, . . . , σn) means M [G] (cid:15) ϕ[σG
n ]).
We will then cross our ﬁngers that the converse also works.

1 , . . . , σG

We proceed by induction on the formula complexity. It turns out in this case that the

atomic formula (base cases) are hardest and themselves require induction on ranks.
For some motivation, let’s consider how we should deﬁne p (cid:15) τ1 ∈ τ2 given that we’ve
already deﬁned p (cid:15) τ1 = τ2. We need to ensure this holds iﬀ
∀M -generic G with p ∈ G : M [G] (cid:15) τ G

1 ∈ τ G
2 .

So it suﬃces to ensure that any generic G (cid:51) p hits a condition q which forces τ G
a member τ G of τ G
hold if and only if

1 to equal
2 . In other words, we want to choose the deﬁnition of p (cid:13) τ1 ∈ τ2 to

{q ∈ P | ∃(cid:104)τ, r(cid:105) ∈ τ2 (q ≤ r ∧ q (cid:13) (τ = τ1))}

1You might say this is a good thing, here’s why. We’re trying to show that ¬CH is consistent with ZFC,
and we’ve started with a model M of the real universe V . But for all we know CH might be true in V
(what if V = L?), in which case it would also be true of M .

Nonetheless, we boldly construct M [G] an extension of the model M . In order for it to behave
diﬀerently from M , it has to be out of reach of M . Conversely, if M could compute everything about
M [G], then M [G] would have to conform to M ’s beliefs.

That’s why we worked so hard to make sure G ∈ M [G] but G /∈ M .

830

Napkin, by Evan Chen (v1.5.20190718)

is dense below in p. In other words, if the set is dense, then the generic must hit q, so it
must hit r, meaning that (cid:104)τr(cid:105) ∈ τ2 will get interpreted such that τ G ∈ τ G
2 , and moreover
the q ∈ G will force τ1 = τ .

Now let’s write down the deﬁnition. . . In what follows, the (cid:13) omits the M and P.

Deﬁnition 86.5.1. Let M be a countable transitive model of ZFC. Let P ∈ M be a
partial order. For p ∈ P and ϕ(σ1, . . . , σn) a formula in LST, we write τ (cid:13) ϕ(σ1, . . . , σn)
to mean the following, deﬁned by induction on formula complexity plus rank.
(1) p (cid:13) τ1 = τ2 means

(i) For all (cid:104)σ1, q1(cid:105) ∈ τ1 the set

Dσ1,q1 := {r | r ≤ q1 → ∃(cid:104)σ2, q2(cid:105) ∈ τ2 (r ≤ q2 ∧ r (cid:13) (σ1 = σ2))} .

is dense in p. (This encodes “τ1 ⊆ τ2”.)

(ii) For all (cid:104)σ2, q2(cid:105) ∈ τ2, the set Dσ2,q2 deﬁned similarly is dense below p.

(2) p (cid:13) τ1 ∈ τ2 means

{q ∈ P | ∃(cid:104)τ, r(cid:105) ∈ τ2 (q ≤ r ∧ q (cid:13) (τ = τ1))}

is dense below p.

(3) p (cid:13) ϕ ∧ ψ means p (cid:13) ϕ and p (cid:13) ψ.
(4) p (cid:13) ¬ϕ means ∀q ≤ p, q (cid:54)(cid:13) ϕ.
(5) p (cid:13) ∃xϕ(x, σ1, . . . , σn) means that the set

{q | ∃τ (q (cid:13) ϕ(τ, σ1, . . . , σn))}

is dense below p.

This is deﬁnable in M ! All we’ve referred to is P and names, which are in M . (Note
that being dense is deﬁnable.) Actually, in parts (3) through (5) of the deﬁnition above,
we use induction on formula complexity. But in the atomic cases (1) and (2) we are doing
induction on the ranks of the names.

So, the construction above gives us one direction (I’ve omitted tons of details, but. . . ).
Now, how do we get the converse: that a sentence is true if and only if something

forces it? Well, by induction, we can actually show:

Lemma 86.5.2 (Consistency and Persistence)

We have

(1) (Consistency) If p (cid:13) ϕ and q ≤ p then q (cid:13) ϕ.
(2) (Persistence) If {q | q (cid:13) ϕ} is dense below p then p (cid:13) ϕ.

You can prove both of these by induction on formula complexity. From this we
get:

86 Forcing

831

Corollary 86.5.3 (Completeness)

The set {p | p (cid:13) ϕ or p (cid:13) ¬ϕ} is dense.
Proof. We claim that whenever p (cid:54)(cid:13) ϕ then for some p ≤ p we have p (cid:13) ¬ϕ; this will
By the contrapositive of the previous lemma, {q | q (cid:13) ϕ} is not dense below p, meaning
for some p ≤ p, every q ≤ p gives q (cid:54)(cid:13) ϕ. By the deﬁnition of p (cid:15) ¬ϕ, we have p (cid:15) ¬ϕ.

establish the corollary.

And this gives the converse: the M -generic G has to hit some condition that passes
judgment, one way or the other. This completes the proof of the fundamental theorem.

§86.6 The remaining axioms

Theorem 86.6.1 (The generic extension satisﬁes ZFC)
Suppose M is a transitive model of ZFC. Let P ∈ M be a poset, and G ⊆ P is an
M -generic ﬁlter. Then

M [G] (cid:15) ZFC.

Proof. We’ll just do Comprehension, as the other remaining axioms are similar.

Suppose σG, σG

1 , . . . , σG

n ∈ M [G] are a set and parameters, and ϕ(x, x1, . . . , xn) is an

LST formula. We want to show that the set

A =(cid:8)x ∈ σG | M [G] (cid:15) ϕ[x, σG

1 , . . . , σG

n ](cid:9)

is in M [G]; i.e. it is the interpretation of some name.

Note that every element of σG is of the form ρG for some ρ ∈ dom(σ) (a bit of abuse
here, σ is a bunch of pairs of names and p’s, and the domain is just the set of names).
So by the fundamental theorem of forcing, we may write

A =(cid:8)ρG | ρ ∈ dom(σ) and ∃p ∈ G (p (cid:13) ρ ∈ σ ∧ ϕ(ρ, σ1, . . . , σn))(cid:9) .

To show A ∈ M [G] we have to write down a τ such that the name τ G coincides with A.
We claim that

τ = {(cid:104)ρ, p(cid:105) ∈ dom(σ) × P | p (cid:13) ρ ∈ σ ∧ ϕ(ρ, σ1, . . . , σn)}

is the correct choice. It’s actually clear that τ G = A by construction; the “content” is
showing that τ is in actually a name of M , which follows from M (cid:15) Comprehension.

So really, the point of the fundamental theorem of forcing is just to let us write down

this τ ; it lets us show that τ is in NameM without actually referencing G.

§86.7 A few harder problems to think about
Problem 86A. For a ﬁlter G and M a transitive model of ZFC, show that M [G] (cid:15) Union.

Problem 86B (Rasiowa-Sikorski lemma). Show that in a countable transitive model
M of ZFC, one can ﬁnd an M -generic ﬁlter on any partial order.

87 Breaking the continuum hypothesis

We now use the technique of forcing to break the Contiuum Hypothesis by choosing a
good poset P. As I mentioned earlier, one can also build a model where the Continuum
Hypothesis is true; this is called the constructible universe, i.e. the model V = L. However,
I think it’s more fun when things break. . .

§87.1 Adding in reals

Starting with a countable transitive model M .

about cardinal collapse later.

We want to choose P ∈ M such that (ℵ2)M many real numbers appear, and then worry
Recall the earlier situation where we set P to be the inﬁnite complete binary tree; its
nodes can be thought of as partial functions n → 2 where n < ω. Then G itself is a path
down this tree; i.e. it can be encoded as a total function G : ω → 2, and corresponds to a
real number.

We want to do something similar, but with ω2 many real numbers instead of just one.

In light of this, consider in M the poset

P = Add (ω2, ω) := ({p : ω2 × ω → 2, dom(p) < ω} ,⊇) .

These elements (conditions) are “partial functions”: we take some ﬁnite subset of ω2 × ω
and map it into 2 = {0, 1}. Moreover, we say p ≤ q if dom(p) ⊇ dom(q) and the two
functions agree over dom(q).

Question 87.1.1. What is 1P here?

Exercise 87.1.2. Show that a generic G can be encoded as a function ω2 × ω → 2.

Lemma 87.1.3 (G encodes distinct real numbers)
For α ∈ ω2 deﬁne

Gα = {n | G (α, n) = 0} ∈ P(N).

Then Gα (cid:54)= Gβ for any α (cid:54)= β.

833

∅0100011011000001010011100101110111G834

Napkin, by Evan Chen (v1.5.20190718)

Proof. We claim that the set

D = {q | ∃n ∈ ω : q (α, n) (cid:54)= q (β, n) are both deﬁned}

is dense.

Question 87.1.4. Check this. (Use the fact that the domains are all ﬁnite.)

Since G is an M -generic it hits this dense set D. Hence Gα (cid:54)= Gβ.
Since G ∈ M [G] and M [G] (cid:15) ZFC, it follows that each Gα is in M [G]. So there are at
least ℵM

real numbers in M . We are done once we can show there is no cardinal collapse.

2

§87.2 The countable chain condition

It remains to show that with P = Add(ω, ω2), we have that

In that case, since M [G] will have ℵM

To do this, we’ll rely on a combinatorial property of P:

many reals, we will be done.

2

ℵM [G]
= ℵM
2 .
2 = ℵM [G]

2

Deﬁnition 87.2.1. We say that A ⊆ P is a strong antichain if for any distinct p and
q in A, we have p ⊥ q.

Example 87.2.2 (Example of an antichain)
In the inﬁnite binary tree, the set A = {00, 01, 10, 11} is a strong antichain (in fact
maximal by inclusion).

This is stronger than the notion of “antichain” than you might be used to!1 We don’t
merely require that every two elements are incomparable, but that they are in fact
incompatible.

Question 87.2.3. Draw a ﬁnite poset and an antichain of it which is not strong.

Deﬁnition 87.2.4. A poset P has the κ-chain condition (where κ is a cardinal) if
all strong antichains in P have size less than κ. The special case κ = ℵ1 is called the
countable chain condition, because it implies that every strong antichain is countable.

We are going to show that if the poset has the κ-chain condition then it preserves
all cardinals greater than κ. In particular, the countable chain condition will show that
P preserves all the cardinals. Then, we’ll show that Add(ω, ω2) does indeed have this
property. This will complete the proof.

We isolate a useful lemma:

1In the context of forcing, some authors use “antichain” to refer to “strong antichain”. I think this is

lame.

87 Breaking the continuum hypothesis

835

Lemma 87.2.5 (Possible values argument)
Suppose M is a transitive model of ZFC and P is a partial order such that P has
the κ-chain condition in M . Let X, Y ∈ M and let f : X → Y be some function in
M [G], but f /∈ M .
Then there exists a function F ∈ M , with F : X → P(Y ) and such that for any
x ∈ X,

f (x) ∈ F (x)

and

|F (x)|M < κ.

What this is saying is that if f is some new function that’s generated, M is still able to
pin down the values of f to at most κ many values.

Proof. The idea behind the proof is easy: any possible value of f gives us some condition
in the poset P which forces it. Since distinct values must have incompatible conditions,
the κ-chain condition guarantees there are at most κ such values.

Here are the details. Let ˙f , ˇX, ˇY be names for f , X, Y . Start with a condition p such

that p forces the sentence

“ ˙f is a function from ˇX to ˇY ”.

We’ll work just below here.

For each x ∈ X, we can consider (using the Axiom of Choice) a maximal strong
antichain A(x) of incompatible conditions q ≤ p which forces f (x) to equal some value
y ∈ Y . Then, we let F (x) collect all the resulting y-values. These are all possible values,
and there are less than κ of them.

§87.3 Preserving cardinals

2

= ℵM

2 . It turns out that to verify this, one can check a weaker result.

As we saw earlier, cardinal collapse can still occur. For the Continuum Hypothesis
we want to avoid this possible, so we can add in ℵM
2 many real numbers and have
ℵM [G]
Deﬁnition 87.3.1. For M a transitive model of ZFC and P ∈ M a poset, we say P
preserves cardinals if ∀G ⊆ P an M -generic, the model M and M [G] agree on the
sentence “κ is a cardinal” for every κ. Similarly we say P preserves regular cardinals
if M and M [G] agree on the sentence “κ is a regular cardinal” for every κ.

Intuition: In a model M , it’s possible that two ordinals which are in bijection in V are
no longer in bijection in M . Similarly, it might be the case that some cardinal κ ∈ M
is regular, but stops being regular in V because some function f : κ → κ is coﬁnal but
happened to only exist in V . In still other words, “κ is a regular cardinal ” turns out to
be a Π1 statement too.

Fortunately, each implies the other. We quote the following without proof.

Proposition 87.3.2 (Preserving cardinals ⇐⇒ preserving regular cardinals)
Let M be a transitive model of ZFC. Let P ∈ M be a poset. Then for any λ, P
preserves cardinalities less than or equal to λ if and only if P preserves regular
cardinals less than or equal to λ. Moreover the same holds if we replace “less than
or equal to” by “greater than or equal to”.

836

Napkin, by Evan Chen (v1.5.20190718)

Thus, to show that P preserves cardinality and coﬁnalities it suﬃces to show that P

preserves regularity. The following theorem lets us do this:

Theorem 87.3.3 (Chain conditions preserve regular cardinals)
Let M be a transitive model of ZFC, and let P ∈ M be a poset. Suppose M satisﬁes
the sentence “P has the κ chain condition and κ is regular”. Then P preserves
regularity greater than or equal to κ.

Proof. Use the Possible Values Argument. Problem 87A.

In particular, if P has the countable chain condition then P preserves all the cardinals
(and coﬁnalities). Therefore, it remains to show that Add(ω, ω2) satisﬁes the countable
chain condition.

§87.4 Inﬁnite combinatorics

We now prove that Add(ω, ω2) satisﬁes the countable chain condition. This is purely
combinatorial, and so we work brieﬂy.

Deﬁnition 87.4.1. Suppose C is an uncountable collection of ﬁnite sets. C is a ∆-
system if there exists a root R with the condition that for any distinct X and Y in C,
we have X ∩ Y = R.

Lemma 87.4.2 (∆-System lemma)
Suppose C is an uncountable collection of ﬁnite sets. Then ∃C ⊆ C such that C is
an uncountable ∆-system.

Proof. There exists an integer n such that C has uncountably many guys of length n.
So we can throw away all the other sets, and just assume that all sets in C have size n.
We now proceed by induction on n. The base case n = 1 is trivial, since we can just

take R = ∅. For the inductive step we consider two cases.

First, assume there exists an a ∈ C contained in uncountably many F ∈ C. Throw
away all the other guys. Then we can just delete a, and apply the inductive hypothesis.
Now assume that for every a, only countably many members of C have a in them.
We claim we can even get a C with R = ∅. First, pick F0 ∈ C. It’s straightforward to
construct an F1 such that F1 ∩ F0 = ∅. And we can just construct F2, F3, . . .

Lemma 87.4.3

For all κ, Add(ω, κ) satisﬁes the countable chain condition.

Proof. Assume not. Let

be a strong antichain. Let

{pα : α < ω1}

Let C ⊆ C be such that C is uncountable, and C is a ∆-system with root R. Then let

C = {dom(pα) : α < ω1} .

B = {pα : dom(pα) ∈ R} .

87 Breaking the continuum hypothesis

837

Each pα ∈ B is a function pα : R → {0, 1}, so there are two that are the same.

Thus, we have proven that the Continuum Hypothesis cannot be proven in ZFC.

§87.5 A few harder problems to think about

Problem 87A. Let M be a transitive model of ZFC, and let P ∈ M be a poset. Suppose
M satisﬁes the sentence “P has the κ chain condition and κ is regular”. Show that P
preserves regularity greater than or equal to κ.

XXIII

Backmatter

Part XXIII: Contents

A Pedagogical comments and references

841
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 841
A.1 Basic algebra and topology
A.2 Second-year topics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 842
A.3 Advanced topics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 843
A.4 Topics not in Napkin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 844

B Hints to selected problems

C Sketches of selected solutions

845

855

D Glossary of notations

877
D.1 General . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 877
D.2 Functions and sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 877
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 878
D.3 Abstract and linear algebra
D.4 Quantum computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 879
. . . . . . . . . . . . . . . . . . . . . . . . . 879
D.5 Topology and real/complex analysis
. . . . . . . . . . . . . . . . . . . . . . . . . . . 880
D.6 Measure theory and probability
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 880
D.7 Algebraic topology
D.8 Category theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 881
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 882
D.9 Diﬀerential geometry
D.10 Algebraic number theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 882
D.11 Representation theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 883
D.12 Algebraic geometry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 884
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 885
D.13 Set theory

E Terminology on sets and functions

887
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 887
E.1 Sets
E.2 Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 888
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 890
E.3 Equivalence relations

A Pedagogical comments and references

Here are some higher-level comments on the way speciﬁc topics were presented, as well

as pointers to further reading.

§A.1 Basic algebra and topology

§A.1.i Linear algebra and multivariable calculus

Following the comments in Section 9.9, I dislike most presentations of linear algebra and
multivariable calculus since they miss the two key ideas, namely:

 In linear algebra, we study linear maps between spaces.

 In calculus, we approximate functions at points by linear functions.

Thus, I believe linear algebra should always be taught before multivariable calculus. In
particular, I do not recommend most linear algebra or multivariable calculus books.

For linear algebra, I’ve heard that [Ax97] follows this approach, hence the appropriate
name “Linear Algebra Done Right”. I followed with heavy modiﬁcations the proceedings
of Math 55a, see [Ga14].

For multivariable calculus and diﬀerential geometry, I found the notes [Sj05] to be
unusually well-written. I referred to it frequently while I was enrolled in Math 55b [Ga15].

§A.1.ii General topology

My personal view on spaces is that every space I ever work with is either metrizable or is
the Zariski topology.

I adopted the approach of [Pu02], using metric topology ﬁrst. I ﬁnd that metric spaces
are far more intuitive, and are a much better way to get a picture of what open / closed
/ compact etc. sets look like. This is the approach history took; general topology grew
out of metric topology.

I personally dislike starting any general topology class by deﬁning what a general
topological space is, because it doesn’t communicate a good picture of open and closed
sets to draw pictures of.

§A.1.iii Groups and commutative algebra

I teach groups before commutative rings but might convert later. Rings have better
examples, don’t have the confusion of multiplicative notation for additive groups, and
modding out by ideals is more intuitive.

There’s a speciﬁc thing I have a qualm with in group theory: the way that the concept
of a normal subgroup is introduced. Only [Go11] does something similar to what I do.
Most other people simply deﬁne a normal subgroup N as one with gN g−1 and then
proceed to deﬁne modding out, without taking the time to explain where this deﬁnition
comes from. I remember distinctly this concept as the ﬁrst time in learning math where
I didn’t understand what was going on. Only in hindsight do I see where this deﬁnition
came from; I tried hard to make sure my own presentation didn’t have this issue.

841

842

Napkin, by Evan Chen (v1.5.20190718)

I deliberately don’t include a chapter on just commutative algebra; other than the
chapter on rings and ideals. The reason is that I always found it easier to learn
commutative algebra theorems on the ﬂy, in the context of something like algebraic
number theory or algebraic geometry. For example, I ﬁnally understand why radicals and
the Nullstellensatz were important when I saw how they were used in algebraic geometry.
Before then, I never understood why I cared about them.

§A.1.iv Calculus

I do real analysis by using metric and general topology as the main ingredient, since I
think it’s the most useful later on and the most enlightening. In some senses, I am still
following [Pu02].

§A.2 Second-year topics

§A.2.i Measure theory and probability

The main inspiration for these lectures is Vadim Gorin’s 18.175 at MIT; [Go18] has really
nice lecture notes taken by Tony Zhang. I go into a bit more details of the measure
theory, and (for now) less into the probability. But I think probability is a great way to
motivate measure theory anyways, and conversely, it’s the right setting in to which state
things like the central limit theorem.

I also found [Ch08] quite helpful, as another possible reference.

§A.2.ii Complex analysis

I picked the approach of presenting the Cauchy-Goursat theorem as given (rather than
proving a weaker version by Stoke’s theorem, or whatever), and then deriving the key
result that holomorphic functions are analytic from it. I think this most closely mirrors
the “real-life” use of complex analysis, i.e. the computation of contour integrals.

The main reference for this chapter was [Ya12], which I recommend.

§A.2.iii Category theory

I enthusiastically recommend [Le14], from which my chapters are based, and which
contains much more than I had time to cover.

You might try reading chapters 2-4 in reverse order though: I found that limits were

much more intuitive than adjoints. But your mileage may vary.

The category theory will make more sense as you learn more examples of structures: it

will help to have read, say, the chapters on groups, rings, and modules.

§A.2.iv Quantum algorithms

The exposition given here is based oﬀ a full semester at MIT taught by Seth Lloyd, in
18.435J [Ll15]. It is written in a far more mathematical perspective.

I only deal with ﬁnite-dimensional Hilbert spaces, because that is all that is needed for
Shor’s algorithm, which is the point of this chapter. This is not an exposition intended
for someone who wishes to seriously study quantum mechanics (though it might be a
reasonable ﬁrst read): the main purpose is to give students a little appreciation for what
this “Shor’s algorithm” that everyone keeps talking about is.

A Pedagogical comments and references

843

§A.2.v Representation theory

I staunchly support teaching the representation of algebras ﬁrst, and then specializing
to the case of groups by looking at k[G]. The primary inﬂuence for the chapters here
is [Et11], and you might think of what I have here as just some selections from the ﬁrst
four chapters of this source.

§A.2.vi Set theory

Set theory is far oﬀ the beaten path. The notes I have written are based oﬀ the class I
took at Harvard College, Math 145a [Ko14].

My general impression is that the way I present set theory (trying to remain intuitive

and informal in a logical mineﬁeld) is not standard. Possible other reference: [Mi14].

§A.3 Advanced topics

§A.3.i Algebraic topology

I cover the fundamental group π1 ﬁrst, because I think the subject is more intuitive
this way. A possible reference in this topic is [Mu00]. Only later do I do the much more
involved homology groups. The famous standard reference for algebraic topology is
[Ha02], which is what almost everyone uses these days. But I also found [Ma13a] to be
very helpful, particularly in the part about cohomology rings.

I don’t actually do very much algebraic topology. In particular, I think the main reason
to learn algebraic topology is to see the construction of the homology and cohomology
groups from the chain complex, and watch the long exact sequence in action. The concept
of a (co)chain complex comes up often in other contexts as well, like the cohomology of
sheaves or Galois cohomology. Algebraic topology is by far the most natural one.

I use category theory extensively, being a category-lover.

§A.3.ii Algebraic number theory

I learned from [Og10], using [Le02] for the part about the Chebotarev density theorem.
When possible I try to keep the algebraic number theory chapter close at heart to

an “olympiad spirit”. Factoring in rings like Z[i] and Z[√−5] is very much an olympiad-

ﬂavored topic at heart: one is led naturally to the idea of factoring in general rings of
integers, around which the presentation is built. As a reward for the entire buildup, the
exposition ﬁnishes with the application of the Chebotarev density theorem to IMO 2003,
Problem 6.

§A.3.iii Algebraic geometry

My preferred introduction to algebraic geometry is [Ga03] for a ﬁrst read and [Va17] for
the serious version. Both sets of lecture notes are essentially self-contained.

I would like to confess now that I know relatively little algebraic geometry, and in my
personal opinion the parts on algebraic geometry are the weakest part of the Napkin.
This is reﬂected in my work here: in the entire set of notes I only barely ﬁnish deﬁning a
scheme, the ﬁrst central deﬁnition of the subject.

Nonetheless, I will foolishly still make some remarks about my own studies. I think

there are three main approaches to beginning the study of schemes:

844

Napkin, by Evan Chen (v1.5.20190718)

 Only looking at aﬃne and projective varieties, as part of an “introductory” class,

typically an undergraduate course.

 Studying aﬃne and projective varieties closely and using them as the motivating

example of a scheme, and then developing algebraic geometry from there.

 Jumping straight into the deﬁnition of a scheme, as in the well-respected and

challenging [Va17].

I have gone with the second approach, I think that if you don’t know what a scheme is,
then you haven’t learned algebraic geometry. But on the other hand I think the deﬁnition
of a scheme is diﬃcult to digest without having a good handle ﬁrst on varieties.

These opinions are based on my personal experience of having tried to learn the subject

through all three approaches over a period of a year. Your mileage may vary.

I made the decision to, at least for the second part, focus mostly on aﬃne schemes.
These already generalize varieties in several ways, and I think the jump is too much if
one starts then gluing schemes together. I would rather that the student ﬁrst feel like
they really understand how an aﬃne scheme works, before going on into the world where
they now have a general scheme X which is locally aﬃne (but probably not itself aﬃne).
The entire chapter dedicated to a gazillion examples of aﬃne schemes is a hint of this.

§A.4 Topics not in Napkin

§A.4.i Analytic number theory

I never had time to write up notes in Napkin for these. If you’re interested though,
I recommend [Hi13]. They are highly accessible and delightful to read. The only real
prerequisites are a good handle on Cauchy’s residue formula.

B Hints to selected problems

1A. Orders.

1B. Copy the proof of Fermat’s little theorem, using Lemma 1.2.5.

1C. For the former, decide where the isomorphism should send r and s, and the rest

will follow through. For the latter, look at orders.

1D(cid:63). Generated groups.
1F†. Use n = |G|.
1G. Draw inspiration from D6.

1H. Look at the group of 2 × 2 matrices mod p with determinant ±1.
2B. No.

2C. You can do this with bare hands. You can also use composition.

2D. ±x for good choices of ±.
2E. Project gaps onto the y-axis. Use the fact that uncountably many positive reals

cannot have ﬁnite sum.

3A. Write it out: φ(ab) = φ(a)φ(b).

3B. Yes, no.

3C. No.

3D. gcd(1000, 999) = 1.

3F. Find an example of order 8.

3G. Try to show G is the dihedral group of order 18. There is not much group theory

content here — just manipulation.

3H. Get yourself a list of English homophones, I guess. Don’t try too hard. Letter v is

the worst; maybe f elt = veldt?

4A. R = R[i].
4B. The isomorphism is given by x (cid:55)→ (1, 0) and 1 − x (cid:55)→ (0, 1).
4E. For (b) homomorphism is uniquely determined by the choice of ψ(x) ∈ R
4H. I think the result is true if you add the assumption A is Noetherian, so look for

trouble by picking A not Noetherian.

5A. Yes.

5B. The kernel is an ideal of K!

5C(cid:63). This is just a deﬁnition chase.

845

846

Napkin, by Evan Chen (v1.5.20190718)

5D(cid:63). Fermat’s little theorem type argument; cancellation holds in integral domains.

5E(cid:63). Just keep on adding in elements to get an ascending chain.

5F. Use the fact that both are PID’s.
5G. Show that the quotient Z[√2]/I has ﬁnitely many elements for any nonzero prime

ideal I. Therefore, the quotient is an integral domain, it is also a ﬁeld, and thus I
was a maximal ideal.

6B. (a): M is complete and bounded but not totally bounded. N is all no. For (b)

show that M ∼= R ∼= N .

6C†. As a set, we let M be the sequence of Cauchy sequences (xn) in M , modulo the

relation that (xn) ∼ (yn) if limn d(xn, yn) = 0.

6D. The standard solution seems to be via the so-called “Baire category theorem”.

7D. Let p be any point. If there is a real number r such that d(p, q) (cid:54)= r for any q ∈ M ,

then the r-neighborhood of p is clopen.

7E. Note that pZ is closed for each p. If there were ﬁnitely many primes, then(cid:83) pZ =
Z \ {−1, 1} would have to be closed; i.e. {−1, 1} would be open, but all open sets
here are inﬁnite.
7F. The balls at 0 should be of the form n! · Z.
7G. Appeal to Q.

8A. [0, 1] is compact.

8B. If and only if it is ﬁnite.

8E. Suppose pi = (xi, yi) is a sequence in X × Y (i = 1, 2, . . . ). Take a sub-sequence
such that the x-coordinate converges (throwing out some terms). Then take a
sub-sequence of that sub-sequence such that y-coordinate converges (throwing out
more terms).

8F†. Mimic the proof of Theorem 8.2.2. The totally bounded condition lets you do

Pigeonhole.

8H. The answer to both parts is no.

For (a) use Problem 8D.

For (b), color each circle in the partition based on whether it contains p but not q,
q but not p, or both.

9A†. Use the rank-nullity theorem. Also consider the zero map.

9C. a + b√5 (cid:55)→ √5a + 5b.
9E. Plug in y = −1, 0, 1. Use dimensions of R[x].
9F. Interpret as V ⊕ V → W for suitable V , W .
9H(cid:63). Use the fact that the inﬁnite chain of subspaces

ker T ⊆ ker T 2 ⊆ ker T 3 ⊆ . . .

and the similar chain for im T must eventually stabilize (for dimension reasons).

B Hints to selected problems

847

10E. Only 0 is. Look at degree.

10F. All of them are!

11A. Follows by writing T in an eigenbasis: then the diagonal entries are the eigenvalues.

11B†. Again one can just take a basis.
11C†. One solution is to just take a basis. Otherwise, interpret T ⊗ S (cid:55)→ Tr(T ◦ S) as a

linear map (V ∨ ⊗ W ) ⊗ (W ∨ ⊗ V ) → k, and verify that it is commutative.

11D. Look at the trace of T .

12A. The point is that

(v1 + cv2) ∧ v2 ··· ∧ vn = v1 ∧ v2 ··· ∧ vn + c(v2 ∧ v2 ··· ∧ vn)

and the latter term is zero.

12B. You can either do this by writing T in matrix form, or you can use the wedge

deﬁnition of det T with the basis given by Jordan form.

12C. This is actually immediate by taking any basis in which X is upper-triangular!

12D. You don’t need eigenvalues (though they could work also). In one direction, recall
that (by Problem 9B†) we can replace “isomorphism” by “injective”. In the other,
if T is an isomorphism, let S be the inverse map and look at det(S ◦ T ).

12E. Consider 1000 × 1000 matrix M with entries 0 on diagonal and ±1 oﬀ-diagonal.

Mod 2.

12F. There is a family of solutions other than just a = b = c = d.

One can solve the problem using Cayley-Hamilton. A more “bare-hands” approach
is to show the matrix is invertible (unless a = b = c = d) and then diagonalize the

matrix as M =(cid:20) s −q

p (cid:21)(cid:20)λ1

−r

0

r

0

λ2(cid:21)(cid:20)p q

s(cid:21) =(cid:20)psλ1 − qrλ2

pr(λ2 − λ1) psλ2 − qrλ1(cid:21).

qs(λ1 − λ2)

12G. Take bases, and do a fairly long calculation.

13B(cid:63). Fix an orthonormal basis e1, . . . , en. Use the fact that Rn is complete.

13C. Dot products in F2.

13D(cid:63). Deﬁne it on simple tensors then extend linearly.

13E. k = nn. Endow tensor products with an inner form. Note that “zero entry
somewhere on its diagonal” is equivalent to the product of those entries being zero.

14A. Use Parseval again, but this time on f (x) = x2.
14B. Deﬁne the Boolean function D : {±1}3 → R by D(a, b, c) = ab + bc + ca. Write out

the value of D(a, b, c) for each (a, b, c). Then, evaluate its expected value.

15A(cid:63). You can prove the result just by taking a basis e1, . . . , en of V and showing that it

is a linear map sending e1 to the basis (e∨1 )∨.

15B. Use Theorem 9.7.6 and it will be immediate (the four quantities equal the k in the

theorem).

848

Napkin, by Evan Chen (v1.5.20190718)

15C†. This actually is just the previous problem in disguise! The row rank is dim im T ∨

and the column rank is dim ker T .

15F. If there is a polynomial, check T T † = T †T directly. If T is normal, diagonalize it.

16A. Just apply Burnside’s lemma directly to get the answer of 198 (the relevant group

is D14).

16B. There are multiple ways to see this. One is to just do the algebraic manipulation.

Another is to use Cayley’s theorem to embed G into a symmetric group.

16C. Double-count pairs (g, x) with g · x = x.
16E†. Let G act on the left cosets {gH | g ∈ G} by left multiplication: g(cid:48) · gH = g(cid:48)gH.
Consider the orbit O of the coset H. By the orbit-stabilizer theorem, |O| divides
|G|. But |O| ≤ p also. So either O = {H} or O contains all cosets. The ﬁrst case
is impossible.

17B. Count Sylow 8 and 7 groups and let them intersect.

17C. Construct a non-abelian group such that all elements have order three.

17D. First, if G abelian it’s trivial. Otherwise, let Z(G) be the center of the group, which
is always a normal subgroup of G. Do a mod p argument via conjugation (or use
the class equation).

18A†. In the structure theorem, k/(si) ∈ {0, k}.
18B†. By theorem V ∼=(cid:76)i k[x]/(si) for some polynomials si. Write each block in the

form described.

18C†. Copy the previous proof, except using the other form of the structure theorem.

Since k[x] is algebraically closed each pi is a linear factor.

18D. The structure theorem is an anti-result here: it more or less implies that ﬁnitely
generated abelian groups won’t work. So, look for an inﬁnitely generated example.

19B†. For any a ∈ A, the map v (cid:55)→ a · v is intertwining.
19C(cid:63). For part (b), pick a basis and do T (cid:55)→ (T (e1), . . . , T (en)).
19D(cid:63). Right multiplication.

19E. Apply Problem 9H(cid:63).
20A. They are all one-dimensional, n of them. What are the homomorphisms Z/nZ →

C×?

20B. The span of (1, 0) is a subrepresentation.

20C. This is actually easy.

20D. There are only two one-dimensional ones (corresponding to the only two homomor-

phisms D10 → C×). So the remaining ones are two-dimensional.

21A†. Obvious. Let W =(cid:76) V mi

i

(possible since C[G] semisimple) thus χW =(cid:80)i miχVi.

21B. Use the previous problem, with χW = χ2

.

reﬂ0

B Hints to selected problems

849

21C. Characters. Note that |χW| = 1 everywhere.
21D. There are ﬁve conjugacy classes, 1, −1 and ±i, ±j, ±k. Given four of the represen-

tations, orthogonality can give you the ﬁfth one.

21E(cid:63). Write as

r(cid:88)i=1

(gh−1) = χ(cid:76)

i Vi⊗V ∨

i

χVi⊗V ∨

i

(gh−1) = χC[G](gh−1).

Now look at the usual basis for C[G].

(|→(cid:105)A ⊗ |←(cid:105)B − |←(cid:105)A |→(cid:105)B).

23A. Rewrite |Ψ−(cid:105) = − 1√2
23B. −1, 1, 1, 1. When we multiply them all together, we get that idA ⊗ idB ⊗ idC has
measurement −1, which is the paradox. What this means is that the values of
the measurements are created when we make the observation, and not prepared in
advance.

24A. One way is to create CCNOT using a few Fredkin gates.

24B. Plug in |ψ(cid:105) = |0(cid:105), |ψ(cid:105) = |1(cid:105), |ψ(cid:105) = |→(cid:105) and derive a contradiction.
24C. First show that the box sends |x1(cid:105)⊗···⊗|xm(cid:105)⊗|←(cid:105) to (−1)f (x1,...,xm)(|x1(cid:105)⊗···⊗

|xm(cid:105) ⊗ |←(cid:105)).

24D†. This is direct computation.

26B. Iﬀ the sequence is convergent!

26D. The nth partial sum is

1

1−r (1 − rn+1).

26F. This is a very tricky algebraic manipulation. Try setting an = x1 + ··· + xn for

xi ≥ 0.

26G. This is trickier than it looks. We have xn = exn − exn+1 but it requires some care
to prove convergences. Helpful hint: et ≥ t + 1 for all real numbers t, therefore all
xn’s are nonnegative.

26H. The limit always exists and equals zero. Consequently, f is continuous exactly at

irrational points.

29B†. Because you know all derivatives of sin and cos, you can compute their Taylor
series, which converge everywhere on R. At the same time, exp was deﬁned as a
Taylor series, so you can also compute it. Write them all out and compare.

29C†. Use repeated Rolle’s theorem. You don’t need any of the theory in this chapter to
solve this, so it could have been stated much earlier; but then it would be quite
unmotivated.

29D. Use Taylor’s theorem.

30A. Contradiction and mean value theorem (again!).

30B(cid:63). For every positive integer n, take a partition where every rectangle has width
w = b−a
n . Use the mean value theorem to construct a tagged partition such
that the ﬁrst rectangle has area f (a + w) − f (a), the second rectangle has area
f (a + 2w) − f (a + w), and so on; thus the total area is f (b) − f (a).

850

Napkin, by Evan Chen (v1.5.20190718)

30D. Write this as 1

Riemann integral.

n(cid:80)n

k=1

1

1+ k
n

. Then you can interpret it as a rectangle sum of a certain

31A(cid:63). Look at the Taylor series of f , and use Cauchy’s diﬀerentiation formula to show

that each of the larger coeﬃcients must be zero.

31B(cid:63). Proceed by contradiction, meaning there exists a sequence z1, z2,··· → z where
0 = f (z1) = f (z2) = . . . all distinct. Prove that f = 0 on an open neighborhood of
z by looking at the Taylor series of f and pulling out factors of z.

31C(cid:63). Take the interior of the agreeing points; show that this set is closed, which implies

the conclusion.

31E. Liouville. Look at

1

f (z)−w .

32C. This is called a “wedge contour”. Try to integrate over a wedge shape consisting
n . Take the limit as r → ∞

of a sector of a circle of radius r, with central angle 2π
then.

32D. It’s lima→∞(cid:82) a

35B. Show that

−a

cos x
x2+1 dx. For each a, construct a semicircle.

µ∗(S) =

S = ∅
S bounded and nonempty

0
1
∞ S not bounded.

This lets you solve (b) readily; I think the answer is just unbounded sets, ∅, and
one-point sets.

38A. You can read it oﬀ Theorem 38.3.1.

38B. After Pontryagin duality, we need to show G compact implies (cid:98)G discrete and G
discrete implies (cid:98)G compact. Both do not need anything fancy: they are topological

40A. This is actually trickier than it appears, you cannot just push quantiﬁers (contrary

facts.

to the name), but have to focus on ε = 1/m for m = 1, 2, . . . .
The problem is saying for each ε > 0, if n > Nε, we have µ(ω : |X(ω) − Xn(ω)| ≤
ε) = 1. For each m there are some measure zero “bad worlds”; take the union.

42B. Simply induct, with the work having been done on the k = 2 case.

43B. This is just a summation. You will need the fact that mixed partials are symmetric.

44A†. Direct application of Stokes’ theorem to α = f dx + g dy.

44B. This is just an exercises in sigma notation.

44C. This is a straightforward (but annoying) computation.

44D. We would want αp(v) = (cid:107)v(cid:107).

44E. Show that d2 = 0 implies(cid:82)∂c α = 0 for exact α. Draw an annulus.

46B. Note that p(x) is a minimal polynomial for r, but so is q(x) = xdeg pp(1/x). So q

and p must be multiples of each other.

B Hints to selected problems

851

46C(cid:63). (cid:12)(cid:12) 1

n (ε1 + ··· + εn)(cid:12)(cid:12) ≤ 1.

46D†. Only the obvious ones. Assume cos(qπ) ∈ Q. Let ζ be a root of unity (algebraic
integer as ζ N + 1 for some N ) and note that 2 cos(qθ) = ζ + ζ N−1 is both an
algebraic integer and a rational number.

46E. View as roots of unity. Note 1

2 isn’t an algebraic integer.

46F. Let α = α1, α2, . . . , αn be its conjugates. Look at the polynomial (x−αe

across e ∈ N. Pigeonhole principle on all possible polynomials.

1) . . . (x−αe
n)

47A(cid:63). The norm is multiplicative and equal to product of Galois conjugates.

47B(cid:63). It’s isomorphic to K.
47C. Taking the standard norm on Q(√2) will destroy it.
47D. Norm in Q( 3√2).
47E†. Obviously Z[ζp] ⊆ OK, so our goal is to show the reverse inclusion. Show that for any
α ∈ OK, the trace of α(1−ζp) is divisible by p. Given x = a0+a1ζp+···+ap−2ζ p−2 ∈
OK (where ai ∈ Q), consider (1 − ζp)x.

48C. Copy the proof of the usual Fermat’s little theorem.

48D†. Clear denominators!

48E. (a) is straightforward. For (b) work mod p. For (c) use norms.

49A. Repeat the previous procedure.

49B. You should get a group of order three.

49C. Mimic the proof of part (a) of Minkowski’s theorem.

49D. Linear algebra.

49E. Factor in Q(i).
49F. Factor p, show that the class group of Q(√−5) has order two.

50A(cid:63). Direct linear algebra computation.

50B(cid:63). Let M be the “embedding” matrix. Look at M(cid:62)M , where M(cid:62) is the transpose

matrix.

50C(cid:63). Vandermonde matrices.

50D. MK ≥ 1 must hold. Bash.
52A(cid:63). Look at the image of ζp.

52B. Repeated quadratic extensions have degree 2, so one can only get powers of two.

52D. Hint: σ(x2) = σ(x)2 ≥ 0 plus Cauchy’s Functional Equation.
52E. By induction, suﬃces to show Q(α, β) = Q(γ) for some γ in terms of α and β. For

all but ﬁnitely many rational λ, the choice γ = α + λβ will work.

852

Napkin, by Evan Chen (v1.5.20190718)

53A†. The Fibonacci sequence is given by Fn = αn−βn

and β = 1−√5
are the two roots of P (X) def= X 2 − X − 1. Show the polynomial P (X) is irreducible
modulo 127; then work in the splitting ﬁeld of P , namely Fp2.
Show that Fp = −1, Fp+1 = 0, F2p+1 = 1, F2p+2 = 0. (Look at the action of
Gal(Fp2/Fp) on the roots of P .)

α−β where α = 1+√5

2

2

54A†. Show that no rational prime p can remain inert if Gal(K/Q) is not cyclic. Indeed,

if p is inert then Dp ∼= Gal(K/Q).

55A. Modify the end of the proof of quadratic reciprocity.

55C†. Chebotarev Density on Q(ζm).
55E. By primitive roots, it’s the same as the action of ×3 on Z/(p − 1)Z. Let ζ be a
(p − 1)st root of unity. Take d =(cid:81)i<j(ζ i − ζ j), think about Q(d), and ﬁgure out
how to act on it by x (cid:55)→ x3.
56A†. Pick m so that f(L/Q) | m∞.
56B†. Apply the Takagi existence theorem with m = 1.

57C†. Prove and use the fact that a quotients of compact spaces remain compact.

61A. The category A × 2 has “redundant arrows”.
64A. Take the n − 1st homology groups.
64B. Build F as follows: draw the ray from x through f (x) and intersect it with the

boundary Sn−1.

65A. Induction on m, using hemispheres.

65B. One strategy is induction on p, with base case p = 1. Another strategy is to let U

be the desired space and let V be the union of p non intersecting balls.

65C(cid:63). Use Theorem 65.2.5. Note that Rn \ {0} is homotopy equivalent to Sn−1.
65D(cid:63). Find a new short exact sequence to apply Theorem 65.2.1 to.
65E(cid:63). It’s possible to use two cylinders with U and V . This time the matrix is(cid:20)1

or some variant though; in particular, it’s injective, so (cid:101)H2(X) = 0.

66B. Use Theorem 65.2.5.

1

1 −1(cid:21)

67A†. CPn has no cells in adjacent dimensions, so all dk maps must be zero.
67B. The space Sn − {x0} is contractible.
67D. You won’t need to refer to any elements. Start with

H2(X) ∼= H2(X 3) ∼= H2(X 2)/ ker(cid:2)H2(X 2) (cid:16) H2(X 3)(cid:3) ,

say. Take note of the marked injective and surjective arrows.

67E†. There is one cell of each dimension. Show that the degree of dk is deg(id)+deg(−id),

hence dk is zero or ·2 depending on whether k is even or odd.

B Hints to selected problems

853

69A†. Write H k(M ; Z) in terms of Hk(M ) using the UCT, and analyze the ranks.

69B. Use the previous result on Betti numbers.

69C. Use the Z/2Z cohomologies, and ﬁnd the cup product.
69D. Assume that r : Sm × Sn → Sm ∨ Sn is such a map. Show that the induced map
H•(Sm ∨ Sn; Z) → H•(Sm × Sn; Z) between their cohomology rings is monic (since
there exists an inverse map i).

70A. Squares are nonnegative.

70B. This is actually an equivalent formulation of the Weak Nullestellensatz.

70C. Use the weak Nullstellensatz on n + 1 dimensions. Given f vanishing on everything,

consider xn+1f − 1.

73B. You will need to know about complex numbers in Euclidean geometry to solve this

problem.

74B†. Use the standard aﬃne charts.

74C. Examine the global regular functions.
74D. Assume f was an isomorphism. Then it gives an isomorphism f (cid:93) : OV (V ) →
OX (X) = C[x, y]. Thus we may write OV (V ) = C[a, b], where f (cid:93)(a) = x and
f (cid:93)(b) = y. Let f (p) = q where V(a, b) = {q}. Use the deﬁnition of pullback to
prove p ∈ V(x, y), contradiction.

75C. The stalk is R at points in the closure of {p}, and 0 elsewhere.
75D. Show that the complement {p | [s]p = 0} is open.
76B. Consider zero divisors.

76C(cid:63). Only one! A proof will be given a few chapters later.

76D. No. Imagine two axes.

77A. Galois conjugates.
78B. k[x, y] × k[z, z−1].
78D. It’s isomorphic to R!
80A. Use the fact that AﬀSch (cid:39) CRing.
84A. supk∈ω |Vk|.
84B. Rearrange the coﬁnal maps to be nondecreasing.

85C†. This is very similar to the proof of L¨owenheim-Skolem. For a sentence φ, let fφ
send α to the least β < κ such that for all (cid:126)b ∈ Vα, if there exists a ∈ M such that
Vκ (cid:15) φ[a,(cid:126)b] then ∃a ∈ Vβ such that Vκ (cid:15) φ[a,(cid:126)b]. (To prove this β exists, use the fact
that κ is coﬁnal.) Then, take the supremum over the countably many sentences for
each α.

86B. Let D1, D2, . . . be the dense sets (there are countably many of them).
87A. Assume not, and take λ > κ regular in M ; if f : λ → λ, use the Possible Values

Argument on f to generate a function in M that breaks coﬁnality of λ.

C Sketches of selected solutions

1A. The point is that ♥ is a group, G (cid:40) ♥ a subgroup and G ∼= ♥. This can only occur
if |♥| = ∞; otherwise, a proper subgroup would have strictly smaller size than the
original.

1B. Let {g1, g2, . . . , gn} denote the elements of G. For any g ∈ G, this is the same as
the set {gg1, . . . , ggn}. Taking the entire product and exploiting commutativity
gives gn · g1g2 . . . gn = g1g2 . . . gn, hence gn = 1.

1C. One can check manually that D6 ∼= S3, using the map r (cid:55)→ (1 2 3) and s (cid:55)→ (1 2).

On the other hand D24 contains an element of order 12 while S4 does not.

1D(cid:63). Let G be a group of order p, and 1 (cid:54)= g ∈ G. Look at the group H generated by g

and use Lagrange’s theorem.

1F†. The idea is that each element g ∈ G can be thought of as a permutation G → G by

x (cid:55)→ gx.

1G. We have www = bb, bww = wb, wwb = bw, bwb = ww. Interpret these as elements

of D6.

1H. Look at the group G of 2 × 2 matrices mod p with determinant ±1 (whose entries

are the integers mod p). Let g =(cid:20)1 1

1 0(cid:21) and then use g|G| = 1G.

2B. In Q, no singleton set is open, whereas in N, they all are (in fact N is discrete).
2C. For subtraction, the map x (cid:55)→ −x is continuous so you can view it as a composed

map

R × R

(id,−x)

R × R

+

R

(a, b)

(a,−b)

a − b.

Similarly, if you are willing to believe x (cid:55)→ 1/x is a continuous function, then
division is composition

R × R>0

(id,1/x)

R × R>0

×

R

(a, b)

(a, 1/b)

a/b.

If for some reason you are suspicious that x (cid:55)→ 1/x is continuous, then here is a
proof using sequential continuity. Suppose xn → x with xn > 0 and x > 0 (since x
needs to be in R>0 too). Then

(cid:12)(cid:12)(cid:12)(cid:12)

xn(cid:12)(cid:12)(cid:12)(cid:12) = |xn − x|

|xxn|

1

1
x −

.

855

856

Napkin, by Evan Chen (v1.5.20190718)

If n is large enough, then |xn| > x/2; so the denominator is at least x2/2, and
hence the whole fraction is at most 2

x2 |xn − x|, which tends to zero as n → ∞.

2D. Let f (x) = x for x ∈ Q and f (x) = −x for irrational x.
2E. Assume for contradiction it is completely discontinuous; by scaling set f (0) = 0,
f (1) = 1 and focus just on f : [0, 1] → [0, 1]. Since it’s discontinuous everywhere,
for every x ∈ [0, 1] there’s an εx > 0 such that the continuity condition fails. Since
the function is strictly increasing, that can only happen if the function misses all
y-values in the interval (f x − εx, f x) or (f x, f x + εx) (or both).
Projecting these missing intervals to the y-axis you ﬁnd uncountably many intervals
(one for each x ∈ [0, 1]) all of which are disjoint. In particular, summing the εx you
get that a sum of uncountably many positive reals is 1.
But in general it isn’t possible for an uncountable family F of positive reals to have
ﬁnite sum. Indeed, just classify the reals into buckets 1
k−1 . If the sum is
actually ﬁnite then each bucket is ﬁnite, so the collection F must be countable,
contradiction.

k ≤ x < 1

3A. Abelian groups: abab = a2b2 ⇐⇒ ab = ba.
3B. Yes to (a): you can check this directly from the ghg−1 deﬁnition. For example,
for (a) it is enough to compute (ras)rn(ras)−1 = r−n ∈ H. The quotient group is
Z/2Z.
The answer is no for (b) by following Example 3.5.2.

3C. A subgroup of order 3 must be generated by an element of order 3, since 3 is prime.
So we may assume WLOG that H = (cid:104)(1 2 3)(cid:105) (by renaming elements appropriately).
But then let g = (3 4); one can check gHg−1 (cid:54)= H.

3D. G/ ker G is isomorphic to a subgroup of H. The order of the former divides 1000;
the order of the latter divides 999. This can only occur if G/ ker G = {1} so
ker G = G.

3F. Quaternion group.

3G. The answer is |G| = 18.

First, observe that by induction we have

anc = ca8n

for all n ≥ 1. We then note that

a(bc) = (ab)c
a · ca6 = c2a4 · c
ca8 · a6 = c2a4 · c

a14 = c(a4c) = c2a32.

Hence we conclude c2 = a−18. Then ab = c2a4 =⇒ b = a−15.

C Sketches of selected solutions

857

In that case, if c2018 = b2019, we conclude 1 = a2018·18−2019·15 = a6039. Finally,

bc = ca6
a−15c = ca6
a−15c2 = c(a6c) = c2a48

a−33 = a30
=⇒ a63 = 1.

Since gcd(6039, 63) = 9, we ﬁnd a9 = 1, hence ﬁnally c2 = 1. So the presentation
above simpliﬁes to

G =(cid:10)a, c | a9 = c2 = 1, ac = ca−1(cid:11)

which is the presentation of the dihedral group of order 18. This completes the
proof.

3H. You can ﬁnd many solutions by searching “homophone group”; one is https:

//math.stackexchange.com/q/843966/229197.

4A. This is just R[i] = C. The isomorphism is given by x (cid:55)→ i, which has kernel (x2 + 1).
4H. Nope! Pick

A = Z[x1, x2, . . . ]
B = Z[x1, x2, . . . , εx1, εx2, . . . ]
C = Z[x1, x2, . . . , ε].

where ε (cid:54)= 0 but ε2 = 0. I think the result is true if you add the assumption A is
Noetherian.

5C(cid:63). Consider ab ∈ φpre(I), meaning φ(ab) = φ(a)φ(b) ∈ I. Since I is prime, either
φ(a) ∈ I or φ(b) ∈ I. In the former case we get a ∈ φpre(I) as needed; the latter
case we get b ∈ φpre(I).

5D(cid:63). Let x ∈ R with x (cid:54)= 0. Look at the powers x, x2, . . . . By pigeonhole, eventually

two of them coincide. So assume xm = xn where m < n, or equivalently

0 = x · x · ··· · x ·(cid:0)xn−m − 1(cid:1) .

Since x (cid:54)= 0, we get xn−m − 1 = 0, or xn−m = 1. So xn−m−1 is an inverse for x.
This means every nonzero element has an inverse, ergo R is a ﬁeld.

5E(cid:63). For part (b), look at the poset of proper ideals. Apply Zorn’s lemma (again using a
union trick to verify the condition; be sure to verify that the union is proper!). In
part (a) we are given no ascending inﬁnite chains, so no need to use Zorn’s lemma.

5F. The ideal (0) is of course prime in both. Also, both rings are PID’s.

For C[x] we get a prime ideal (x − z) for each z ∈ C.
For R[x] a prime ideal (x − a) for each a ∈ R and a prime ideal (x2 − ax + b) for
each quadratic with two conjugate real roots.

858

Napkin, by Evan Chen (v1.5.20190718)

6B. Part (a) is essentially by deﬁnition. The space M is bounded since no distances
exceed 1, but not totally bounded since we can’t cover M with ﬁnitely many
1
2 -neighborhoods. The space M is complete since a sequence of real numbers
converges in M if it converges in the usual sense. As for N , the sequence −1, −2,
. . . is Cauchy but fails to converge; and it is obviously not bounded.
To show (b), the identity map (!) is an homeomorphism M ∼= R and R ∼= N , since
it is continuous.
This illustrates that M ∼= N despite the fact that M is both complete and bounded
but N is neither complete nor bounded. On the other hand, we will later see that
complete and totally bounded implies compact, which is a very strong property
preserved under homeomorphism.

7F. Let d(x, y) = 2017−n, where n is the largest integer such that n! divides |x − y|.
7G. You can pick a rational number in each interval and there are only countably many

rational numbers. Done!

8A. Compactness is preserved under homeomorphism, but [0, 1] is compact while (0, 1)

is not.

8E. Suppose pi = (xi, yi) is a sequence in X × Y (i = 1, 2, . . . ). Looking on the X side,
some subsequence converges: for the sake of illustration say it’s x1, x4, x9, x16,··· →
x. Then look at the corresponding sequence y1, y4, y9, y16, . . . . Using compact-
ness of Y , it has a convergent subsequence, say y1, y16, y81, y256,··· → y. Then
p1, p16, p81, . . . will converge to (x, y).

One common mistake is to just conclude that (xn) has a convergent subsequence
and that (yn) does too. But these sequences could be totally unrelated. For this
proof to work, you do need to apply compactness of X ﬁrst, and then compactness
of Y on the resulting ﬁltered sequence like we did here.

8H. Part (a) follows by the Cantor intersection theorem (Problem 8D). Assume for
contradiction such a partition existed. Take any of the circles C0, and let K0 denote
the closed disk with boundary C0. Now take the circle C1 passing through the
center of C0, and let K1 denote the closed disk with boundary C1. If we repeat in
this way, we get a nested sequence K0 ⊇ K1 ⊇ . . . and the radii of Ci approach
which is impossible.

zero (since each is at most half the previous once). Thus some point p lies in(cid:84)n Kn

Now for part (b), again assume for contradiction a partition into circles exists.
Color a circle magenta if it contains p but not q and color a circle cyan if it contains
q but not p. Color p itself magenta and q itself cyan as well. Finally, color a circle
neon yellow if it contains both p and q. (When we refer to coloring a circle, we
mean to color all the points on it.)

By repeating the argument in (a) there are no circles enclosing neither p nor q.
Hence every point is either magenta, cyan, or neon yellow. Now note that given any
magenta circle, its interior is completely magenta. Actually, the magenta circles
can be totally ordered by inclusion (since they can’t intersect). So we consider two
cases:

 If there is a maximal magenta circle (i.e. a magenta circle not contained in any
other magenta circle) then the set of all magenta points is just a closed disk.

C Sketches of selected solutions

859

 If there is no maximal magenta circle, then the set of magenta points can also
be expressed as the union over all magenta circles of their interiors. This is a
union of open sets, so it is itself open.

We conclude the set of magenta points is either a closed disk or an open set.
Similarly for the set of cyan points. Moreover, the set of such points is convex.

To ﬁnish the problem:

 Suppose there are no neon yellow points.

If the magenta points form a
closed disk, then the cyan points are R2 minus a disk which is not convex.
Contradiction. So the magenta points must be open. Similarly the cyan points
must be open. But R2 is connected, so it can’t be written as the union of two
open sets.

 Now suppose there are neon yellow points. We claim there is a neon yellow
circle minimal by inclusion. If not, then repeat the argument of (a) to get a
contradiction, since any neon yellow circle must have diameter the distance
from p to q. So we can ﬁnd a neon yellow circle C whose interior is all magenta
and cyan. Now repeat the argument of the previous part, replacing R2 by the
interior of C .

T injective T surjective T isomorphism

9A†.

If dim V > dim W . . .
If dim V = dim W . . .
If dim V < dim W . . .

never

sometimes
sometimes

sometimes
sometimes

never

never

sometimes

never

Each “never” is by the rank-nullity theorem. Each counterexample is obtained
by the zero map sending every element of V to zero; this map is certainly neither
injective or surjective.

9B†. It essentially follows by Theorem 9.7.6.

1 0(cid:21).
9C. Since 1 (cid:55)→ √5 and √5 (cid:55)→ 5, the matrix is(cid:20)0 5

9F. Let V be the space of real polynomials with degree at most d/2 (which has
dimension 1 + (cid:98)d/2(cid:99)), and W be the space of real polynomials modulo P (which
has dimension d). Then dim(V ⊕ V ) > dim W . So the linear map V ⊕ V → W
by (A, B) (cid:55)→ A + Q · B has a kernel of positive dimension (by rank-nullity, for
example).

9H(cid:63). Consider

{0} (cid:40) ker S ⊆ ker S2 ⊆ ker S3 ⊆ . . . and V (cid:41) im S ⊇ im S2 ⊇ im S3 ⊇ . . . .

For dimension reasons, these subspaces must eventually stabilize: for some large
integer N , ker T N = ker T N +1 = . . . and im T N = im T N +1 = im T N +2 = . . . .

When this happens, ker T N(cid:84) im T N = {0}, since T N is an automorphism of im T N .
On the other hand, by Rank-Nullity we also have dim ker T N + dim im T n = dim V .
Thus for dimension reasons, V = ker T N ⊕ im T N .

10A. It’s just dim V = 2018. After all, you are adding the dimensions of the Jordan

blocks. . .

10B. (a): if you express T as a matrix in such a basis, one gets a diagonal matrix. (b):
this is just saying each Jordan block has dimension 1, which is what we wanted.
(We are implicitly using uniqueness of Jordan form here.)

860

Napkin, by Evan Chen (v1.5.20190718)

10C. The +1 eigenspace is spanned by e1 + e2. The −1 eigenspace is spanned by e1 − e2.
10D. The +1 eigenspace is spanned by 1 + x2 and x. The −1 eigenspace is spanned by

1 − x2.

10E. Constant functions diﬀerentiate to zero, and these are the only 0-eigenvectors.
There can be no other eigenvectors, since if deg p > 0 then deg p(cid:48) = deg p − 1, so if
p(cid:48) is a constant real multiple of p we must have p(cid:48) = 0, ergo p is constant.

10F. ecx is an example of a c-eigenvector for every c. If you know diﬀerential equations,

these generate all examples!

11C†. Although we could give a coordinate calculation, we instead opt to give a cleaner

proof. This amounts to drawing the diagram

p

o m

c



Hom(W, W ) 

(W ∨ ⊗ V ) ⊗ (V ∨ ⊗ W ) - (V ∨ ⊗ W ) ⊗ (W ∨ ⊗ V )
o s e

co

m

p

ose

-

?
W ∨ ⊗ W

Tr

ev

-

?
k



-

- Hom(V, V )

?
V ∨ ⊗ V



ev

?
k

-

T r

id

It is easy to check that the center rectangle commutes, by checking it on pure
tensors ξW ⊗ v ⊗ ξV ⊗ w. So the outer hexagon commutes and we’re done. This is
really the same as the proof with bases; what it amounts to is checking the assertion
is true for matrices that have a 1 somewhere and 0 elsewhere, then extending by
linearity.

11D. See https://mks.mff.cuni.cz/kalva/putnam/psoln/psol886.html.

12D. Recall that (by Problem 9B†) we can replace “isomorphism” by “injective”.

If T (v) = 0 for any nonzero v, then by taking a basis for which e1 = v, we ﬁnd
Λn(T ) will map e1 ∧ . . . to 0 ∧ T (e2) ∧ ··· = 0, hence is the zero map, so det T = 0.
Conversely, if T is an isomorphism, we let S denote the inverse map. Then
1 = det(id) = det(S ◦ T ) = det S det T , so det T (cid:54)= 0.

12E. We proceed by contradiction. Let v be a vector of length 1000 whose entries are
weight of cows. Assume the existence of a matrix M such that M v = 0, with entries
0 on diagonal and ±1 oﬀ-diagonal. But det M (mod 2) is equal to the number of
derangements of {1, . . . , 1000}, which is odd. Thus det M is odd and in particular
not zero, so M is invertible. Thus M v = 0 =⇒ v = 0, contradiction.

12F. The answer is

(cid:20)t

t

t

t(cid:21)

and

(cid:20)−3t −t
3t(cid:21)

t

for t ∈ R. These work by taking k = 3.
Now to see these are the only ones, consider an arithmetic matrix
M =(cid:20)

a + 2e a + 3e(cid:21) .

a + e

a

C Sketches of selected solutions

861

with e (cid:54)= 0. Its characteristic polynomial is t2 − (2a + 3e)t − 2e2, with discriminant
(2a + 3e)2 + 8e2, so it has two distinct real roots; moreover, since −2e2 ≤ 0 either
one of the roots is zero or they are of opposite signs. Now we can diagonalize M
by writing

M =(cid:20) s −q

p (cid:21)(cid:20)λ1

−r

0

λ2(cid:21)(cid:20)p q

s(cid:21) =(cid:20)psλ1 − qrλ2

pr(λ2 − λ1) psλ2 − qrλ1(cid:21)

qs(λ1 − λ2)

0

r

where ps − qr = 1. By using the fact the diagonal entries have sum equalling the
oﬀ-diagonal entries, we obtain that

(ps − qr)(λ1 + λ2) = (qs − pr)(λ1 − λ2) =⇒ qs − pr =

λ1 + λ2
λ1 − λ2

.

Now if M k ∈ S too then the same calculation gives

qs − pr =

λk
1 + λk
2
λk
1 − λk

2

.

Let x = λ1/λ2 < 0 (since −2e2 < 0). We appropriately get

x + 1
x − 1

=

xk + 1
xk − 1

=⇒

2

x − 1

=

2

xk − 1

=⇒ x = xk =⇒ x = −1 or x = 0

and k odd. If x = 0 we get e = 0 and if x = −1 we get 2a + 3e = 0, which gives
the curve of solutions that we claimed.

A slicker approach is to note that by Cayley-Hamilton. Assume that e (cid:54)= 0, so
M has two distinct real eigenvalues as above. We have M k = cM + did for some
constants c and d (since M satisﬁes some quadratic polynomial). Since M ∈ S,
M k ∈ S we obtain d = 0. Thus M k = cM , so it follows the eigenvalues of M are
negatives of each other. That means Tr M = 0, and the rest is clear.

12G. Pick a basis e1, . . . , en of V . Let T have matrix (xij), and let m = dim V . Let δij
be the Kronecker delta. Also, let Fix(σ) denote the ﬁxed points of a permutation
σ and let NoFix(σ) denote the non-ﬁxed points.

862

Napkin, by Evan Chen (v1.5.20190718)

Expanding then gives

t(cid:89)k=1

−xikik

det(a · id − T )

σ ﬁxes ik

σ ﬁxes (ik)

−xiσ(i)

(a · −xii)

m(cid:89)i=1(cid:0)a · δiσ(i) − xiσ(i)(cid:1)(cid:33)
(cid:32)sign(σ) ·
m(cid:89)i=1(cid:0)a · δiσ(i) − xiσ(i)(cid:1)(cid:33)
sign(σ) · (cid:89)i /∈(ik)
n(cid:89)i∈(ik)
(a − xii)
−xiσ(i) (cid:89)i∈Fix σ
−xiσ(i)
|Fix(σ)|(cid:88)t=0
a|Fix(σ)|−t · (cid:88)i1<···<it∈Fix(σ)
(cid:88)X⊆{1,...,m}

am−t−|Fix(σ)|

(cid:89)i∈X

−xiσ(i)

= (cid:88)σ∈Sm(cid:32)sign(σ) ·
m(cid:88)s=0 (cid:88)1≤i1<···<is≤m (cid:88)σ∈Sm
m(cid:88)s=0 (cid:88)1≤i1<···<is≤m (cid:88)σ∈Sm
sign(σ) · (cid:89)i∈NoFix(σ)
= (cid:88)σ∈Sm
sign(σ) · (cid:89)i∈NoFix(σ)
= (cid:88)σ∈Sm


|Fix(σ)|(cid:88)t=0
= (cid:88)σ∈Sm
am−n
sign(σ) (cid:88)X⊆{1,...,m}
am−n(−1)n (cid:88)X⊆{1,...,m}

|X|=n (cid:88)σ∈Sm

(cid:88)σ∈Sm

NoFix(σ)⊆X

m(cid:88)n=0

m(cid:88)n=0

sign(σ)

|X|=n

xiσ(i) .

sign(σ)(cid:89)i∈X

(cid:89)i∈X

−xiσ(i)





=

=

=

=

NoFix(σ)⊆X

X has exactly t ﬁxed



NoFix(σ)⊆X

Hence it’s the same to show that

|X|=n (cid:88)σ∈Sm
(cid:88)X⊆{1,...,m}

NoFix(σ)⊆X

sign(σ)(cid:89)i∈X

xiσ(i) = TrΛn(V ) (Λn(T ))

holds for every n.

C Sketches of selected solutions

863

We can expand the deﬁnition of trace as using basis elements as

eik(cid:33)(cid:33)

Tr (Λn(T )) = (cid:88)1≤i1<···<in≤m(cid:32) n(cid:94)k=1
= (cid:88)1≤i1<···<in≤m(cid:32) n(cid:94)k=1
= (cid:88)1≤i1<···<in≤m(cid:32) n(cid:94)k=1
= (cid:88)1≤i1<···<in≤m (cid:88)π∈Sn
= (cid:88)X⊆{1,...,m}
|X|=n (cid:88)π∈SX

eik(cid:33)∨(cid:32)Λn(T )(cid:32) n(cid:94)k=1
T (eik )(cid:33)
eik(cid:33)∨(cid:32) n(cid:94)k=1
xikjej

eik(cid:33)∨
m(cid:88)j=1
n(cid:94)k=1
n(cid:89)k=1
sign(π)(cid:89)i∈X

xiπ(k)k

sign(π)

xtπ(t)

Hence it remains to show that the permutations over X are in bijection with the
permutations over Sm which ﬁx {1, . . . , m} − X, which is clear, and moreover, the
signs clearly coincide.

13C. Interpret clubs as vectors in the vector space Fn

2 . Consider a “dot product” to show

that all k vectors are linearly independent. Thus k ≤ dim Fn

2 = n.

13D(cid:63). The inner form given by

(cid:104)v1 ⊗ w1, v2 ⊗ w2(cid:105)V ⊗W = (cid:104)v1, v2(cid:105)V (cid:104)w1, w2(cid:105)W

on pure tensors, then extending linearly. For (b) take ei ⊗ fj for 1 ≤ i ≤ n,
1 ≤ j ≤ m.

14B. Deﬁne the Boolean function D : {±1}3 → R by
D(a, b, c) = ab + bc + ca =(cid:40)3

−1 a, b, c not all equal.

a, b, c all equal

.

Thus paradoxical outcomes arise when D(f (x•), g(y•), h(z•)) = 3. Now, we compute
that for randomly selected x•, y•, z• that

ED(f (x•), g(y•), h(z•)) = E(cid:88)S (cid:88)T (cid:16)(cid:98)f (S)(cid:98)g(T ) +(cid:98)g(S)(cid:98)h(T ) +(cid:98)h(S)(cid:98)f (T )(cid:17) (χS(x•)χT (y•))
=(cid:88)S (cid:88)T (cid:16)(cid:98)f (S)(cid:98)g(T ) +(cid:98)g(S)(cid:98)h(T ) +(cid:98)h(S)(cid:98)f (T )(cid:17) E (χS(x•)χT (y•)) .

Now we observe that:

 If S (cid:54)= T , then EχS(x•)χT (y•) = 0, since if say s ∈ S, s /∈ T then xs aﬀects
the parity of the product with 50% either way, and is independent of any other
variables in the product.

 On the other hand, suppose S = T . Then

χS(x•)χT (y•) =(cid:89)s∈S

xsys.

864

Napkin, by Evan Chen (v1.5.20190718)

3 and −1 with probability 2
Note that xsys is equal to 1 with probability 1
3
(since (xs, ys, zs) is uniform from 3! = 6 choices, which we can enumerate).
From this an inductive calculation on |S| gives that

(cid:89)s∈S

Thus

xsys =(cid:40)+1 with probability 1
2 (1 + (−1/3)|S|)
−1 with probability 1
2 (1 − (−1/3)|S|).
xsys(cid:33) =(cid:18)−
3(cid:19)|S|

E(cid:32)(cid:89)s∈S

1

.

Piecing this altogether, we now have that

Then, we obtain that

ED(f (x•), g(y•), h(z•)) =(cid:16)(cid:98)f (S)(cid:98)g(T ) +(cid:98)g(S)(cid:98)h(T ) +(cid:98)h(S)(cid:98)f (T )(cid:17)(cid:18)−
3(cid:19)|S|
4(cid:88)S (cid:16)(cid:98)f (S)(cid:98)g(T ) +(cid:98)g(S)(cid:98)h(T ) +(cid:98)h(S)(cid:98)f (T )(cid:17)(cid:98)f (S)2(cid:18)−

(1 + D(f (x•), g(y•), h(z•)))
1

1
4

1
4

E

=

+

1

Comparing this with the deﬁnition of D gives the desired result.

.

1

3(cid:19)|S|

.

15B. By Theorem 9.7.6, we may select e1, . . . , en a basis of V and f1, . . . , fm a basis of
W such that T (ei) = fi for i ≤ k and T (ei) = 0 for i > k. Then T ∨(f∨i ) = e∨i
for
i ≤ k and T ∨(f∨i ) = 0 for i > k. All four quantities are above are then equal to k.
15F. First, suppose T ∗ = p(T ). Then T ∗T = p(T ) · T = T · p(T ) = T T ∗ and we’re done.
Conversely, suppose T is diagonalizable in a way compatible with the inner form
(OK since V is ﬁnite dimensional). Consider the orthonormal basis. Then T consists
of eigenvalues on the main diagonals and zeros elsewhere, say

λ1
0
...
0

0
λ2
...
0

. . .
0
. . .
0
...
. . .
. . . λn

In that case, we ﬁnd that for any polynomial q we have

T =
q(T ) =
T ∗ =

0
...
0

.



.



q(λ1)

0

q(λ2)

...
0

. . .
. . .
. . .
. . .

0
0
...

q(λn)

.



λ1
0
...
0

0
λ2
...
0

0
. . .
0
. . .
...
. . .
. . . λn

and

So we simply require a polynomial q such that q(λi) = λi for every i. Since there are
ﬁnitely many λi, we can construct such a polynomial using Lagrange interpolation.

C Sketches of selected solutions

865

17B. If not, there exists eight Sylow 7-groups and seven Sylow 8-groups (since n7 ≡ 1
(mod 7), n8 | 7, and n7, n8 > 1). But no two of thees 8 + 7 = 15 groups can intersect
at an element other than 1G, which is clearly absurd.

17C. One example is upper triangular matrices with entries in mod 3.

17D. Let G be said group. If G is abelian then all subgroups are normal, and since G is
simple, G can’t have any subgroups at all. We can clearly ﬁnd an element of order
p, hence G has a subgroup of order p, which can only happen if n = 1, G ∼= Z/pZ.
Thus it suﬃces to show G can’t be abelian. For this, we can use the class equation,
but let’s avoid that and do it directly:
Assume not and let Z(G) = {g ∈ G | xg = gx ∀x ∈ G} be the center of the group.
Since Z(G) is normal in G, and G is simple, we see Z(G) = {1G}. But then let G
act on itself by conjugation: g · x = gxg−1. This breaks G into a bunch of orbits
O0 = {1G}, O1, O2, . . . , and since 1G is the only ﬁxed point by deﬁnition, all other
orbits have size greater than 1. The Orbit-stabilizer theorem says that each orbit
now has size dividing pn, so they must all have size zero mod p.
But then summing across all orbits (which partition G), we obtain |G| ≡ 1 (mod p),
which is a contradiction.

19D(cid:63). The operators are those of the form T (a) = ab for some ﬁxed b ∈ A. One can check
these work, since for c ∈ A we have T (c · a) = cab = c · T (a). To see they are the
only ones, note that T (a) = T (a · 1A) = a · T (1A) for any a ∈ A.

20C. Pick any v ∈ V , then the subspace spanned by elements g·v for v ∈ V is G-invariant;

this is a ﬁnite-dimensional subspace, so it must equal all of V .

21B. Csign ⊕ C2 ⊕ reﬂ0 ⊕(reﬂ0 ⊗Csign).
21C. First, observe that |χW (g)| = 1 for all g ∈ G.

(cid:104)χV ⊗W , χV ⊗W(cid:105) = (cid:104)χV χW , χV χW(cid:105)

1

|G|(cid:88)g∈G
|G|(cid:88)g∈G

|χV (g)|2 |χW (g)|2

|χV (g)|2

=

=

1

= (cid:104)χV , χV (cid:105) = 1.

21D. The table is given by

1

1 −1 ±i ±j ±k
Q8
Ctriv 1
1
1
1
Ci
1 −1 −1
1
1
Cj
1 −1
1 −1
1
Ck
1 −1 −1
1
1
C2
2 −2
0
0
0

The one-dimensional representations (ﬁrst four rows) follows by considering the
homomorphism Q8 → C×. The last row is two-dimensional and can be recovered
by using the orthogonality formula.

866

Napkin, by Evan Chen (v1.5.20190718)

23A. By a straightforward computation, we have |Ψ−(cid:105) = − 1√2

(|→(cid:105)A ⊗ |←(cid:105)B − |←(cid:105)A |→(cid:105)B).
Now, |→(cid:105)A⊗|→(cid:105)B, |→(cid:105)A⊗|←(cid:105)B span one eigenspace of σA
x ⊗idB, and |←(cid:105)A⊗|→(cid:105)B,
|←(cid:105)A ⊗ |←(cid:105)B span the other. So this is the same as before: +1 gives |←(cid:105)B and −1
gives |←(cid:105)A.

24A. To show the Fredkin gate is universal it suﬃces to reversibly create a CCNOT gate

with it. We write the system

(z,¬z,−) = Fred(z, 1, 0)
(x, a,−) = Fred(x, 1, 0)
(y, b,−) = Fred(y, a, 0)
(−, c,−) = Fred(b, 0, 1)
(−, d,−) = Fred(c, z,¬z).
Direct computation shows that d = z + xy (mod 2).

24C. Put |←(cid:105) = 1√2

(|0(cid:105) − |1(cid:105)). Then we have that Uf sends

|x1(cid:105) . . .|xm(cid:105)|0(cid:105) − |x1(cid:105) . . .|xm(cid:105)|1(cid:105)

Uf

(cid:55)−−→ ±|x1(cid:105) . . .|xm(cid:105)|0(cid:105) ∓ |x1(cid:105) . . .|xm(cid:105)|1(cid:105)

the sign being +, − exactly when f (x1, . . . , xm) = 1.
Now, upon inputting |0(cid:105) . . .|0(cid:105)|1(cid:105), we ﬁnd that H⊗m+1 maps it to

2−n/2 (cid:88)x1,...,xn

|x1(cid:105) . . .|xn(cid:105)|←(cid:105) .

Then the image under Uf is

2−n/2 (cid:88)x1,...,xn

(−1)f (x1,...,xn) |x1(cid:105) . . .|xn(cid:105)|←(cid:105) .

We now discard the last qubit, leaving us with

2−n/2 (cid:88)x1,...,xn

(−1)f (x1,...,xn) |x1(cid:105) . . .|xn(cid:105) .

Applying H⊗m to this, we get

(−1)f (x1,...,xn) ·(cid:32)2−n/2 (cid:88)y1,...,yn

2−n/2 (cid:88)x1,...,xn
since H |0(cid:105) = 1√2
(|0(cid:105) − |1(cid:105)), so minus signs arise exactly
if xi = 0 and yi = 0 simultaneously, hence the term (−1)x1y1+···+xnyn. Swapping
the order of summation, we get

(−1)x1y1+···+xnyn |y1(cid:105)|y2(cid:105) . . .|yn(cid:105)(cid:33)

(|0(cid:105) + |1(cid:105)) while H |1(cid:105) = 1√2

2−n (cid:88)y1,...,yn

C(y1, . . . , yn)|y1(cid:105)|y2(cid:105) . . .|yn(cid:105)

where Cy1,...,yn = (cid:80)x1,...,xn(−1)f (x1,...,xn)+x1y1+···+xnyn. Now, we ﬁnally consider

two cases.

C Sketches of selected solutions

867

 If f is the constant function, then we ﬁnd that

C(y1, . . . , yn) =(cid:40)±1

0

y1 = ··· = yn = 0
otherwise.

To see this, note that the result is clear for y1 = ··· = yn = 0; otherwise, if
WLOG y1 = 1, then the terms for x1 = 0 exactly cancel the terms for x1 = 0,
pair by pair. Thus in this state, the measurements all result in |0(cid:105) . . .|0(cid:105).

 On the other hand if f is balanced, we derive that

C(0, . . . , 0) = 0.

Thus no measurements result in |0(cid:105) . . .|0(cid:105).

In this way, we can tell whether f is balanced or not.

26E. This is an application of Cauchy convergence, since one can show that

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
N(cid:88)n=M

(−1)nan(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ amin{M,N}.

Indeed, if M and N are even (for simplicity; other cases identical) then

aM − aM +1 + aM +2 − . . . = aM − (aM +1 − aM +2) − (aM +3 − aM +4)

− ··· − (aN−1 − aN )

≤ aM

aM − aM +1 + aM +2 − . . . = aM − aM +1 + (aM +2 − aM +3) + (aM +4 − aM +5)

+ ··· + (aN−2 − aN +1) + aN

≥ −aM +1.

In this way we see that the sequence of partial sums is Cauchy, hence converges to
some limit.

26F. To capture (an)n monotonic and bounded, write an = x1 + ··· + xn for some xi.

Then x2, . . . are all the same sign and so(cid:80)|xi| = A < ∞ for some constant A.
We now prove that the partial sums of(cid:80) anbn are a Cauchy sequence. Consider

any ε > 0. Let K be such that the tails of bn starting after K have absolute value

868

Napkin, by Evan Chen (v1.5.20190718)

less than ε

A . Then for any N > M ≥ K we have

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
N(cid:88)k=M

akbk(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

bkxj(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
=(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
k(cid:88)j=1
N(cid:88)k=M
bkxj(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
=(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
N(cid:88)k=max{j,M}
N(cid:88)j=1
=(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
bk(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
N(cid:88)j=1
N(cid:88)k=max{j,M}
bk(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
|xj|(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
N(cid:88)j=1
N(cid:88)k=max{j,M}
N(cid:88)j=1

|xj| ·

xj ·

≤

<

ε
A

< ε

as desired.

26G. The answer is e − 1.

We begin by noting xn+1 = log(exn − xn) ≥ log 1 = 0, owing to et ≥ 1 + t. So
xn ≥ 0 for all n.
Next notice that

xn+1 = log (exn − xn) < log exn = xn.

So (xn)n is strictly decreasing in addition to nonnegative. Thus it must converge
to some limit L.

Third, observe that

xn = exn − exn+1 =⇒ x0 + x1 + ··· + xn = ex0 − exn = e − exn < e.
Since the partial sums are bounded by e, and xi ≥ 0, we conclude L = 0.
Finally, the limit of the partial sums is then

lim
n→∞

e − exn = e − e0 = e − 1.

31B(cid:63). Proceed by contradiction, meaning there exists a sequence z1, z2,··· → z where
0 = f (z1) = f (z2) = . . . all distinct. WLOG set z = 0. Look at the Taylor
series of f around z = 0. Since it isn’t uniformly zero by assumption, write it as
aN zN + aN +1zN +1 + . . . , aN (cid:54)= 0. But by continuity of h(z) = aN + aN +1z + . . .
there is some open neighborhood of zero where h(z) (cid:54)= 0.

31C(cid:63). Let S be the interior of the points satisfying f = g. By deﬁnition S is open. By
if zi → z and zi ∈ S, then f = g in some open

the previous part, S is closed:
neighborhood of z, hence z ∈ S. Since S is clopen and nonempty, S = U .

31E. Suppose we want to show that there’s a point in the image within ε of a given a

point w ∈ C. Look at

1

f (z)−w and use Liouville’s theorem.

C Sketches of selected solutions

869

32C. See https://math.stackexchange.com/q/242514/229197, which does it with

2019 replaced by 3.

38A. It is the counting measure.

40A. For each positive integer m, consider what happens when ε = 1/m. Then, by

hypothesis, there is a threshold Nm such that the anomaly set

Am :=(cid:26)ω : |X(ω) − Xn(ω)| ≥

1
m

for some n > Nm(cid:27)

has measure µ(Am) = 0. Hence, the countable union A =(cid:83)m≥1 Am has measure

zero too.
So the complement of A has measure 1. For any world ω /∈ A, we then have

n |X(ω) − Xn(ω)| = 1
lim

because when n > Nm that absolute value is always at most 1/m (as ω /∈ Am).

40B. https://math.stackexchange.com/a/2201906/229197

48C. If α ≡ 0 (mod p) it’s clear, so assume this isn’t the case. Then OK/p is a ﬁnite
ﬁeld with N(p) elements. Looking at (OK/p)∗, it’s a multiplicative group with
N(p) − 1 elements, so αN(p)−1 ≡ 1 (mod p), as desired.

48D†. Suppose it’s generated by some elements in K; we can write them as βi
αi

for

αi, βi ∈ A. Hence

J =(cid:40)(cid:88)i

βi

αi | αi, βi, γi ∈ OK(cid:41) .

γi ·

Now “clear denominators”. Set α = α1 . . . αn, and show that αI is an integral
ideal.

48E. For part (a), note that the pi are prime just because

OK/pi ∼= (Z[x]/f )/(p, fi) ∼= Fp[x]/(fi)

is a ﬁeld, since the fi are irreducible.
We check (b). Computing the product modulo p yields1

r(cid:89)i=1

(fi(α))ei ≡ (f (α)) ≡ 0

(mod p)

so we’ve shown that I ⊆ (p).
Finally, we prove (c) with a size argument. The idea is that I and (p) really should
have the same size; to nail this down we’ll use the ideal norm. Since (p) divides I, we

e(cid:48)
can write (p) =(cid:81)r
i where e(cid:48)i ≤ ei for each i. Remark OK/(p) ∼= Z/pZ[x]/(f )
i=1 p
has size pdeg f . Similarly, OK/(fi) has degree pdeg fi for each i. Compute N((p))
using the e(cid:48)i now and compare the results.
√
7)(3, 1 − √
√
1For example, suppose we want to know that (3, 1 +
7, 3 − 3

√
7) is contained in (3). We could
7, 6). But if all we care about is that every
do the full computation and get (9, 3 + 3
element is divisible by 3, we could have just taken “mod 3” at the beginning and looked at just
(1 +

7) = (6); all the other products we get will obviously have factors of 3.

√
7)(1 − √

i

870

Napkin, by Evan Chen (v1.5.20190718)

49F. Let K = Q(√−5). Check that ClK has order two using the Minkowski bound;
moreover ∆K = 20. Now note that OK = Z[√−5], and x2 + 5 factors mod p as
(x + k)(x − k); hence in OK we have (p) = (p,√−5 + k)(p,√−5 − k) = p1p2, say.
For p > 5 the prime p does not ramify and we have p1 (cid:54)= p2, since ∆K = 20.
Then (p2) = p2
2. Because the class group has order two, both ideals on the
right hand side are principal, and because p1 (cid:54)= p2 they are distinct. Thus p2 is
a nontrivial product of two elements of OK; from this we can extract the desired
factorization.

1 · p2

52A(cid:63). It’s just Z/p − 1Z, since ζp needs to get sent to one (any) of the p − 1 primitive

roots of unity.

52E. http://www.math.cornell.edu/~kbrown/6310/primitive.pdf

53A†. Recall that the Fibonacci sequence is given by
αn − βn
α − β

Fn =

where α = 1+√5
Let p = 127 and work modulo p. As

2

2

and β = 1−√5
p(cid:19) =(cid:16) p
(cid:18) 5

5(cid:17) =(cid:18) 2

5(cid:19) = −1

are the two roots of P (X) := X 2 − X − 1.

we see 5 is not a quadratic residue mod 127. Thus the polynomial P (X), viewed as
a polynomial in Fp[X], is irreducible (intuitively, α and β are not elements of Fp).
Accordingly we will work in the ﬁnite ﬁeld Fp2, which is the Fp-splitting ﬁeld of
P (X). In other words we interpret α and β as elements of Fp2 which do not lie in
Fp.

Let σ : Fp2 → Fp2 by t (cid:55)→ tp be the nontrivial element of Gal(cid:0)Fp2/Fp(cid:1); in other

words, σ is the non-identity automorphism of Fp2. Since the ﬁxed points of σ are
the elements of Fp, this means σ does not ﬁx either root of P ; thus we must have

αp = σ(α) = β
βp = σ(β) = α.

Now, compute

Fp =

Fp+1 =

F2p+1 =

F2p+2 =

αp − βp
β − α
=
α − β
α − β
αp+1 − βp+1
=
α2p+1 − β2p+1
α2p+2 − β2p+2

α − β

α − β

=

=

= −1.
αβ − βα
= 0.
α − β
β2α − α2β
β2α2 − α2β2

α − β

α − β

α − β

= −αβ = 1.

= 0.

Consequently, the period must divide 2p + 2 but not p + 1.

We now use for the ﬁrst time the exact numerical value p = 127 to see the period
divides 2p + 2 = 256 = 28, but not p + 1 = 128 = 27. (Previously we only used the
fact that (5/p) = −1.) Thus the period must be exactly 256.

C Sketches of selected solutions

871

55A. It is still true that

(cid:18) 2
q(cid:19) = 1 ⇐⇒ σ2 ∈ H ⇐⇒ 2 splits in Z(cid:104) 1

2 (1 +(cid:112)q∗)(cid:105).

Now, 2 splits in the ring if and only if t2− t− 1
4 (1− q∗) factors mod 2. This happens
if and only if q∗ ≡ 1 (mod 8). One can check this is exactly if q ≡ ±1 (mod 8),
which gives the conclusion.

55C†. Let K = Gal(Q(ζm)/Q). One can show that Gal(K/Q) ∼= (Z/mZ)× exactly as
before. In particular, Gal(K/Q) is abelian and therefore its conjugacy classes are
singleton sets; there are φ(m) of them.

As long as p is suﬃciently large, it is unramiﬁed and σp = Frobp for any p above p
(as mth roots of unity will be distinct modulo p; diﬀerentiate xm − 1 mod p again).
55E. This solution is by David Corwin. By primitive roots, it’s the same as the action

of ×3 on Z/(p − 1)Z. Let ζ be a (p − 1)st root of unity.
Consider

d = (cid:89)0≤i<j<p−1

(ζ i − ζ j).

This is the square root of the discriminant of the polynomial X p−1 − 1; in other
words d2 ∈ Z. In fact, by elementary methods one can compute

(−1)(p−1

2 )d2 = −(p − 1)p−1

Now take the extension K = Q(d), noting that

 If p ≡ 3 (mod 4), then d = (p − 1)
 If p ≡ 1 (mod 4), then d = i(p − 1)

1

2 (p−1), so K = Q.
2 (p−1), so K = Q(i).

1

Either way, in OK, let p be a prime ideal above (3) ⊆ OK. Let σ = Frobp then be
the unique element such that σ(x) = x3 (mod p) for all x. Then, we observe that

σ(d) ≡ (cid:89)0≤i<j<p−1

(ζ 3i − ζ 3j) ≡(cid:40)+d if π is even

−d if π is odd

(mod p).

Now if K = Q, then σ is the identity, thus σ even. Conversely, if K = Q(i), then 3
does not split, so σ(d) = −d (actually σ is complex conjugation) thus π is odd.
Note the condition that p ≡ 2 (mod 3) is used only to guarantee that π is actually a
permutation (and thus d (cid:54)= 0); it does not play any substantial role in the solution.
56A†. Suppose f(L/Q) | m∞ for some m. Then by the example from earlier we have the

chain

PQ(m∞) = H(Q(ζ)/Q, m∞) ⊆ H(L/Q, m) ⊆ IQ(m∞).

So by inclusion reversal we’re done.

56B†. Apply the Takagi existence theorem with m = 1 to obtain an unramiﬁed extension

E/K such that H(E/K, 1) = PK(1). We claim this works:

 To see it is maximal by inclusion, note that any other extension M/K with
this property has conductor 1 (no primes divide the conductor), and then we
have PK(1) = H(E/K, 1) ⊆ H(M/K, 1) ⊆ IK(1), so inclusion reversal gives
M ⊆ E.

872

Napkin, by Evan Chen (v1.5.20190718)

 We have Gal(L/K) ∼= IK(1)/PK(1) = CK(1) the class group.
 The isomorphism in the previous part is given by the Artin symbol. So p

splits completely if and only if(cid:16) L/K

in CK(1)).

p (cid:17) = id if and only if p is principal (trivial

This completes the proof.

61A. The main observation is that in A × 2, you have the arrows in A (of the form
(f, id2)), and then the arrows crossing the two copies of A (of the form (idA, 0 ≤ 1)).
But there are some more arrows (f, 0 ≤ 1): nonetheless, they can be thought of as
compositions

(f, 0 ≤ 1) = (f, id2) ◦ (idA, 0 ≤ 1) = (idA, 0 ≤ 1) ◦ (f, id2).

Now we want to specify a functor α : A × 2, we only have to specify where each of
these two more basic things goes. The conditions on α already tells us that (f, id2)
should be mapped to F (f ) or G(f ) (depending on whether the arrow above is in
A × {0} or A × {1}), and specifying the arrow (idA, 0 ≤ 1) amounts to specifying
the Ath component. Where does naturality come in?

The above discussion transfers to products of categories in general: you really only
have to think about (f, id) and (id, g) arrows to get the general arrow (f, g) =
(f, id) ◦ (id, g) = (id, g) ◦ (f, id).

which is clearly not possible.

64A. Applying the functor Hn−1 we get that the composition Z → 0 → Z is the identity
65B. The answer is (cid:101)Hn−1(X) ∼= Z⊕p, with all other groups vanishing. For p = 1,
Rn − {∗} ∼= Sn−1 so we’re done. For all other p, draw a hyperplane dividing the

p points into two halves with a points on one side and b points on the other (so
a + b = p). Set U and V and use induction.

Alternatively, let U be the desired space and let V be the union of p disjoint balls,
one around every point. Then U ∪ V = Rn has all reduced homology groups trivial.
From the Mayer-Vietoris sequence we can read (cid:101)Hk(U ∩ V ) ∼= (cid:101)Hk(U )∩(cid:101)Hk(V ). Then
U ∩ V is p punctured balls, which are each the same as Sn−1. One can read the
conclusion from here.

65C(cid:63). It is Z for k = n and 0 otherwise.

65D(cid:63). Use the short exact sequence 0 → Cn(A, B) → Cn(X, B) → Cn(X, A) → 0.
66B. We have an exact sequence

.

(cid:101)H1(R)
(cid:124) (cid:123)(cid:122) (cid:125)=0

→ (cid:101)H1(R, Q) → (cid:101)H0(Q) → (cid:101)H0(R)
(cid:124) (cid:123)(cid:122) (cid:125)=0

Now, since Q is path-disconnected (i.e. no two of its points are path-connected) it

follows that (cid:101)H0(Q) consists of countably inﬁnitely many copies of Z.

67D. For concreteness, let’s just look at the homology at H2(X 2, X 1) and show it’s

C Sketches of selected solutions

873

isomorphic to H2(X). According to the diagram

H2(X) ∼= H2(X 3)

∼= H2(X 2)/ im ∂3

∼= H2(X 2)/ ker(cid:2)H2(X 2) (cid:16) H2(X 3)(cid:3)
∼= im(cid:2)H2(X 2) (cid:44)→ H2(X 2, X 1)(cid:3) / im ∂3

∼= ker(∂2)/ im ∂3
∼= ker d2/ im d3.

69D. See [Ma13a, Example 3.3.14, pages 68-69].

70A. If V = V(I) with I = (f1, . . . , fm) (as usual there are ﬁnitely many polynomials

since R[x1, . . . , xn] is Noetherian) then we can take f = f 2

1 + ··· + f 2
m.

70B. Let I be an ideal, and let m be a maximal ideal contained in it. (If you are worried
about the existence of m, it follows from Krull’s Theorem, Problem 5E(cid:63)). Then
m = (x1 − a1, . . . , xn − an) by Weak Nullstellensatz. Consequently, (a1, . . . , an) is
the unique point of V(m), and hence this point is also in V(I).

70C. The point is is to check that if f vanishes on all of V(I), then f ∈ √I.

Take a set of generators f1, . . . , fm, in the original ring C[x1, . . . , xn]; we may
assume it’s ﬁnite by the Hilbert basis theorem.
We’re going to do a trick now: consider S = C[x1, . . . , xn, xn+1] instead. Consider
the ideal I(cid:48) ⊆ S in the bigger ring generated by {f1, . . . , fm} and the polynomial
xn+1f − 1. The point of the last guy is that its zero locus does not touch our copy
xn+1 = 0 of An nor any point in the “projection” of f through An+1 (one can think
of this as V(I) in the smaller ring direct multiplied with C). Thus V(I(cid:48)) = ∅, and
by the weak Nullstellensatz we in fact have I(cid:48) = C[x1, . . . , xn+1]. So

1 = g1f1 + ··· + gmfm + gm+1 (xn+1f − 1) .

Now the hack: replace every instance of xn+1 by 1
nators. Thus for some large enough integer N we can get
f N = f N (g1f1 + ··· + gmfm)

f , and then clear all denomi-

which eliminates any fractional powers of f in the right-hand side. It follows that
f N ∈ I.

73A. From the exactness, hI (d) = hI (d − k) + hI+(f )(d), and it follows that

χI+(f )(d) = χI (d) − χI (d − k).

Let m = dimVpr(I) ≥ 1. Now dimVpr(I + (f )) = m − 1, so and cnew = deg I + (f )
then we have

deg(I + (f ))dm−1 + . . .

(m − 1)!

from which we read oﬀ

deg(I + (f )) =

as needed.

=

1
m!

(deg I(dm − (d − k)m) + lower order terms)

(m − 1)!

m!

· k(cid:18)m

1(cid:19) deg I = k deg I

874

Napkin, by Evan Chen (v1.5.20190718)

73B. In complex numbers with ABC the unit circle, it is equivalent to solving the two

cubic equations

(p − a)(p − b)(p − c) = (abc)2(q − 1/a)(q − 1/b)(q − 1/c)

(p + b − c − bcq)

0 =(cid:89)cyc

(p + c − b − bcq) +(cid:89)cyc

in p and q = p. Viewing this as two cubic curves in (p, q) ∈ C2, by Bezout’s theorem
it follows there are at most nine solutions (unless both curves are not irreducible,
but it’s easy to check the ﬁrst one cannot be factored). Moreover it is easy to name
nine solutions (for ABC scalene): the three vertices, the three excenters, and I, O,
H. Hence the answer is just those three triangle centers I, O and H.

74C. If they were isomorphic, we would have OV (V ) ∼= OW (W ). For irreducible pro-
jective varieties, OW (W ) ∼= C, while for aﬃne varieties OV (V ) ∼= C[V ]. Thus we

conclude V must be a single point.

74D. Assume for contradiction there is an aﬃne variety V and an isomorphism

Then taking the pullback we get a ring isomorphism

f : X → V.

f (cid:93) : OV (V ) → OX (X) = C[x, y].

Now let OV (V ) = C[a, b] where f (cid:93)(a) = x, f (cid:93)(b) = y. In particular, we actually
have to have V ∼= A2.
Now in the aﬃne variety V we can take V(a) and V(b); these have nonempty
intersection since (a, b) is a maximal ideal in OV (V ). Call this point q, and let p
be a point with f (p) = q.

Then

0 = a(q) = (f (cid:93)a)(p) = x(p)

and so p ∈ V(x) ⊆ X. Similarly, p ∈ V(y) ⊆ X, but this is a contradiction since
V(x, y) = ∅.

75A. Because the stalks are preserved by sheaﬁﬁcation, there is essentially nothing to

prove: both sides correspond to sequences of compatible F -germs over U .

76A. One should get A[1/60] = Z/7Z.

76B. If and only if S has no zero divisors.

76D. Take A = C[x, y]/(xy).

80A. Since Z is the initial object of CRing, it follows Spec Z is the ﬁnal object of AﬀSch.

p gets sent to the characteristic of the ﬁeld OX,p/mX,p.

85C†. For a sentence φ let

fφ : κ → κ
send α to the least β < κ such that for all (cid:126)b ∈ Vα, if there exists a ∈ Vκ such that
Vκ (cid:15) φ[a,(cid:126)b] then ∃a ∈ Vβ such that Vκ (cid:15) φ[a,(cid:126)b].
We claim this is well-deﬁned. There are only |Vα|n many possible choices of (cid:126)b,
and in particular there are fewer than κ of these (since we know that |Vα| < κ;

C Sketches of selected solutions

875

compare Problem 84C(cid:63)). Otherwise, we can construct a coﬁnal map from |V n
α | into
κ by mapping each vector (cid:126)b into a β for which the proposition fails. And that’s
impossible since κ is regular!

In other words, what we’ve done is ﬁx φ and then use Tarski-Vaught on all the
(cid:126)b ∈ V n

α . Now let g : κ → κ be deﬁned by

α (cid:55)→ sup fφ(α).

Since κ is regular and there are only countably many formulas, g(α) is well-deﬁned.

Check that if α has the property that g maps α into itself (in other words, α is
closed under g), then by the Tarski-Vaught test, we have Vα ≺ Vκ.
So it suﬃces to show there are arbitrarily large α < κ which are closed under g.
Fix α0. Let α1 = g(α0), et cetera and deﬁne

α = sup
n<ω

αn.

This α is closed under g, and by making α0 arbitrarily large we can make α as
large as we like.

86B. Since M is countable, there are only countably many dense sets (they live in M !),

say

D1, D2, . . . , Dn, . . . ∈ M.

Using Choice, let p1 ∈ D1, and then let p2 ≤ p1 such that p2 ∈ D2 (this is possible
since D2 is dense), and so on. In this way we can inductively exhibit a chain

p1 ≥ p2 ≥ p3 ≥ . . .

with pi ∈ Di for every i.
Hence, we want to generate a ﬁlter from the {pi}. Just take the upwards closure –
let G be the set of q ∈ P such that q ≥ pn for some n. By construction, G is a ﬁlter
(this is actually trivial). Moreover, G intersects all the dense sets by construction.

87A. It suﬃces to show that P preserves regularity greater than or equal to κ. Consider
λ > κ which is regular in M , and suppose for contradiction that λ is not regular
in M [G]. That’s the same as saying that there is a function f ∈ M [G], f : λ → λ
coﬁnal, with λ < λ. Then by the Possible Values Argument, there exists a function
F ∈ M from λ → P(λ) such that f (α) ∈ F (α) and |F (α)|M < κ for every α.
Now we work in M again. Note for each α ∈ λ, F (α) is bounded in λ since λ is
regular in M and greater than |F (α)|. Now look at the function λ → λ in M by
just

This is coﬁnal in M , contradiction.

α (cid:55)→ ∪F (α) < λ.

D Glossary of notations

§D.1 General

 ∀: for all
 ∃: there exists
 sign(σ): sign of permutation σ

 X =⇒ Y : X implies Y

§D.2 Functions and sets

 f img(S) is the image of f : X → Y for S ⊆ X.
 f−1(y) is the inverse for f : X → Y when y ∈ Y .
 f pre(T ) is the pre-image for f : X → Y when T ⊆ Y .
 f(cid:22)S is the restriction of f : X → Y to S ⊆ X.

 f n is the function f applied n times

Below are some common sets. These may also be thought of as groups, rings, ﬁelds

etc. in the obvious way.

 C: set of complex numbers

 R: set of real numbers

 N: set of positive integers

 Q: set of rational numbers

 Z: set of integers

 ∅: empty set

Some common notation with sets:

 A ⊂ B: A is any subset of B
 A ⊆ B: A is any subset of B
 A ⊆ B: A is a proper subset of B
 S × T : Cartesian product of sets S and T
 S \ T : diﬀerence of sets S and T
 S ∪ T : set union of S and T
 S ∩ T : set intersection of S and T

877

878

Napkin, by Evan Chen (v1.5.20190718)

 S (cid:116) T : disjoint union of S and T
 |S|: cardinality of S
 S/∼: if ∼ is an equivalence relation on S, this is the set of equivalence classes
 x + S: denotes the set {x + s | s ∈ S}.
 xS: denotes the set {xs | s ∈ S}.

§D.3 Abstract and linear algebra

Some common groups/rings/ﬁelds:

 Z/nZ: cyclic group of order n

 (Z/nZ)×: set of units of Z/nZ.
 Sn: symmetric group on {1, . . . , n}
 D2n: dihedral group of order 2n.

 0, 1: trivial group (depending on context)

 Fp: integers modulo p

Notation with groups:

 1G: identity element of the group G
 N (cid:69) G: subgroup N is normal in G.

 G/N : quotient group of G by the normal subgroup N

 Z(G): center of group G

 NG(H): normalizer of the subgroup H of G

 G × H: product group of G and H
 G ⊕ H: also product group, but often used when G and H are abelian (and hence

we can think of them as Z-modules)

 StabG(x): the stabilizer of x ∈ X, if X is acted on by G
 FixPt g, the set of ﬁxed points by g ∈ G (under a group action)

Notation with rings:

 R/I: quotient of ring R by ideal I

 (a1, . . . , an): ideal generated by the ai

 R×: the group of units of R

 R[x1, . . . , xn]: polynomial ring in xi, or ring obtained by adjoining the xi to R

 F (x1, . . . , xn): ﬁeld obtained by adjoining xi to F

 Rd: dth graded part of a graded (pseudo)ring R

879

D Glossary of notations

Linear algebra:

 V ⊕ W : direct sum
 V ⊕n: direct sum of V , n times

 V ⊗ W : tensor product
 V ⊗n: tensor product of V , n times

 V ∨: dual space

 T ∨: dual map (for T a vector space)

 T †: conjugate transpose (for T a vector space)

 (cid:104)−,−(cid:105): a bilinear form
 Mat(V ): endomorphisms of V , i.e. Homk(V, V )

 e1, . . . , en: the “standard basis” of k⊕n

§D.4 Quantum computation

 |ψ(cid:105): a vector in some vector space H
 (cid:104)ψ|: a vector in some vector space H∨, dual to |ψ(cid:105).
 (cid:104)φ|ψ(cid:105): evaluation of an element (cid:104)φ| ∈ H∨ at |φ(cid:105) ∈ H.
 |↑(cid:105), |↓(cid:105): spin z-up, spin z-down
 |→(cid:105), |←(cid:105): spin x-up, spin x-down
 |⊗(cid:105), |(cid:12)(cid:105): spin y-up, spin y-down

§D.5 Topology and real/complex analysis

Common topological spaces:

 S1: the unit circle

 Sn: surface of an n-sphere (in Rn+1)

 Dn+1: closed n + 1 dimensional ball (in Rn+1)
 RPn: real projective n-space
 CPn: complex projective n-space

Some topological notation:

 ∂Y : boundary of a set Y (in some topological space)

 X/S: quotient topology of X by S ⊆ X
 X × Y : product topology of spaces X and Y
 X (cid:113) Y : disjoint union of spaces X and Y

880

Napkin, by Evan Chen (v1.5.20190718)

 X ∨ Y : wedge product of (pointed) spaces X and Y

Real analysis (calculus 101):

 lim inf: limit inﬁmum

 lim sup: limit supremum

 inf: inﬁmum

 sup: supremum

 Zp: p-adic integers

 Qp: p-adic numbers

 f(cid:48): derivative of f

Complex analysis:

a f (x) dx: Riemann integral of f on [a, b]

 (cid:82) b
 (cid:82)α f dz: contour integral of f along path α

 Res(f ; p): the residue of a meromorphic function f at point p

 I(γ, p): winding number of γ around p.

§D.6 Measure theory and probability

 A cm: the σ-algebra of Caratheory-measurable sets

 B(X): the Borel space for X

 µcm: the induced measure on A cm.

 λ: Lebesgue measure

 1A: the indicator function for A

 limn→∞ fn: pointwise limit of fn

 (cid:82)Ω f dµ: the Lebesgue integral of f
 (cid:98)G: Pontryagin dual for G

§D.7 Algebraic topology

 α (cid:39) β: for paths, this indicates path homotopy
 ∗: path concatenation
 π1(X) = π1(X, x0): the fundamental group of (pointed) space X

 πn(X) = πn(X, x0): the nth homotopy group of (pointed) space X

 f(cid:93): the induced map π1(X) → π1(Y ) of f : X → Y
 ∆n: the standard n-simplex

D Glossary of notations

881

 ∂σ: the boundary of a singular n-simplex σ

 Hn(A•): the nth homology group of the chain complex A•
 Hn(X): the nth homology group of a space X

 (cid:101)Hn(X): the nth reduced homology group of X

 Hn(X, A): the nth relative homology group of X and A ⊆ X
 f∗: the induced map on Hn(A•) → Hn(B•) of f : A• → B•, or Hn(X) → Hn(Y )

for f : X → Y

 χ(X): Euler characteristic of a space X

 H n(A•): the nth cohomology group of a cochain complex A•
 H n(A•; G): the nth cohomology group of the cochain complex obtained by applying

Hom(−, G) to A•

 H n(X; G): the nth cohomology group/ring of X with G-coeﬃcients

 (cid:101)H n(X; G): the nth reduced cohomology group/ring of X with G-coeﬃcients
 H n(X, A; G): the nth relative cohomology group/ring of X and A ⊂ X with
 f (cid:93): the induced map on H n(A•) → H n(B•) of f : A• → B•, or H n(X) → H n(Y )

G-coeﬃcients

for f : X → Y

 Ext(−,−): the Ext functor
 φ (cid:94) ψ: cup product of cochains φ and ψ

§D.8 Category theory

Some common categories (in alphabetical order):

 Grp: category of groups

 CRing: category of commutative rings

 Top: category of topological spaces

 Top∗: category of pointed topological spaces
 Vectk: category of k-vector spaces

 FDVectk: category of ﬁnite-dimensional vector spaces

 Set: category of sets

 hTop: category of topological spaces, whose morphisms are homotopy classes of

maps

 hTop∗: pointed version of hTop
 hPairTop: category of pairs (X, A) with morphisms being pair-homotopy equivalence

classes

882

Napkin, by Evan Chen (v1.5.20190718)

 OpenSets(X): the category of open sets of X, as a poset

Operations with categories:

 objA: objects of the category A
 Aop: opposite category
 A × B: product category
 [A,B]: category of functors from A to B
 ker f : Ker f → B: for f : A → B, categorical kernel
 coker f : A → Coker f : for f : A → B, categorical cokernel
 im f : A → Im f : for f : A → B, categorical image

§D.9 Diﬀerential geometry

 Df : total derivative of f

 (Df )p: total derivate of f at point p

 ∂f
∂ei

: ith partial derivative

 αp: evaluating a k-form α at p

 (cid:82)c α: integration of the diﬀerential form α over a cell c

 dα: exterior derivative of a k-form α

 φ∗α: pullback of k-form α by φ

§D.10 Algebraic number theory

 Q: ring of algebraic numbers

 Z: ring of algebraic integers

 F : algebraic closure of a ﬁeld F

 NK/Q(α): the norm of α in extension K/Q
 TrK/Q(α): the trace of α in extension K/Q
 OK: ring of integers in K
 a + b: sum of two ideals a and b

 ab: ideal generated by products of elements in ideals a and b

 a | b: ideal a divides ideal b
 a−1: the inverse of a in the ideal group

 N(I): ideal norm

 ClK: class group of K

D Glossary of notations

883

 ∆K: discriminant of number ﬁeld K

 µ(OK): set of roots of unity contained in OK
 [K : F ]: degree of a ﬁeld extension

 Aut(K/F ): set of ﬁeld automorphisms of K ﬁxing F

 Gal(K/F ): Galois group of K/F

 Dp: decomposition group of prime ideal p

 Ip: inertia group of prime ideal p

 Frobp: Frobenius element of p (element of Gal(K/Q))

 PK(m): ray of principal ideals of a modulus m

 IK(m): fractional ideals of a modulus m

 CK(m): ray class group of a modulus m

 (cid:16) L/K

• (cid:17): the Artin symbol

 Ram(L/K): primes of K ramifying in L

 f(L/K): the conductor of L/K

§D.11 Representation theory

 k[G]: group algebra

 V ⊕ W : direct sum of representations V = (V, ρV ) and W = (W, ρW ) of an algebra

A

 V ∨: dual representation of a representation V = (V, ρV )

 Reg(A): regular representation of an algebra A

 Homrep(V, W ): algebra of morphisms V → W of representations
 χV : the character A → k attached to an A-representation V
 Classes(G): set of conjugacy classes of G

 Funclass(G): the complex vector space of functions Classes(G) → C
 V ⊗ W : tensor product of representations V = (V, ρV ) and W = (W, ρW ) of a

group G (rather than an algebra)

 Ctriv: the trivial representation

 Csign: the sign representation

884

Napkin, by Evan Chen (v1.5.20190718)

§D.12 Algebraic geometry

 V(−): vanishing locus of a set or ideal
 An: n-dimensional (complex) aﬃne space
 √I: radical of an ideal I

 C[V ]: coordinate ring of an aﬃne variety V

 OV (U ): ring of rational functions on U
 D(f ): distinguished open set

 CPn: complex projective n-space (ambient space for projective varieties)

 (x0 : ··· : xn): coordinates of projective space
 Ui: standard aﬃne charts

 Vpr(−): projective vanishing locus.
 hI , hV : Hilbert function of an ideal I or projective variety V

 π(cid:93) or π(cid:93)

U : the pullback OY → OX (πpre(U )) obtained from π : X → Y

 Fp: the stalk of a (pre-)sheaf F at a point p

 [s]p : the germ of s ∈ F (U ) at the point p
 OX,p: shorthand for (OX )p.
 F sh: sheaﬁﬁcation of pre-sheaf F

 αp : Fp → Gp: morphism of stalks obtained from α : F → G
 mX,p: the maximal ideal of OX,p
 Spec A: the spectrum of a ring A

 S−1A: localization of ring A at a set S

 A[1/f ]: localization of ring A away from element f

 Ap: localization of ring A at prime ideal p

 f (p): the value of f at p, i.e. f (mod p)

 κ(p): the residue ﬁeld of Spec A at the element p.

 π(cid:93)

p: the induced map of stalks in π(cid:93).

D Glossary of notations

§D.13 Set theory

 ZFC: standard theory of ZFC

885

 ZFC+: standard theory of ZFC, plus the sentence “there exists a strongly inaccessible

cardinal”

 2S or P(S): power set of S
 A ∧ B: A and B
 A ∨ B: A or B
 ¬A: not A
 V : class of all sets (von Neumann universe)

 ω: the ﬁrst inﬁnite ordinal, also the set of nonnegative integers

 Vα: level of the von Neumann universe

 On: class of ordinals

 (cid:83) A: the union of elements inside A

 A ≈ B: sets A and B are equinumerous
 ℵα: the aleph numbers
 cof λ: the coﬁnality of λ
 M (cid:15) φ[b1, . . . , bn]: model M satisﬁes sentence φ with parameters b1, . . . , bn

 ∆n, Σn, Πn: levels of the Levy hierarchy
 M1 ⊆ M2: M1 is a substructure of M2
 M1 ≺ M2: M1 is an elementary substructure of M2
 p (cid:107) q: elements p and q of a poset P are compatible
 p ⊥ q: elements p and q of a poset P are incompatible
 Nameα: the hierarchy of P-names

 τ G: interpretation of a name τ by ﬁlter G
 M [G]: the model obtained from a forcing poset G ⊆ P
 p (cid:13) ϕ(σ1, . . . , σn): p ∈ P forces the sentence ϕ
 ˇx: the name giving an x ∈ M when interpreted
 ˙G: the name giving G when interpreted

E Terminology on sets and functions

This appendix will cover some notions on sets and functions such as “bijections”,

“equivalence classes”, and so on.

Remark for experts: I am not dealing with foundational issues in this chapter. See
Chapter 82 (and onwards) if that’s what you’re interested in. Consequently I will not
prove most assertions.

§E.1 Sets

A set for us will just be a collection of elements (whatever they may be). For example,
the set N = {1, 2, 3, 4, . . .} is the positive integers, and Z = {. . . ,−2,−1, 0, 1, 2, . . .} is
the set of all integers. As another example, we have a set of humans:

H = {x | x is a featherless biped} .

(Here the “|” means “such that”.)

There’s also a set with no elements, which we call the empty set. It’s denoted by ∅.
It’s conventional to use capital letters for sets (like H), and lowercase letters for

elements of sets (like x).

Deﬁnition E.1.1. We write x ∈ S to mean “x is in S”, for example 3 ∈ N.
Deﬁnition E.1.2. If every element of a set A is also in a set B, then we say A is a
subset of B, and denote this by A ⊆ B. If moreover A (cid:54)= B, we say A is a proper
subset and write A (cid:40) B. (This is analogous to ≤ and <.)
Given a set A, the set of all subsets is denoted 2A or P(A) and called the power set
of A.

Example E.1.3 (Examples of subsets)
(a) {1, 2, 3} ⊆ N ⊆ Z.
(b) ∅ ⊆ A for any set A. (Why?)
(c) A ⊆ A for any set A.
(d) If A = {1, 2} then 2A = {∅,{1},{2},{1, 2}}.

Deﬁnition E.1.4. We write

 A ∪ B for the set of elements in either A or B (possibly both), called the union of

A and B.

 A ∩ B for the set of elements in both A and B, and called the intersection of A

and B.

 A \ B for the set of elements in A but not in B.

887

888

Napkin, by Evan Chen (v1.5.20190718)

Example E.1.5 (Examples of set operations)
Let A = {1, 2, 3} and B = {3, 4, 5}. Then

A ∪ B = {1, 2, 3, 4, 5}
A ∩ B = {3}
A \ B = {1, 2}.

Exercise E.1.6. Convince yourself: for any sets A and B, we have A ∩ B ⊆ A ⊆ A ∪ B.

Here are some commonly recurring sets:

 C is the set of complex numbers, like 3.2 + √2i.
 R is the set of real numbers, like √2 or π.
 N is the set of positive integers, like 5 or 9.

 Q is the set of rational numbers, like 7/3.
 Z is the set of integers, like −2 or 8.

(These are pronounced in the way you would expect: “see”, “are”, “en”, “cue”, “zed”.)

§E.2 Functions

Given two sets A and B, a function f from A to B is a mapping of every element of A
to some element of B.

f

We call A the domain of f , and B the codomain. We write this as f : A → B or
−→ B.

A

Abuse of Notation E.2.1. If the name f is not important, we will often just write
A → B.

We write f (a) = b or a (cid:55)→ b to signal that f takes a to b.
If B has 0 as an element and f (a) = 0, we often say a is a root or zero of f , and that

f vanishes at a.

§E.2.i Injective / surjective / bijective functions

Deﬁnition E.2.2. A function f : A → B is injective if it is “one-to-one” in the following
sense: if f (a) = f (a(cid:48)) then a = a(cid:48). In other words, for any b ∈ B, there is at most one
a ∈ A such that f (a) = b.

Often, we will write f : A (cid:44)→ B to emphasize this.

Deﬁnition E.2.3. A function f : A → B is surjective if it is “onto” in the following
sense: for any b ∈ B there is at least one a ∈ A such that f (a) = b.

Often, we will write f : A (cid:16) B to emphasize this.

Deﬁnition E.2.4. A function f : A → B is bijective if it is both injective and surjective.
In other words, for each b ∈ B, there is exactly one a ∈ A such that f (a) = b.

E Terminology on sets and functions

889

Example E.2.5 (Examples of functions)
By “human” I mean “living featherless biped”.

(a) There’s a function taking every human to their age in years (rounded to the
nearest integer). This function is not injective, because for example there are
many people with age 20. This function is also not surjective: no one has age
10000.

(b) There’s a function taking every USA citizen to their social security number. This
is also not surjective (no one has SSN equal to 3), but at least it is injective
(no two people have the same SSN).

Example E.2.6 (Examples of bijections)
(a) Let A = {1, 2, 3, 4, 5} and B = {6, 7, 8, 9, 10}. Then the function f : A → B by

a (cid:55)→ a + 5 is a bijection.

(b) In a classroom with 30 seats, there is exactly one student in every seat. Thus the
function taking each student to the seat they’re in is a bijection; in particular,
there are exactly 30 students.

Remark E.2.7 — Assume for convenience that A and B are ﬁnite sets. Then:

 If f : A (cid:44)→ B is injective, then the size of A is at most the size of B.
 If f : A (cid:16) B is surjective, then the size of A is at least the size of B.

 If f : A → B is a bijection, then the size of A equals the size of B.

Now, notice that if f : A → B is a bijection, then we can “apply f backwards”: (for
example, rather than mapping each student to the seat they’re in, we map each seat to
the student sitting in it). This is called an inverse function; we denote it f−1 : B → A.
§E.2.ii Images and pre-images

f

−→ Y be a function.

Let X
Deﬁnition E.2.8. Suppose T ⊆ Y . The pre-image f pre(T ) is the set of all x ∈ X such
that f (x) ∈ T . Thus, f pre(T ) is a subset of X.

Example E.2.9 (Examples of pre-image)
Let f : H → Z be the age function from earlier. Then
(a) f pre({13, 14, 15, 16, 17, 18, 19}) is the set of teenagers.
(b) f pre({0}) is the set of newborns.
(c) f pre({1000, 1001, 1002, . . .}) = ∅, as I don’t think anyone is that old.

Abuse of Notation E.2.10. By abuse of notation, we may abbreviate f pre({y}) to
f pre(y). So for example, f pre({0}) above becomes shortened to f pre(0).

890

Napkin, by Evan Chen (v1.5.20190718)

The dual notion is:

Deﬁnition E.2.11. Suppose S ⊆ X. The image f img(S) is the set of all things of the
form f (s).

Example E.2.12 (Examples of images)
Let A = {1, 2, 3, 4, 5} and B = Z. Consider a function f : A → B given by

f (1) = 17

f (2) = 17 f (3) = 19 f (4) = 30 f (5) = 234.

(a) The image f img({1, 2, 3}) is the set {17, 19}.
(b) The image f img(A) is the set {17, 19, 30, 234}.

Question E.2.13. Suppose f : A (cid:16) B is surjective. What is f img(A)?

§E.3 Equivalence relations

Let X be a ﬁxed set now. A binary relation ∼ on X assigns a truth value “true” or “false”
to x ∼ y for each x or y. Now an equivalence relation ∼ on X is a binary relation
which satisﬁes the following axioms:

 Reﬂexive: we have x ∼ x.
 Symmetric: if x ∼ y then y ∼ x
 Transitive: if x ∼ y and y ∼ z then x ∼ z.

An equivalence class is then a set of all things equivalent to each other. One can show
that X becomes partitioned by these equivalence classes:

Example E.3.1 (Example of an equivalence relation)
Let N denote the set of positive integers. Then suppose we declare a ∼ b if a and b
have the same last digit, for example 131 ∼ 211, 45 ∼ 125, and so on.
Then ∼ is an equivalence relation. It partitions N into ten equivalence classes, one
for each trailing digit.

Often, the set of equivalence classes will be denoted X/∼ (pronounced “X mod sim”).

Image Attributions

[1207]

[ca]

[Ee]

[Fr]

[Ge]

[gk]

[Go08]

[Go09]

[Ho]

[In]

[Kr]

[Mu]

[Na]

[Or]

[To]

[Wa]

[Wo]

127“rect”. Cantor set in seven iterations. Public domain. 2007. url: https:
//en.wikipedia.org/wiki/File:Cantor_set_in_seven_iterations.svg.
Pop-up casket. Omega exp. Public domain. url: https://commons.wikimedia.
org/wiki/File:Omega-exp-omega-labeled.svg.
Eeyore22. Weierstrass function. Public domain. url: https : / / commons .
wikimedia.org/wiki/File:WeierstrassFunction.svg.
Fropuﬀ. Klein bottle. Public domain. url: https://en.wikipedia.org/wiki/
File:KleinBottle-01.png.
Topological Girl’s Generation. Topological Girl’s Generation. url: http://
topologicalgirlsgeneration.tumblr.com/.
g.kov. Normal surface vector. url: http : / / tex . stackexchange . com / a /
235142/76888.
Abstruse Goose. Math Text. CC 3.0. 2008. url: http://abstrusegoose.com/
12.
Abstruse Goose. Zornaholic. CC 3.0. 2009. url: http://abstrusegoose.com/
133.
George Hodan. Apple. Public domain. url: http://www.publicdomainpictures.
net/view-image.php?image=20117.
Inductiveload. Klein Bottle Folding. Public domain. url: https://commons.
wikimedia.org/wiki/File:Klein_Bottle_Folding_1.svg.
Petr Kratochvil. Velociraptor. Public domain. url: http://www.publicdomainpictures.
net/view-image.php?image=93881.
Randall Munroe. Rolle’s theorem. CC 2.5. url: https://xkcd.com/2042/.
Krish Navedala. Stokes patch. Public domain. url: https://en.wikipedia.
org/wiki/File:Stokes_patch.svg.
Ben Orlin. The Math Major Who Never Reads Math. url: http://mathwithbaddrawings.
com/2015/03/17/the-math-major-who-never-reads-math/.
Toahigherlevel. Projection color torus. Public domain. url: https : / / en .
wikipedia.org/wiki/File:Projection_color_torus.jpg.

Bill Watterson. Calvin and Hobbes. I think this is fair use.
Wordslaugh. Covering spcae diagram. CC 3.0. url: https://commons.wikimedia.
org/wiki/File:Covering_space_diagram.svg.

891

Bibliography

[Ax97]

[Ba10]

[Ch08]

[Et11]

[Ga03]

[Ga14]

[Ga15]

[Go11]

[Go18]

[Ha02]

[Hi13]

[Ko14]

[Le]

[Le02]

[Le14]

[Ll15]

Sheldon Axler. Linear algebra done right. New York: Springer, 1997. isbn:
978-0387982588.

Joseph Bak. Complex analysis. New York: Springer Science+Business Media,
LLC, 2010. isbn: 978-1441972873.

Steve Cheng. “A Crash Course on the Lebesgue Integral and Measure Theory”.
Apr. 2008. url: https://www.gold-saucer.org/math/lebesgue/lebesgue-
new.pdf.
Pavel Etingof. “Introduction to Representation Theory”. 2011. url: http :
//math.mit.edu/~etingof/replect.pdf.
Andreas Gathmann. “Algebraic Geometry”. 2003. url: http://www.mathematik.
uni-kl.de/agag/mitglieder/professoren/gathmann/notes/alggeom/.

Dennis Gaitsgory. “Math 55a: Honors Abstract and Linear Algebra”. 2014.
url: http://web.evanchen.cc/coursework.html.
Dennis Gaitsgory. “Math 55b: Honors Real and Complex Analysis”. 2015. url:
http://web.evanchen.cc/coursework.html.
Timothy Gowers. “Normal subgroups and quotient groups”. 2011. url: https:
//gowers.wordpress.com/2011/11/20/normal-subgroups-and-quotient-
groups/.
Vadim Gorin. “18.175: Theory of Probability”. 2018. url: https://www.mit.
edu/~txz/links.html.
Allen Hatcher. Algebraic topology. Cambridge, New York: Cambridge University
Press, 2002. isbn: 0-521-79160-X. url: http : / / opac . inria . fr / record =
b1122188.
A. J. Hildebrand. “Introduction to Analytic Number Theory”. 2013. url:
http://www.math.illinois.edu/~hildebr/ant/.
Peter Koellner. “Math 145a: Set Theory I”. 2014. url: http://web.evanchen.
cc/coursework.html.
Holden Lee. “Number Theory”. url: https : / / github . com / holdenlee /
number-theory.
Hendrik Lenstra. “The Chebotarev Density Theorem”. 2002. url: http://
websites.math.leidenuniv.nl/algebra/.

Tom Leinster. Basic category theory. Cambridge: Cambridge University Press,
2014. isbn: 978-1107044241. url: https://arxiv.org/abs/1612.09375.
Seth Lloyd. “18.435J: Quantum Computation”. 2015. url: http : / / web .
evanchen.cc/coursework.html.

[Ma13a] Laurentiu Maxim. “Math 752 Topology Lecture Notes”. 2013. url: https:

//www.math.wisc.edu/~maxim/752notes.pdf.

[Ma13b] Maxima. “Burnside’s Lemma, post 6”. 2013. url: http://www.aops.com/

Forum/viewtopic.php?p=3089768#p3089768.

[Mi14]

Alexandre Miquel. “An Axiomatic Presentation of the Method of Forcing”.
2014. url: http://www.fing.edu.uy/~amiquel/forcing/.

893

894

[Mu00]

[Og10]

[Pu02]

Napkin, by Evan Chen (v1.5.20190718)

James Munkres. Topology. 2nd. Prentice-Hall, Inc., Jan. 2000. isbn: 9788120320468.
url: http://amazon.com/o/ASIN/8120320468/.
Frederique Oggier. “Algebraic Number Theory”. 2010. url: http://www1.
spms.ntu.edu.sg/~frederique/ANT10.pdf.
C. C. Pugh. Real mathematical analysis. New York: Springer, 2002. isbn:
978-0387952970.

[Sj05]

[Ul08]

[Sc07] W.H. Schikhof. Ultrametric Calculus: An Introduction to P-Adic Analysis.
Cambridge Studies in Advanced Mathematics. Cambridge University Press,
2007. isbn: 9780521032872. url: https://books.google.com/books?id=
cBT05R7TH1QC.
Reyer Sjamaar. “Manifolds and Diﬀerential Forms”. 2005. url: http://www.
math.cornell.edu/~sjamaar/classes/3210/notes.html.
Brooke Ullery. “Minkowski Theory and the Class Number”. 2008. url: http://
www.math.uchicago.edu/~may/VIGRE/VIGRE2008/REUPapers/Ullery.pdf.
Ravi Vakil. “The Rising Sea: Foundations of Algebraic Geometry”. Nov. 2017.
url: http://math.stanford.edu/~vakil/216blog/.
Andrew Yang. “Math 43: Complex Analysis”. 2012. url: https : / / math .
dartmouth.edu/~m43s12/syllabus.html.

[Va17]

[Ya12]

